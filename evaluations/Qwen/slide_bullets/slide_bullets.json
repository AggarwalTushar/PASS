[
    "Slide Topic: Introduction to Sentiment-to-Sentiment Translation\nSummary:\n- Changes sentence sentiment while preserving non-emotional content.\n- A specialized style transfer task in NLP.\n- Applications include review sentiment transformation and news rewriting.\n- Lacks parallel training data, hindering performance.\n- Important for adapting text sentiment in various contexts.\n\nSlide Topic: Challenges in Sentiment-to-Sentiment Translation\nSummary:\n- Emotion and semantics mixed in dense hidden vectors.\n- Implicit separation of emotional and semantic information is difficult.\n- Lack of supervised parallel data complicates translation.\n- Modifying underlying sentiment risks losing non-emotional semantics.\n- Current methods struggle to preserve meaning while altering sentiment.\n\nSlide Topic: Proposed Cycled Reinforcement Learning Approach\nSummary:\n- Addresses lack of parallel data issue\n- Uses cycled reinforcement learning method\n- Includes neutralization and emotionalization modules\n- Neutralization module filters out emotional words\n- Emotionalization module adds sentiment to neutral content\n- Aids in sentiment-to-sentiment translation tasks\n\nSlide Topic: Neutralization Module\nSummary:\n- Identifies non-emotional words in sentences.\n- Uses LSTM to assess word neutrality probability.\n- Feeds non-emotional words to emotionalization module.\n- Trained to enhance extraction of non-emotional content.\n- Improves overall emotionalization module performance.\n\nSlide Topic: Emotionalization Module\nSummary:\n- Adds sentiment to neutral semantic content\n- Uses bi-decoder based encoder-decoder framework\n- Framework includes one encoder and two decoders\n- One decoder adds positive sentiment\n- Other decoder adds negative sentiment\n- Input sentiment signal selects decoder to use\n\nSlide Topic: Cycled Reinforcement Learning Mechanism\nSummary:\n- Two modules trained using cycled method.\n- Neutralization module converts emotional input to semantic content.\n- Emotionalization module reconstructs original sentence from sentiment and semantic content.\n- Discrete choice of neutral words makes loss non-differentiable.\n- Formulated as reinforcement learning problem.\n- Policy gradient trains neutralization module.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Two review datasets with user ratings per review.\n- Ratings above three are positive; below three are negative.\n- Positive and negative reviews are not paired.\n- Reviews exceeding 20 words are filtered out.\n- First sentence of each review paired with its sentiment label.\n- Text-sentiment pairs created for analysis.\n\nSlide Topic: Evaluation Metrics and Results\nSummary:\n- Two evaluations conducted: automatic and human.\n- Automatic evaluation assesses sentiment transformation accuracy.\n- BLEU score measures content preservation performance.\n- Human evaluation scores transformed text on sentiment.\n- Human evaluation also assesses semantic similarity.\n- Evaluations performed on designated test set.\n\nSlide Topic: Human Evaluation and Comparison with Baselines\nSummary:\n- Table 2 shows human evaluation results.\n- Proposed method improves semantic preservation.\n- Yelp dataset score increased from 3.87 to 5.08.\n- Amazon dataset score increased from 3.22 to 4.67.\n- Results indicate significant improvement over baselines.\n- Proposed model achieves best overall performance.\n\nSlide Topic: Error Analysis and Future Work\nSummary:\n- Method outperforms current systems.\n- Fails with sentiment-conflicted sentences.\n- Neutral sentences also pose challenges.\n- Incomplete removal of original sentiment noted.\n- Unseen emotional words and implicit sentiment contribute.\n- Handling complex sentiment expressions is future work.",
    "Slide Topic: Introduction to Constituency Parsing and Its Importance\nSummary:\n- Constituency parsing is crucial in natural language processing.\n- It involves devising fast and accurate parsing algorithms.\n- Parsing incorporates linguistic prior into various tasks.\n- Used in relation extraction and paraphrase detection.\n- Supports natural language inference and machine translation.\n- A long-standing challenge in the field.\n\nSlide Topic: Overview of Existing Parsing Approaches and Their Limitations\nSummary:\n- Neural approaches achieve competitive results using dense input representations.\n- Transition-based models make local decisions, enabling fast decoding.\n- Transition-based models suffer from compounding errors.\n- Chart-based models use structured loss functions during training.\n- Chart-based models benefit from exact inference via the CYK algorithm.\n- Chart-based models have higher computational costs during decoding.\n\nSlide Topic: Proposed Model: Syntactic Distances for Constituency Parsing\nSummary:\n- Novel fully-parallel model for constituency parsing\n- Based on concept of 'syntactic distance'\n- Trained using margin rank loss\n- Reproduces ordering of split points induced by ground-truth distances\n- Works in parallel, enhancing modern computing architectures\n- Facilitates easy parallelization on GPUs\n\nSlide Topic: Algorithm for Converting Parse Trees to Syntactic Distances\nSummary:\n- Two algorithms proposed for parse tree conversion.\n- First algorithm converts parse tree to compact representation.\n- Representation based on distances between consecutive words.\n- Syntactic distances defined for each possible split point.\n- Order induced by distances specifies recursive sentence splits.\n- Specifies order for splitting sentence into smaller constituents.\n\nSlide Topic: Algorithm for Reconstructing Parse Trees from Syntactic Distances\nSummary:\n- Algorithm reconstructs unique binary tree from model output (d, \u0109, t).\n- Selects split point i with maximum d_i.\n- Assigns predicted label \u0109_i to the span.\n- Ensures efficient tree reconstruction process.\n- Running time complexity is O(n log n).\n\nSlide Topic: Model Architecture for Predicting Syntactic Distances and Constituent Labels\nSummary:\n- Uses stacked recurrent (LSTM) and convolutional layers.\n- Maps words and tags to embedding sequences.\n- Concatenates embeddings as inputs for bidirectional LSTM layers.\n- Predicts constituent labels via a 2-layer network with softmax output.\n- Calculates syntactic distances using a 2-layer feed-forward network.\n- Outputs syntactic distances with a single output unit.\n\nSlide Topic: Training Objective and Loss Functions\nSummary:\n- Training objective combines syntactic distances and constituent labels' prediction losses.\n- Uses standard softmax classifier with cross-entropy loss for labels.\n- Proposes minimizing pair-wise learning-to-rank loss for syntactic distances.\n- Pair-wise loss inspired by methods from Burges et al., 2005.\n- Encourages model to reproduce full ranking order of ground-truth distances.\n- Enhances model's ability to accurately predict syntactic structures.\n\nSlide Topic: Experimental Results and Performance Evaluation\nSummary:\n- Evaluated on WSJ part of PTB and CTB datasets\n- Achieves 91.8 F1 on PTB without external data\n- Achieves 86.5 F1 on CTB without external data\n- Single-model constituency parsing performance\n- Matches and sometimes outperforms recent parsing models\n- Competes with chart-based and transition-based models\n\nSlide Topic: Ablation Study and Impact of Model Components\nSummary:\n- Ablation study removes components from optimally trained network.\n- Each ablated version is re-trained from scratch.\n- Assesses individual component contributions to model performance.\n- MSE loss model underperforms significantly.\n- Rank loss model outperforms MSE loss model.\n- Reveals critical components for model success.\n\nSlide Topic: Parsing Speed and Efficiency\nSummary:\n- Syntactic distance predictions batched in modern GPU architectures.\n- Distance to tree conversion uses O(n log n) divide-and-conquer algorithm.\n- Neural network runs on NVIDIA Titan XP GPU.\n- Distance to tree inference operates on Intel Core i7-6850K CPU.\n- Achieves 111.1 sentences per second.\n- Significantly more efficient than previous models.",
    "Slide Topic: Historical Context of Translation\nSummary:\n- Translation vital in multilingual societies since ancient laws, religions, and trade.\n- Evidence of scribe translations dates back to Hammurabi's era, around 4000 years ago.\n- Biblical references to translation include Esther 1:22 and 8:9.\n- Translators aimed to remain invisible for millennia.\n- Seamless translation was the standard.\n- Translated works were meant to appear originally written in the target language.\n\nSlide Topic: Translation Universals and Interference\nSummary:\n- Translations have unique characteristics distinct from original texts.\n- These differences are statistical, not due to poor translation.\n- Simplification: Complex source structures become simpler in target.\n- Standardization: Over-conformity to target language norms occurs.\n- Explicitation: Implicit source structures become more explicit in target.\n- Interference: Source language influences the translation product.\n\nSlide Topic: Methodology and Dataset\nSummary:\n- Uses EuroParl corpus, European Parliament proceedings and translations.\n- Covers languages of the European Union from 1999 to 2011.\n- Includes 21 official EU languages.\n- Establishes translation direction via speaker language validation.\n- Translates all speeches from original language to all others.\n- Uses English as an intermediate language for translations.\n\nSlide Topic: Feature Selection for Translation Analysis\nSummary:\n- Original and translated texts are represented as feature vectors.\n- Features focus on text structure, not content.\n- Top-1,000 most frequent POS trigrams reflect shallow syntax.\n- Function words (FW) indicate grammar patterns across texts.\n- Cohesive markers are overrepresented in translated texts.\n- Features help measure source-language interference in translations.\n\nSlide Topic: Evaluation of Phylogenetic Trees\nSummary:\n- Trees' quality assessed by similarity to gold standard.\n- Two metrics: unweighted and weighted.\n- Unweighted metric evaluates structural similarity.\n- Weighted metric considers structure and branch length.\n- L2-norm adapted for leaf-pair distance calculation.\n- Weighted distance sums edge weights on shortest leaf path.\n\nSlide Topic: Identification of Source Language\nSummary:\n- Machines outperform humans in identifying source languages of translated texts.\n- Experiment conducted across 14 languages from 3 language families.\n- Limited by available data, 100 chunks of 1,000 tokens each were extracted.\n- Classified translated English and French texts into 14 classes.\n- POS-trigrams feature set used for classification.\n- Achieved 75.61% accuracy on English translations, baseline at 7.14%.\n\nSlide Topic: Reconstruction of Phylogenetic Language Trees\nSummary:\n- Generated trees from English and French translations.\n- Hypothesized source language interference in translations.\n- Trees constructed using POS-trigrams were most accurate.\n- Accurate trees resembled the gold IE tree standard.\n- POS-trigrams closely linked to source-language interference.\n- Best trees derived from European language translations.\n\nSlide Topic: Analysis of Linguistic Phenomena in Translations\nSummary:\n- Source-language traces dominate translation products.\n- These traces facilitate reconstruction of source language history.\n- Inspecting phenomena to understand interference characteristics.\n- Patterns reflecting phenomena computed in texts translated to English.\n- Languages analyzed include Germanic, Romance, and Baltic-Slavic families.\n- Averages computed over each language family for comparative analysis.\n\nSlide Topic: Conclusion and Future Research Directions\nSummary:\n- Translation distortions are not random.\n- Reflects language topology clearly.\n- Professional translations conform to common standards.\n- Native speakers edit translations, enhancing accuracy.\n- Results hold after multiple translation phases.\n- Extending research to literary texts in Hebrew.",
    "Slide Topic: Introduction to Cross-Language Plagiarism\nSummary:\n- Plagiarism is a significant issue in higher education.\n- Monolingual plagiarism detection is well-established.\n- Internet and machine translation tools have enabled cross-language plagiarism.\n- Cross-language plagiarism involves translating plagiarized content.\n- Detecting cross-language plagiarism is challenging.\n- Suspect documents differ in language from their sources.\n\nSlide Topic: Challenges in Cross-Language Plagiarism Detection\nSummary:\n- Source and suspicious texts are in different languages.\n- Traditional methods rely on single-language features.\n- Lexical comparison is ineffective across languages.\n- Syntactic structures vary significantly between languages.\n- Translation inaccuracies complicate similarity detection.\n- Contextual understanding is harder to automate.\n\nSlide Topic: Use of Word Embeddings for Plagiarism Detection\nSummary:\n- Word embeddings represent words in a continuous space.\n- Captures context, synonymy, and morphological closeness.\n- Calculates similarity between sentences in any language.\n- Useful for detecting plagiarism across languages.\n- Utilizes MultiVec toolkit for computation and management.\n- Enhances accuracy in identifying paraphrased content.\n\nSlide Topic: State-of-the-Art Methods for Cross-Language Textual Similarity\nSummary:\n- CL-CnG uses character n-grams for similarity.\n- CL-CTS applies conceptual thesaurus for cross-language texts.\n- CL-ASA aligns texts to measure similarity.\n- CL-ESA performs explicit semantic analysis across languages.\n- T+MA combines translation with monolingual analysis.\n\nSlide Topic: Evaluation Protocol and Dataset\nSummary:\n- Distance matrix of size N x M is constructed.\n- Each textual unit compared to itself and M-1 random units.\n- Dataset is multilingual, including French, English, and Spanish.\n- Based on both parallel and comparable corpora.\n- Sources include Wikipedia, conference papers, product reviews.\n- Incorporates EuroParl and JRC datasets.\n\nSlide Topic: Proposed Methods: CL-CTS-WE and CL-WES\nSummary:\n- CL-CTS-WE replaces lexical resources with word embeddings.\n- CL-CTS-WE uses distributed representations for words.\n- CL-WES compares sentences in different languages directly.\n- CL-WES builds bilingual common representation vectors.\n- CL-WES applies cosine similarity for comparison.\n- Both methods enhance cross-lingual sentence similarity.\n\nSlide Topic: Syntax Weighting in Distributed Representations (CL-WESS)\nSummary:\n- CL-WESS enhances CL-WES by incorporating syntax flavor.\n- Textual units are syntactically tagged.\n- Weights are assigned to each tag type.\n- Weights are optimized to improve F1 score.\n- Final vector representation uses syntactic aggregation.\n- Aggregation formula computes representation based on part-of-speech weights.\n\nSlide Topic: Combining Multiple Methods for Improved Detection\nSummary:\n- Combines multiple methods to enhance cross-language similarity detection.\n- Uses weighted fusion to assign weights to each method's similarity scores.\n- Implements decision tree fusion for optimal method combination.\n- Employs C4.5 algorithm to determine the best decision tree structure.\n- Aims to improve overall detection performance through method integration.\n\nSlide Topic: Results and Discussion on Method Performance\nSummary:\n- Word embeddings enhance CL-CTS performance.\n- CL-CTS-WE gains +3.83% on chunks, +3.19% on sentences.\n- CL-WESS outperforms CL-C3G, best single method on corpus.\n- CL-WESS is syntactically weighted version of CL-WES.\n- Decision tree fusion achieves 89.15% F1 score at chunk level.\n- Decision tree fusion reaches 88.50% F1 score at sentence level.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Augmenting baseline approaches with word embeddings improves cross-language textual similarity detection.\n- CL-WESS outperforms previous state-of-the-art methods.\n- CL-WESS shows the most promising results among the tested approaches.\n- Future work will focus on enhancing CL-WESS.\n- Analysis of syntactic weights will be a key area of improvement.\n- Adapting weights based on the plagiarist's stylometry will be explored.",
    "Slide Topic: Introduction to Taylor's Law and Its Application in Natural Language\nSummary:\n- Taylor's Law describes variance-mean power law in event clustering.\n- Law quantifies how variance grows with respect to mean.\n- Applied across diverse domains, rarely in natural language.\n- Study applies Taylor's Law to lexical fluctuation in texts.\n- Focuses on word co-occurrence and burstiness within texts.\n- Provides new insights into text structure and language patterns.\n\nSlide Topic: Methodology for Measuring Taylor's Exponent in Texts\nSummary:\n- Define a discrete time series X of words.\n- Segment X by a fixed length \u2206t.\n- Count occurrences of each word in every segment.\n- Calculate mean \u00b5 and standard deviation \u03c3 for each word.\n- Plot \u03c3 distribution against \u00b5 to find Taylor's exponent \u03b1.\n- \u03b1 typically ranges between 0.5 and 1.0 across various domains.\n\nSlide Topic: Analysis of Taylor Exponents Across Different Languages and Text Types\nSummary:\n- Taylor exponent slightly dependent on data size\n- Largest datasets: The New York Times (1.5 billion words), Mainichi (24 years)\n- Exponent shows slight decrease trend with increased data size\n- Median and quantiles calculated for various data types\n- Randomized sequences yielded exponent of 0.50 from 10 samples\n- Real data exponents ranged from 0.53 to 0.68 for Project Gutenberg texts\n\nSlide Topic: Comparison of Taylor Exponents Between Natural Language and Other Data Sources\nSummary:\n- Taylor exponents vary by data source.\n- Analysis includes enwiki8, child-directed speech, programming languages, and music.\n- Distributions differ from natural language texts.\n- Exponents are significantly larger in non-language datasets.\n- Indicates more frequent fixed-form expressions in non-language data.\n- Natural language texts show less frequent fixed-form expressions.\n\nSlide Topic: Impact of Data Size and Segment Size on Taylor Exponents\nSummary:\n- Taylor exponent varies with \u2206t.\n- Mean exponent plotted for various \u2206t in Figure 6.\n- Exponent grows as segment size increases.\n- Words occur in bursts across all scales.\n- Larger segments include many or few word instances.\n- This leads to greater variance growth.\n\nSlide Topic: Evaluation of Machine-Generated Text Using Taylor's Exponent\nSummary:\n- Taylor's law behavior found in real texts.\n- Baseline language models confirm findings.\n- IID processes have a Taylor exponent of 0.50.\n- Non-trivial processes also often show an exponent of 0.50.\n- LSTM model generated Shakespeare-like text with exponent 0.50.\n- Character-level models fail to reproduce word-level clustering.\n\nSlide Topic: Implications of Taylor's Law for Understanding Linguistic Complexity\nSummary:\n- Taylor exponent quantifies natural language complexity.\n- Reflects lexical fluctuation in language models.\n- Initially 0.5 for very small segment sizes.\n- Useful in computational linguistics for model assessment.\n- Explains word appearance probability in small segments.\n- Demonstrates potential for quantifying underlying word coherence.",
    "Slide Topic: Introduction to Crisis Response through Social Media Analysis\nSummary:\n- Analyze social media data for sudden crises\n- Twitter posts offer timely, useful information\n- Reports include injuries, deaths, and infrastructure damage\n- Posts highlight urgent needs like food, shelter, and medical aid\n- Humanitarian organizations benefit from real-time data\n- Reduces human and economic losses through swift response\n\nSlide Topic: Challenges in Real-Time Tweet Classification During Crises\nSummary:\n- Classify tweets in real-time during crises.\n- Focus on predefined categories like relevant vs. non-relevant.\n- Deep neural networks (DNNs) excel in NLP classification.\n- DNN success requires large labeled datasets.\n- Labeled data scarcity at crisis onset limits DNN effectiveness.\n- Real-time classification faces significant data availability challenges.\n\nSlide Topic: Proposed Deep Learning Framework for Domain Adaptation and Semi-Supervised Learning\nSummary:\n- Novel model integrates domain adaptation and semi-supervised learning.\n- Unified deep learning framework proposed.\n- Basic task-solving network is a convolutional neural network.\n- Includes a network for semi-supervised learning.\n- Incorporates a network for domain adaptation.\n- Framework aims to enhance model performance across different tasks.\n\nSlide Topic: Convolutional Neural Network Architecture for Feature Extraction\nSummary:\n- Input: Tweet t = (w1, ..., wn) from vocabulary V\n- First layer maps words to R^d using embedding matrix E\n- E initialized with pre-trained crisis dataset embeddings\n- Output matrix X \u2208 R^(n\u00d7d) from lookup layer\n- X passes through convolution and pooling layers\n- Layers learn higher-level feature representations\n\nSlide Topic: Semi-Supervised Learning Component Using Graph-Based Embedding\nSummary:\n- Induces structural similarity between source and target event instances.\n- Uses graph-based semi-supervised deep learning framework.\n- Framework outperforms existing methods on multiple datasets.\n- 'Similarity' graph G encodes relations between training instances.\n- Network learns internal representations (embeddings) using G.\n- Applies to both labeled and unlabeled training instances.\n\nSlide Topic: Domain Adversarial Training for Invariant Feature Learning\nSummary:\n- Uses domain adversarial training on neural networks.\n- Incorporates a domain discriminator branch.\n- Discriminator takes shared internal representation z as input.\n- Aims to distinguish input domain (DS or DT).\n- Defined by a sigmoid function.\n- Employs negative log-probability as discrimination loss.\n\nSlide Topic: Training Algorithm and Hyperparameter Settings\nSummary:\n- Uses AdaDelta for adaptive model parameter updates.\n- Performs three gradient updates per iteration for different loss components.\n- Balancing network components is the main challenge in adversarial training.\n- Domain adversary loss weight (\u03bbd) fixed at 1e-8.\n- Semi-supervised loss weight (\u03bbg) fixed at 1e-2.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Two real-world Twitter datasets used: 2015 Nepal Earthquake (NEQ) and 2013 Queensland Floods (QFL).\n- Millions of tweets collected via Twitter Streaming API using event-specific keywords/hashtags.\n- Datasets annotated using paid workers from the Crowdflower platform.\n- Annotation task classified tweets into relevant and non-relevant categories.\n- Experimental setup evaluated system performance on labeled examples.\n\nSlide Topic: Results and Discussion on Semi-Supervised Learning\nSummary:\n- Table 2 compares supervised, self-training semi-supervised, and graph-based semi-supervised approaches.\n- Graph-based semi-supervised approach outperforms both baselines.\n- Improvement ranges from 4% to 13% in F1 scores.\n- Nepal dataset shows a 4% absolute improvement.\n- Queensland dataset shows a 13% absolute improvement.\n- Results highlight the effectiveness of graph-based methods.\n\nSlide Topic: Results and Discussion on Domain Adaptation\nSummary:\n- Domain adversarial training shows F1 gains from 1.8% to 4.1%.\n- Adversarial training induces domain-invariant features effectively.\n- Internal representations improve with adversarial training.\n- Combining semi-supervised learning enhances results further.\n- Additional F1 score gains range from 5% to 7%.\n- Unsupervised domain adaptation contributes to overall improvement.",
    "Slide Topic: Introduction to Abstract Meaning Representation (AMR)\nSummary:\n- AMR is a semantic graph representation.\n- Nodes represent concepts; edges represent semantic relations.\n- AMR abstracts away from sentence syntax.\n- Graphs, not trees, due to multiple parent nodes.\n- Multiple parent nodes are called reentrancies.\n- Example: reentrancy between \"finger\" and \"he\" in a graph.\n\nSlide Topic: AMR-to-Text Generation and Encoder/Decoder Architectures\nSummary:\n- AMR-to-text converts AMR graphs to natural language.\n- Encoder/decoder architectures are used for this task.\n- Konstas et al. (2017) convert AMR graphs to sequences.\n- Song et al. (2018) and Beck et al. (2018) encode AMR as graphs.\n- Graph encoding captures reentrant structures in AMR.\n- Attention mechanisms enhance model performance in AMR-to-text.\n\nSlide Topic: Handling Reentrancies in AMR Graphs\nSummary:\n- Reentrancies are central but challenging in AMR.\n- Previous methods remove reentrancies for simpler data structures.\n- Some approaches keep reentrancies without analyzing their impact.\n- Impact of reentrancies on performance remains unclear.\n- Explicit encoding of reentrancies for generation is not yet proven beneficial.\n\nSlide Topic: Input Representations for AMR Graphs\nSummary:\n- AMRs are rooted, directed graphs.\n- Tree structures require discarding reentrancies.\n- Sequential AMRs linearize and anonymize graphs.\n- Linearization converts graphs into sequences.\n- Process loses reentrancy information.\n- Based on Konstas et al. (2017) methodology.\n\nSlide Topic: Encoder Architectures for AMR-to-Text Generation\nSummary:\n- RNN Encoders use BiLSTM to process sequential linearization.\n- Tree LSTM Encoders require AMR graphs to be preprocessed into tree structures.\n- GCN Encoders learn node embeddings by examining neighboring nodes.\n- GCN Encoders naturally handle graph structures without preprocessing.\n- Each architecture leverages unique graph properties for AMR-to-text conversion.\n\nSlide Topic: Stacking Encoders for Enhanced AMR-to-Text Generation\nSummary:\n- Combines structural info from Tree LSTMs and GCNs\n- Integrates sequential info extracted by BiLSTMs\n- Experiments stack structural encoder before RNN\n- Achieves better scores with this stacking order\n- GCN-based models show significant improvement\n- Enhances AMR-to-text generation quality\n\nSlide Topic: Experimental Setup and Evaluation Metrics\nSummary:\n- BLEU and METEOR used as evaluation metrics.\n- Systems implemented in PyTorch using OpenNMT-py framework.\n- Hyperparameters tuned on LDC2015E86 development set.\n- Training conducted with SGD, initial learning rate 1, decay to 0.8.\n- Batch size set to 100.\n\nSlide Topic: Impact of Reentrancies on Model Performance\nSummary:\n- Tested hypothesis on graph encoders and reentrancies.\n- Evaluated scenarios based on reentrancy numbers.\n- Graph encoders outperform others with high reentrancy.\n- Widest performance gap at more than six reentrancies.\n- Explicit reentrancy encoding benefits model performance.\n- Overall scores underestimate encoding benefits.\n\nSlide Topic: Handling Long-range Dependencies in AMR Graphs\nSummary:\n- Long sequences challenge interactions between distant items.\n- Graph encoder outperforms sequential and tree encoders.\n- Performance gap widens with longer dependencies.\n- Longer dependencies crucial for improving results.\n- Graph encoders benefit more from handling long dependencies.\n- Enhances AMR graph accuracy significantly.",
    "Slide Topic: Introduction to Temporal and Causal Relations\nSummary:\n- Events understanding is key in natural language processing.\n- Identifying relations between events is crucial.\n- Supports applications like story completion and summarization.\n- Focuses on temporal and causal relations extraction.\n- Enhances timeline construction and event sequencing.\n- Essential for advanced narrative and data analysis.\n\nSlide Topic: Joint Extraction of Temporal and Causal Relations\nSummary:\n- Temporal and causal relations often influence each other.\n- Decision on one relation is often based on evidence from the other.\n- Identifying temporal relations can be challenging without explicit markers.\n- Weak syntactic connection between events complicates temporal relation identification.\n- Causal relations can clarify temporal sequences when direct evidence is lacking.\n- Causal relation dictates the temporal order of events in complex cases.\n\nSlide Topic: Challenges in Jointly Studying Temporal and Causal Relations\nSummary:\n- Lack of jointly annotated data hinders joint study.\n- Developed a new jointly annotated dataset.\n- Augmented Event-Causality dataset with temporal annotations.\n- New dataset enables statistical improvements.\n- Improvements shown on both temporal and causal relations.\n- Used a proposed joint framework for analysis.\n\nSlide Topic: Proposed Joint Framework: Temporal and Causal Reasoning (TCR)\nSummary:\n- Proposes a joint framework for Temporal and Causal Reasoning (TCR).\n- Integrates temporal and causal extraction systems.\n- Uses a Constrained Conditional Model (CCM) framework.\n- Employs Integer Linear Programming (ILP) for inference.\n- Enforces declarative constraints during the inference phase.\n- Inspired by examples of temporal and causal reasoning.\n\nSlide Topic: Temporal Component of TCR\nSummary:\n- Introduce temporal component for TCR.\n- Design transitivity constraints for temporal logic.\n- Apply prior knowledge to enhance performance.\n- Incorporate causal relations into the framework.\n- Complete TCR joint inference framework.\n- Transform joint problem into ILP for solution.\n\nSlide Topic: Incorporating Causal Relations into TCR\nSummary:\n- Joint inference framework for temporal relations established.\n- Causal relations vectorized and integrated into the framework.\n- Scoring function for causality added to enhance model.\n- Full inference formula now includes causal scores.\n- Search space for causal relations depends on temporal labels.\n- Constraints ensure logical consistency between causal and temporal relations.\n\nSlide Topic: Scoring Functions and ILP Formulation\nSummary:\n- Classifiers trained using averaged perceptron algorithm.\n- Features from Do et al. (2012) and Ning et al. (2017) utilized.\n- Soft-max scores used in scoring functions.\n- Scoring function SEE uses learned weights and feature vectors.\n- SC function reuses SEE features, learns averaged perceptron.\n- Incorporates prior statistics from a large corpus for probabilistic event ordering.\n\nSlide Topic: Experiments and Results on TB-Dense\nSummary:\n- Standard train/dev/test split used for TB-Dense.\n- Parameters tuned to optimize F1 performance on dev set.\n- Gold events and time expressions utilized.\n- Ablation study in Table 2 shows step-by-step F1 score improvement.\n- System 5 achieved best precision, recall, and F1 scores.\n- Incorporation of high-precision rules (Y2) further improved temporal performance.\n\nSlide Topic: Joint Performance on New Dataset\nSummary:\n- New dataset annotated with temporal and causal relations.\n- Split into 20 training and 5 testing documents.\n- Training parameters tuned via 5-fold cross-validation.\n- Joint framework outperforms individual components.\n- Temporal performance improved in precision, recall, and F1.\n- Causal accuracy improved from 70.5% to 77.3%.\n\nSlide Topic: Discussion and Future Work\nSummary:\n- System shows low precision and high recall on TB-Dense dataset.\n- New dataset yields more balanced precision and recall.\n- Annotation scheme differences may explain performance variations.\n- Temporal improvements are small but statistically significant.\n- Joint framework effectively synthesizes both temporal and causal signals.\n- Causality encodes global information, complementing local temporal context.",
    "Slide Topic: Introduction to BabbleLabble Framework\nSummary:\n- Converts natural language and unlabeled data into a training set.\n- Comprises a semantic parser, filter bank, and label aggregator.\n- Semantic parser translates explanations into logical forms (LFs).\n- Filter bank removes incorrect LFs without ground truth labels.\n- Label matrix from LFs is aggregated into consistent labels.\n- Resulting labeled examples train a discriminative model.\n\nSlide Topic: Semantic Parser for Natural Language Explanations\nSummary:\n- Parser converts natural language explanations into logical forms (LFs).\n- LFs are functions mapping inputs to {-1, 0, 1} for binary classification.\n- Uses a rule-based approach without requiring training.\n- Rules are in the form \u03b1 \u2192 \u03b2, allowing token substitution.\n- Recursively constructs valid parses for explanation spans.\n- Outputs all valid LFs corresponding to the full explanation.\n\nSlide Topic: Filter Bank for Improving LF Quality\nSummary:\n- Removes incorrect LFs without additional labels.\n- Comprises semantic and pragmatic filters.\n- Semantic filter checks LF consistency with examples.\n- Pragmatic filters remove constant, redundant, or correlated LFs.\n- Removes 86% of incorrect parses across three tasks.\n- Remaining incorrect parses have accuracy within 2.5% of correct ones.\n\nSlide Topic: Label Aggregator for Combining LF Outputs\nSummary:\n- Combines multiple LF suggested labels into a single probabilistic label.\n- Handles potentially conflicting labels from different LFs.\n- Implements function f: {\u22121, 0, 1}^m\u00d7n \u2192 [0, 1]^n for m LFs and n examples.\n- Naive majority vote fails due to varying LF accuracy and coverage.\n- Uses data programming to model true labels and LF outputs.\n- Represents relationships as a factor graph for accurate label aggregation.\n\nSlide Topic: Discriminative Model for Final Classification\nSummary:\n- Uses noisily-labeled training set from label aggregator.\n- Trains an arbitrary discriminative model for classification.\n- Incorporates features beyond those in labeling functions.\n- Enhances model's ability to use informative, unidentified features.\n- Utilizes logistic regression classifier with generic features.\n- Features defined over dependency paths for classification.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Evaluates BabbbleLabble on three relation extraction tasks: Spouse, Disease, Protein.\n- Each task trains a classifier to predict entity relationships.\n- Dataset statistics, examples, and explanations provided.\n- Uses existing implementations of label aggregator, feature library, and classifier.\n- Implements methods described in Sections 2.4-2.5.\n- Utilizes open-source project Snorkel for implementation details.\n\nSlide Topic: High Bandwidth Supervision with BabbleLabble\nSummary:\n- BabbleLabble uses 30 explanations for classifier training.\n- Collecting 30 explanations takes as long as 60 labels.\n- BabbleLabble achieves given F1 scores with fewer user inputs.\n- Up to 100 times fewer inputs needed for the Spouse task.\n- Explanations apply to many unlabeled examples.\n- Each user input implicitly contributes multiple noisy labels.\n\nSlide Topic: Utility of Incorrect Parses and Filter Bank Effectiveness\nSummary:\n- LF correctness measured by exact match with manual parses.\n- Filter bank removes over 95% of incorrect LFs across tasks.\n- Final LF sets are 86% correct on average.\n- Correct and incorrect LFs passing filter yield similar end-task accuracy.\n- Average difference in end-task accuracy is less than 2.5%.\n\nSlide Topic: Comparison of Using LFs as Functions vs. Features\nSummary:\n- Logical forms can be used as features or functions.\n- Srivastava et al. (2017) suggest using them as features in a linear classifier.\n- We use logical forms as functions for weak supervision in data programming.\n- Data programming outperforms feature-based approach by 9.5 F1 points with rule-based parser.\n- Performance gap is 4.5 F1 points with a perfect parser.\n- Data programming better utilizes unlabeled data.\n\nSlide Topic: Related Work and Discussion on Weak Supervision\nSummary:\n- Focuses on modeling natural language explanations and learning from weak supervision.\n- Closest related work involves converting natural language into classifier features.\n- Our approach converts explanations into labeling functions.\n- Weak supervision has gained attention, especially in relation extraction.\n- Distant supervision uses existing knowledge bases to probabilistically create training sets.\n- Extensions aggregate various supervision sources using generative models from noisy labels.",
    "Slide Topic: Introduction to Abstractive Sentence Summarization\nSummary:\n- Online information growth demands effective summarization.\n- Focuses on abstractive sentence summarization.\n- Generates a shorter sentence version.\n- Preserves original meaning through generation.\n- Useful for creating appealing headlines.\n- Enhances comprehension and information retention.\n\nSlide Topic: Challenges with Pure Seq2Seq Models\nSummary:\n- Pure Seq2Seq models rely solely on source text for summaries.\n- Performance declines with increased generation length.\n- Models sometimes lose control over output.\n- 3% of summaries contain less than 3 words.\n- Some summaries repeat words excessively.\n- Results reduce the informativeness and readability of summaries.\n\nSlide Topic: Template-Based Summarization and Its Limitations\nSummary:\n- Traditional approach for abstract summarization\n- Templates are incomplete sentences filled manually\n- Summaries are fluent and informative due to human crafting\n- Template creation is time-consuming and requires domain expertise\n- Exhaustive template development across all domains is impractical\n- Limits scalability and adaptability to new information domains\n\nSlide Topic: Proposed Re 3 Sum System\nSummary:\n- Inspired by retrieve-based systems\n- Uses golden summaries as soft templates\n- No rules needed for new summaries\n- Combines seq2seq and template-based approaches\n- System named Re 3 Sum\n- Includes Retrieve, Rerank, and Rewrite modules\n\nSlide Topic: Retrieve Module: Finding Candidate Templates\nSummary:\n- Identifies candidate templates from training corpus.\n- Assumes similar sentences share similar summary patterns.\n- Finds analogies of input sentence in corpus.\n- Uses summaries of analogies as candidate templates.\n- Employs Lucene for efficient indexing and searching.\n- Selects top 30 search results as candidate templates.\n\nSlide Topic: Joint Rerank and Rewrite Modules\nSummary:\n- Encode source sentence and soft template into hidden states.\n- Use hidden states for measuring text relevance.\n- Jointly conduct reranking and rewriting with shared encoding.\n- Employ a Bidirectional Recurrent Neural Network (BiRNN) encoder.\n- BiRNN consists of forward and backward RNNs.\n- Represent source sentence and soft template in hidden states.\n\nSlide Topic: Evaluation Metrics and Baselines\nSummary:\n- ROUGE adopted for automatic evaluation since 2004.\n- Standard metric for DUC shared tasks.\n- Measures summary quality via lexical overlap.\n- Reports ROUGE-1 (unigram), ROUGE-2 (bigram), ROUGE-L (LCS) F1 scores.\n- Evaluates candidate summary against actual summaries.\n- Combines ROUGE scores to measure template relevance.\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- Re3Sum outperforms state-of-the-art seq2seq models in informativeness.\n- Soft templates show high competitiveness in summary generation.\n- High-quality external summaries enhance stability and readability.\n- Linguistic quality of generated summaries is measured across various aspects.\n- Re3Sum's performance is nearly identical to soft templates in length and brevity.\n- Soft templates effectively guide the summary generation process.\n\nSlide Topic: Effect of Soft Templates on Model Performance\nSummary:\n- Different soft templates fed into Rewriting module of Re3Sum.\n- Higher quality templates lead to higher ROUGE scores.\n- Random templates yield zero ROUGE-2 score.\n- Model generates acceptable summaries even with random templates.\n- Re3Sum can judge and ignore seriously irrelevant templates.\n- Joint learning with Rerank model is crucial.\n\nSlide Topic: Comparison with State-of-the-Art Models\nSummary:\n- Compared models: OpenNMT, FTSum, Pipeline\n- OpenNMT: Standard attentional seq2seq model\n- FTSum: Enhances summary faithfulness and informativeness\n- Pipeline: Baseline with Re3Sum architecture\n- OpenNMT results: Distinguished by 'O' (official) and 'I' (our experiment)\n- Pipeline trains rerank and rewrite modules sequentially",
    "Slide Topic: Introduction to Event Detection (ED)\nSummary:\n- ED identifies event triggers and types from sentences.\n- Example: 'married' is a trigger for 'Marry' event.\n- Focuses on extracting meaningful events from text.\n- Challenges arise from context-dependent meanings.\n- Crucial for understanding actions and states in text.\n- Enhances automated text analysis and information extraction.\n\nSlide Topic: Challenges in Event Detection\nSummary:\n- Identifying events from isolated sentences is difficult.\n- Same event trigger can represent different event types.\n- Context determines the event type accurately.\n- 'Leave' may indicate 'Transport' or 'End-Position' event.\n- Sentences within the same context are thematically related.\n- Document-level information is crucial for accurate detection.\n\nSlide Topic: Existing Methods for Event Detection\nSummary:\n- Methods categorized into feature-based and representation-based\n- Feature-based relies on manually designed features\n- Representation-based uses distributed representations for semantic capture\n- Most methods focus on sentence-level contextual information\n- Neglect of document-level information limits accuracy\n- Document-level context is critical for precise event detection\n\nSlide Topic: Proposed DEEB-RNN Model\nSummary:\n- Introduces DEEB-RNN for sentence-level event detection\n- Uses hierarchical, supervised attention mechanism\n- Focuses on event triggers at word level\n- Identifies event-containing sentences at sentence level\n- Learns document embeddings through attention\n- Enhances event detection accuracy in sentences\n\nSlide Topic: EDODEL Module: Document Embedding Learning\nSummary:\n- Learns document embeddings using hierarchical and supervised attention.\n- Employs word-level Bi-GRU encoder with attention on event triggers.\n- Utilizes sentence-level Bi-GRU encoder with attention on event-containing sentences.\n- Constructs gold attention signals at word and sentence levels.\n- Supervises learning process through these attention signals.\n\nSlide Topic: DEED Module: Document-level Enhanced Event Detection\nSummary:\n- Uses learned document embeddings to tag trigger candidates.\n- Employs Bi-GRU encoder for multi-word event triggers.\n- Softmax output layer provides probability vector for each word.\n- Vector indicates likelihood of a word being an event trigger.\n- Supports detection of various event types within documents.\n- Enhances accuracy in document-level event detection.\n\nSlide Topic: Joint Training of DEEB-RNN Model\nSummary:\n- DEEB-RNN integrates document embedding and event detection losses.\n- Joint loss function combines cross-entropy and square error.\n- Cross-entropy error targets event type prediction accuracy.\n- Square error measures attention accuracy at word and sentence levels.\n- Hyper-parameters balance loss components.\n- Training optimizes both embedding and detection modules jointly.\n\nSlide Topic: Experimental Setup and Baseline Models\nSummary:\n- DEEB-RNN model validated on ACE-2005 dataset\n- Dataset split into training, validation, and test sets\n- Compared against state-of-the-art methods\n- Baseline models include feature-based and representation-based methods\n- Specific baselines: Sentence-level, Joint Local, JRNN, Skip-CNN, ANN-S2\n- Additional baselines: Cross-event, PSL, DLRNN\n\nSlide Topic: Impact of Different Attention Strategies\nSummary:\n- Experiments on ACE-2005 dataset show varied attention strategies impact DEEB-RNN model performance.\n- DEEB-RNN3, using gold attention signals at word and sentence levels, performs best.\n- Supervised attention significantly enhances event detection accuracy.\n- Word-level and sentence-level attention signals are crucial for optimal performance.\n- Results highlight the importance of attention strategies in model effectiveness.\n\nSlide Topic: Performance Comparison and Analysis\nSummary:\n- DEEB-RNN outperforms current state-of-the-art methods.\n- Superior in recall and F1-measure.\n- Utilizes hierarchical and supervised attention mechanism.\n- Automatically captures event types without manual rules.\n- Learns event detection-oriented document embeddings.\n- Exploits document-level information effectively.",
    "Slide Topic: Introduction to Neural Machine Translation (NMT) and Its Advantages Over SMT\nSummary:\n- NMT uses Recurrent Neural Networks (RNNs) for direct translation.\n- Architecture includes an encoder and decoder, known as Encoder-Decoder approach.\n- NMT shows significant improvements over traditional Statistical Machine Translation (SMT).\n- Performs best with large training datasets.\n- Stacked LSTM units enhance performance in both encoder and decoder.\n- Outperforms phrase-based SMT in various language pairs.\n\nSlide Topic: Evaluation of NMT Models in English-Japanese Translation\nSummary:\n- NM\u0422 models evaluated for English-Japanese translation.\n- ASP\u0415C-JE corpus used for training data.\n- Focus on English-to-Japanese translation performance.\n- Simple NM\u0422 models outperform all SMT baselines.\n- Demonstrates NM\u0422 effectiveness in this language pair.\n\nSlide Topic: Impact of Different Recurrent Units on NMT Performance\nSummary:\n- Various recurrent units tested in NMT architectures.\n- Soft-attention models with LSTM units outperform multi-layer Encoder-Decoder models.\n- LSTM units show consistent superior performance.\n- Soft-attention models with GRU units perform significantly lower.\n- Choice of recurrent units significantly impacts NMT performance.\n- LSTM\u4f18\u4e8eGRU\u5728NMT\u6027\u80fd\u4e2d\u3002\n\nSlide Topic: Effect of Pre-reordering Training Data on NMT Models\nSummary:\n- Examined impact of pre-ordered data on NMT models.\n- Pre-ordered data does not benefit soft-attention models.\n- Performance slightly degrades with pre-ordered data.\n- Perplexity does not improve with pre-ordered training.\n- Observation holds for direct translation performance measurement.\n- Pre-reordering may not aid English-Japanese NMT.\n\nSlide Topic: Recovering Unknown Words in NMT Translations\nSummary:\n- NMT struggles with fixed vocabulary size in output layer.\n- Proposes workaround using back-off system for unknown words.\n- Involves translating input with both NMT and baseline SMT systems.\n- Compares NMT and SMT translation outputs.\n- Replaces unknown words in NMT output.\n- Uses rarest word from SMT output missing in NMT output.\n\nSlide Topic: Qualitative Analysis of NMT Translation Errors\nSummary:\n- Analysis conducted on held-out development data.\n- NMT models often omit information in translations.\n- Omissions lead to loss of sentence meaning.\n- Errors indicate room for model improvements.\n- Focus needed on preserving complete input meaning.\n- Qualitative insights guide future NMT enhancements.\n\nSlide Topic: Soft-Attention Mechanism in NMT\nSummary:\n- Focuses on parts of input, enhancing efficiency.\n- Makes computational graph differentiable.\n- Trained using standard backpropagation.\n- Mitigates performance drop with longer sentences.\n- Improves upon basic Encoder-Decoder approach.\n- Enhances model's ability to handle sequence data.\n\nSlide Topic: Experimental Setup and Model Architectures\nSummary:\n- Four models with recurrent layers of 1024 neurons each\n- Word embedding size set to 1000\n- Training via SGD with learning rate decay\n- Batch size of 60 and initial learning rate of 1\n- Gradient clipping to ensure L2 norm below 3\n- Beam size 20 for translation, fallback to 1000 if needed\n\nSlide Topic: Evaluation Metrics and Results\nSummary:\n- Perplexity on valid data correlated strongly with translation performance.\n- Submitted systems outperformed SMT baselines significantly.\n- System S2 added pre-reordered LSTM Search models to the ensemble.\n- System S2 achieved a 0.24 BLEU improvement.\n- Human evaluation confirmed effectiveness of NMT models.\n- NMT models effective in English-Japanese translation task.",
    "Slide Topic: Introduction to Knowledge Bases and Semantic Parsing\nSummary:\n- Knowledge bases (KB) store facts in structured databases.\n- Large-scale KBs like DBpedia and Freebase support open-domain QA.\n- State-of-the-art KB-QA relies on semantic parsing.\n- Semantic parsing maps questions to formal logic forms.\n- Questions are translated into KB queries for retrieval.\n- Answers are obtained by executing the generated queries.\n\nSlide Topic: Challenges in Traditional Semantic Parsing\nSummary:\n- Decoupling from knowledge base causes issues.\n- Generic meaning representations often mismatch.\n- Predicate differences between logic form and KB create problems.\n- Close language-KB schema still faces challenges.\n- Large KB vocabulary complicates predicate selection.\n- Utterance relation mapping to KB remains difficult.\n\nSlide Topic: Proposed Semantic Parsing Framework\nSummary:\n- Inspired by Yao and Van Durme (2014) and Bao et al. (2014)\n- Leverages knowledge base tightly for input question parsing\n- Defines query graph mapped to \u03bb-calculus logical form\n- Query graph closely related to \u03bb-DCS (Liang, 2013)\n- Semantic parsing reduced to query graph generation\n- Formulated as a search problem with staged states and actions\n\nSlide Topic: Query Graph Design and Its Relation to Knowledge Base\nSummary:\n- Four node types: grounded entity, existential variable, lambda variable, aggregation function.\n- Grounded entities are existing entities in knowledge base K.\n- Existential and lambda variables are un-grounded entities.\n- Lambda variables map to answer entities.\n- Aggregation function operates on specific entities.\n- Aggregation captures numerical properties.\n\nSlide Topic: Staged Query Graph Generation Process\nSummary:\n- Formalize query graph generation as a search problem.\n- States: Empty graph (\u03c6), single-node entity graph (Se), core chain (Sp), complex graph (Sc).\n- Actions: Add entity (Ae), add predicate (Ap), add constraint (Ac), add attribute (Aa).\n- Actions grow the graph by adding nodes and edges.\n- Process evolves through stages, enhancing graph complexity.\n\nSlide Topic: Deep Convolutional Neural Networks for Semantic Matching\nSummary:\n- Addresses semantic variety in question phrasing.\n- Mitigates mismatch between natural language and knowledge base predicates.\n- Employs Siamese neural networks for core inferential chain identification.\n- Two neural networks process pattern and inferential chain separately.\n- Outputs are k-dimensional vectors representing each input.\n- Semantic similarity computed using cosine distance function.\n\nSlide Topic: Augmenting Constraints and Aggregations in Query Graphs\nSummary:\n- Simple query graphs consist of inferential chains.\n- Executed against knowledge base K to retrieve answers.\n- Constraints can be added to refine answer entities.\n- Two types of actions: attaching entities to variable nodes (A_c) and aggregations (A_a).\n- A_c links entities to variables using valid predicates.\n- Aggregations describe constraints over entire answer sets, e.g., 'first'.\n\nSlide Topic: Learning the Reward Function for Query Graph Generation\nSummary:\n- Reward function \u03b3(s) assesses query graph correctness.\n- Uses log-linear model for learning.\n- Features match graph portions to input question.\n- Features correspond to staged actions.\n- Actions include Topic Entity, Core Inferential Chain.\n- Actions also include Constraints & Aggregations, Overall.\n\nSlide Topic: Experimental Results and Performance Analysis\nSummary:\n- STAGG outperforms existing methods by 7.2%.\n- STAGG's staged design contributes to its success.\n- Topic entity linking is the first critical stage.\n- Precision and recall of entities impact final results.\n- High-quality entity detection significantly enhances performance.\n- Detailed component analysis reveals individual contributions.\n\nSlide Topic: Error Analysis and Future Work\nSummary:\n- Approach outperforms existing methods but has room for improvement.\n- Accuracy for question answering is only slightly above 50%.\n- 100 incorrect query graphs sampled and errors categorized.\n- One third of errors due to label issues, not real mistakes.\n- Label errors account for 2% of issues.\n- 17% due to incomplete labels, 15% due to acceptable alternative answers.",
    "Slide Topic: Increasing Retraction Rates and Their Impact on Scientific Literature\nSummary:\n- Retraction rates are rising relative to overall publication growth.\n- Retractions help remove unethical and erroneous research.\n- Cited retracted articles pose a significant challenge.\n- Plagiarism and data fabrication are common retraction causes.\n- Continued citations interfere with purging bad studies.\n- Impact on scientific integrity and literature accuracy is critical.\n\nSlide Topic: Challenges Posed by Citations of Retracted Articles\nSummary:\n- Retracted articles often cited, undermining literature integrity.\n- Citations can legitimize unethical research practices.\n- High pre-retraction citations increase post-retraction citations.\n- Such citations jeopardize the scientific validation process.\n- Retracted studies may still influence current research findings.\n- Hinders efforts to remove flawed studies from academic discourse.\n\nSlide Topic: Types of Retraction Citations and Their Implications\nSummary:\n- Two types of retraction citations exist.\n- Pre-retraction citations occur before an article is retracted.\n- Post-retraction citations happen after retraction notices.\n- Both types jeopardize scientific integrity.\n- High pre-retraction citations increase post-retraction citations.\n- Misuse of retracted articles as legitimate references persists.\n\nSlide Topic: Case Study: Scott S. Reuben and the Persistence of Citations Post-Retraction\nSummary:\n- Scott S. Reuben fabricated data in 25 studies.\n- Resulted in mass retractions of his articles.\n- Reuben was convicted for scientific misconduct.\n- Popularity of his articles did not decrease post-retraction.\n- High citation rates persisted even 5 years later.\n- Highlights the persistence of flawed research in literature.\n\nSlide Topic: Methodology for Identifying and Analyzing Post-Retraction Citations\nSummary:\n- Accessed ScienceDirect, Elsevier's full-text database in October 2014.\n- Queried database for 'RETRACTED' in article titles and retraction notices.\n- Each retracted article in ScienceDirect is marked with 'RETRACTED'.\n- Journals include a retraction notice explaining the retraction reasons.\n- Manually coded each article with a 'retracted by' field.\n- Field represents the person(s) requesting the retraction.\n\nSlide Topic: Analysis of Citations for a Retracted Article in Cell\nSummary:\n- Article published in Cell in 2010, retracted in 2014.\n- Retraction due to irregularities in graphs and image misrepresentation.\n- Editors noted substandard figure preparation.\n- No impact on results' validity, according to the retraction notice.\n- Analyzed 36 recent citations tracked in 2015 and 2016.\n- Citations predominantly positive despite retraction.\n\nSlide Topic: The S\u00e9ralini Affair: A Controversial Case of Retraction and Its Aftermath\nSummary:\n- Study published in 2012 on GM crops and rats.\n- Found increased tumors in rats fed GM crops.\n- Validity of findings and use of animals debated.\n- Media and government scrutiny led to retraction.\n- Sparked anti-GM food movement.\n- Ongoing demand for GM food labeling.\n\nSlide Topic: Republishing Retracted Articles and Its Impact on Scientific Credibility\nSummary:\n- Article republished in 2014 by Environmental Sciences Europe.\n- Stirred controversy in the scientific community.\n- Scientists expressed concerns over its reappearance.\n- Received 17 citations in 2015 and 2016, mostly positive.\n- Republishing questioned scientific credibility.\n- Criticism towards peer-review practices of retracting editors.\n\nSlide Topic: Citations of a Retracted Article on Resveratrol and Data Manipulation\nSummary:\n- Article retracted in 2014 due to data manipulation.\n- Editors examined claims with three methodologists.\n- Findings revealed irregularities and falsification of results.\n- Authors failed to respond appropriately to findings.\n- Article was officially retracted from the journal.\n- Despite retraction, article continued to be cited.\n\nSlide Topic: Recommendations for Handling Citations of Retracted Articles\nSummary:\n- Use reference checks on all submitted articles.\n- Detect and remove citations of retracted articles.\n- Request author explanation for citing retracted papers.\n- Ensure explanations clearly appear in the paper.\n- Delete retracted articles from publishers' websites.\n- Restrict free access to retracted articles online.",
    "Slide Topic: Introduction to Word Embeddings and Their Analysis\nSummary:\n- Word embeddings are vector representations of words.\n- Ubiquitous in natural language processing (NLP) applications.\n- Analysis of embedding spaces is crucial for understanding their utility.\n- Facilitates proper use in downstream NLP tasks.\n- Many analyses focus on nearest neighborhoods.\n- Proxies for semantic information in embeddings.\n\nSlide Topic: Limitations of Neighborhood-Based Semantic Analysis\nSummary:\n- Neighborhood-based analysis relies on unreliable nearest neighborhoods.\n- Nearest neighborhoods can be inconsistent and inaccurate.\n- Focuses on semantic content of embedding spaces.\n- Does not evaluate characteristics of the feature space itself.\n- Limits comprehensive understanding of data semantics.\n\nSlide Topic: Geometric Analysis of Word Embeddings\nSummary:\n- Geometric analysis provides insights into word embeddings.\n- It examines the distribution of embeddings (Mimno & Thompson, 2017).\n- Correlates embedding properties with performance (Chandrasekaran et al., 2018).\n- Enhances understanding of semantic neighborhoods.\n- Adds information on the continuous feature space.\n- Improves characterization of embedding properties.\n\nSlide Topic: Transformations Applied to Pretrained Embeddings\nSummary:\n- Analyzed four components of word embedding geometry.\n- Transformed embeddings to expose subsets to downstream models.\n- Used Word2Vec, GloVe, and FastText pretrained embeddings.\n- Applied transformations to each set of embeddings.\n- Evaluated transformed embeddings on standard tasks.\n- Measured changes in task performance.\n\nSlide Topic: Affine Transformations and Their Impact\nSummary:\n- Affine transformations used for word embedding post-processing.\n- Includes translations, reflections, rotations, and homotheties.\n- Aligns with prior work on embedding analysis.\n- Enhances embeddings for specific tasks.\n- Highly interpretable in relation to original space.\n- Supports improved understanding and utilization of embeddings.\n\nSlide Topic: Cosine Distance Encoding (CDE) Transformation\nSummary:\n- CDE transforms feature distribution in R^d.\n- Represents word vectors as pairwise distance matrix.\n- Uses distances to embeddings of 10K most frequent words.\n- Yields transformation: R^d \u2192 R^104.\n- Autoencoder maps back to R^d for compatibility.\n- Obscures original feature distribution for obfuscation.\n\nSlide Topic: Nearest Neighbor Encoding (NNE) Transformation\nSummary:\n- NNE discards most global pairwise distance information.\n- Retains information about nearest neighborhoods only.\n- Maps from R^d to R^|V| dimensions.\n- Outputs form a sparse adjacency matrix.\n- Describes a directed nearest neighbor graph.\n- Combines outputs for each v \u2208 V to create the matrix.\n\nSlide Topic: Evaluation of Transformed Embeddings\nSummary:\n- Evaluate transformed embeddings' utility as input features.\n- Assess contributions of geometric aspects from Section 3.\n- Use intrinsic evaluations modeling linguistic info directly.\n- Apply extrinsic evaluations in downstream applications.\n- Test on standard evaluation battery for comprehensive analysis.\n- Transform embeddings using defined sequence of operations.\n\nSlide Topic: Analysis and Discussion of Evaluation Results\nSummary:\n- Models robust to simple linear transformations.\n- Rotations, dilations, and reflections minimally affect both intrinsic and extrinsic tasks.\n- Extrinsic evaluations show insensitivity to translations.\n- Translations can be modeled using bias terms.\n- Intrinsic tasks more sensitive to shifts relative to origin.\n- Lack of learned models and reliance on cosine similarity increase sensitivity in intrinsic tasks.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Training methods should focus on local geometric structure.\n- Tuning embeddings must consider downstream task specificity.\n- Recent contextualized embeddings like ELMo and BERT generate dynamic embeddings.\n- Future work should improve sampling methods for local structures.\n- Evaluation methods need to be refined for better geometric accuracy.\n- Specialized applications demand more precise embedding tuning.",
    "Slide Topic: Introduction to the Problem and Neural Models\nSummary:\n- Long-term goal: machines answering arbitrary user-generated questions.\n- Current methods can locate likely documents with answers.\n- Automatically extracting answers from texts is still challenging.\n- Neural models show promise in answering questions given related text.\n- Neural models could be key to solving automatic answer extraction.\n- Ongoing research aims to enhance neural models' capabilities.\n\nSlide Topic: Pipelined Approach for Paragraph Selection\nSummary:\n- Proposes pipelined QA system for paragraph selection.\n- Selects paragraph with smallest TF-IDF cosine distance to question.\n- Computes document frequencies using individual paragraphs.\n- Uses linear classifier for multiple input documents.\n- Classifier features include TF-IDF score, paragraph position, preceding tokens, and question word inclusion.\n- Enhances accuracy in multi-document scenarios.\n\nSlide Topic: Handling Noisy Labels in Distantly Supervised Data\nSummary:\n- Labels all text spans matching answer text.\n- Risks training model to select unwanted spans.\n- Uses summed objective function.\n- Function similar to Kadlec et al. (2016).\n- Optimizes negative log-likelihood.\n- Focuses on selecting correct answer spans.\n\nSlide Topic: Model Architecture and Components\nSummary:\n- Embedding layer uses pre-trained word vectors and character-derived embeddings.\n- Pre-process layer employs a shared bi-directional GRU for question and passage embeddings.\n- Attention layer utilizes BiDAF for query-aware context representation.\n- Self-attention layer applies residual self-attention.\n- Prediction layer uses bi-directional GRU and linear layer for answer start and end scores.\n- Dropout rate of 0.2 applied to GRU inputs and attention mechanisms.\n\nSlide Topic: Confidence Method for Multi-Paragraph Settings\nSummary:\n- Uses un-normalized, un-exponentiated score as confidence measure.\n- Runs model on each paragraph separately.\n- Selects answer span with highest confidence score.\n- Experiments with four training approaches.\n- Approaches include Shared-Normalization, Merge, No-Answer Option, Sigmoid.\n\nSlide Topic: Training Methods for Comparable Confidence Scores\nSummary:\n- Four approaches tested: Shared-Normalization, Merge, No-Answer Option, Sigmoid.\n- All methods include sampling paragraphs without answers as training points.\n- Shared-Normalization modifies objective function for cross-paragraph score normalization.\n- Merge concatenates paragraphs from the same context during training.\n- No-Answer Option lets model choose a 'no-answer' option per paragraph.\n- Sigmoid uses sigmoid loss objective function.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Evaluated on TriviaQA Unfiltered, TriviaQA Wiki, TriviaQA Web, and SQuAD.\n- TriviaQA Web uses all 530k training examples without subsampling.\n- Restructured TriviaQA documents by merging small paragraphs.\n- Set maximum paragraph size to 400 unless otherwise noted.\n- Used paragraph separator tokens with learned embeddings between merged paragraphs.\n- Preserved formatting information through separator tokens.\n\nSlide Topic: Results and Performance Analysis\nSummary:\n- Model outperforms prior work on TriviaQA web and wiki.\n- Achieves 61.55 EM and 67.61 F1 on TriviaQA dev set.\n- Outperforms previous best by 10 points on TriviaQA.\n- Reaches 72.37 F1 and 64.08 EM on SQuAD with 15 paragraphs.\n- Scores 59.14 EM and 67.34 F1 on document-level SQuAD.\n- Significantly outperforms Chen et al. (2017a) by 9.44 EM.\n\nSlide Topic: End-to-End QA System Demonstration\nSummary:\n- Experiment tests model in an end-to-end QA system.\n- Retrieves up to 10 web documents using Bing search.\n- Includes Wikipedia articles on entities identified by TAGME.\n- Selects 16 most relevant paragraphs using linear paragraph ranker.\n- Tests on curated questions from Baudis (2015) in TREC QA tasks.\n- Achieves 53.31% accuracy, demonstrating advanced QA capabilities.\n\nSlide Topic: Discussion and Future Directions\nSummary:\n- Models trained on single answer-containing paragraphs perform poorly with multiple paragraphs.\n- Shared-norm approach outperforms others, especially on SQuAD and TriviaQA unfiltered.\n- Minimal performance impact when only one paragraph is used, preserving per-paragraph performance.\n- Shared-norm model accuracy does not drop with additional paragraphs, resolving distraction issues.\n- Future work should advance sentence and paragraph reading comprehension.\n- Incorporating mechanisms to handle document-level coreferences is recommended.",
    "Slide Topic: Introduction to Diversity in Paraphrasing\nSummary:\n- Diversity in paraphrasing is crucial in NLP.\n- Enhances data augmentation and conversational agents.\n- Applies to text simplification and document summarization.\n- Supports QA systems and information retrieval.\n- Previous models focus on semantic similarity.\n- Need for diverse paraphrasing approaches is growing.\n\nSlide Topic: Challenges in Existing Paraphrasing Approaches\nSummary:\n- Models use top-k beam search for paraphrases.\n- Results often lack structural diversity.\n- Sentences show minor, word-level changes.\n- Some works address diversity in NLP.\n- Retaining source fidelity remains challenging.\n- Current methods are restrictive in this aspect.\n\nSlide Topic: Monotone Submodular Function Maximization for Paraphrasing\nSummary:\n- Addresses diverse paraphrasing challenge.\n- Balances fidelity retention and diversity maximization.\n- Submodular objective enhances control over diversity and fidelity.\n- Greedy algorithm solves monotone submodular function maximization.\n- Guarantees near-optimal diverse paraphrasing solutions.\n- Efficient approach for generating varied yet faithful paraphrases.\n\nSlide Topic: Diverse Paraphraser using Submodularity (DiPS) Methodology\nSummary:\n- Introduces DiPS for generating diverse paraphrases.\n- Utilizes a novel submodular objective function.\n- Built on the SEQ2SEQ framework for paraphrasing.\n- Enhances quality of paraphrases through submodular optimization.\n- Applies during decoding phase for improved output diversity.\n- Targets high-quality, varied paraphrases efficiently.\n\nSlide Topic: Fidelity and Diversity Objectives in DiPS\nSummary:\n- Fidelity achieved through syntactic and semantic sentence similarity.\n- Diversity encouraged by rewarding sequences with more distinct n-grams.\n- Structural variations ensured using edit distance-based function.\n- Aim to balance fidelity and diversity for high-quality generations.\n- Complementary notions enhance sentence accuracy and variety.\n\nSlide Topic: Experiments and Evaluation of DiPS\nSummary:\n- DiPS evaluated against recent paraphrasing models.\n- Outperforms baselines in fidelity metrics (BLEU, METEOR, TERp).\n- Tested against multiple diversity-inducing schemes.\n- Generates paraphrases used as augmented samples.\n- Evaluated in paraphrase recognition task.\n- Achieves high performance on Quora-PR dataset.\n\nSlide Topic: Data Augmentation Using DiPS\nSummary:\n- Evaluates high-quality paraphrases in classification tasks\n- Tasks include intent and question classification\n- DiPS generates high structural variations\n- Maintains fidelity without compromising quality\n- Leads to significant accuracy gains\n- Benefits both types of classification models\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Proposed DIPS model for high-quality paraphrasing.\n- Utilizes a novel submodular objective function.\n- Demonstrated effectiveness across multiple standard datasets.\n- Generates diverse, meaning-preserving paraphrases.\n- Useful for data augmentation.\n- Future work includes expanding dataset diversity.",
    "Slide Topic: Introduction to Geolocation Prediction in Social Media\nSummary:\n- Geolocation prediction forecasts user or document locations.\n- Crucial for disaster, disease, and political analyses.\n- Applied to Wikipedia, Flickr, Facebook, and Twitter.\n- Twitter preferred due to suitable characteristics.\n- Location data enhances social media analysis accuracy.\n- Proposes a neural network model for Twitter geolocation.\n\nSlide Topic: Text-based Approaches for Geolocation Prediction\nSummary:\n- Word distributions over locations estimate user geolocations.\n- Maximum likelihood estimation predicts locations using word distributions.\n- Language modeling minimizes KL-divergence for location prediction.\n- Supervised machine learning uses word features for text-based geolocation.\n- Multinomial Naive Bayes and logistic regression are effective methods.\n- Neural networks with stacked denoising autoencoders enhance prediction accuracy.\n\nSlide Topic: User Network-based Approaches for Geolocation Prediction\nSummary:\n- User interactions form a network on social media.\n- User network information aids in geolocation prediction.\n- Twitter friend and follower data predict user locations.\n- Various algorithms like SVM and Bayesian networks are used.\n- Mention information also supports geolocation prediction.\n- Label propagation and social coefficient models enhance accuracy.\n\nSlide Topic: Metadata-based Approaches for Geolocation Prediction\nSummary:\n- Metadata, like location fields, aids geolocation prediction.\n- Decent accuracy achieved using location fields alone.\n- Combining metadata with texts extends prediction methods.\n- Dynamic weighted ensemble method enhances prediction accuracy.\n- Polygon stacking and stacking techniques improve prediction.\n- Neural network with average pooling strengthens geolocation prediction.\n\nSlide Topic: Proposed Neural Network Model for Geolocation Prediction\nSummary:\n- Combines text, metadata, and user network info.\n- Uses complex neural network for integration.\n- Neural networks effectively capture complex representations.\n- Aims for unified representations with attention mechanism.\n- Superior to earlier ensemble approaches.\n- Enhances geolocation prediction accuracy.\n\nSlide Topic: Evaluation of the Proposed Model\nSummary:\n- Evaluated using accuracy, accuracy @ 161, median error distance, and mean error distance.\n- Achieved 2.8% accuracy increase on TwitterUS.\n- Achieved 2.4% accuracy @ 161 increase on TwitterUS.\n- Achieved 4.8% accuracy increase on W-NUT.\n- Achieved 6.6% accuracy @ 161 increase on W-NUT.\n- Compared against baseline model MADCEL-B-LR-STACK.\n\nSlide Topic: Analysis of Attention Probabilities in the Proposed Model\nSummary:\n- Analyzed Attention U and Attention UN states\n- Higher probabilities assigned to timeline representations\n- Twitter US shows more informative timelines than W-NUT\n- Twitter US timelines contain more tweets\n- Findings support model's attention allocation rationale\n- Insights gained into model's unification processes\n\nSlide Topic: Limitations and Future Directions of the Proposed Model\nSummary:\n- Model accuracy at 70-72% even at Accuracy@161.\n- High confidence errors occur with location changes or travel tweets.\n- Errors noted when location fields are incorrect due to house moves.\n- Future work includes handling multiple locations.\n- Expanding model to account for temporal states.\n- Aiming to capture location changes and states like traveling.",
    "Slide Topic: Introduction to Automatic Machine Translation Evaluation Metrics\nSummary:\n- Automatic MT evaluation metrics face criticism.\n- Higher BLEU scores correlate with better human judgments.\n- Consistency in correlation supports conventional wisdom.\n- BLEU score is a widely used metric.\n- Significant higher BLEU scores suggest better translations.\n- Lower scoring translations are generally less preferred.\n\nSlide Topic: BLEU Metric and Its Limitations\nSummary:\n- BLEU measures translation quality by n-gram matches.\n- Uses brevity penalty (BP) to adjust for short translations.\n- Scores range from 0 to 1, with 1 being perfect match.\n- Combines precision with BP for a balanced score.\n- Formula: BLEU = BP \u00d7 exp(\u03a3 w_n log p_n).\n- Limited in assessing fluency and meaning accuracy.\n\nSlide Topic: Real-World Example of BLEU Score Discrepancy\nSummary:\n- High BLEU score but poor manual evaluation in MT task.\n- Hypothesis translation BLEU: 64.03 with BP of 0.905.\n- Baseline translation BLEU: 43.29 with BP of 0.854.\n- Subtle difference between hypothesis and baseline translations.\n- Five crowd-sourced evaluators preferred baseline translation.\n- Example from WAT 2015 Asian Translation Workshop.\n\nSlide Topic: RIBES Metric and Its Limitations\nSummary:\n- RIBES adds rank correlation to unigram matches.\n- Addresses reordering issues without needing higher n-grams.\n- Hypothesis translation penalized minimally despite errors.\n- RIBES scores: 94.04 (hypothesis), 86.33 (baseline).\n- Human evaluators favored baseline translation unanimously.\n- RIBES fails to fully account for significant translation errors.\n\nSlide Topic: Alternative Approaches to BLEU and RIBES\nSummary:\n- Metrics vary in how they measure n-gram matches.\n- METEOR allows matching of paraphrases and stems.\n- ROUGE-S uses skip-gram matches for evaluation.\n- Recent models use semantic similarity for matching.\n- Neural networks trained on skip-grams improve accuracy.\n- Surface n-gram matching enhancements don't solve BLEU's core issues.\n\nSlide Topic: Experimental Setup for WAT 2015 Shared Task\nSummary:\n- Task: Korean to Japanese patent translation\n- Data: 1 million sentences from JPO Patent Corpus\n- Tool: Phrase-based SMT using Moses toolkit\n- Alignment: IBM model 4 with MGIZA++\n- Models: Bidirectional lexicalized reordering, 5-gram language model\n- Tuning: MERT for decoding parameters\n\nSlide Topic: Human Evaluation Methodology\nSummary:\n- Scores acquired via Lancers crowdsourcing platform\n- Evaluators randomly assigned test set documents\n- Shown source, hypothesis, and baseline translations\n- Five non-expert evaluators judged each document\n- Judgments on better translation or tie determined wins/losses\n- HUMAN score calculated: 100 \u00d7 (W - L) / (W + L + T)\n\nSlide Topic: Results and Analysis of the WAT 2015 Submission\nSummary:\n- MERT tuning method is non-deterministic.\n- Repeated tuning for higher BLEU score submissions.\n- Submitted three sets of output for evaluation.\n- Two sets underwent manual evaluation.\n- Our system outperformed the baseline in BLEU scores.\n- Improvement due to larger training data and second development set inclusion.\n\nSlide Topic: Segment Level Meta-Evaluation of BLEU and RIBES\nSummary:\n- Conducted segment-level meta-evaluation using BLEU and RIBES.\n- Figures 1 and 2 show correlations with human judgments.\n- High BLEU improvements (>60) rated marginally better than baseline.\n- Some translations with >40 BLEU improvements tied with baseline.\n- Many segments with BLEU improvements rated poorer than baseline.\n- High BLEU improvements often tied with baseline translations.\n\nSlide Topic: Conclusion and Future Directions\nSummary:\n- High BLEU and RIBES scores do not always correlate with human judgment.\n- Demonstrated using a real-world case from WAT 2015 patent shared task.\n- Identified factors contributing to poor correlation between scores and human judgment.\n- Conducted segment-level meta-evaluation to pinpoint worse system translations.\n- Found instances where high BLEU/RIBES improvements were deemed worse than baseline.\n- Aims to improve automatic translation evaluation metrics based on findings.",
    "Slide Topic: Introduction to Controversial Content and Its Detection\nSummary:\n- Controversial content sparks mixed feedback.\n- Can improve community health through spirited debate.\n- Detection aids both community members and moderators.\n- Alerts moderators to potentially contentious discussions.\n- Context of community influences content's controversial nature.\n- What's controversial varies across different audiences.\n\nSlide Topic: Community-Specific Controversiality and Its Challenges\nSummary:\n- 'Break up' is controversial in relationships subreddit.\n- Same topic lacks controversy in Ask Women subreddit.\n- 'Crossfit' is highly controversial in Fitness subreddit.\n- Controversial topics vary widely between communities.\n- Some topics are not discussed in certain communities.\n- Community moderators cannot predict which posts will attract controversy.\n\nSlide Topic: Proposed Early-Detection Approach for Controversial Content\nSummary:\n- Uses initiating post content and initial responding comments.\n- Incorporates both textual and structural features.\n- Combines previously disjoint research streams.\n- Analyzed over 15,000 discussion trees.\n- Covers six different subreddits.\n- Shows quick improvement in predictive performance.\n\nSlide Topic: Dataset and Methodology for Controversy Labeling\nSummary:\n- Data sourced from Reddit, covering diverse subcommunities.\n- Dataset includes public posts and comments from 2007 to Feb. 2014.\n- Combines Baumgartner's datasets with additional scraping efforts.\n- Focuses on six text-based subreddits.\n- Subreddits vary in style and topic for comprehensive coverage.\n- Aims to capture community-specific controversial content.\n\nSlide Topic: Quantitative and Qualitative Validation of Controversy Labels\nSummary:\n- Binary labels: controversial vs. non-controversial\n- Discard posts with >5% variability across 10 API queries\n- Exclude posts with no variation in upvote ratio or score\n- Sort surviving posts by upvote percentage within each community\n- Label top quartile as 'non-controversial'\n- Label bottom quartile as 'controversial' if downvotes approach upvotes\n\nSlide Topic: Analysis of Early Discussion Threads for Controversy Prediction\nSummary:\n- Analyze early comments for controversy prediction.\n- Focus on comments within one hour of original post.\n- Introduce C-RATE features: comment rate and number.\n- Introduce C-TREE features: structural aspects of discussion trees.\n- C-TREE features improve model fit in most cases.\n- Exception: personal finance category shows no significant improvement.\n\nSlide Topic: Comparison of Text Models for Controversy Classification\nSummary:\n- Compares text-only models for classifying controversial posts.\n- Models access full post titles and bodies.\n- BERT-based models perform best overall.\n- HAND+W2V also shows strong performance.\n- Adding hand-designed features to BERT does not improve performance.\n- BERT-MP-512 selected for further experiments.\n\nSlide Topic: Impact of Post-Time Metadata on Controversy Prediction\nSummary:\n- Post-time metadata features added to TEXT for performance evaluation.\n- TIME features consistently improve performance across all communities.\n- Adding author features to TIME + TEXT does not enhance performance.\n- TEXT + TIME models perform best.\n- TEXT + TIME models set as strong post-time baseline.\n\nSlide Topic: Incorporation of Early Discussion Features for Improved Prediction\nSummary:\n- Augmented post-time features with early-discussion sets.\n- Access to comments from increasing observation periods.\n- Trained linear classifiers combining TEXT + TIME features.\n- Swept observation periods from 15 to 180 minutes in 15-minute intervals.\n- Improved performance in five out of six communities.\n- Significant performance exceeded post-time baseline in under three hours.\n\nSlide Topic: Domain Transfer Experiments and Feature Transferability\nSummary:\n- Models trained on one subreddit tested on another.\n- Text features strong in-domain but brittle and community-specific.\n- Rate and structural comment tree features less predictive in-domain.\n- Text features do not transfer well between communities.\n- Structural features transfer better across different communities.\n- Experiment highlights varying transferability of different features.",
    "Slide Topic: Introduction to Automatic Evaluation in Machine Translation\nSummary:\n- Automatic evaluation is preferred for determining MT system performance.\n- Manual evaluation is costly and time-consuming.\n- Automatic metrics correlate well with manual evaluation.\n- Essential for hyperparameter tuning and architecture search.\n- Facilitates objective and reproducible performance measurement.\n- Enables easier cross-paper comparisons.\n\nSlide Topic: WMT Metrics Shared Task Setup and Objectives\nSummary:\n- Automatic metric compares MT output with manual reference translations.\n- Produces system-level scores or segment-level scores, or both.\n- System-level score provides an overall score for the MT system.\n- Segment-level scores assess each output translation individually.\n- Joint task with Quality Estimation (QE) organizers introduced.\n- QE methods estimate translation quality without human-produced references.\n\nSlide Topic: Manual Quality Assessment Methods and Golden Truth\nSummary:\n- Direct Assessment (DA) used as 'golden truth' source.\n- DA collects quality scores on a 1-100 scale for MT outputs.\n- Scores standardized per annotator for consistency.\n- System-level evaluation averages standardized scores.\n- Segment-level evaluation requires converting DA to relative rankings.\n- Scores assess MT system performance comprehensively.\n\nSlide Topic: Baseline Metrics and Their Implementation\nSummary:\n- BLEU, NIST, TER, WER, PER, CDER, chrF, chrF+ used.\n- sacreBLEU-BLEU and sacreBLEU-chrF for system-level only.\n- sentBLEU for segment-level only; chrF for both levels.\n- Chinese word segmentation not supported by tokenization scripts.\n- Pre-process MT outputs and reference translations for Chinese.\n- Separate Chinese characters and non-Chinese parts in pre-processing.\n\nSlide Topic: Submitted Metrics and Their Descriptions\nSummary:\n- Metrics overview for evaluation provided.\n- B.E.E.R uses linear model combining sub-word and global word order features.\n- BERT-r compares MT output with reference using contextual embeddings.\n- CharacTER is a character-level metric inspired by Translation Edit Rate.\n- EED is a character-based metric derived from CDER.\n- ESIM, a neural model for NLI, adapted for MT evaluation.\n- hLEPORb, hLEPORa use length, precision, recall, and position penalties.\n- PR-eP filters pseudo-references for better gold reference matching.\n- WMD-O measures distance between distributions in semantic vector space.\n- YiSi evaluates MT quality across languages with varying resource levels.\n\nSlide Topic: System-Level Evaluation Results and Analysis\nSummary:\n- Discuss system-level results for news task systems.\n- Tables 3, 4, and 5 show metric correlations for news test 2019.\n- Underlying texts are from WMT19 News Translation test set.\n- Use Williams significance test for correlation differences.\n- Highlight non-significantly outperformed metrics in bold.\n- MT systems are those participating in WMT19 News Translation Task.\n\nSlide Topic: Segment-Level Evaluation Results and Analysis\nSummary:\n- Relies on manual judgments from News Translation Task evaluation.\n- Converts DA scores to d-aRR preferences for competing translations.\n- Measures metric quality using Kendall's Tau-like formula.\n- Key change from WMT14-WMT16 to WMT17-WMT19: shift from RR to d-aRR.\n- Addresses treatment of ties in evaluation methodology.\n- Results shown in Tables 6 and 7.\n\nSlide Topic: Discussion on Metric Performance Across Different MT Systems\nSummary:\n- Metric performance varies with underlying texts and systems.\n- Strong MT systems are hard to distinguish, even for human assessors.\n- Metrics correlate better with human judgment when systems span a wide performance range.\n- Plots of Pearson correlation were created for top MT systems.\n- Figure 5 shows a sample plot.\n- Detailed plots for all language pairs and metrics are provided.\n\nSlide Topic: QE Systems as Metrics: Performance and Challenges\nSummary:\n- Standard reference-based metrics show better correlations.\n- 'QE as a Metric' correlations range from 0.028 to 0.947 across language pairs.\n- System-level 'QE as a Metric' correlations are highly unstable.\n- Segment-level 'QE as a Metric' correlations range from -0.153 to 0.351.\n- Instability is observed for a single metric across different language pairs.\n- Performance varies significantly between monolingual and bilingual golden truth.\n\nSlide Topic: Future Directions and Insights for MT Evaluation Metrics\nSummary:\n- Investigate BLEU's effectiveness in current models.\n- Explore why BLEU fails with state-of-the-art outputs.\n- Metrics like BERT-r, ESIM, YiSi use more semantic features.\n- Future metrics should focus on semantic feature analysis.\n- Evaluate if contextual embeddings achieve semantic understanding.\n- Assess if semantic understanding drives metric performance gains.",
    "Slide Topic: Introduction to the Split-and-Rephrase Task\nSummary:\n- Processing long, complex sentences is difficult.\n- Task aims to break complex sentences into simpler ones.\n- Simplified sentences must retain original meaning.\n- Narayanan et al. (2017) introduced the task.\n- Dataset, evaluation methods, and baseline systems were provided.\n- Task is named 'Split-and-Rephrase'.\n\nSlide Topic: Challenges in Processing Complex Sentences\nSummary:\n- Complex sentences are hard to process.\n- Challenges exist for both humans and NLP systems.\n- Parsing and machine translation are particularly affected.\n- Long sentences can confuse meaning.\n- Breaking down sentences improves understanding.\n- Automatic simplification systems are highly desirable.\n\nSlide Topic: Dataset and Evaluation Methodology\nSummary:\n- Dataset contains 1,066,115 instances.\n- Each instance maps a complex sentence to simpler sentences.\n- Includes RDF triples for semantic description.\n- Evaluation follows Narayan et al. (2017).\n- Uses averaged multi-reference BLEU score.\n- Scores each prediction for accuracy.\n\nSlide Topic: Initial SEQ2SEQ Model Performance and Limitations\nSummary:\n- Vanilla SEQ2SEQ model with attention achieves 77.5 BLEU.\n- Substantially outperforms Narayan et al. (2017) text-to-text baseline.\n- Approaches the best RDF-aware method of Narayan et al. (2017).\n- Manual inspection reveals unwanted behaviors in outputs.\n- Performance indicates potential but highlights need for improvements.\n\nSlide Topic: Analysis of Model Behavior and Memorization\nSummary:\n- Manual inspection reveals unwanted behaviors in outputs.\n- Outputs contain unsupported facts not present in inputs.\n- Facts are often repeated across multiple output sentences.\n- Some input facts are omitted in the output.\n- Model memorizes entity-fact pairs rather than learning to rephrase.\n- Behavior indicates memorization over understanding and rephrasing.\n\nSlide Topic: Proposed Data Split for Better Generalization\nSummary:\n- New train-dev-test split for split-and-rephrase model.\n- 5,554 distinct complex sentences divided randomly.\n- RDF information ensures all relation types in training set.\n- Each RDF triplet appears in only one split.\n- Enhances model's generalization and expected behavior.\n\nSlide Topic: Copy-Augmented SEQ2SEQ Model\nSummary:\n- SEQ2SEQ models augmented with a copy mechanism.\n- Copy mechanism proven beneficial in summarization and language modeling.\n- Hypothesis: Biasing towards copying improves performance.\n- Many words in simple sentences correspond to entities.\n- Entities often appear in complex sentences.\n- Aims to enhance split-and-rephrase task performance.\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- Models outperform Narayan et al. (2017) by up to 8.68 BLEU on original split.\n- No RDF triples used in our models.\n- Vanilla SEQ2SEQ models fail on new split.\n- Copy-augmented models perform better on new split.\n- Macro-average number of simple sentences per prediction presented.\n- Average number of tokens per simple sentence analyzed.\n\nSlide Topic: Conclusions and Future Research Directions\nSummary:\n- SEQ2SEQ model scores high without true split-and-rephrase capability.\n- Introduced a more challenging data split to address this issue.\n- Cheating SEQ2SEQ models fail on the new data split.\n- Copy-mechanism augmentation improves performance on both splits.\n- Establishes a new competitive baseline for the task.\n- Split-and-rephrase task remains unsolved on the new split.",
    "Slide Topic: Introduction to Neural Network Mappings in Cross-Modal Retrieval\nSummary:\n- Neural networks bridge modalities in cross-modal retrieval.\n- Trained to predict vectors of one modality from another.\n- Used in zero-shot learning, multimodal representations, word translation.\n- At test time, operations like retrieval use nearest neighbors.\n- Success depends on semantic similarity of predicted and target vectors.\n- Mapping accuracy is crucial for system performance.\n\nSlide Topic: The Phenomenon of Input Vector Preservation in Mapped Vectors\nSummary:\n- Recent work shows input modality information propagates into predicted modality.\n- Mapped vectors often resemble input vectors more than target vectors semantically.\n- This contradicts the goal of a cross-modal map.\n- Phenomenon linked to continuity and topology-preserving properties of neural networks.\n- Ensures closely related points are mapped closely together.\n- Challenges the effectiveness of cross-modal mapping in diverse applications.\n\nSlide Topic: Proposed Measure for Quantifying Semantic Similarity\nSummary:\n- Measures similarity between paired vector sets' neighborhood structures.\n- Introduces mean nearest neighbor overlap measure (mNNO).\n- Quantifies semantic similarity between two vector sets.\n- Focuses on paired vectors' predicted and target sets.\n- Offers a more realistic estimate of semantic similarity.\n- Enhances understanding of vector sets' semantic relationships.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Experiments conducted across three benchmarks: ImageNet, IAPR TC-12, Wiki.\n- Benchmarks cover text at word, sentence, and document levels.\n- Utilizes state-of-the-art text and image features.\n- Features include GloVe, word2vec, bi-GRU for text.\n- Features also include VGG-128 and ResNet for images.\n- Aims to learn image-to-text and text-to-image neural mappings.\n\nSlide Topic: Results of Experiment 1: Neighborhood Structure Analysis\nSummary:\n- Mapped vectors closely resemble input vectors' semantic structure.\n- Mapped vectors less resemble target vectors' structure.\n- Test mean nearest neighbor overlap (mNNO) indicates similarity.\n- Higher mNNO between input and mapped vectors.\n- Lower mNNO between target and mapped vectors.\n- Results summarized in Table 1.\n\nSlide Topic: Results of Experiment 2: Semantic Structure Preservation with Untrained Networks\nSummary:\n- Untrained networks preserve semantic structure of input vectors.\n- Semantic information naturally preserved in feed-forward nets.\n- Mapped vectors maintain similarity to original inputs.\n- Performance compared in concept similarity tasks.\n- Results shown in Table 2.\n- Supports hypothesis of inherent semantic preservation.\n\nSlide Topic: Discussion on the Implications of the Findings\nSummary:\n- Reveals a largely overlooked phenomenon.\n- Relevant to cross-modal/cross-space applications.\n- Includes retrieval, zero-shot learning, image annotation.\n- Neural net cross-modal mappings analyzed.\n- Mapped vectors resemble input vectors semantically.\n- Challenges assumptions about target vector alignment.\n\nSlide Topic: Advocacy for Semantic-Based Evaluation Criteria\nSummary:\n- Encourage better architectures to bridge modalities/spaces.\n- Advocate for semantic-based criteria in evaluation.\n- Focus on quality of predicted vectors.\n- Propose neighborhood-based measure for evaluation.\n- Shift from purely geometric measures like MSE.\n- Enhance understanding of data semantics in predictions.",
    "Slide Topic: Introduction to End-to-End Differentiable Functions in NLP\nSummary:\n- End-to-end differentiable functions dominate NLP learning.\n- These functions enable gradient-based optimization.\n- Traditional NLP used modular, pipeline processing stages.\n- Pipelines made discrete decisions at each stage.\n- Neural learning is incompatible with hard decision pipelines.\n- Researchers now favor abandoning earlier-stage processing.\n\nSlide Topic: Challenges of Integrating Pipelines with Neural Architectures\nSummary:\n- Pipelines make discrete decisions at each stage.\n- Neural learning struggles with discrete decision-making processes.\n- Researchers often abandon earlier-stage processing in pipelines.\n- Linguistic and domain-specific preprocessing still offers benefits.\n- Pipelines can be integrated as layers in neural architectures.\n- Integration enhances NLP tasks by preserving preprocessing advantages.\n\nSlide Topic: Overview of Existing Solutions for Backpropagation Through Discrete Decisions\nSummary:\n- Reinforcement learning, notably REINFORCE, is a solution.\n- Structured attention (SA) replaces argmax with sampling or marginalization.\n- Not all argmax operations have efficient sampling or marginalization methods.\n- Inspection of intermediate outputs is easier for hard decisions than for posteriors.\n- Straight-through estimator (STE) treats discrete decisions as differentiable.\n- STE ignores constraints on argmax problems, like unique syntactic parents for words.\n\nSlide Topic: Introduction to SPIGOT: Structured Projection of Intermediate Gradients Optimization Technique\nSummary:\n- Introduces SPIGOT for optimizing loss functions.\n- Uses a proxy for gradients respecting argmax constraints.\n- Unlike STE, SPIGOT aims to honor optimization constraints.\n- Applies to any layer expressible as a constrained maximization.\n- Feasible set must be projectable for SPIGOT application.\n- Effective even with approximate maximization and projection.\n\nSlide Topic: Application of SPIGOT in NLP Pipelines\nSummary:\n- SPIGOT uses structured argmax as an intermediate layer.\n- Two architectures: semantic parsing with syntactic parsing in the middle.\n- Another architecture: sentiment analysis with semantic parsing in the middle.\n- Architectures trained with a joint objective.\n- Training data for intermediate and end tasks are distinct.\n- Intermediate task parameters influenced by both training data parts.\n\nSlide Topic: Detailed Explanation of SPIGOT's Mechanism and Implementation\nSummary:\n- Treats argmax layer as differentiable in neural networks.\n- Enables placement of syntactic parsers within network layers.\n- Forward calculation calls parser, passing parse tree to next layer.\n- Backward computation is challenging due to discrete outputs.\n- Argmax function is piecewise constant, with gradient zero or undefined.\n- Introduces gradient proxy for loss function to enable backpropagation.\n\nSlide Topic: Experiments and Results: Syntactic-then-Semantic Parsing\nSummary:\n- Conducted syntactic parsing followed by semantic dependency parsing.\n- Evaluated method using syntactic trees in semantic parsing and sentiment classification.\n- SPIGOT outperforms all baselines in semantic dependency parsing.\n- STE and SPIGOT use hard syntactic features, outperforming PIPELINE.\n- SA only outperforms PIPELINE on out-of-domain PSD test set.\n- SPIGOT shows more than 0.3% absolute labeled F1 improvement over STE on DM.\n\nSlide Topic: Experiments and Results: Semantic Dependencies for Sentiment Classification\nSummary:\n- Experiment uses semantic dependency graphs to enhance sentiment classification.\n- No efficient algorithm exists for margin inference under determinism constraints.\n- Table 2 compares SPIGOT method to three baselines.\n- Pipeline semantic dependency predictions improve accuracy by 0.9%.\n- SPIGOT outperforms all baseline methods.\n- STE performs slightly worse than fixed pre-trained PIPELINE.\n\nSlide Topic: Analysis of Intermediate Model Behavior Under End-Task Training Signal\nSummary:\n- Examines effect of end-task training signal on intermediate model.\n- Uses syntactic-then-semantic parsing model as case study.\n- Compares pipelined system to one jointly trained using SPIGOT.\n- Analyzes instances where both annotations are available.\n- SPIGOT reduces syntactic parsing accuracy to 89.6 from 97.4.\n- Achieves 1.1% absolute gain in labeled F1 for semantic parsing.\n\nSlide Topic: Comparison with Existing Methods and Discussion of SPIGOT's Advantages\nSummary:\n- SPIGOT outperforms STE on DM by over 0.3% labeled F1.\n- Similar performance to STE on in-domain PSD, but 0.5% better on out-of-domain.\n- STE performs slightly worse than a fixed pre-trained pipeline.\n- SPIGOT shows stronger performance than baselines in both settings.\n- Outperforms state-of-the-art systems in semantic dependency parsing.",
    "Slide Topic: Introduction to Discourse-Aware Machine Translation\nSummary:\n- Discourse phenomena crucial for coherent translations.\n- Traditional systems often ignore context, translating sentences in isolation.\n- Recent research shows context-aware models improve translation performance.\n- Focus on understanding discourse phenomena in neural machine translation.\n- Simple discourse-aware model outperforms discourse-agnostic baseline.\n- Demonstrated improvements on English-Russian subtitles dataset.\n\nSlide Topic: Traditional Approaches vs. Context-Aware NMT\nSummary:\n- Traditional research focused on specific discourse phenomena.\n- Special-purpose features were engineered for modeling.\n- Neural Machine Translation (NMT) models use additional context.\n- Context-aware NMT allows for predictive feature induction.\n- Performance improvements are seen with context-aware NMT.\n- Uncertainty remains on which discourse phenomena are effectively modeled.\n\nSlide Topic: Proposed Context-Aware Model Architecture\nSummary:\n- Based on Transformer architecture\n- Handles additional context effectively\n- Source and context sentences encoded separately\n- Single attention layer with gating function used\n- Produces context-aware source sentence representation\n- Enhances accuracy and interpretability\n\nSlide Topic: Experimental Setup and Data\nSummary:\n- Used OpenSubtitles2018 corpus for English and Russian.\n- Applied data cleaning, selected 2 million training instances.\n- Randomly selected 10,000 instances for development and testing.\n- Encoded sentences using byte-pair encoding.\n- Source and target vocabularies consist of about 32,000 tokens.\n- Used parameters and optimizer similar to the original Transformer.\n\nSlide Topic: Overall Performance and Analysis\nSummary:\n- Best model shows 0.7 BLEU improvement over discourse-agnostic model.\n- Concatenation baseline is less accurate than our context-aware model.\n- Our model is more amendable to analysis.\n- Shuffled context sentences lead to significant performance drop.\n- Model relies on context information to enhance translation quality.\n- Demonstrates effectiveness and reliance on contextual data.\n\nSlide Topic: Analysis of Contextual Information Utilization\nSummary:\n- Model focuses on <bos> and <eos> tokens.\n- Attends to words for passing contextual information.\n- Heavily relies on contextual history for pronoun translation.\n- Context is crucial at sentence beginnings.\n- Shorter sentences benefit more from context.\n- Word attention is selective, not constant.\n\nSlide Topic: Impact of Context on Pronoun Translation\nSummary:\n- Model heavily relies on contextual history for pronoun translation.\n- Leads to empirical improvements in translation accuracy.\n- Larger performance gains when antecedent contains a noun.\n- Indicates model's learned attention to context for translation.\n- Contextual history significantly enhances pronoun translation quality.\n\nSlide Topic: Latent Anaphora Resolution through Attention Mechanism\nSummary:\n- Model exploits context for ambiguous pronoun resolution.\n- Attention mechanism acts as latent anaphora resolution.\n- Experiments show attention weights align well with reference system.\n- Alignment is better than heuristic methods.\n- Indicates model performs anaphora resolution effectively.\n- Context-aware approach enhances pronoun antecedent identification.\n\nSlide Topic: Comparison with External Coreference Resolution Systems\nSummary:\n- Neural system implicitly learns coreference without feature engineering.\n- Model agreement with ground truth is 72%.\n- Performance is 5% below dedicated coreference system.\n- Significantly outperforms best heuristic methods.\n- Confirms latent anaphora resolution capability.",
    "Slide Topic: Introduction to Weblio Pre-reordering SMT System\nSummary:\n- Describes Weblio Pre-reordering Statistical Machine Translation (SMT) System\n- Uses pre-reordering method from Zhu et al. (2014)\n- Automatically learns pre-reordering model from word alignments and parse trees\n- Integrates statistical language model for reordering parse tree node layers\n- Applied in English-Japanese translation task\n- Presented in WAT 2014 workshop\n\nSlide Topic: Head-restructured CFG Parse Tree (HRCFG Tree) for Reordering\nSummary:\n- Addresses reordering from SVO to SOV.\n- Captures long-distance word reordering patterns.\n- Injects sentence-level dependencies into CFG parse trees.\n- Maintains local grammatical structures unchanged.\n- Explicitly annotates subject, object, and verb parts.\n- Enhances model's ability to capture reordering patterns.\n\nSlide Topic: Reordering Model Integrated with Language Model\nSummary:\n- Follows Zhu et al. (2014) model structure\n- Integrates language model for optimal node layer order\n- Uses bilingual training data and GIZA++ for word alignments\n- Determines best order for each node layer based on target sentence alignment\n- Reorders non-terminal tags, replacing some with head words\n- Trains language model to estimate tag sequence likelihood in target language\n\nSlide Topic: N-best Reordering Implementation\nSummary:\n- Best sentence order derived from 1-best orders of each node layer.\n- Language model usually provides the best reordering.\n- Translation phase sometimes reveals the best reordering.\n- Example: \"The rocket is launched by NASA\" shows two plausible reorderings.\n- Collect N-best reordering results for all node layers.\n- Rank reorderings by accumulated language model score.\n\nSlide Topic: Experimental Setup and Methodology\nSummary:\n- Used 1-best parse trees for training and testing.\n- Applied Stanford tokenizer and Berkeley parser for CFG parse trees.\n- Generated dependency parse trees using Stanford rules.\n- Constructed HR_CFG trees from both parse tree types.\n- Tokenized Japanese text with KyTea.\n- Trained reordering model on 1.5M bilingual text for English-to-Japanese translation.\n- Set distortion limit to 6 for all experiments.\n\nSlide Topic: Experiment Results and Analysis\nSummary:\n- Experiments combined N-best parse trees and reordering results.\n- Table 2 lists automatic evaluation scores for different system settings.\n- 'N-best parse' systems use 30 highest-scoring parse trees.\n- 'N-best reordering' systems accept 10 highest-scoring reordering results per parse tree.\n- System 4 generated up to 300 reordering results per sentence.\n- Official evaluation scores, including BLEU and RIBES, are detailed in Table 3.\n\nSlide Topic: Evaluation of Pre-reordering Performance\nSummary:\n- Performance evaluated using Kendall's \u03c4 from word alignments.\n- \u03c4 estimated following Isozaki et al. (2010) method.\n- Average \u03c4 for natural order: 0.30; for adjusted order: 0.71.\n- 20.30% aligned words skipped in natural order; 14.06% in pre-ordered.\n- Figure 5 shows distribution of \u03c4, indicating word order improvements.\n- Fully identical word order sentences increased from 1.8% to 15% post-reordering.\n\nSlide Topic: Error Analysis of Pre-reordering\nSummary:\n- Pre-reordering SMT system outperforms baseline systems.\n- Translation still suffers from reordering model defects.\n- Model is a language model built on sequences with non-terminal tags and words.\n- Involving words makes reordering flexible but sparse.\n- Rare or unknown words often lead to incorrect reordering.\n- Errors are widespread in reordering results for ASPEC test corpus.\n\nSlide Topic: Context-aware Machine Translation Efforts\nSummary:\n- Addresses phrase selection in English-Japanese translation.\n- Japanese words often have multiple translations, especially in katakana.\n- Example: 'remote control' translates to 'ENKAKUSEIGYO' or 'RIMOKON'.\n- Translation preference varies by domain, shown in Figure 6.\n- Experimented with interpolating general and domain-specific language models.\n- Best perplexity achieved with 0.5 weight for in-domain model, per Figure 7.\n\nSlide Topic: Conclusion and Future Research Directions\nSummary:\n- Applied reordering model in Weblio Pre-reordering SMT system.\n- Improved automatic evaluation scores with multiple parse tree candidates.\n- Human evaluation showed no improvement with N-best parse trees.\n- Reordering model remains unstable for some simple cases.\n- Interpolating general and in-domain language models improves translation quality.\n- Future work will explore performance limits of pre-reordering models across language pairs.",
    "Slide Topic: Introduction to QA-SRL and Its Challenges\nSummary:\n- QA-SRL predicts sentence structures using question-answer pairs.\n- Method is intuitive for non-experts.\n- Matches traditional predicate-argument annotation schemes closely.\n- High recall is challenging to achieve.\n- Easy to gather answerable questions.\n- Hard to label every possible question for each verb.\n\nSlide Topic: Crowdsourcing QA-SRL Annotations\nSummary:\n- New scalable approach for QA-SRL annotations introduced.\n- Streamlined web interface with autosuggest mechanism.\n- Automatic quality control boosts recall.\n- Validation stage ensures high precision.\n- Produced QA-SRL Bank 2.0 in 9 days.\n- Dataset includes 265,140 question-answer pairs across 3 domains.\n\nSlide Topic: Annotation Pipeline and Interface Design\nSummary:\n- Pipeline includes generation and validation steps.\n- Generation: Worker writes QA-SRL questions for a marked verb.\n- Validation: Multiple workers answer or invalidate questions.\n- Interface includes an autocomplete dropdown for questions.\n- Autocomplete reduces keystrokes for subsequent questions.\n- Enhances efficiency and accuracy in annotation tasks.\n\nSlide Topic: Quality Control and Validation\nSummary:\n- 89.5% of questions rated as valid by validators.\n- Agreement rate on invalid questions is 90.9%.\n- Fleiss's Kappa for initial validation is 0.51.\n- Higher-density re-run primed validators to be more critical.\n- 76.5% of questions considered valid in the re-run.\n- Agreement rate in re-run is 83.7%, Fleiss's Kappa 0.55.\n\nSlide Topic: Models for QA-SRL Parsing\nSummary:\n- Three-step pipeline: predicate identification, span detection, question generation.\n- Predicate identification locates verbal predicates in text.\n- Span detection uses sequence tagging (BIO) and span-based models.\n- Question generation employs local and sequence models.\n- Local model predicts words for each slot independently.\n- Sequence model shares information between slots using RNN.\n\nSlide Topic: Span Detection Models\nSummary:\n- Two models explored: sequence-tagging with BIO encoding\n- Second model: span-based, assigns probability to every span\n- Span-based model outperforms BIO model in precision\n- Span-based model also shows better recall\n- Performance difference is less pronounced under IOU matching\n- Span-based model offers significant overall improvement\n\nSlide Topic: Question Generation Models\nSummary:\n- Two models introduced: local and sequence models.\n- Local model predicts each slot independently.\n- Sequence model uses RNN to share information between slots.\n- Sequential model has significantly higher exact match accuracy.\n- Word-level accuracy is comparable between both models.\n- Local model excels in learning slot-level posteriors.\n\nSlide Topic: Evaluation Metrics and Results\nSummary:\n- Span detection evaluated using modified precision and recall.\n- Question generation accuracy measured by exact match rate.\n- Relaxed match counts WH, SBJ, OBJ, and Misc slots.\n- Sequential model shows significantly higher exact match accuracy.\n- Word-level accuracy is roughly comparable between models.\n\nSlide Topic: Data Expansion and Retraining\nSummary:\n- Low probability cutoff detects more spans.\n- Generates extra QA pairs for annotated sentences.\n- Uses 3 validators in crowdsourcing pipeline.\n- Increased valid questions by 20% in train and dev sets.\n- Retrained model on expanded training set.\n- Modest improvements in span detection and question generation.\n\nSlide Topic: Final Human Evaluation and System Output\nSummary:\n- Final human evaluation via crowdsourced validation\n- Model uses span-based detection and sequential question generation\n- Trained with expanded data for improved accuracy\n- Achieves 82.64% question accuracy\n- Achieves 77.61% span accuracy\n- Generates 2 questions per verb, similar to original annotation",
    "Slide Topic: Introduction to Human-Computer Conversation\nSummary:\n- Critical AI and NLP task\n- Two research streams: task-oriented dialog and general-purpose dialog\n- Task-oriented dialog aids specific task completion\n- General-purpose dialog aims for natural, wide-ranging conversations\n- Task-oriented examples: buying tickets, shopping\n- General-purpose covers open-domain topics\n\nSlide Topic: Challenges in General Purpose Dialog\nSummary:\n- Dialog differs from machine translation.\n- Translation has a 1-to-1 semantic relationship.\n- Dialog allows varied inputs for general responses.\n- \"I don't know\" can respond to many utterances.\n- Seq2Seq models treat all pairs uniformly.\n- Models favor frequent general responses.\n\nSlide Topic: Previous Approaches to Addressing General Responses\nSummary:\n- Few efforts in literature address this issue.\n- Li et al. (2016) proposed using Maximum Mutual Information (MMI) to penalize general responses.\n- Xing et al. (2017) pre-defined a set of topics from an external corpus.\n- Topics from external corpus guide Seq2Seq model generation.\n- Consistency between external and conversation corpus topics is challenging.\n- Inconsistencies introduce additional noise in response generation.\n\nSlide Topic: Proposed Specificity Controlled Seq2Seq Model (SC-Seq2Seq)\nSummary:\n- Novel mechanism for controlled response generation.\n- Handles varying specificity in utterance-response relationships.\n- Employs Seq2Seq framework with explicit specificity control variable.\n- Control variable represents agent's response purpose.\n- Interacts with word usage through Gaussian Kernel layer.\n- Guides model to generate responses at different specificity levels.\n\nSlide Topic: Encoder-Decoder Framework in SC-Seq2Seq\nSummary:\n- Encoder maps input utterance X into a compact vector.\n- Vector captures essential topics of the input.\n- Decoder generates response Y based on input's hidden representations.\n- Response generation considers a specificity level denoted by control variable s.\n- Probability of generating target word y_t is a mixture of semantic and specificity-based probabilities.\n- p(y_t) = \u03b2p_M(y_t) + \u03b3p_S(y_t), where \u03b2 and \u03b3 are coefficients.\n\nSlide Topic: Distant Supervision Techniques for Training SC-Seq2Seq\nSummary:\n- Model trained by maximizing log likelihood of responses.\n- Training set D uses triples (X, Y, s).\n- s is an explicit control variable.\n- Two distant supervision methods introduced.\n- Methods: Normalized Inverse Response Frequency (NIRF).\n- Methods: Normalized Inverse Word Frequency (NIWF).\n\nSlide Topic: Experimental Setup and Baseline Methods\nSummary:\n- Experiments conducted on public STC dataset from NTCIR-13.\n- Proposed SC-Seq2Seq model compared against state-of-the-art baselines.\n- Baseline 1: Seq2Seq-att uses standard Seq2Seq with attention mechanism.\n- Baseline 2: MMI-bidi reorders responses using Maximum Mutual Information.\n- Baseline 3: MARM models latent responding mechanisms probabilistically.\n- Baseline 4: Seq2Seq+IDF extends Seq2Seq-att by optimizing specificity.\n\nSlide Topic: Evaluation Metrics and Results\nSummary:\n- Both automatic and human evaluations are used.\n- Distinct-1 & Distinct-2 measure unique unigrams and bigrams.\n- BLEU correlates strongly with human evaluations.\n- BLEU-n assesses n-gram precision against reference sentences.\n- Average & Extrema project responses into vectors for similarity.\n- Cosine similarity is computed between generated and ground truth vectors.\n\nSlide Topic: Human Evaluation and Case Studies\nSummary:\n- SC-Seq2Seq NIWF, s=1 generates most informative and interesting responses.\n- SC-Seq2Seq NIWF, s=0 produces the most general responses.\n- MARM generates the poorest quality responses.\n- MARM highlights drawbacks of unknown latent responding mechanisms.\n- Kappa values for our models exceed 0.4, indicating moderate agreement.\n- Table 4 details human evaluation results.\n\nSlide Topic: Analysis of Usage Representations\nSummary:\n- Analyzed word usage representations in SC-Seq2Seq NIWF model.\n- Randomly sampled 500 words for visualization.\n- Applied t-SNE to visualize usage and semantic embeddings.\n- Distributions in usage and semantic spaces are significantly different.\n- '\u8102\u80aa\u809d' and '\u4e45\u5750' are close in usage space, far from '\u80d6'.\n- In semantic space, '\u8102\u80aa\u809d' is close to '\u80d6', far from '\u4e45\u5750'.",
    "Slide Topic: Introduction to Neural Machine Translation (NMT) and Coverage Issues\nSummary:\n- NMT emerged as a successful translation paradigm.\n- More fluent than previous statistical systems.\n- Adequacy remains a major concern.\n- Common mistake: dropping source words.\n- Common mistake: repeating words in translation.\n- Coverage issues impact translation quality.\n\nSlide Topic: Previous Approaches to Mitigate Coverage Problems in NMT\nSummary:\n- Wu et al. (2016) used coverage and length penalties during beam search.\n- Penalties affect only existing translation hypotheses in the beam.\n- Architectural changes include coverage vectors to track attention history.\n- Gating architectures control the amount of source context provided.\n- Adaptive attention mechanisms adjust source context dynamically.\n- Reconstruction loss is added to improve model performance.\n\nSlide Topic: Proposed Solution: Constrained Sparsemax Transformation\nSummary:\n- Maintains overall architecture, modifies attention transformation.\n- Replaces traditional softmax with sparsity-promoting transformations.\n- Bounds attention amount per word using source word fertility values.\n- Combines sparsity and bounded attention concepts.\n- Introduces constrained sparsemax for attention weights.\n- Produces compact and interpretable alignments.\n\nSlide Topic: Sparsemax Transformation Details\nSummary:\n- Sparsemax is the Euclidean projection of scores onto the probability simplex.\n- Projection hits simplex boundary, creating sparse probability distribution.\n- Allows decoder to focus on few words, assigning zero probability to others.\n- Sparsemax evaluated in O(J) time.\n- Gradient backpropagation takes sublinear time.\n- Introduced by Martins and Astudillo in 2016.\n\nSlide Topic: Constrained Softmax Transformation Details\nSummary:\n- Proposed by Martins and Kreutzer (2017) for easy-first sequence tagging.\n- Returns distribution closest to softmax(z) with bounded attention probabilities by u.\n- Utilizes the concept of fertility in attention mechanism.\n- Each source word j gets a credit of fj units of attention.\n- Attention credits are consumed during the decoding process.\n- Once credit is exhausted, the word receives zero attention thereafter.\n\nSlide Topic: Constrained Sparsemax Transformation: Definition and Properties\nSummary:\n- Novel transformation offers sparse and bounded probabilities.\n- Defined as solution to a constrained optimization problem.\n- Minimizes Euclidean distance between scores and probability distribution.\n- Subject to constraints that distribution is bounded by fertility values.\n- Solution form: \u03b1_j = max{0, min{u_j, z_j \u2212 \u03c4}}.\n- \u03c4 acts as a normalization constant.\n\nSlide Topic: Fertility Bounds and Implementation Details\nSummary:\n- Three fertility setting methods: CONSTANT, GUIDED, PREDICTED\n- CONSTANT sets all source word fertilities to a fixed integer f\n- GUIDED trains a word aligner based on IBM Model 2\n- GUIDED sets fertilities to maximal observed values in training data\n- At test time, uses expected fertilities according to the model\n- Appends an additional <SINK> token with unbounded fertility\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Evaluated on DE-EN, JA-EN, and RO-EN language pairs\n- Training sets contain 153,326, 329,882, and 560,767 parallel sentences\n- Data tokenized using Moses scripts\n- Preprocessed with subword units, joint vocabulary, 32k merge operations\n- Implementation based on OpenNMT-py toolkit with default parameters\n\nSlide Topic: Evaluation Metrics and Results\nSummary:\n- Tokenized BLEU and METEOR used for evaluation.\n- Introduced new metrics: REP-score and DROP-score.\n- Sparse models show higher BLEU and METEOR scores.\n- Sparse models also perform better on REP and DROP scores.\n- Sparse attention improves coverage to some extent.\n- Sparse models outperform soft models in all metrics.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Proposed new approach to address coverage problem in NMT\n- Replaced soft max attentional transformation with sparse and constrained alternatives\n- Introduced sparsemax, constrained softmax, and new constrained sparsemax\n- Incorporated model for fertility prediction\n- Achieved sparse alignments, avoiding repeated words in translation\n- Future work includes further optimization and testing on diverse datasets",
    "Slide Topic: Introduction to Multilingual Corpora and Their Applications\nSummary:\n- Multilingual corpora support various NLP tasks.\n- Used for extracting parallel lexicons.\n- Feed statistical machine translation systems.\n- Enhance multilingual retrieval models.\n- Domain-specific corpora improve coverage in SMT.\n- Wikipedia serves as a multilingual data source.\n\nSlide Topic: Challenges in Extracting Comparable Corpora from Wikipedia\nSummary:\n- Extracting specific domain corpora from Wikipedia is complex.\n- Wikipedia categories form a densely connected, overlapping graph.\n- Many articles are misclassified or lack appropriate categorization.\n- Over-categorization of articles further complicates the process.\n- Cycles and redundant categories add to the extraction difficulty.\n- Complexity hinders the creation of clean, comparable corpora.\n\nSlide Topic: Proposed Platform for Extracting Comparable Corpora from Wikipedia\nSummary:\n- Platform extracts comparable corpora from Wikipedia.\n- Input is top category of the domain (e.g., Sport).\n- Defines domain vocabulary using Wikipedia.\n- Performs breadth-first search on category graph.\n- Identifies categories likely belonging to the domain.\n- Ensures simple and effective extraction process.\n\nSlide Topic: Graph Exploration Strategy for Domain-Specific Corpora Extraction\nSummary:\n- Starts from top category of a domain\n- Uses breadth-first search on Wikipedia category graph\n- Avoids loops and re-traversing paths\n- Sets stopping criterion to limit exploration\n- Scores explored categories for domain relevance\n- Stops when scores indicate domain-specific corpora extracted\n\nSlide Topic: Parallel Sentence Extraction from Comparable Corpora\nSummary:\n- Articles linked by interlanguage are analyzed.\n- Cross-language sentence pairs' similarities are estimated.\n- Various text similarity measures are applied.\n- Measure quality impacts corpus purity.\n- Purity affects translator quality.\n- Cosine and length factor measures compute similarities.\n\nSlide Topic: Evaluation of Extracted Corpora in Statistical Machine Translation\nSummary:\n- Quality of obtained corpora impacts SMT performance.\n- Trained SMT systems using Wikipedia and Europarl.\n- Systems translated both domain-specific and out-of-domain texts.\n- Wikipedia translators outperformed baseline with EP.\n- Useful SMT corpora can be extracted with F1 < 0.5.\n- Automated extraction enhances SMT system training.\n\nSlide Topic: Impact of Domain-Specific Corpora on Translation Quality\nSummary:\n- Domain-specific training improves translation quality.\n- Wikipedia and Europarl corpora show significant enhancements.\n- Combined system doubles BLEU score across domains.\n- Improvement reaches up to 30 points in BLEU score.\n- Even low F1 corpora (F1 < 0.5) are beneficial.\n- Confirming utility in Statistical Machine Translation (SMT).\n\nSlide Topic: Comparison of Different Similarity Measures for Parallel Sentence Extraction\nSummary:\n- Character 3-grams yield best performance.\n- Performance comparable to monolingual methods.\n- F1 score of 0.36 achieved with character 3-grams.\n- Other measures include pseudo-cognates, word 1-grams, and length factor.\n- Translator not mandatory for sentence selection.\n- Character 3-grams outperform other similarity measures.\n\nSlide Topic: Conclusions and Future Work on Improving Parallel Sentence Identification\nSummary:\n- Model extracts in-domain comparable corpora from Wikipedia.\n- Extracted corpus useful for multiple NLP tasks.\n- Current model focuses on parallel sentence identification.\n- Aims to develop a more robust identification model.\n- Designing indirect evaluation schemes for model validation.\n- Future work includes enhancing model performance.",
    "Slide Topic: Introduction to Bilingual Lexicon Induction\nSummary:\n- New methods for low-resource language pairs.\n- Limited sentence-aligned parallel data available.\n- Parallel data creation is costly.\n- Methods utilizing readily available data are preferred.\n- Advances in unsupervised lexicon induction show promise.\n- Beneficial for low-resource contexts.\n\nSlide Topic: Challenges and Advancements in Low-Resource Language Pairs\nSummary:\n- Parallel data creation is costly.\n- Methods using less data are preferred.\n- Large seed dictionaries are often required.\n- Recent work aims to reduce this requirement.\n- Document-aligned data is utilized for training.\n- Iterative self-learning uses small, auto-generated numeral lexicons.\n\nSlide Topic: Embedding-Based Methods for Bilingual Lexicon Induction\nSummary:\n- Use multilingual word embeddings in a shared latent space.\n- Each language's vocabulary is embedded in the same space.\n- Initial seed lexicon created from identical numerals.\n- Seed lexicon is small and automatically generated.\n- Iterative self-learning framework refines the seed lexicon.\n- Linear mapping learned between monolingual embedding spaces.\n\nSlide Topic: Incorporating Linguistic Features: Orthographic Information\nSummary:\n- Identifies words with common ancestry.\n- Detects borrowed words across languages.\n- Enhances performance in earlier models.\n- Particularly effective for related languages.\n- Not yet applied to modern methods.\n- Improves accuracy through orthographic analysis.\n\nSlide Topic: Orthographic Extension of Word Embeddings\nSummary:\n- Augments word embeddings for both languages.\n- Uses self-learning framework for enhanced embeddings.\n- Appends vector to each word's embedding.\n- Vector length equals union of both languages' alphabets.\n- Each vector position corresponds to a letter.\n- Position value is the letter count in the word.\n\nSlide Topic: Orthographic Similarity Adjustment\nSummary:\n- Adjusts word pair similarity scores during dictionary induction.\n- Uses dot product of word embeddings for similarity quantification.\n- Incorporates orthographic similarity measure into the score.\n- Orthographic similarity based on normalized string edit distance.\n- Enhances self-learning framework's accuracy in similarity assessment.\n- Improves dictionary induction by considering spelling similarities.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Uses datasets from Artetxe et al. (2017)\n- Includes English-Italian, English-German, English-Finnish pairs\n- Each dataset has monolingual word embeddings for both languages\n- Contains a bilingual dictionary split into training and test sets\n- Training set not used as input dictionary\n- Uses an automatically-generated dictionary of numeral identity translations\n\nSlide Topic: Results and Discussion\nSummary:\n- Orthographic extensions of word embeddings were used.\n- Embedding size extended by alphabet union size.\n- English-Italian union size: 199.\n- English-German union size: 200.\n- English-Finnish union size: 287.\n- Compared methods against Arteutex et al. (2017) using scaling factors.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Presented two techniques for improving embedding-based bilingual lexicon induction.\n- Techniques utilize orthographic information without parallel data.\n- Methods enhance accuracy for low-resource language pairs.\n- Combined and embedding extension methods show significant gains.\n- Future work aims to apply techniques to languages with different alphabets.\n- Plan to extend other unsupervised bilingual lexicon induction systems.",
    "Slide Topic: Introduction to the Scientific Publication Lifecycle and Research Challenges\nSummary:\n- Scientific publication lifecycle involves multiple researcher activities.\n- Key activities include literature review, research work, and result dissemination.\n- Dissemination occurs through conferences and journals.\n- Each activity requires specific expertise and experience.\n- Less experienced researchers face difficulties in completing research activities.\n- Assistance from supervisors, experts, and librarians is crucial for skill development.\n\nSlide Topic: Overview of Academic Assistive Systems and Their Role in Research\nSummary:\n- Alleviate expertise gap between experts and novices in research.\n- Include search systems with faceted user interfaces.\n- Utilize bibliometric tools for citation network visualization.\n- Feature scientific paper recommendation systems.\n- Recommend articles for citation contexts in manuscripts.\n- Suggest suitable conference venues for research publication.\n\nSlide Topic: Rec4LRW System: A Tool for Literature Review and Manuscript Preparation\nSummary:\n- Supports literature review and manuscript preparation tasks.\n- Builds initial reading list of research papers.\n- Finds similar papers based on selected set.\n- Shortlists papers for manuscript inclusion.\n- Uses seven features/criteria for recommendation.\n- Enhances efficiency in research and writing processes.\n\nSlide Topic: Dataset and User Interface Features of Rec4LRW\nSummary:\n- Dataset sourced from ACM Digital Library (ACM DL) snapshot.\n- Includes papers from 1951 to 2011, covering proceedings and journals.\n- Sample set comprises 103,739 articles and 2,320,345 references.\n- Papers selected based on full text and metadata availability.\n- UI highlights include author-specified keywords and reference count.\n- System provides paper summary if abstract is missing.\n\nSlide Topic: Technique for Shortlisting Papers Based on Article-Type Preference\nSummary:\n- Aids researchers in identifying impactful papers.\n- Considers citation counts and uniqueness for shortlisting.\n- Uses Girvan-Newman algorithm for clustering citation networks.\n- Identifies communities within citation networks.\n- Top cited papers from clusters are shortlisted.\n- Algorithm implemented as Edge Betweenness Clustering in JUNG library.\n\nSlide Topic: User Evaluation Study Design and Participants\nSummary:\n- Online pre-screening identified potential participants.\n- Participants must have experience writing conference or journal papers.\n- Study period: November 2015 to January 2016.\n- 116 out of 230 eligible participants completed the study.\n- 57 participants were PhD/Masters students.\n- 59 participants were research, academic staff, and librarians.\n\nSlide Topic: Agreement Percentages and Quality Measures from User Evaluation\nSummary:\n- Figure 2 shows AP for seven measures by participant groups.\n- AP above 75% indicates higher agreement.\n- Students show consistently higher AP than staff.\n- Highest AP difference: Usefulness (82.00% for students, 64.15% for staff).\n- Quality measures with highest AP: Importance (85.96% for students, 77.97% for staff).\n- Shortlisting_Feature also shows high AP (84.21% for students, 74.58% for staff).\n\nSlide Topic: Qualitative Feedback Analysis: Preferred and Critical Aspects\nSummary:\n- 68 participants provided feedback on preferred aspects.\n- 24% preferred paper shortlisting based on article type.\n- Participants found the quality of shortlisted papers satisfactory.\n- 41 participants gave critical feedback.\n- 16% found the procedure of adding 30 papers uninteresting.\n- Total participants: 116.\n\nSlide Topic: Conclusion and Future Directions for Rec4LRW System\nSummary:\n- Novices and experts show a significant gap in task knowledge and skills for literature review.\n- Rec4LRW aims to bridge this gap by shortlisting articles for manuscript inclusion.\n- Uses community detection algorithm on citation networks to identify key paper communities.\n- Varies shortlisted paper count based on article type.\n- Students evaluated the task positively across all measures.\n- High agreement on the availability of important papers among shortlisted ones.",
    "Slide Topic: Introduction to Copy & Paste in NLP Archives\nSummary:\n- Copy & paste revolutionized document creation.\n- Facilitates vast NLP archives for study.\n- Enables analysis of written and spoken materials.\n- Highlights reuse and plagiarism levels in NLP.\n- Archives provide insights into historical and current practices.\n\nSlide Topic: Context and Previous Studies on Reuse and Plagiarism\nSummary:\n- Previous studies initiated at ACL's 50th anniversary workshop.\n- Gupta and Ross focused on text reuse and plagiarism.\n- Human judges struggle to detect sources without prior knowledge.\n- Automatic plagiarism detection aids in identifying potential sources.\n- Annual PAN competition advances plagiarism detection research.\n- Study on reuse and plagiarism in LREC papers from 1998 to 2014 conducted.\n\nSlide Topic: Objectives of the Study\nSummary:\n- Not to present state-of-the-art or compare metrics.\n- Focus on extrinsic detection for near-matches between texts.\n- Contrast with intrinsic detection of single-author text inconsistencies.\n- Primary goal: Analyze entry-level detection methods.\n- Investigate meaningful differences between raw strings and linguistic parsing.\n- Secondary goal: Study practices specific to our field.\n\nSlide Topic: The NLP4NLP Corpus and Its Preprocessing\nSummary:\n- Contains 65,003 documents from NLP conferences and journals.\n- Documents span 50 years, from 1965 to 2015.\n- Metadata and content are formatted in BibTeX and PDF.\n- Text extraction uses PDFBox and Tesseract OCR for image PDFs.\n- Quality filters ensure content is at least 900 characters and 91% in-vocabulary.\n- Corpus includes English and French texts, with minor German and Russian content.\n\nSlide Topic: Definitions of Reuse and Plagiarism\nSummary:\n- Define terms to avoid misunderstandings.\n- Self-reuse: Copy & paste with citation from own work.\n- Self-plagiarism: Copy & paste without citation from own work.\n- Reuse: Copy & paste with citation from others' work.\n- Plagiarism: Copy & paste without citation from others' work.\n- Self-plagiarism term used despite seeming contradiction.\n\nSlide Topic: Algorithm for Detection of Copy & Paste\nSummary:\n- Algorithm compares word sequences for copy detection.\n- Builds sliding windows of 5-7 lemmas for source corpus.\n- Records windows as character string keys in local document index.\n- Constructs 'focus index' by gathering all local indexes.\n- Filters and records only focus index windows in non-source documents.\n- Computes similarity score using Jaccard distance for document pairs.\n\nSlide Topic: Algorithm Comparison: Raw Text vs. Linguistic Parsing\nSummary:\n- Raw text comparison fails due to PDF formatting issues.\n- Linguistic parsing accounts for variations like hyphens, case, and abbreviations.\n- Entity linking normalizes names and abbreviations for consistent comparison.\n- Only sentences with verbs are considered post-processing.\n- Linguistic parsing yields 158 more pairs than raw text comparison.\n- Parsing is computationally longer but more accurate, reducing silent system errors.\n\nSlide Topic: Tuning Parameters for the Detection Algorithm\nSummary:\n- Three parameters tuned: window size, distance function, and threshold.\n- No gold standard available; relied on PAN contest articles for initial parameters.\n- Selected window size of seven tokens to reduce common sequence overload.\n- Used the widely adopted Jaccard distance for computation.\n- Set threshold at 0.04 to reduce noise while detecting meaningful paper pairs.\n- Added a secondary threshold of 50 shared windows to address OCR and short content issues.\n\nSlide Topic: Results and Analysis of Self-Reuse and Self-Plagiarism\nSummary:\n- 12,493 documents (18%) show self-reuse or self-plagiarism.\n- 61% of cases involve no citation of the source paper.\n- 205 papers have identical titles; 130 have same titles and author lists.\n- Major sources are ISCA, IEEE-ICASSP, ACL, and journals like TASLP.\n- Self-reuse flows predominantly from conferences to journals.\n- Similarity scores range from 4% to 97%, with 4,500 couples having scores \u226510%.\n\nSlide Topic: Results and Analysis of Reuse and Plagiarism\nSummary:\n- 261 papers (0.4% of total) reused fragments from other authors.\n- 60% of reuse cases (156 papers) were potential plagiarism, 0.23% of total.\n- Manual checking revealed 12 cases of self-reuse and 25 of self-plagiarism.\n- 107 reuse cases and 117 potential plagiarism cases studied closely.\n- Most cases explained by shared references, lab collaboration, or project overlap.\n- Only 3 cases of possible plagiarism remained unexplained, with similarity scores up to 42%.",
    "Slide Topic: Introduction to Mental State Tracking in Stories\nSummary:\n- Understanding stories involves reasoning about causal links and character mental states.\n- These relationships are often implicit, making them hard for machine readers.\n- Humans trivially infer mental states and causal links in stories.\n- Machine readers, both statistical and neural, struggle with this reasoning.\n- New annotation formalism is introduced to label mental states in stories.\n- Focuses on dense labeling of common sense short stories for better analysis.\n\nSlide Topic: Challenges in Understanding Commonsense Stories\nSummary:\n- First study to annotate full mental state effects in stories.\n- Requires interpreting discourse.\n- Involves understanding implicit causal effects.\n- Needs comprehension of formal psychology theory categories.\n- Poses unique challenges in annotation.\n\nSlide Topic: Annotation Formalism for Mental States\nSummary:\n- Provides full chain of motivations and emotional reactions for each character.\n- Includes pre- and post-event conditions for all story events.\n- Captures state changes for entities, even if not directly mentioned.\n- Accounts for implied effects not stated in the story.\n- Uses formal labels from multiple psychological theories.\n- Includes open text descriptions for comprehensive mapping.\n\nSlide Topic: Theoretical Foundations: Motivation and Emotion Theories\nSummary:\n- Derive labels for annotating causal dynamics in story characters.\n- Use Maslow's hierarchy of needs and Reiss's basic motives for motivation.\n- Compile 5 coarse-grained and 19 fine-grained motivation categories.\n- Employ Plutchik's wheel of emotions for emotional categorization.\n- Plutchik's theory is widely used in emotion classification literature.\n\nSlide Topic: Annotation Framework and Pipeline\nSummary:\n- Introduces new annotation framework with a pipeline structure.\n- Pipeline consists of smaller, isolated tasks.\n- Tasks include entity, action, and affect resolution.\n- Also includes motivation and emotional reaction annotation.\n- Designed for completion by crowdsourced workers.\n- Workers sourced from Amazon Mechanical Turk.\n\nSlide Topic: Dataset Statistics and Insights\nSummary:\n- 300k low-level annotations for motivation and emotion\n- Covers 15,000 stories\n- Includes over 150,000 character-line pairs\n- 56k pairs annotated for motivation\n- 105k pairs annotated for emotion change\n- Split into training, development, and test sets\n\nSlide Topic: New Tasks and Baseline Models\nSummary:\n- Dataset supports multiple new tasks through diverse annotations.\n- Three task types outlined: state classification, annotation classification, explanation generation.\n- Eight tasks total for evaluation purposes.\n- State classification assesses condition identification.\n- Annotation classification evaluates label prediction accuracy.\n- Explanation generation focuses on creating descriptive outputs.\n- Baseline models include word embedding encoders, neural networks, and memory networks.\n\nSlide Topic: Experimental Setup and Results\nSummary:\n- Tested models on Maslow, Reiss, and Plutchik states categorization.\n- All models outperformed random baseline despite task difficulty.\n- Best models effectively predict Maslow's physiological needs.\n- Models excel in identifying Reiss's food motives.\n- Top performers accurately forecast Plutchik's joy reactions.\n- Results indicate models' capability in complex emotional state prediction.\n\nSlide Topic: Impact of Context and Pretraining on Model Performance\nSummary:\n- Entity-specific context boosts model performance.\n- Models learn to condition on a character's past experiences.\n- Past experiences aid in current mental state classification.\n- Pretraining encoder parameters improves performance across tasks.\n- Free response annotations from training set enhance pretraining.\n- Performance boost observed for all models and prediction tasks.\n\nSlide Topic: Related Work and Future Directions\nSummary:\n- Previous projects explored incorporating emotion theories into NLP tasks.\n- Our work introduces a new dataset for tracking emotional reactions and motivations in stories.\n- Dataset focuses on character emotional responses and motivations.\n- Seeks to enhance common sense understanding of emotional reactions.\n- Aims to serve as a future test bed for evaluating models.\n- Supports assessment of models trained on various resources.",
    "Slide Topic: Introduction to Similarity Search and Semantic Hashing\nSummary:\n- Similarity search finds nearest neighbors in large datasets.\n- Also known as nearest-neighbor search, it identifies most similar items.\n- Semantic hashing accelerates similarity search through binary codes.\n- Each item is represented by a discrete, similarity-preserving hash code.\n- Hamming distances between codes measure similarity between items.\n- Efficient evaluation of similarity reduces search time significantly.\n\nSlide Topic: Challenges in Existing Text Hashing Approaches\nSummary:\n- Two-stage training leads to suboptimal local optima.\n- Modules are optimized separately, reducing overall effectiveness.\n- Binary constraints are handled inadequately by truncation.\n- Truncation causes loss of information from continuous representations.\n- Separate binarization process further degrades information integrity.\n\nSlide Topic: Proposed Neural Variational Inference Framework for Text Hashing\nSummary:\n- Framework learns binary latent codes for documents end-to-end.\n- Represents hashing codes as deterministic or stochastic Bernoulli variables.\n- Optimizes inference and generative networks jointly.\n- Uses variational lower bound for optimization.\n- Maximizes marginal distribution of input documents.\n- Enhances text hashing through neural variational inference.\n\nSlide Topic: Training with Binary Latent Variables\nSummary:\n- Maximize variational lower bound for encoder and decoder parameters.\n- Approximate posterior distribution aligns with multivariate Bernoulli prior.\n- Use straight-through (ST) estimator for binary latent variables' gradients.\n- First loss term backpropagates into encoder network.\n- Fine-tune hash function through gradient backpropagation.\n- Enhance model's ability to generate binary latent representations.\n\nSlide Topic: Injecting Data-dependent Noise to Latent Variables\nSummary:\n- Inspired by rate-distortion theory\n- Proposes data-dependent noise injection in latent codes\n- Occurs during the decoding stage\n- Adapts to balance rate and distortion\n- Minimizes bits used and reconstruction error\n- Enhances retrieval performance\n\nSlide Topic: Supervised Hashing Extension of NASH\nSummary:\n- NASH extends to supervised hashing.\n- Learns mapping from z to labels y.\n- Employs joint training objective.\n- Balances variational lower bound.\n- Minimizes discriminative loss.\n- Enhances semantic hashing accuracy.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Three datasets used: Reuters 21578, 20 Newsgroups, TMC\n- Model trained with Adam optimizer, learning rate 1 \u00d7 10\u207b\u00b3\n- Dropout applied to encoder network output\n- Training datasets are publicly available\n- Latent dimension z tested at 8, 16, 32, 64, 128\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- NASH models outperform baselines on all datasets.\n- Models effectively assign similar hashing codes to documents with the same label.\n- Decoder network noise injection enhances robustness.\n- Data-dependent noise yields even better results.\n- Improved binary representations are more resilient.\n\nSlide Topic: Ablation Study and Effect of Stochastic Sampling\nSummary:\n- Stochastic sampling enhances learned representations.\n- Ablation study confirms the benefit of stochastic sampling.\n- Increased non-linearity in the encoder improves results.\n- Non-linear encoder networks yield better empirical outcomes.\n- Non-linear decoder networks may produce less meaningful binary representations.\n- Decoder non-linearity can negatively affect binary representation quality.\n\nSlide Topic: Qualitative Analysis of Learned Semantic Information\nSummary:\n- Learned matrix E groups semantically similar words.\n- NASH model demonstrates effective semantic abstraction.\n- Framework captures useful semantic information.\n- Achieves abstraction under binary/discrete constraints.\n- Shows capability in organizing related concepts together.",
    "Slide Topic: Introduction to Deep Neural Networks and Domain Adaptation Challenges\nSummary:\n- DNNs excel in supervised NLP tasks with labeled data.\n- Challenges arise when learning from unlabeled data under domain shifts.\n- Real-world applications often face distribution differences between training and test data.\n- State-of-the-art domain adaptation methods often exploit task-specific characteristics.\n- General approaches risk overfitting due to evaluation on limited datasets.\n- Need for more generalized and widely applicable domain adaptation techniques.\n\nSlide Topic: Evaluation of Traditional Bootstrapping Algorithms in Neural NLP Models\nSummary:\n- Re-evaluates bootstrapping algorithms in DNN context.\n- Algorithms treat models as black boxes, easily adaptable.\n- Focuses on self-training, tri-training, and tri-training with disagreement.\n- Tests on sequence prediction and classification tasks.\n- Includes POS tagging and sentiment analysis for evaluation.\n- Assesses performance across different NLP task characteristics.\n\nSlide Topic: Self-Training Method and Its Variants for Neural Networks\nSummary:\n- Self-training uses model's predictions on unlabeled data.\n- Focuses on most confident predictions for training.\n- Introduced in 1995 by Yarowsky as a bootstrapping approach.\n- Success varies; errors can be amplified over time.\n- Errors are worsened under domain shift conditions.\n- Novel multi-task tri-training method introduced for improvement.\n\nSlide Topic: Tri-Training and Its Variants for Domain Adaptation\nSummary:\n- Tri-training reduces prediction bias using three independently trained models.\n- Models agree on predictions for unlabeled data to improve accuracy.\n- Tri-training with disagreement focuses on models' weak points.\n- Labeled data is not skewed by easy data points in this variant.\n- This variant is more data-efficient than traditional tri-training.\n- Achieves competitive results in part-of-speech tagging tasks.\n\nSlide Topic: Multi-Task Tri-Training: A Novel Approach to Reduce Complexity\nSummary:\n- Proposes Multi-Task Tri-Training (MT-Tri) to reduce time and space complexity.\n- Leverages multi-task learning (MTL) insights for knowledge sharing.\n- Shares model parameters and trains models jointly.\n- Softmax layers remain model-specific, updating only for respective model inputs.\n- Reduces need for separate storage and training of each model.\n- Demonstrated in Figure 1 for POS tagging application.\n\nSlide Topic: Experimental Setup and Datasets for POS Tagging and Sentiment Analysis\nSummary:\n- Evaluate methods across different domains using two unsupervised datasets.\n- Use datasets for sequence labeling and classification tasks.\n- Sentiment analysis evaluated on Amazon reviews dataset.\n- POS tagging uses SANCL 2012 shared task dataset.\n- See Table 1 for data statistics.\n- Aim to identify robust methods across domains.\n\nSlide Topic: Results and Analysis of Sentiment Analysis Experiments\nSummary:\n- 12 domain adaptation scenarios analyzed.\n- Tri-training shows best average results on two target domains.\n- Tri-training outperforms state-of-the-art methods on average.\n- MT-Tri surpasses state-of-the-art on 3/4 domains.\n- MT-Tri is the overall best method.\n- Improvement is mainly due to B->E and D->E scenarios.\n\nSlide Topic: Results and Analysis of POS Tagging Experiments\nSummary:\n- Low-data setup results in Table 3.\n- Tri-training with disagreement tops at 89.70% accuracy.\n- Classic tri-training closely follows in low-data regime.\n- Baseline outperformed on 4/5 domains in low-data.\n- High-data setup: classic tri-training is best (Table 4).\n- Traditional tri-training complements word embedding initialization.\n\nSlide Topic: Comparison with State-of-the-Art Methods and Baselines\nSummary:\n- Compared to top literature results.\n- Classic tri-training outperforms task-agnostic state-of-the-art methods.\n- Tri-training and MT-Tri model outperform DANN in sentiment analysis.\n- Classic tri-training excels in POS tagging.\n- Superior performance on OOVs and low-frequency tokens noted.\n- Indicates lower susceptibility to error propagation.\n\nSlide Topic: Conclusions and Future Work on Bootstrapping Algorithms in NLP\nSummary:\n- Traditional tri-training outperforms recent methods in semi-supervised learning under domain shift.\n- Tri-training excels but suffers from high time and space complexity.\n- Proposed multi-task tri-training model improves efficiency and performance.\n- New model outperforms alternatives in sentiment analysis tasks.\n- Emphasize comparing neural approaches with strong baselines.\n- Reporting results across multiple runs is crucial for reliability.",
    "Slide Topic: Introduction to Cross-Lingual Transfer Learning\nSummary:\n- Transfers knowledge from data-rich to data-poor languages.\n- Compensates for lack of training data in low-resource languages.\n- Most methods use a single source language for knowledge transfer.\n- Limits transferable knowledge to only one source language.\n- Existing approaches do not adequately account for transfer quality.\n- Languages' contributions are typically weighted uniformly.\n\nSlide Topic: Challenges in Cross-Lingual Transfer Learning\nSummary:\n- Cross-lingual transfer learning faces significant hurdles.\n- Performance drops with a large number of source languages.\n- Distantly related target languages pose additional challenges.\n- Uniform ensembling methods are ineffective in NER tasks.\n- Tested across a diverse set of 41 languages.\n- Customized approaches are needed for better outcomes.\n\nSlide Topic: Proposed Zero-Shot Multilingual Transfer Method (BEA uns)\nSummary:\n- Inspired by truth inference in crowdsourcing\n- Adapts ideas to multilingual transfer setting\n- Learns quality of transfer and language-specific errors\n- Infers best labeling in target language\n- Utilizes Bayesian graphical model\n- Aims for zero-shot multilingual transfer capability\n\nSlide Topic: Handling Few-Shot Transfer with RaRe\nSummary:\n- Proposes RaRe for exploiting limited annotations.\n- Ranks systems and uses top models' outputs.\n- Combines model outputs with gold data for retraining.\n- Implements process using mini-batch scheduling.\n- Randomly samples labels from transfer model with probability \u03c9.\n- Fine-tunes model with small supervised dataset for accuracy.\n\nSlide Topic: Experimental Setup and Data\nSummary:\n- Evaluated on a subset of the Wikiann NER corpus.\n- Used 41 out of 282 languages.\n- Languages chosen based on overlap with multilingual word embedding resources.\n- Employed fastText 300-dimensional Wikipedia embeddings for language-independent features.\n- Mapped embeddings to English space using character-identical words as seeds.\n- Applied Procrustes rotation method for learning bilingual embedding spaces from MUSE.\n\nSlide Topic: Results and Analysis of Direct Transfer\nSummary:\n- Direct transfer using a single model often fails.\n- Oracle choice of source language model performs much better.\n- Best source language is not always closely related.\n- Script role surpasses language family importance.\n- Direct transfer works well between Slavic languages with same alphabet.\n- Alphabet mismatch significantly hinders direct transfer effectiveness.\n\nSlide Topic: Impact of Source Language Choice on Transfer Performance\nSummary:\n- Model performance varies with source language choice and quantity.\n- MV ent, BEA ent sup, and RaRe use small labeled datasets for ranking.\n- BEA ent uns, oracle accesses perfect ranking based on real F1 scores.\n- BEA uns \u00d7 2 ranks models using its own estimates, fully unsupervised.\n- Supervised models rely on labeled data for source model evaluation.\n- Unsupervised models estimate performance without labeled data guidance.\n\nSlide Topic: Comparison with Related Work\nSummary:\n- Two approaches: representation and annotation projection.\n- Representation projection learns transferable models in high-resource languages.\n- Annotation projection applied to POS tagging.\n- Multi-source transfer improves over single-source transfer.\n- Multi-source transfer can introduce noise.\n- Noise results from linguistic differences between languages.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Proposed two new multilingual transfer models: RaRe and BEA.\n- BEA unsupervised method simplifies data annotation in target language.\n- BEA handles noisy annotations and outperforms competitive baselines.\n- Light supervision further enhances performance.\n- RaRe, based on ranking transfer and retraining, shows consistent improvements.\n- Future work will explore deeper integration of supervised and unsupervised techniques.",
    "Slide Topic: Introduction to the Problem and Model\nSummary:\n- Agent faces challenges in executing instructions.\n- Must ground language to observed environment.\n- Requires reasoning about discourse dependencies.\n- Needs to generate actions for high-level goals.\n- Example: moving and mixing chemicals in beakers.\n- Model maps sequences of instructions to actions.\n\nSlide Topic: Challenges in Executing Sequences of Instructions\nSummary:\n- Contextual cues from interaction history are crucial.\n- Instructions often reference previously mentioned objects or actions.\n- World state defines available actions and objects.\n- Actions are constrained by the current world state.\n- Both contexts evolve continuously during interaction.\n- Instruction history expands with new commands.\n\nSlide Topic: Model Architecture and Attention Mechanism\nSummary:\n- Inputs are transformed into continuous vector representations.\n- Each input set of vectors is processed using an attention function.\n- A single bidirectional RNN with LSTM encodes instructions.\n- Decoder generates actions based on encoded instructions.\n- At each step, attention function updates decoder state and predicts next action.\n- Attention mechanism prioritizes relevant parts of input vectors for action generation.\n\nSlide Topic: Learning Approach and SESTRA Algorithm\nSummary:\n- Uses exploration-based learning to maximize immediate expected reward.\n- Observes agent behavior under current policy and computes expected reward.\n- Iterates over training data T times, sampling actions up to M.\n- Rolls out executions and observes immediate rewards for all actions.\n- Computes entropy and accumulates gradient using rewards and entropy.\n- Updates parameters using RMSProp algorithm.\n\nSlide Topic: Reward Function and Objective\nSummary:\n- Reward function R(j)_i: S \u00d7 A \u00d7 S \u2192 R defined per example j and instruction i.\n- R(j)_i(s, a, s') = P(j)_i(s, a, s') + \u03c6(j)_i(s') - \u03c6(j)_i(s).\n- \u03c6(j)_i(s') - \u03c6(j)_i(s) computes edit distance between state s and goal.\n- Potential-based shaping term \u03c6 enhances learning signal in non-terminal states.\n- Objective maximizes immediate expected reward across all actions.\n- Entropy regularization is used to encourage exploration.\n\nSlide Topic: SCONE Domains and Data\nSummary:\n- SCONE has three domains: Alchemy, Scene, and Tangrams.\n- Each interaction includes five instructions.\n- Table 1 provides data statistics.\n- Table 2 details discourse reference analysis.\n- Alchemy involves seven numbered beakers with colored chemicals.\n- State encodings use RNNs for beaker data and index embeddings.\n\nSlide Topic: Experimental Setup and Evaluation\nSummary:\n- Task completion accuracy evaluated by exact match of final state to annotated goal state.\n- Accuracy reported for complete interactions (5 utts), first three utterances (3 utts), and single instructions (Ins).\n- Single instruction execution starts from annotated start state.\n- Performance reported for policy gradient and contextual bandit baseline systems.\n- Both systems use reward with shaping term and our model.\n- Supervised learning results generated heuristically and evaluated by maximum-likelihood estimate.\n\nSlide Topic: Ablation Studies and Model Performance\nSummary:\n- Table 4 shows development results and model ablation studies.\n- Removing previous instructions or both states reduces performance across all domains.\n- Removing only initial or current state shows mixed results across domains.\n- Access to both initial and current states improves performance for Alchemy.\n- For other domains, access to both states reduces performance.\n- Increased parameters outweigh marginal information for some domains.\n\nSlide Topic: Error Analysis and Model Limitations\nSummary:\n- Analyzed instruction-level errors in best models with correct initial state.\n- Studied 50 examples per domain to identify failure types.\n- Table 5 shows counts of major error categories.\n- Considered multiple reference resolution errors.\n- State reference errors occur when resolving references to world state fails.\n- Distinguish between feasible multi-turn reference errors and those requiring past states.",
    "Slide Topic: Introduction to Language Identification and Its Applications\nSummary:\n- Identifies text or speech language in natural language processing.\n- Improves letter-to-phoneme conversion accuracy.\n- Enhances transliteration systems for semantic rules variation.\n- Accounts for different languages when target is Chinese.\n- Clusters training data by source language for better machine transliteration.\n- Increases overall accuracy in language-dependent applications.\n\nSlide Topic: Traditional N-gram Approaches for Language Identification\nSummary:\n- N-gram models popular for language identification.\n- Cavnaar and Trenkle (1994) apply n-grams for text categorization.\n- Character-level models created using n-grams up to a max length.\n- New text classified by generating its n-gram frequency profile.\n- Text assigned to class with most similar language model.\n- Similarity determined by summing differences in n-gram ranks.\n\nSlide Topic: SVMs for Language Identification of Short Texts\nSummary:\n- Use character n-gram occurrences as features\n- Include word length as an additional feature\n- SVMs handle large feature sets effectively\n- SVMs learn to weigh features appropriately\n- Avoid using language models for name language determination\n- Suitable for identifying languages in short texts\n\nSlide Topic: Experimental Setup and Data Sets\nSummary:\n- Two corpora used: Transfermarkt for soccer player names, CEJ for first names and surnames.\n- 10% of data reserved for testing.\n- Remaining 90% used for training.\n- 10-fold cross-validation for parameter optimization.\n- Tested SVM-based approach on both datasets.\n\nSlide Topic: Evaluation of SVMs on Transfermarkt Corpus\nSummary:\n- SVMs outperform language models on Transfermarkt corpus.\n- SVMs show higher accuracy on last names.\n- Language models are less accurate on full names.\n- Kernel differences in SVMs are not statistically significant.\n\nSlide Topic: Evaluation of SVMs on CEJ Corpus\nSummary:\n- RBF and sigmoid kernels were too slow due to corpus size.\n- Only linear kernel was tested.\n- Results compared to previous language models.\n- Error rate reduced by over 50%.\n- See Table 2 for detailed comparison.\n\nSlide Topic: Application of Language Identification to Machine Transliteration\nSummary:\n- Machine transliteration benefits from language identification.\n- Language determines word pronunciation.\n- Language identification aids in accurate transliteration.\n- Tested language identification as a preprocessing step.\n- Results showed potential for improved transliteration accuracy.\n\nSlide Topic: Results and Analysis of Transliteration Experiment\nSummary:\n- Two separate models trained on split data.\n- Combined top-1 accuracy: 46.0%.\n- Single model accuracy on full data: 47.0%.\n- No statistically significant difference noted.\n- Performance decrease attributed to reduced training data.\n- Separate models face data scarcity challenges.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Novel approach to language identification of names proposed.\n- SVM with n-gram counts outperforms language model-based methods.\n- Superior performance demonstrated in experimental results.\n- Future work will explore integrating language identification in machine transliteration.\n- Aims to enhance accuracy and applicability in multilingual environments.",
    "Slide Topic: Introduction to Extractive Summarization\nSummary:\n- Extracts key word sequences from source text.\n- Concatenates selected sequences to form summary.\n- Focuses on identifying salient information.\n- Uses human abstracts for supervised learning.\n- Labels source words appearing in abstract as 1.\n- Aims to improve automated summarization accuracy.\n\nSlide Topic: Challenges with Existing Supervised Approaches\nSummary:\n- Most source words are tagged 0, only a few are 1.\n- Human abstracts are short, often omitting source words.\n- Labels are not always accurate.\n- Words labeled 0 may be paraphrases or generalizations.\n- Mislabeling occurs frequently with related source words.\n\nSlide Topic: Proposed Training Paradigm Using Cloze-Style Questions\nSummary:\n- New training method for extractive summarization explored.\n- Human abstracts converted into Cloze-style comprehension questions.\n- Questions feature a sentence with a blank as the question body.\n- Answers are entities or keywords from the abstract.\n- Questions require specific content knowledge, not general world knowledge.\n- Encourages summaries to preserve relevant, salient source content.\n\nSlide Topic: Attention Mechanism for Question-Answering\nSummary:\n- Identifies relevant parts of text for answering questions.\n- Uses \u03b1k,i \u221d exp(qkWa hSi) to measure word importance.\n- \u03b1k,i characterizes importance via a bilinear term.\n- Constructs context vector ck as a weighted summary.\n- ck sums words relevant to the k-th question.\n- ck aids in predicting the answer accurately.\n\nSlide Topic: Reinforcement Learning for Extractive Summarization\nSummary:\n- Extends Lei et al. (2016) work using reinforcement learning.\n- Focuses on generating fluent, generic document summaries.\n- Shifts from rationale generation to summary exploration.\n- Employs a question-answering task to achieve the goal.\n- QA performance is secondary to summary quality.\n- Aims to improve extractive summarization through RL.\n\nSlide Topic: Reward Function Components\nSummary:\n- Final reward function R(Y) combines multiple components.\n- Components include Ra(Y), Rb(Y), Rf(Y), Rs(Y).\n- \u03b3, \u03b1, \u03b2 are coefficients for interpolation.\n- R(Y) = Ra(Y) + \u03b3Rb(Y) \u2212 \u03b1Rf(Y) \u2212 \u03b2Rs(Y).\n- Coefficients tuned for optimal performance.\n- Linear interpolation ensures smooth reward scaling.\n\nSlide Topic: Comparison of Entity and Keyword-Based QA Pairs\nSummary:\n- 'Entity Q' uses QA pairs with named entities as answers.\n- 'Keyword Q' uses QA pairs with sentence root words as answers.\n- Both methods outperform baseline systems on benchmark dataset.\n- 'Entity Q' yields an R-2 F-score of 11.5.\n- 'Keyword Q' yields an R-2 F-score of 11.6.\n- Scores based on R-1, R-2, and R-L metrics.\n\nSlide Topic: Experimental Setup and Results\nSummary:\n- CNN dataset used for training, validation, and testing.\n- Dataset includes news articles paired with human-written highlights.\n- Methods compared against state-of-the-art systems.\n- Both extractive and abstractive approaches evaluated.\n- Achieved superior R-2 F-scores of 11.5 and 11.6.\n- Outperformed baseline systems on benchmark dataset.\n\nSlide Topic: Future Work and Usability Studies\nSummary:\n- Investigate automatic clustering of summary segments.\n- Each cluster captures a unique aspect of content.\n- Clusters can be color-highlighted for clarity.\n- Conduct usability study inspired by Narayan et al. (2018).\n- Test how well highlights help users answer key questions.\n- Provide alternative strategy to evaluate against baselines.",
    "Slide Topic: Introduction to Multimodal Social Media and Name Tagging\nSummary:\n- Social platforms like Snapchat, Twitter, Instagram, and Pinterest are integral to daily communication.\n- These platforms are evolving to become more multimodal, integrating images, videos, audio, and text.\n- Multimodal content enhances expressive capabilities on social media.\n- Name Tagging is crucial for understanding language in multimodal contexts.\n- It involves identifying and tagging names in both image and textual content.\n- Name Tagging supports tasks like Question Answering, Summarization, Searching, and Recommendation.\n\nSlide Topic: Challenges in Name Tagging for Social Media\nSummary:\n- Short messages limit context around names.\n- Linguistic variations and slang are common.\n- Typographical errors frequently occur.\n- Colloquial language use is widespread.\n- Hashtags and informal abbreviations complicate tagging.\n- Task difficulty exceeds traditional news wire name tagging.\n\nSlide Topic: Proposed Model: Visual Attention and Modulation Gate\nSummary:\n- End-to-end model for name tagging in multimodal social media data.\n- Uses image-sentence pairs without human-designed features.\n- Incorporates a Visual Attention component for decision-making insight.\n- Combines textual and visual features using a modulation gate.\n- Employs BLSTM for textual feature extraction and CRF for tag prediction.\n- Gate architecture modulates word-level multimodal features.\n\nSlide Topic: BLSTM-CRF Sequence Labeling Model\nSummary:\n- Models name tagging as a sequence labeling task.\n- Predicts label sequence L from word sequence S.\n- Uses Bidirectional LSTM to capture long-range dependencies.\n- Combines past and future contexts for each word.\n- Concatenates left and right context representations.\n- Enhances tagging accuracy with bidirectional information.\n\nSlide Topic: Visual Feature Representation and Attention Mechanism\nSummary:\n- CNNs extract image representations.\n- ResNet used for regional and whole image features.\n- Global visual representation is reasonable but not optimal.\n- Parts of images may be more relevant than the whole.\n- Visual attention mechanism addresses this limitation.\n- Effective for vision-language tasks like Image Captioning and Visual Question Answering.\n\nSlide Topic: Datasets for Multimodal Name Tagging\nSummary:\n- Two datasets from Twitter and Snapchat\n- Four entity types: Location, Person, Organization, Miscellaneous\n- Each instance pairs a sentence with an image\n- Names in sentences manually tagged by three experts\n- Twitter dataset includes tweets and associated images\n- Snapchat dataset consists of captions and images from public stories\n\nSlide Topic: Experimental Setup and Training Details\nSummary:\n- Tokenization follows Owoputi et al. (2013), except for separating '#' from words.\n- Use 100-dimensional GloVe embeddings initialized from 2 billion tweets.\n- Fine-tune word embeddings and pre-trained ResNet during training.\n- Optimize using mini-batch SGD with batch size 20 and momentum 0.9.\n- Set initial learning rate \u03b7\u2080 = 0.03 with decay rate \u03c1 = 0.01.\n- Apply gradient clipping of 5.0 to prevent exploding gradients.\n\nSlide Topic: Results and Performance Analysis\nSummary:\n- Baseline model: BLSTM-CRF with sentences only\n- Proposed model: BLSTM-CRF + Visual Attention + Gate\n- Proposed model achieves best F1 scores on both datasets\n- Visual features validate entity types effectively\n- Images with persons likely contain person names\n- Images with soccer fields likely contain sports team names\n\nSlide Topic: Attention Visualization and Error Analysis\nSummary:\n- Figure 5 shows successful attention visualization examples.\n- Model focuses on correct regions with well-aligned images and sentences.\n- Figure 6 illustrates three types of failures.\n- Failures due to misalignment between visual and textual data.\n- Failures occur with blurry images.\n- Incorrect model attention leads to tagging errors.\n- Name tagging benefits from visual features in aligned contexts but varies in social media.\n\nSlide Topic: Related Work and Future Directions\nSummary:\n- Neural architectures outperform traditional methods in named tagging.\n- Existing methods focus on news wire, neglecting social media.\n- Our model is inspired by (Lample et al., 2016).\n- Introduced new visual attention component and modulation control gate.\n- Future work includes expanding to fine-grained Named Tagging.\n- Entity Liking is another potential future research direction.",
    "Slide Topic: Introduction to Text Coherence\nSummary:\n- Coherence enhances text communication.\n- Reduces reader cognitive load.\n- Easier reading with coherent texts.\n- No single definition of coherence.\n- Multiple definitions highlight key aspects.\n- Crucial for effective text understanding.\n\nSlide Topic: Definitions and Aspects of Coherence\nSummary:\n- Conveys clear purpose and meaning.\n- Integrated as a whole, not isolated sentences.\n- Sentences centered around a theme or topic.\n- Exhibits discourse and cohesion between sentences.\n- Organization aids in anticipating upcoming information.\n- Easier to read than less coherent texts.\n\nSlide Topic: Existing Computational Models of Text Coherence\nSummary:\n- Studies use supervised learning for text coherence.\n- Entity Grid model and its extension, Role Matrix, are key.\n- Models biased towards original text ordering.\n- Poor performance on disorganized texts due to training.\n- Supervised models face data sparsity and high computational costs.\n- Unsupervised Entity Graph model proposed to address these issues.\n\nSlide Topic: Proposed Unsupervised Text Coherence Model\nSummary:\n- Addresses limitations of past research.\n- Captures contribution of related but not identical entities.\n- Text encoded into graph G(V, E).\n- Vertices V represent sentences in text.\n- Edges E show semantic relations between sentences.\n- Directed, weighted edges indicate sentence connections.\n\nSlide Topic: Graph Construction Methods for Semantic Similarity\nSummary:\n- Three methods proposed for graph construction based on semantic similarity.\n- PAV method: Edges based on preceding adjacent sentence similarity.\n- SSV method: Edges to preceding and following vertices by semantic similarity.\n- MSV method: Multiple outgoing edges per vertex based on cosine similarity threshold.\n- Methods vary in edge creation based on sentence adjacency and similarity thresholds.\n\nSlide Topic: Text Coherence Measure Calculation\nSummary:\n- Graph constructed using one of three methods\n- Text coherence measure (tc) calculated from graph\n- Average outgoing edge weights from each vertex\n- tc is the average of these weights\n- Higher tc indicates more coherent text\n- Graph method choice impacts tc accuracy\n\nSlide Topic: Evaluation and Results of Document Discrimination Task\nSummary:\n- Two tasks evaluated: document discrimination and insertion.\n- Discrimination task involves identifying original text from a pair.\n- Texts are original and randomly permuted sentences.\n- PAV method outperformed unsupervised baseline (Entity Graph).\n- PAV did not surpass supervised baseline (Entity Grid).\n- Performance difference was statistically significant (p < 0.05).\n\nSlide Topic: Evaluation and Results of Insertion Task\nSummary:\n- Task evaluates measure coherence by estimating original sentence position.\n- Sentence is randomly removed from text for evaluation.\n- PAV method outperforms both supervised and unsupervised baselines.\n- Evaluation focuses on accuracy of sentence position estimation.\n- PAV shows better performance, though not statistically significant.\n- Statistical significance threshold set at p < 0.05.\n\nSlide Topic: Discussion on Performance and Limitations\nSummary:\n- PAV achieved 0.774 accuracy in document discrimination.\n- PAV scored 0.356 in the insertion task.\n- Large performance gap exists between tasks.\n- Error analysis suggests room for improvement.\n- Refined sentence vector representation could enhance performance.\n- Capturing distant sentence relations improves semantic similarity.",
    "Slide Topic: Introduction to the Study of Language Use and Occupational Class\nSummary:\n- Online social networks offer broad context for text analysis.\n- Occupational status influences language use.\n- Hypothesis: Social media language indicates user's occupational class.\n- Study focuses on Twitter to predict user's occupational class.\n- Analysis uses platform-related attributes and generated content (tweets).\n- Findings applicable in sociology and targeted job recruitment.\n\nSlide Topic: Standard Occupational Classification (SOC) Taxonomy\nSummary:\n- SOC taxonomy maps Twitter users to occupations.\n- Developed by UK Office of National Statistics.\n- Jobs classified hierarchically by skill and content.\n- Nine major groups coded 1-9.\n- Sub-major groups coded with 2 digits, minor groups with 3.\n- Unit groups, coded with 4 digits, represent specific jobs.\n\nSlide Topic: Creation of the Twitter Data Set for Occupational Class Prediction\nSummary:\n- No suitable public data sets exist for this task.\n- Created a new data set mapping Twitter users to their occupations.\n- Used profile information to identify users with disclosed occupations.\n- Retrieved up to 200 accounts per job title using Twitter's Search API.\n- Data set includes 5,191 users across 55 minor groups and 9 major SOC groups.\n- Final data set contains 10,796,836 tweets, collected around August 5, 2014.\n\nSlide Topic: Feature Extraction for User Occupational Class Prediction\nSummary:\n- Features divided into user level and textual categories.\n- User level features include general user info and tweet statistics.\n- Textual features derived from aggregated user tweets.\n- Bio field excluded to prevent data collection bias.\n- SVD Word Embeddings (SVD-E) and NPMI Clusters (SVD-C) used.\n- Neural Embeddings (W2V-E) and Neural Clusters (W2V-C) utilized.\n\nSlide Topic: Classification with Gaussian Processes\nSummary:\n- Gaussian Processes (GP) offer a Bayesian non-parametric framework.\n- Kernels define covariance in response values based on inputs.\n- Binary classification uses a logistic function to squash real-valued outputs.\n- GP inference targets the distribution of latent variables for test cases.\n- One-vs-all classification is performed for each class.\n- Class with the highest likelihood determines the final label.\n\nSlide Topic: Experimental Results and Predictive Accuracy\nSummary:\n- Users classified into nine possible classes.\n- Data split into training (80%), validation (10%), and testing (10%) sets.\n- Validation set used for LR and SVM parameter tuning.\n- User-level features not useful for predicting job class.\n- Textual features improve performance over baseline.\n- GPs outperform SVMs slightly in predictive accuracy.\n\nSlide Topic: Error Analysis of Classification Results\nSummary:\n- Confusion matrix in Figure 1 illustrates classifier errors.\n- Class 4 often misclassified as Class 2 or 3.\n- Similar occupations in Classes 2, 3, and 4 cause confusion.\n- Doctors and nurses, accountants and assistants show classification overlap.\n- Misclassification mainly occurs between adjacent classes.\n- Model captures general user skill level effectively.\n\nSlide Topic: Qualitative Analysis of Word Clusters and Topic Usage\nSummary:\n- Word clusters built from reference corpus used in GP classifier.\n- Best performing model (W2V-C-200) used for analysis.\n- Assumption: Language and topic usage vary across occupational classes.\n- GP length scale parameters inversely proportional to topic relevance.\n- Parameters rank topic importance, addressing hypothesis.\n- Table 4 shows top 10 informative topics ranked by MRR across classifiers.\n\nSlide Topic: Comparison with Related Work\nSummary:\n- Occupation prediction studied in psychology and economics.\n- French (1959) linked student measures to future occupations.\n- Predictors included math/verbal skills, family status, and personality.\n- Sociolinguistic studies suggest social status influences language use.\n- Li et al. (2014) extracted user profiles from Twitter.\n- Focus was on employer information, not occupation type.\n\nSlide Topic: Conclusions and Future Directions\nSummary:\n- First large-scale study on inferring user occupational class via social media language use.\n- Introduced extensive labeled dataset extracted from Twitter.\n- Used GP framework for classification, balancing predictive performance and feature interpretability.\n- Achieved good predictive accuracy, showing occupation influences text use.\n- Qualitative analysis captured both occupation-specific interests and general class-based behaviors.\n- Study reflects Twitter population, acknowledging potential self-mention bias.",
    "Slide Topic: Introduction to Social Sensors and Twitter-based Influenza Surveillance\nSummary:\n- Social media platforms increase personal information sharing.\n- Twitter's micro-blogging format attracts researchers.\n- Twitter serves as a valuable personal information resource.\n- Study focuses on detecting seasonal influenza epidemics.\n- Influenza detection is a popular Twitter application.\n- Over 30 Twitter-based influenza detection systems exist globally.\n\nSlide Topic: Dataset Collection and Preprocessing\nSummary:\n- Collected 7.7 million influenza-related tweets from August 2012 to January 2016.\n- Filtered out retweets and links to clean data.\n- Used Japanese morphological parser (JUMAN) to extract stem forms.\n- Extracted 27,588 unique words.\n- Analyzed word frequency per day.\n- Built a word matrix (days \u00d7 words) for analysis.\n\nSlide Topic: Identification of Forecasting Words and Time Lag Estimation\nSummary:\n- 'Fever' indicates an upcoming influenza outbreak.\n- Symptoms precede influenza diagnosis.\n- Designate symptom words as forecasting words.\n- Estimate optimal time lag for each forecasting word.\n- Incorporate time lags into the model.\n- Measure time lag using cross-correlation of word frequency and patient numbers.\n\nSlide Topic: Construction of Time-shifted Word Matrix\nSummary:\n- Cross-correlation identifies optimal time shifts for word frequencies.\n- Time-shifted words form matrix X.\n- Initial word matrix (\u03c4 = 0) shows 50 randomly selected words.\n- Initial matrix displays irregular vertical lines due to low-correlation words.\n- Time shift operation aligns words with IDS C reports.\n- Resulting matrix exhibits clear horizontal lines.\n\nSlide Topic: Nowcasting Model: Parameter Estimation and Performance\nSummary:\n- Linear model parameters estimated by minimizing squared error.\n- Vocabulary size |V| significantly exceeds sample size T.\n- L2-penalized regression outperforms L1-penalized (Lasso) in prediction.\n- Elastic Net combines L1 and L2 penalties for optimal performance.\n- Proposed model achieves best nowcasting performance to date.\n- Correlation ratio of the model is 0.93.\n\nSlide Topic: Forecasting Model: Extension and Evaluation\nSummary:\n- Nowcasting model extends to forecasting naturally.\n- Shift word frequency at least \u2206f days for future patient prediction.\n- Forecasting accuracy results shown in Figure 4.\n- Both models outperform baseline for up to 3 weeks.\n- One-week prediction accuracy nearly matches \u03c4min = 0 case.\n- Accuracy declines beyond 3 weeks into the future.\n\nSlide Topic: Discussion on Seasonal Variability and Model Limitations\nSummary:\n- Proposed time shift operation fits IDS C reports.\n- Model trained in Season 3 shows exceptions.\n- Time shift width inconsistency between training and other seasons.\n- Inconsistency affects forecasting performance.\n- Word 'shot' has high weight in Season 3 but low correlation in others.\n- Decrease in forecasting accuracy due to seasonal variability.\n\nSlide Topic: Comparison with Related Work and Future Directions\nSummary:\n- Novel system uses Twitter data for influenza surveillance.\n- First model to predict future epidemics based on social media.\n- Model adaptable for predicting other events.\n- Incorporates time shift parameter for each word.\n- Determines optimal time shift widths first.\n- Explores weight parameters using L1 or elastic net.",
    "Slide Topic: Introduction to Simultaneous Interpretation and Cognitive Challenges\nSummary:\n- Simultaneous Interpretation (SI) requires real-time language translation.\n- Interpreters face significant cognitive and attentional demands.\n- Strategies like summarization aid in efficient message conveyance.\n- Human memory and stamina limit interpreter performance.\n- Errors increase exponentially after brief interpreting periods.\n- Accuracy and precision are critical in SI despite challenges.\n\nSlide Topic: Role of Computer-Assisted Interpretation (CAI) in Supporting Interpreters\nSummary:\n- CAI offers live, interactive terminology support to interpreters.\n- Enhances the quality of interpreter output.\n- Risk of overwhelming interpreters with too much information.\n- Ideal support level varies based on interpreter performance.\n- Systems minimize distraction by offering help only when needed.\n- Balances cognitive load to maintain interpreter efficiency.\n\nSlide Topic: Quality Estimation (QE) for Machine Translation and Its Adaptation for Simultaneous Interpretation\nSummary:\n- QE estimates quality of MT outputs based on source and generated sentences.\n- Default feature set in QuesT++ includes 17 features like token count and n-gram frequency.\n- QE effectively evaluates MT output quality.\n- SI output differs from MT with pauses, hesitations, and paraphrases.\n- Adaptation of QE for SI requires considering unique SI characteristics.\n- Enhancing QE for SI improves real-time translation quality assessment.\n\nSlide Topic: Interpretation-Specific Features for Enhancing QE Accuracy\nSummary:\n- Augment baseline feature set with four new types.\n- Include ratio of pauses/hesitations/incomplete words.\n- Feature ratio of non-specific words usage.\n- Incorporate ratio of 'quasi-'cognates.\n- Consider ratio of word count as a feature.\n- Capture unique aspects of SI output for better accuracy.\n\nSlide Topic: Evaluation Metrics for Simultaneous Interpretation Quality\nSummary:\n- Novice interpreters are assessed on omissions, additions, and inaccuracies.\n- Metrics focus on lexical items and longer phrases for accuracy.\n- No large annotated corpus exists for these measures.\n- Metrics align with issues addressed in machine translation evaluation.\n- METEOR metric is used for its sophistication and content-function word distinction.\n- METEOR better handles disparities between machine translation and simultaneous interpretation.\n\nSlide Topic: Data Sources and Corpus Details for Training and Testing QE Models\nSummary:\n- EN-JA data from NAIST TED SI corpus\n- Seven TED Talks transcribed by interpreters of varying proficiency\n- EN-FR and EN-IT data from EPITC corpus\n- Source and interpreter transcripts from European Parliament speeches\n- Includes translations of source speech transcripts\n- Combined data for comprehensive training and testing\n\nSlide Topic: Experiments and Results: Comparing Baseline, Trimmed, and Proposed Feature Sets\nSummary:\n- Pearson's r used to measure prediction accuracy.\n- Compared baseline, trimmed, and proposed feature sets.\n- Proposed model outperforms in all language settings.\n- EN-JA (A-Rank) shows +0.104 gain.\n- Highest accuracy on EN-FR dataset.\n- Gains statistically significant at p < 0.05.\n\nSlide Topic: Analysis of Results and Insights into Feature Contributions\nSummary:\n- Ablation study on full feature set conducted.\n- Qualitative comparison analysis performed.\n- Utterance length differences benefit Japanese and French.\n- Cognates are particularly useful in EN-FR and EN-IT.\n- Pauses and hesitations aid EN-FR and EN-IT, but hinder EN-JA.\n\nSlide Topic: Qualitative Example of QE Model Performance\nSummary:\n- EN-IT example with METEOR score of 0.079\n- Score lower than average across all datasets (0.262)\n- Baseline model predicted score of 0.127\n- Proposed model scored 0.066\n- Example shows potential for CAI system assistance\n- Interpreter struggles with accurate rendition\n\nSlide Topic: Conclusion and Future Directions for Improving Interpreter Performance Evaluation\nSummary:\n- Novel QE application evaluates interpreter output effectively.\n- Enables CAI systems to offer targeted assistance to interpreters.\n- METEOR used for evaluating interpreter performance.\n- Immediate application in enhancing interpreter support systems.\n- Future work: developing fine-grained measures for performance.\n- Aims to assess various aspects of interpreter performance comprehensively.",
    "Slide Topic: Introduction to Deep Learning for Sentence Representations\nSummary:\n- Deep learning enables automatic learning of continuous sentence representations.\n- Representations can be task-specific or universal.\n- Task-specific representations focus on classification or relation tasks.\n- Universal representations perform reasonably well across various tasks.\n- Representations can be learned through supervised or unsupervised methods.\n- Supervised learning uses manually annotated training sets.\n\nSlide Topic: Overview of Sentence Representation Learning Methods\nSummary:\n- Early RNN models showed embeddings reflecting semantic and syntactic structures.\n- Hill et al. (2016) evaluated sentence representations across models using classification tasks.\n- Semantic similarity was linked to closeness in representation space by Hill et al. (2016).\n- Shih et al. (2016) investigated syntactic properties of NMT system representations.\n- Syntactic labels at sentence and word levels were predicted by Shih et al. (2016).\n- Syntax trees were generated from representations by Shih et al. (2016).\n\nSlide Topic: Proposed Compound Attention Model Architecture\nSummary:\n- Compound model uses attention in both encoder and decoder.\n- Encoder processes input with bidirectional GRU.\n- Inner attention computes weighted averages of encoder states.\n- Decoder leverages structured sentence embedding.\n- Decoder applies attention to components of embedding.\n- Mechanism similar to classical attention but on fixed-size representation.\n\nSlide Topic: Evaluation of Sentence Representations\nSummary:\n- Continuous sentence representations evaluated through classification and similarity tasks.\n- SentEval tool assesses 10 classification and 7 similarity tasks.\n- Phrase evaluation uses COCO and HyTER Networks sources.\n- Metrics include cluster classification accuracy.\n- Nearest-neighbor paraphrase retrieval accuracy is computed.\n- Inverse Davies-Bouldin index measures clustering quality.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Trained English-to-German and English-to-Czech NMT models using Neural Monkey.\n- Used Multi30K multilingual image caption dataset, extended to 410k sentence pairs.\n- Trained cs models on CzEng 1.7 dataset.\n- Evaluated Infersent model pre-trained on natural language inference task.\n- Included GloVe-BOW baseline by averaging GloVe word vectors.\n- Experimented with various datasets to ensure comprehensive model evaluation.\n\nSlide Topic: Translation Quality Results\nSummary:\n- BLEU scores used for quality estimation\n- ATTN Bahdanau model tops performance\n- Transformer (de only) and ATTN-ATTN follow\n- Non-attentive FINAL Cho model ranks lowest\n- Manual ranking of 5 models on 200 pairs conducted\n- Manual ranking confirms automatic evaluation results\n\nSlide Topic: SentEval Task Performance\nSummary:\n- Results presented via averages and best models.\n- Supervised InferSent outperforms others in most tasks.\n- InferSent excels except on MRPC and TREC tasks.\n- Our models lag behind GloVe-BOW overall.\n- CS-FINAL-CTX surpasses others in NLI tasks.\n- SICK-E and SNLI show best performance for CS-FINAL-CTX.\n\nSlide Topic: Paraphrase Evaluation\nSummary:\n- Use COCO and HyTER Networks as paraphrase sources.\n- Encode datasets to get sentence embeddings with cluster labels.\n- Compute cluster classification accuracy for evaluation.\n- Measure nearest-neighbor paraphrase retrieval accuracy.\n- Calculate inverse Davies-Bouldin index for clustering quality.\n- Evaluate models based on these metrics for paraphrase representation.\n\nSlide Topic: Correlation Analysis Between BLEU and Representation Metrics\nSummary:\n- Heat map of Pearson correlations assesses BLEU and representation metrics.\n- Most representation metrics positively correlate on cs dataset.\n- Representation metrics negatively correlate with BLEU.\n- More attention heads negatively impact representation metrics.\n- Attention heads segment sentences, less effective for paraphrases.\n- Subject-object position swaps in paraphrases challenge representation accuracy.\n\nSlide Topic: Discussion on the Impact of Attention Heads on Representation Quality\nSummary:\n- Higher BLEU scores correlate with poorer meaning representation.\n- Bilingual MT may not provide useful data for representation learning.\n- Systems optimizing for high BLEU scores may neglect sentence meaning.\n- More attention heads negatively correlate with representation quality.\n- Attention heads tend to segment sentences, affecting paraphrase representation.\n- Subject-object position swaps are less effectively represented by segmented sentences.",
    "Slide Topic: Introduction to Speech-to-Text Translation (ST) and Its Challenges\nSummary:\n- ST converts spoken language to text in another language.\n- Useful for documenting low-resource languages and crisis relief.\n- Traditional ST uses ASR followed by MT, needing transcribed data.\n- Low-resource languages often lack required transcribed audio and text.\n- End-to-end ST is promising but demands extensive training resources.\n- Resource limitations pose significant challenges in low-resource settings.\n\nSlide Topic: End-to-End ST in Low-Resource Settings\nSummary:\n- End-to-end ST is useful in scenarios with limited resources.\n- Requires substantial data for effective training.\n- Low-resource settings challenge system accuracy.\n- Example: 20 hours of Spanish-English data.\n- Achieved BLEU score of 5.3.\n- Indicates low accuracy in resource-poor environments.\n\nSlide Topic: Leveraging High-Resource Languages for Low-Resource ST\nSummary:\n- Use high-resource language data to enhance low-resource ST.\n- Train single models on multiple languages for effective ASR.\n- Transfer learning significantly improves MT performance.\n- Leverage transcribed audio in high-resource target languages for low-resource ST.\n- Pre-train model on high-resource ASR task.\n- Transfer and fine-tune parameters for low-resource ST task.\n\nSlide Topic: Model Architecture and Preprocessing Techniques\nSummary:\n- Encoder-decoder model with attention for ASR and ST\n- Speech encoder: two CNN layers followed by three-layer bi-directional LSTM\n- Text decoder: fully-connected layer and three-layer LSTM\n- Speech preprocessing: 13-dimensional MFCCs and speaker-level normalization\n- Text preprocessing: BPE for subword segmentation\n- Model is open-vocabulary and trains quickly\n\nSlide Topic: Experimental Setup and Data Sets\nSummary:\n- English ASR uses Switchboard Telephone Speech Corpus\n- French ASR employs French Speech Corpus from Global Phone\n- Spanish-English ST utilizes Fisher Spanish Speech Corpus\n- Mboshi-French ST uses Mboshi-French parallel corpus\n- Spanish-English dataset: 1.5M word tokens, 17K word types\n- Mboshi-French dataset: ~4 hours of Mboshi speech\n\nSlide Topic: Evaluation Metrics and Baseline Models\nSummary:\n- BLEU scores reported for all models.\n- Word-level unigram precision and recall included.\n- METEOR used for stem, synonym, and paraphrase matches.\n- Naive baseline model predicts K most frequent words.\n- Baseline uses bag of words for each test utterance.\n- Evaluation scores provided for comparison.\n\nSlide Topic: Impact of ASR Pre-Training on Spanish-English ST\nSummary:\n- Fine-tuning English ASR model on Spanish-English ST data improves BLEU score.\n- 20 hours of training data boosts BLEU score from 10.8 to 19.9.\n- Transfer learning significantly enhances precision and recall.\n- Gains in precision and recall range between 10 and 20 points.\n- Even 2.5 hours of ST data with transfer learning achieves 30% precision/recall.\n- Transfer learning is effective in very low-resource scenarios.\n\nSlide Topic: Analysis of Transfer Learning Effects\nSummary:\n- Transferring all English ASR model parameters to Spanish-English ST model is most effective.\n- Speech encoder parameters contribute significantly to the gains.\n- Transferring only decoder parameters does not improve accuracy.\n- Parameter training order might explain the lack of improvement with decoder-only transfer.\n- Models show strong early gains with transfer learning.\n- Achieve higher BLEU scores after just 5 epochs compared to non-transfer learning models.\n\nSlide Topic: Impact of Code-Switching and Cross-Linguistic Pre-Training\nSummary:\n- English ASR model scores 1.1 BLEU on Spanish-to-English translation.\n- Code-switching in Fisher-Spanish dataset aids some 4-gram matches.\n- French ASR data improves Spanish-English translation performance.\n- Pre-training on distinct third language aids low-resource speech translation.\n- Cross-linguistic pre-training enhances model's adaptability and performance.\n\nSlide Topic: Application to Mboshi-French ST\nSummary:\n- Best scores achieved on all metrics.\n- Transferred encoding from English ASR model.\n- Transferred decoding from French ASR model.\n- Framework shows high flexibility.\n- Outperforms baselines with only 4 hours of parallel data.\n- Achieved BLEU score of 7.1, precision/recall of 25%.",
    "Slide Topic: Introduction to Neural Sequence-to-Sequence Models and Their Limitations\nSummary:\n- SEQ2SEQ models excel in tasks like machine translation.\n- Require large parallel training datasets.\n- Research focuses on using non-parallel corpora to enhance performance.\n- Current models struggle to match lead-N baselines.\n- Many models are non-differentiable.\n- Rely on reinforcement learning, which is unstable and inefficient.\n\nSlide Topic: Proposed SEQ 3 Model: A Sequence-to-Sequence-to-Sequence Autoencoder\nSummary:\n- Introduces SEQ 3: a sequence-to-sequence-to-sequence autoencoder.\n- Trained end-to-end using gradient-based optimization.\n- Uses differentiable approximations for categorical sampling.\n- Outperforms reinforcement learning methods.\n- Flexible framework for tasks like machine translation and semantic parsing.\n- Applied to unsupervised abstract sentence compression initially.\n\nSlide Topic: Compressor Component of SEQ 3\nSummary:\n- Receives input text x of N words\n- Generates summary y of M words (M < N)\n- Embedding layer projects x to word embeddings e_s\n- Bidirectional RNN encodes e_s to hidden states h_s\n- Each h_s is concatenation of left-to-right and right-to-left states\n- y is a latent variable representing compressed information\n\nSlide Topic: Differentiable Word Sampling Techniques in SEQ 3\nSummary:\n- Word sampling from categorical distributions is non-differentiable.\n- Gumbel-Softmax (GS) trick used for approximation.\n- GS trick enables differentiable sampling.\n- Adds stochasticity to the sampling process.\n- Facilitates exploration of parameter space.\n- Lowers variance in sampling approximation.\n\nSlide Topic: Reconstructor Component of SEQ 3\nSummary:\n- Operates like a compressor but on summary word embeddings.\n- Aims to reproduce input from the summary.\n- Communicates only through discrete summary words.\n- Encoder processes embeddings e_c1, ..., e_cM of summary words.\n- Initialized similarly to the compressor.\n- Uses transformation of concatenated last hidden states and length vector.\n\nSlide Topic: Loss Functions in SEQ 3\nSummary:\n- Uses multiple loss functions for quality summaries.\n- Reconstruction Loss measures input word likelihood.\n- LM Prior Loss ensures readability through pretraining.\n- Topic Loss rewards summaries with similar topic words.\n- Length Penalty aids in predicting end-of-sequence token.\n- Enhances summary quality and coherence.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- SEQ 3 trained on Gigaword sentence compression dataset\n- Dataset includes news article sentences and headlines\n- Tested on DUC-2003 and DUC-2004 tasks\n- Tasks contain news articles with reference summaries\n- Compared to other unsupervised sentence compression models\n- Evaluated using average F1 of ROUGE-1, ROUGE-2, ROUGE-L\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- SEQ 3 outperforms unsupervised generator on Gigaword dataset.\n- SEQ 3 surpasses LEAD-8 baseline on Gigaword.\n- Performance drops without LM prior, highlighting its importance.\n- SEQ 3 generates coherent summaries effectively.\n- Similar performance on DUC-2003 and DUC-2004 datasets.\n- Does not surpass PREFIX baseline on DUC-2003.\n\nSlide Topic: Model Limitations and Future Work\nSummary:\n- Model copies first words of input in compressed text.\n- Issue\u6e90\u4e8e\u91cd\u6784\u5668\u7684\u81ea\u56de\u5f52\u6027\u8d28\u3002\n- \u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6cd5\u662f\u4fee\u6539SEQ 3\u3002\n- \u7b2c\u4e00\u7f16\u89e3\u7801\u5668\u5c06\u8f93\u5165\u8f6c\u6362\u4e3a\u66f4\u957f\u5e8f\u5217\u3002\n- \u7b2c\u4e8c\u7f16\u89e3\u7801\u5668\u5c1d\u8bd5\u538b\u7f29\u4ee5\u91cd\u6784\u539f\u59cb\u8f93\u5165\u3002\n- \u672a\u6765\u5de5\u4f5c\u5c06\u63a2\u7d22SEQ 3\u5728\u5176\u4ed6\u4efb\u52a1\u4e2d\u7684\u6f5c\u529b\u3002",
    "Slide Topic: Introduction to Domain-Specific Sentiment Analysis\nSummary:\n- Words expressing opinions vary by domain.\n- \"Entertaining\" and \"boring\" are common in movie reviews.\n- These words are rarely used in electronics reviews.\n- Some words are used across domains but change polarity.\n- \"Boring\" is negative in movies but positive in automobiles.\n- Domain-specific analysis improves sentiment accuracy.\n\nSlide Topic: Challenges in Cross-Domain Sentiment Analysis\nSummary:\n- Supervised algorithms struggle with new domains.\n- Performance degrades when moving from source to target.\n- Re-training is often required for each new domain.\n- Manual annotation is costly and time-consuming.\n- Numerous domains exist, increasing complexity.\n- Generalization across domains remains a significant issue.\n\nSlide Topic: Significance of Consistent Polarity Words (SCP) Across Domains\nSummary:\n- SCP words show consistent sentiment across different domains.\n- These words are crucial for cross-domain sentiment analysis.\n- Novel technique uses \u03c7\u00b2 test for identifying SCP words.\n- Cosine-similarity measures context vector similarity between words.\n- Method enhances accuracy in sentiment analysis across domains.\n- SCP identification improves understanding of universal sentiment indicators.\n\nSlide Topic: Identification of SCP Words Using \u03c7\u00b2 Test and Cosine-Similarity\nSummary:\n- \u03c7\u00b2 test identifies significant words for sentiment in labeled data.\n- Cosine-similarity assesses word polarity in unlabeled data.\n- Context vectors of words are compared using cosine-similarity.\n- Positive polarity assigned if cosine-similarity with PosPivot is higher.\n- Negative polarity assigned if cosine-similarity with NegPivot is higher.\n- Method effectively transfers sentiment classification across domains.\n\nSlide Topic: Ensemble-Based Adaptation Algorithm for Cross-Domain Sentiment Classification\nSummary:\n- Learns domain-specific features from target domain.\n- Uses classifier trained on SCP words from source domain.\n- Transfers SCP features from source to target domain.\n- Learns target domain-specific features within the target.\n- Combines classifiers in an ensemble for enhanced performance.\n- Improves cross-domain sentiment classification accuracy.\n\nSlide Topic: Experimental Setup and Dataset\nSummary:\n- Compares SCP-based domain adaptation (our approach) with SCL-based method by Bhatt et al. (2015)\n- Uses four domains: Electronics (E), Kitchen (K), Books (B), and DVD\n- Each domain's data split into train (60%), validation (20%), and test (20%) sets\n- Evaluates performance across different domains for both approaches\n\nSlide Topic: Performance Comparison with Existing Methods\nSummary:\n- System-3 outperforms System-1 and System-2 in most pairs.\n- System-2 slightly outperforms System-3 in K to B and B to D pairs.\n- Accuracy difference between System-2 and System-3 is below 1%.\n- System-5, SCL-based, outperforms System-4, common-unigrams-based.\n\nSlide Topic: Error Analysis and Domain Similarity Impact\nSummary:\n- Domain-specific word overlap boosts cross-domain classifier accuracy.\n- Similar domains like Electronics and Kitchen share many specific words.\n- Higher word overlap results in more accurate target domain classification.\n- Pairing similar domains as source and target improves classifier performance.\n- Domain similarity directly impacts cross-domain classification success.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- SCP words transfer sentiment information across domains.\n- Strong positive correlation (0.78) between SCP words and classification accuracy.\n- SCP words enhance cross-domain sentiment classification.\n- Ensemble of classifiers improves overall accuracy.\n- Future work: Expand SCP application in diverse domains.\n- Explore deeper integration with machine learning models.",
    "Slide Topic: Introduction to Semantic Parsing and Surface Realization\nSummary:\n- Increased interest in semantic parsing and surface realization.\n- Focus on graph-structured semantic representations.\n- Examples include AMR, EDS, and DMRS.\n- AMR introduced by Banarese et al. in 2013.\n- EDS and DMRS developed in 2006 and 2009, respectively.\n- Need for a formal framework to manipulate these graphs.\n\nSlide Topic: Graph Grammar and DAG Automata in NLP\nSummary:\n- Graph grammar and DAG automata proposed for NLP.\n- Graph grammar example: Hyperedge Replacement Grammar (HRG).\n- DAG automata studied by Kamimura and Slutzki, extended by Chiang et al.\n- Focus on DAG transducers for NLG systems.\n- Aim for accurate, efficient, and robust natural language generation.\n- Recent applications enhance NLP system capabilities.\n\nSlide Topic: Type-Logical Semantic Graphs and Their Characteristics\nSummary:\n- Represents meaning using type-logical semantic graphs\n- Grounded in type-logical semantics (Carpenter, 1997)\n- Adjuncts analyzed as higher-order functions\n- Functions consume complex arguments (Kratzer and Heim, 1998)\n- Structure is flat, not hierarchical\n- Distinguishes natural language semantics from syntax\n\nSlide Topic: Challenges with Existing DAG Transducers\nSummary:\n- Only existing DAG transducer for NLG proposed by Quernheim and Knight (2012).\n- Transducer designed for DAG-to-tree transformation in AMR-to-text generation.\n- Unsuitable for meaning graphs from type-logical semantics.\n- Limited handling of hierarchical structures with reentrancies.\n- No method described for acquiring graph recognition and transduction rules.\n- No reported practical generation results.\n\nSlide Topic: Proposed DAG Transducer Design\nSummary:\n- Inspired by declarative programming paradigm\n- Novel DAG transducer for graph-to-program transformation\n- Input: semantic graph\n- Output: program in declarative language\n- Avoids linguistic structure dependency\n- Execution yields surface string effortlessly\n\nSlide Topic: Declarative Programming Language for String Concatenation\nSummary:\n- Syntax defined in BNF format for string calculation.\n- Program consists of zero or more statements.\n- Statement assigns value to a variable.\n- Expression can be a variable, string, or concatenation of expressions.\n- Strings are sequences of characters from \u03a3out, including empty string \u03f5.\n- '+' operator denotes string concatenation.\n\nSlide Topic: Transduction Process: Recognition and Instantiation\nSummary:\n- Transduction involves two steps: Recognition and Instantiation.\n- Recognition finds an edge labeling function \u03c1 that matches rule patterns.\n- Instantiation uses rule(v) to denote the rule applied to node v.\n- For each edge e_i adjacent to v, replace L with \u03c8(\u2113(v)).\n- Replace occurrences of v_l(j,d) in s with new variable x_ij.\n- Generates a new expression for node v.\n\nSlide Topic: DAG Transduction-based NLG for Different Languages\nSummary:\n- Languages vary in morphosyntactic and syntactico-semantic properties.\n- Russian and Arabic use grammatical markers for functions.\n- Chinese encodes information configurationally, not through inflection.\n- Differences impact NLG significantly.\n- DAG transducer suffices for lemma sequence in Chinese.\n- Morphologically rich languages require modeling complex morphological changes.\n\nSlide Topic: Inducing Transduction Rules from EDS Graphs\nSummary:\n- Empirical study on DAG-based NLG feasibility\n- Focus on variable-free MRS representations: EDS\n- Dataset: Deep Bank 1.1\n- EDS-specific constraints must be maintained\n- Head state defined by direction u in input or r in output\n- Only head variables or S can be left sides of statements\n\nSlide Topic: Evaluation and Performance Analysis\nSummary:\n- Uses Deep Bank 1.1 for training DAG transducer and sequence-to-sequence morphyzer.\n- Wikiwoods used as additional dataset to enhance sequence-to-sequence morphyzer.\n- Training, development, and test datasets from Deep Bank, split as recommended.\n- Datasets contain 34,505, 1,758, and 1,444 sentences respectively.\n- Achieves notable accuracy, efficiency, and robustness.\n- Demonstrates feasibility of DAG transducer for NLG and effectiveness of design.",
    "Slide Topic: Introduction to RBMT and SMT Challenges\nSummary:\n- SMT widely used in practical systems.\n- Creating large parallel corpora is costly.\n- Diverse domains increase development expenses.\n- RBMT developed to address these issues.\n- RBMT uses monolingual corpus in target language.\n- Reduces dependency on expensive parallel data.\n\nSlide Topic: Development of RBMT Using Monolingual Corpus\nSummary:\n- Target word selection based on co-occurrence from monolingual corpus.\n- Word sense disambiguation developed using monolingual corpus.\n- Technique applied to Japanese-Korean translation systems.\n- Technique applied to Korean-Japanese translation systems.\n- Enhances accuracy in rule-based machine translation.\n- Utilizes domain-specific target corpus for better results.\n\nSlide Topic: Application of Statistical Post Editing (SPE) to RBMT\nSummary:\n- SPE enhances RBMT through parallel corpora.\n- Domain adaptation benefits confirmed (Suzuki, 2011).\n- SPE approach used in Japanese-English tasks.\n- SPE approach used in Japanese-Chinese tasks.\n- SPE outperformed RBMT in evaluations.\n- Results from last workshop (Nakazawa et al., 2014).\n\nSlide Topic: Overview of Toshiba's Machine Translation System\nSummary:\n- Transfer-based machine translation system\n- Analyzes morphology and syntax/semantics\n- Selects target words and transfers structure\n- Generates syntax and morphology in target language\n- Utilizes extensive rules and word dictionaries\n- Achieves high performance and customization flexibility\n\nSlide Topic: System Combination Using RNNLM\nSummary:\n- Combined SPE and SMT systems using RNNLM for n-best reranking.\n- Reranking uses both basic features and RNNLM score.\n- Tuning involved 100-best candidates from SPE and SMT.\n- Dev-set used for tuning process.\n- MERT tuning ran by adding RNNLM score to basic features.\n- Enhanced system performance through integrated scoring.\n\nSlide Topic: Technical Term Dictionaries and Pre/Postprocessing\nSummary:\n- Selected technical term dictionaries by the same principle as previous workshops.\n- Used an additional patent dictionary for JP Oz-h-ja.\n- The patent dictionary is extracted from JPO Chinese-Japanese dictionary.\n- Employed n-gram probability dictionary for JP Ok-o-ja.\n- N-gram dictionary was created from monolingual patent resources.\n- Aims to resolve word sense disambiguation.\n\nSlide Topic: Experimental Results and Performance Analysis\nSummary:\n- Table 1 shows overall BLEU and RIBES scores for 'scientific papers task'.\n- Table 2 shows scores for 'patents task'.\n- COMB indicates results from system combination.\n- Rerank shows results from RNNLM reranking.\n- SMT uses 100 best, COMB uses 200 best for reranking.\n- SPE improves RBMT translation results on BLEU and RIBES.\n\nSlide Topic: Official Results and Human Evaluation\nSummary:\n- Submitted two results: SPE and SPE-SMT combination.\n- Top two systems evaluated via crowdsourcing.\n- BLEU scores used to rank system performance.\n- Toshiba systems ranked top three in human evaluation.\n- Evaluations covered ja-en, ja-zh, and zht-jp translations.\n\nSlide Topic: Discussion on Evaluation Differences and System Characteristics\nSummary:\n- Analyzed differences between our system and Online A.\n- Online A had the highest HUMAN score in JP Okoj-a.\n- Focused on crowdsourcing results for comparison.\n- Hypothesize significant difference\u6e90\u4e8e\u6570\u5b57\u8868\u8fbe\u7684\u8bc4\u4f30.\n- \u4f8b\u5982'\u30b7\u30b9\u30c6\u30e0(100)'\u548c'\u30b7\u30b9\u30c6\u30e0100'.\n- \u8fd9\u4e9b\u5dee\u5f02\u5f71\u54cd\u4e86\u8bc4\u4f30\u7ed3\u679c.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- SPE and re-ranking with RNNLM outperformed phrase-based SMT in most language pairs.\n- SPE-SMT system combination improved BLEU scores for Japanese-English and Japanese-Chinese pairs.\n- No clear correlation found between automatic and human evaluations.\n- Future work involves developing practical multi-system combinations.\n- Leverage unique characteristics and qualities of each system for better performance.",
    "Slide Topic: Introduction to the ACL Anthology\nSummary:\n- Provides open access to ACL sponsored conferences and journal articles.\n- Hosts computational linguistics literature from sister organizations.\n- Offers text and faceted search of indexed papers.\n- Features author-specific pages and third-party metadata integration.\n- Contains over 43,000 computational linguistics and NLP papers.\n- Serves over 4,500 daily requests.\n\nSlide Topic: Technical Infrastructure and Hosting\nSummary:\n- Framework: Ruby on Rails\n- Search Engine: Solr\n- Database: PostgreSQL\n- Web Server: Nginx/Jetty\n- OS: Debian GNU/Linux\n- Currently hosted at Saarland University, aiming to move under ACL's https://www.aclweb.org/portal/\n\nSlide Topic: Paper Ingestion and Metadata Management\nSummary:\n- Key task: Import, index, and provision newly accepted papers.\n- Papers sourced from recent conference proceedings and journal issues.\n- XML format for bibliographic metadata defined by Anthology.\n- XML extended to support software, posters, videos, and datasets.\n- Paper import process is semi-automated.\n- Integrated with ACL conference submission and printed proceedings management.\n\nSlide Topic: Community Involvement and Volunteer Management\nSummary:\n- Anthology maintained by volunteer efforts coordinated by editor.\n- Volunteers join and leave unpredictably.\n- Knowledge preservation is a key challenge.\n- Editor ensures continuity in operations.\n- Improving documentation for all tasks is a priority.\n- Aims to reduce learning curve for new volunteers.\n\nSlide Topic: Documentation and Future-Proofing Efforts\nSummary:\n- All resources hosted on GitHub.\n- Documentation expanded based on setup issues.\n- Aims to ease learning for new volunteers.\n- Simplifies Anthology server migration.\n- Supports easier creation of mirrors.\n- Reduces setup and update challenges.\n\nSlide Topic: Security and Dependency Management\nSummary:\n- Security is a major concern.\n- Older dependencies expose systems to unpatched bugs.\n- Anthology volunteer group addresses outdated software.\n- Working on making Anthology available via Docker image.\n- Docker simplifies server setup and dependency management.\n- Eases future mirror deployments independently.\n\nSlide Topic: Docker Integration for Enhanced Robustness\nSummary:\n- Anthology volunteer group developing Docker image for entire Anthology.\n- Docker offers virtualized environment for simplified server setup.\n- Eases dependency issues independently of future deployments.\n- Supports consistent and isolated application environments.\n- Facilitates easy mirror provision by member institutions.\n- Enables individuals to experiment with Anthology data effortlessly.\n\nSlide Topic: Challenges and Future Extensions\nSummary:\n- Community support is crucial for the Anthology's future.\n- Extending useful capabilities aligns with research efforts.\n- Current Anthology is stable and meets intended use.\n- Hosting mirrors and indexing new publications pose challenges.\n- Code maintenance and updates are ongoing needs.\n- Expanding Anthology's functionalities is a future goal.\n\nSlide Topic: Anonymous Pre-prints and Community Control\nSummary:\n- Pre-prints gain popularity, aiding scientific progress.\n- Current systems lack anonymity, introducing bias.\n- Author-blind publications combat bias effectively.\n- Anonymous pre-prints could be an author option.\n- Collaboration with existing services can facilitate this.\n- Direct hosting within Anthology is another implementation method.\n\nSlide Topic: Reviewer Matching Using Anthology Data\nSummary:\n- Anthology data supports conference organizers in reviewer assignment.\n- Data includes reviewers' interests and expertise from past publications.\n- Information mining from the Anthology streamlines the matching process.\n- Eliminates need to upload papers to external platforms.\n- Enhances efficiency, potentially improving response rates.\n- Directly leverages existing data for accurate reviewer-paper fit.",
    "Slide Topic: Introduction to Satire and Its Characteristics\nSummary:\n- Satire criticizes through entertainment.\n- Uses humor, irony, sarcasm, and exaggeration.\n- Includes harsh, offensive, or funny words.\n- Parody and caricature are common techniques.\n- Mimics regular news in diction for effect.\n- Aims to provoke thought and laughter.\n\nSlide Topic: The Task of Satire Detection and Its Challenges\nSummary:\n- Distinguish satirical news from regular news automatically.\n- Relevant for applications like knowledge base population.\n- Satirical articles need accurate identification and handling.\n- Computational solution faces significant challenges.\n- Even human readers sometimes misidentify satire.\n- Precision in satire detection is crucial for reliability.\n\nSlide Topic: Limitations of Previous Work in Satire Detection\nSummary:\n- Previous work relies on automatically labeled news articles.\n- Labeling is based on publication sources, not content.\n- This leads models to represent publisher traits, not satire.\n- Misleading interpretation of satire concepts results.\n- Model generalization to new sources is compromised.\n- Actual satire characteristics are poorly captured.\n\nSlide Topic: Proposed Model with Adversarial Training\nSummary:\n- New model uses adversarial training to control for publication source bias.\n- Adversarial component keeps satire detection performance comparable.\n- Model debiased through adversarial training process.\n- Adversarial training ensures focus on satire content.\n- Publication characteristics are de-emphasized in model training.\n- Enhances model's ability to detect satire accurately.\n\nSlide Topic: Dataset Creation and Its Unique Features\nSummary:\n- First German dataset for satire detection\n- Includes articles from various publication sources\n- Largest resource for satire detection available\n- Articles span from January 1, 2000, to May 1, 2018\n- Each publication has unique typical phrases\n- Varied most common words across publications\n\nSlide Topic: Model Architecture and Training Details\nSummary:\n- Publication identifier trained as an adversary.\n- Feature extractor uses word embeddings and bidirectional LSTM.\n- Self-attention layer follows the feature extractor.\n- Satire detector performs binary classification using softmax layer.\n- Publication identifier performs multi-class classification.\n- Model identifies 15 different publication sources.\n\nSlide Topic: Research Questions and Baseline Model\nSummary:\n- Investigate impact of adversarial training on satire classification.\n- Examine if adversarial training reduces model bias towards publication source.\n- Train satire detector on satire task initially.\n- Freeze feature extractor weights post satire training.\n- Train publication classifier using frozen feature extractor.\n- Establish baseline model performance for comparison.\n\nSlide Topic: Evaluation Metrics and Hyperparameter Tuning\nSummary:\n- Use precision, recall, and F1 score for satire detection.\n- Calculate weighted macro precision, recall, and F1 for publication identification.\n- Adam optimizer with initial learning rate 0.0001.\n- Learning rate decay set to 10^-6.\n- Mini-batch gradient descent with batch size 32.\n- Alternate batches for the two model branches during training.\n\nSlide Topic: Results and Analysis of Adversarial Training Impact\nSummary:\n- Model with \u03bb = 0.2 shows comparable satire classification performance.\n- Publication identification performance declines, notably precision drops from 44.2% to 30.8%.\n- Indicates overlapping features (cues) for satire and publication classification.\n- Adversarial training helps in identifying cues for satire.\n- Humor is a key cue identified for satire classification.\n- Trade-off exists between satire and publication classification accuracy.\n\nSlide Topic: Conclusion and Future Directions\nSummary:\n- Simple neural networks detect publication sources, not satire.\n- Proposed model uses adversarial training to mitigate this bias.\n- Future research should explore other confounding variables.\n- Time and region of articles could influence satire detection.\n- Investigating these factors will improve model accuracy.\n- Enhanced models can better understand and detect satire.",
    "Slide Topic: Introduction to Zero-Shot Learning in Text Classification\nSummary:\n- Zero-shot learning classifies unseen document classes.\n- Relevant in dynamic environments with emerging topics.\n- Social media exemplifies rapid topic growth.\n- Models must recognize new topics with general information.\n- Labeled training instances are impractical for each new topic.\n- Enhances adaptability without retraining on new data.\n\nSlide Topic: Challenges and Existing Approaches in Zero-Shot Learning\nSummary:\n- Limited studies exploit semantic attributes, concept ontology, or word embeddings.\n- Text classification lacks comprehensive semantic knowledge application.\n- Some research uses different datasets for training and testing.\n- Similarities exist between training and testing class sets.\n- This similarity undermines strict zero-shot learning conditions.\n- Existing approaches may not fully address zero-shot learning challenges.\n\nSlide Topic: Proposed Two-Phase Framework for Zero-Shot Text Classification\nSummary:\n- Novel two-phase framework proposed for zero-shot text classification.\n- Includes data and feature augmentation techniques.\n- First phase: coarse-grained classification to identify seen or unseen classes.\n- Second phase: fine-grained classification to determine specific class.\n- Both phases utilize convolutional neural networks.\n- No labeled data required for unseen classes.\n\nSlide Topic: Data Augmentation Technique: Topic Translation\nSummary:\n- Technique strengthens framework to detect unseen classes.\n- Translates documents from seen to unseen classes.\n- Uses analogy to create new class representations.\n- Enhances training dataset with augmented unseen classes.\n- Improves model's adaptability to new, unseen data.\n\nSlide Topic: Feature Augmentation Using Integrated Semantic Knowledge\nSummary:\n- Feature augmentation enhances document understanding.\n- Uses semantic knowledge for unseen class generalization.\n- Relationship vectors indicate word-class semantic relatedness.\n- ConceptNet, a general knowledge graph, constructs vectors.\n- Enhances zero-shot reasoning through integrated knowledge.\n- Improves model performance on unseen data.\n\nSlide Topic: Implementation Details and Dataset Information\nSummary:\n- Two datasets used: DBpedia ontology and 20 newsgroups.\n- Vocabulary size limited to 20,000 most frequent words, excluding numbers.\n- Phase 1 CNN layer: filter sizes [3, 4, 5], 400 filters each, 300 dense layer units.\n- Phase 2 CNN layer: filter sizes [2, 4, 8], 600 filters each, 400 and 100 dense layer units.\n\nSlide Topic: Experimental Results and Performance Analysis\nSummary:\n- Phase 1 framework outperformed DOC on both datasets.\n- Data augmentation improved accuracy for unseen classes.\n- Zero-shot classifier in Phase 2 achieved highest accuracy.\n- Best results from combining all three input types.\n- Semantic knowledge integration enhances knowledge transfer.\n- Effective for transferring knowledge from seen to unseen classes.\n\nSlide Topic: Comparison with Baseline Models and Other Approaches\nSummary:\n- Two-phase framework outperforms four baselines.\n- Achieves highest accuracy on both datasets.\n- Maintains accuracy on seen classes.\n- Competes well on unseen classes.\n- Baselines include count-based model, label similarity, RNN Autoencoder, CNN+FC.\n- Performs best across all unseen rates.\n\nSlide Topic: Discussion on the Effectiveness of Data and Feature Augmentation\nSummary:\n- Topic translation improved accuracy in detecting unseen class instances.\n- Feature augmentation enabled knowledge transfer from seen to unseen classes.\n- Framework achieved highest overall accuracy in zero-shot learning.\n- Semantic knowledge integration enhanced model performance.\n- Outperformed all baselines and recent approaches in various settings.\n\nSlide Topic: Future Work and Potential Extensions\nSummary:\n- Extend framework for multi-label classification.\n- Use larger datasets for enhanced accuracy.\n- Explore integration of linguist-defined semantic units.\n- Apply findings in zero-shot learning scenarios.",
    "Slide Topic: Introduction to Natural Language Inference (NLI) Task\nSummary:\n- NLI is crucial for understanding natural language.\n- Task involves two sentences: premise and hypothesis.\n- Goal is to determine entailment, neutral, or contradiction.\n- NLI assesses logical relationship between sentences.\n- Performance improved with high-quality datasets.\n- SNLI and MultiNLI are key benchmark datasets.\n\nSlide Topic: Discourse Marker Prediction (DMP) and Its Role in NLI\nSummary:\n- Discourse markers like 'but' and 'and' express sentence relations.\n- They intuitively correspond to NLI intentions.\n- 'But' links to contradiction, 'so' to entailment.\n- Discourse markers reveal intrinsic sentence connections.\n- Few NLI works utilize this discourse marker information.\n- DMP can enhance NLI by predicting these markers.\n\nSlide Topic: Sentence Encoder Model for Discourse Marker Prediction\nSummary:\n- BookCorpus used for training.\n- Dataset from unpublished novels, domain-neutral.\n- Preprocessed data format: (S1, S2, m).\n- S1 and S2 represent sentence halves.\n- m is the discourse marker connecting S1 and S2.\n- Goal: Predict m given S1 and S2.\n\nSlide Topic: Discourse Marker Augmented Network Architecture\nSummary:\n- Incorporates learned encoder into NLI model.\n- Denotes premise as P and hypothesis as H.\n- Encodes words using concatenation of embeddings.\n- Embeddings include word, character, POS, and NER tags.\n- Uses exact match feature for encoding.\n- Passes sequences into bi-directional LSTM for results.\n\nSlide Topic: Integration of Discourse Marker Knowledge into NLI Model\nSummary:\n- Combine sentence representations with DMK.\n- Use linear function for prediction.\n- Apply softmax function for output.\n- Predict logical relations: entailment, neutral, contradiction.\n- Enhance model accuracy with discourse markers.\n\nSlide Topic: Reinforcement Learning for Label Confidence Optimization\nSummary:\n- New objective function combines ground-truth label probabilities and dataset property rewards.\n- Uses REINFORCE algorithm for optimization.\n- Minimizes negative expected reward to improve model performance.\n- Reward measures distance to annotators' ideal labels.\n- Enhances label confidence through iterative learning.\n- Optimizes for both accuracy and dataset-specific properties.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Stanford CoreNLP toolkit for tokenization, POS, and NER tagging\n- Word embeddings initialized with 300d GloVe\n- POS and NER embeddings have dimensions of 30 and 10, respectively\n- TensorFlow r1.3 used as neural network framework\n- LSTM layers have a hidden size of 300\n- Dropout ratio starts at 0.9, decaying by 0.97 every 5000 steps\n\nSlide Topic: Performance Evaluation and Comparison with State-of-the-Art Models\nSummary:\n- Ensemble model built with 10 single models\n- Models share architecture but have different initial parameters\n- Performance results shown in table's bottom section\n- Achieves 89.6% on SNLI dataset\n- Reaches 80.3% on matched MultiNLI\n- Scores 79.4% on mismatched MultiNLI\n\nSlide Topic: Ablation Analysis and Contribution of Model Components\nSummary:\n- Conducted ablation experiment on SNLI development dataset.\n- Evaluated individual model component contributions.\n- Tested sentence encoder model alone for prediction.\n- Represented sentences by single vectors, used dot product for classification.\n- Results were unsatisfactory.\n- Indicates discourse markers' embeddings alone are insufficient for large datasets.\n\nSlide Topic: Semantic Analysis and Visualization of Model Performance\nSummary:\n- Figure 2 shows model performance on three relation labels.\n- Different discourse marker sets used for pre-training.\n- Markers like 'if' or 'before' contribute less than logically strong words.\n- Some markers still improve model performance despite lower contribution.\n- 'Contradiction' label examples benefit most from pre-trained encoder.\n- Other categories show less significant improvement compared to 'contradiction'.",
    "Slide Topic: Introduction to Metaphor in Language\nSummary:\n- Metaphors enrich language and communication.\n- They influence cognition and decision-making.\n- About one-third of sentences contain metaphors.\n- Metaphors use words to represent other concepts.\n- Metaphors do not use the literal meanings of words.\n- They play a significant role in typical corpora.\n\nSlide Topic: Challenges in Computational Metaphor Processing\nSummary:\n- Models non-literal expressions like metaphors, metonymy, and personification.\n- Enhances NLP tasks such as Machine Translation and Sentiment Analysis.\n- Existing methods often fail to identify metaphors accurately.\n- Exclusion of key contextual words leads to literal interpretations.\n- Misinterpretation occurs when phrases are seen as literal rather than metaphorical.\n- Contextual understanding is crucial for correct metaphor processing.\n\nSlide Topic: Proposed Unsupervised Metaphor Processing Model\nSummary:\n- Proposes an unsupervised model for metaphor processing.\n- Identifies and interprets linguistic metaphors at the word level.\n- Utilizes word embedding methods for model construction.\n- Employs WordNet for acquiring lexical relations.\n- Does not require labeled data for training.\n- Captures metaphor in a data-driven, unsupervised manner.\n\nSlide Topic: Methodology and Technical Details\nSummary:\n- Framework uses word embedding based on CBOW and Skip-gram.\n- Train word embeddings from text data.\n- Separate target word from its context.\n- Construct candidate word set for analysis.\n- Identify best fit word for metaphorical analysis.\n- Compute cosine similarity to determine literal or metaphorical use.\n\nSlide Topic: Evaluation and Comparison with Baselines\nSummary:\n- Model evaluated against three strong baselines.\n- Task focuses on metaphor identification.\n- Outperforms unsupervised learning baselines in phrase and sentence evaluation.\n- Achieves equivalent performance to state-of-the-art deep learning baseline.\n- Superior performance noted at both phrase and sentence levels.\n\nSlide Topic: Metaphor Processing for Machine Translation\nSummary:\n- Experiments evaluated metaphor processing in English-Chinese machine translation.\n- Paraphrasing metaphorically used words improves translation engine performance.\n- Best fit words replace metaphorical terms for enhanced translation accuracy.\n- Model significantly improves metaphor translation quality.\n- Tested on Google Translate and Bing Translator.\n- Demonstrates potential for better handling of figurative language in MT.\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- Model outperforms unsupervised baselines in sentence and phrase evaluations.\n- Identified metaphorical words improve Google translation accuracy by 11%.\n- Identified metaphorical words enhance Bing translation accuracy by 9%.\n- Results indicate model's effectiveness in handling complex language structures.\n- Contributions benefit multiple translation systems, showcasing versatility.\n\nSlide Topic: Future Work and Implications\nSummary:\n- Introduce weighted CBOW for positional learning.\n- Implement weighted Skip-gram for sentence structure.\n- Focus on metaphor processing in practical applications.\n- Practical metaphor processing offers real-world value.\n- Enhance understanding beyond mere metaphor identification.",
    "Slide Topic: Introduction to Sentence Embeddings\nSummary:\n- Neural vector representations dominate NLP.\n- Encode sentences into fixed-length vectors.\n- Various methods proposed in recent years.\n- Evaluated on cosine similarities.\n- Aim to match human semantic judgments.\n- Commonly tested in SemEval competitions.\n\nSlide Topic: Evaluation of Sentence Embeddings\nSummary:\n- Mainly evaluated on cosine similarities mirroring human semantic judgments.\n- Typically assessed through SemEval competitions.\n- SICK dataset benchmarks model effectiveness across lexical, syntactic, and semantic challenges.\n- Measures both similarities and predictive ability of entailment.\n- Aims to cover a broad range of linguistic phenomena.\n\nSlide Topic: Sentence Modification Schemes\nSummary:\n- Use simple transformations for sentence triplet sets.\n- Not-Negation inserts \"not\" before the first verb.\n- Quantifier-Negation adds \"there is no\" to the sentence.\n- Synonym Substitution replaces the verb with a synonym.\n- Embedded Clause Extraction isolates clauses with verbs like \"say\".\n- Passivization changes sentences from active to passive voice.\n\nSlide Topic: Sentence Triplet Generation\nSummary:\n- Five evaluation datasets created using triplet sentences.\n- Negation Detection assesses distinguishing sentences with opposite meanings.\n- Negation Variants test reflection of negation quantifiers.\n- Clause Relatedness measures similarity between sentences and embedded clauses.\n- Argument Sensitivity checks distinction between semantic and structural information.\n- Fixed Point Reorder evaluates impact of word order on sentence meaning.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Datasets listed in Table 1 created.\n- Source sentences from SICK, Penn Tree Bank WSJ, and MSR Paraphrase Corpus.\n- Automatic process modifies sentences.\n- Human annotators double-check for grammar and semantics.\n- WordNet used for synonym substitution.\n- Ensures high-quality dataset modification.\n\nSlide Topic: Embedding Methods Compared\nSummary:\n- Three sentence embedding methods compared\n- GloVe Averaging averages word vectors in a sentence\n- Concatenated P-Mean Embeddings combines different p-means of word vectors\n- Sent2Vec learns word and n-gram embeddings\n- Averages all words and n-grams for sentence vector\n- Produces high-quality sentence vectors\n\nSlide Topic: Negation Detection Results\nSummary:\n- Table 2 shows results for Negation Detection dataset.\n- S, S+, S* represent original, synonym substitution, and not-negation sentence versions.\n- Average cosine similarity scores reported for each embedding method.\n- GloVe Avg. often misled by synonym introduction.\n- GloVe Avg. shows high cosine similarity with original word embeddings.\n- InferSent and SkipThought successfully distinguish negated from unnegated sentences.\n\nSlide Topic: Negation Variants Analysis\nSummary:\n- S, S+, S* denote original, Not-Negation, and Quantifier-Negation sentence versions.\n- Accuracy measures higher similarity between S+ and S* over S and S+.\n- Word embedding averaging and Skip Thought show poor accuracy.\n- InferSent demonstrates better understanding of negation quantifiers.\n- Commonality in NLI datasets aids InferSent's performance.\n\nSlide Topic: Clause Relatedness Evaluation\nSummary:\n- S, S+, S* denote original, Embedded Clause Extraction, Not-Negation, respectively.\n- Sent2vec performs best in distinguishing embedded clauses from negations.\n- Dataset sentence triplets divided into two categories for analysis.\n- SkipThought is more sensitive to word order due to its recurrent architecture.\n- InferSent performs better on sentence structures resembling type a).\n- Performance not significantly better than random guessing overall.\n\nSlide Topic: Argument Sensitivity Analysis\nSummary:\n- S, S+, S* denote original, passivized, and reordered sentence versions.\n- Recurrent architectures consider word order.\n- None of the analyzed approaches excel in this task.\n- Distinguishing semantic from structural information is challenging.\n- Current methods fail to adeptly separate meaning from structure.\n\nSlide Topic: Fixed Point Reorder Evaluation\nSummary:\n- S, S+, S* denote original, semantically equivalent, and fixed point inversion sentences.\n- Sentence embeddings based on GloVe averages fail to distinguish fixed point inversion.\n- Weighted means (Sent2Vec) and P-Means concatenation also do not encode sufficient word order.\n- Sent2Vec considers n-grams, but this does not sufficiently affect results.\n- SkipThought and InferSent perform well when original and equivalent sentences share similar structure.\n- Word order information is inadequately encoded in current sentence embeddings.\n\nSlide Topic: Conclusion and Comparative Analysis\nSummary:\n- Proposed method for inspecting sentence embeddings based on semantic properties.\n- Analyzed three popular embedding methods: SkipThought, InferSent, and simple averaging.\n- SkipThought and InferSent distinguish sentence negation from synonymy effectively.\n- InferSent better identifies semantic equivalence regardless of word order and handles quantifiers well.\n- SkipThought suits tasks where sentence semantics align with structure but fails with different word orders.\n- Dedicated sentence embeddings from neural networks outperform simple word embedding averaging.",
    "Slide Topic: Introduction to Sentence Acceptability and Grammaticality\nSummary:\n- Sentence acceptability measures how natural a sentence sounds to native speakers.\n- It includes semantic, syntactic, and pragmatic plausibility.\n- Acceptability also considers non-linguistic factors like memory limitations.\n- Grammaticality refers to the syntactic well-formedness of a sentence.\n- This research focuses on predicting acceptability judgments.\n- Grammaticality is a narrower focus, not the main interest here.\n\nSlide Topic: Research Objectives and Hypotheses\nSummary:\n- Hypothesis 1: Context improves sentence acceptability judgments.\n- Hypothesis 2: Context aids models in predicting acceptability.\n- Hypothesis 3: Models predict acceptability more accurately with context.\n- Study aims to validate these hypotheses through empirical testing.\n\nSlide Topic: Dataset Construction and Annotation Methodology\nSummary:\n- 100 random English Wikipedia articles selected.\n- One sentence sampled from each article.\n- Moses MT system used for translation.\n- Translated to 4 languages and back to English.\n- Amazon Mechanical Turk gathered acceptability judgments.\n- Sentences presented with and without document context.\n\nSlide Topic: Impact of Document Context on Acceptability Ratings\nSummary:\n- Strong correlation (r = 0.80) between ratings with and without context.\n- Context generally improves sentence acceptability.\n- Ill-formed sentences benefit most from added context.\n- Well-formed sentences see decreased acceptability with context.\n- Context boosts ratings more for less well-formed sentences.\n\nSlide Topic: Language Models for Predicting Sentence Acceptability\nSummary:\n- Experimented with standard LSTM and topic-driven language models (tdlm).\n- Tdlm uses document context as a conditioning variable during training.\n- Tested both models with and without context at test time.\n- Aimed to predict sentence acceptability.\n- Context incorporation enhances model performance.\n- Comparing contextual and non-contextual model outcomes.\n\nSlide Topic: Evaluation of Acceptability Measures Against Human Ratings\nSummary:\n- Models evaluated against crowd-sourced annotated sentences.\n- Sentences judged both in and out of context.\n- Context at test time improves model performance.\n- Tdlm consistently outperforms LSTM.\n- Performance evaluated across both types of human ratings.\n- Results robust across test input variants.\n\nSlide Topic: Comparison of LSTM and Topically-Driven Language Models\nSummary:\n- LSTM and TDLM models compared with and without context.\n- TDLM outperforms LSTM in performance tests.\n- Context incorporation enhances model performance.\n- Training with context further boosts TDLM's effectiveness.\n- Testing with context also improves LSTM results.\n- Context is crucial for better model performance.\n\nSlide Topic: Discussion on the Effect of Context on Model Performance\nSummary:\n- Context positively influences acceptability, especially for ill-formed sentences.\n- For well-formed sentences, context can have a reverse effect.\n- Incorporating context during training or testing improves model performance.\n- Prediction performance declines when tested on judgments collected with context.\n- Context's impact varies between ill-formed and well-formed sentences.\n\nSlide Topic: Related Work and Future Research Directions\nSummary:\n- Previous work focused on automatic detection of grammatical errors.\n- Prior research ignored context in acceptability prediction.\n- Future research will explore context's impact on sentence acceptability.\n- Well-formed sentences' acceptability reduction due to context will be investigated.\n- Experimentation with a broader range of models is planned.\n- Sentence embedding methodologies will be included in future studies.",
    "Slide Topic: Introduction to NMT and Tree-based Systems\nSummary:\n- NMT shows recent promising improvements.\n- Systems categorized by input/output types.\n- String-to-string, tree-to-string, string-to-tree systems exist.\n- Tree-based systems use more syntactic information.\n- They easily incorporate prior knowledge.\n- Tree-based systems offer attractive features.\n\nSlide Topic: Advantages of Tree-based NMT Methods\nSummary:\n- Tree-based methods are a focus in current NMT research.\n- Two main categories: tree-structured neural networks and linearization.\n- Linearization offers a simpler model structure.\n- Supports training with larger corpora.\n- Enables reasonable training time.\n- Preferred for computational efficiency.\n\nSlide Topic: Linearization Methods for Tree-based NMT\nSummary:\n- Sennrich and Haddow (2016) encoded dependency labels and words simultaneously.\n- Utilized syntax information by traversing the constituent tree of the source sentence.\n- Nadedje et al. (2017) used CCG supertag sequence for target side linearization.\n- Aharoni and Goldberg (2017) applied top-down linearization for annotated trees.\n- Methods aim to fully utilize syntax information in tree-to-string NMT.\n- Techniques vary for string-to-tree NMT, focusing on different linearization strategies.\n\nSlide Topic: Packed Forest Representation\nSummary:\n- Represents exponentially many parsing trees compactly.\n- Encodes more candidates than n-best lists.\n- Formally defined as a pair V, E.\n- V is the set of nodes.\n- E is the set of hyperedges.\n- Each node spans words from index i to j.\n\nSlide Topic: Proposed Forest-based NMT Approach\nSummary:\n- Inspired by tree-based NMT methods\n- Uses novel weighted linearization for packed forests\n- Encodes syntactic information efficiently\n- Decodes linearized packed forest\n- Operates under simple sequence-to-sequence framework\n- Enhances translation accuracy and efficiency\n\nSlide Topic: Forest Linearization Algorithm\nSummary:\n- Proposes Algorithm 1 for packed forest linearization.\n- Linearizes from root to leaf nodes recursively.\n- Uses EXPANDSEQ procedure to maintain word order.\n- Preserves word sequential information throughout.\n- Linearizes hyperedge tails immediately after hyperedge.\n- Ensures parent-child information is preserved.\n\nSlide Topic: Encoding the Linearized Forest\nSummary:\n- Linearized packed forest is encoder input\n- Differs from sequence-to-sequence NMT input\n- Input has symbol and score sequences\n- Symbols consist of words and constituent labels\n- Symbols combined using operators (c, \u2297, \u2295, or)\n- Encoder processes this structured input format\n\nSlide Topic: Experimental Setup and Results\nSummary:\n- Evaluated forest-based NMT systems on English-to-Chinese and English-to-Japanese tasks.\n- Both configurations outperformed the s2s baseline system.\n- 1-best and forest-based setups showed improved results.\n- Demonstrates effectiveness of syntactic information in translation.\n- Syntactic trees and forests enhance machine translation quality.\n- Results confirm the utility of our framework.\n\nSlide Topic: Qualitative Analysis of Translation Results\nSummary:\n- s2s model outputs ungrammatical Chinese sentences.\n- Attributive phrase \"Czech border region\" forms a complete sentence.\n- Chinese grammar does not allow attributives to be complete sentences.\n- 1-best constituent tree yields grammatical Chinese.\n- Phrase \"adjacent to neighboring Slovakia\" ignored in translation.\n- Packed forest method correctly translates the ignored phrase.\n\nSlide Topic: Comparison with Related Work\nSummary:\n- Utilizes linearized packed forest for encoding\n- Encoder efficiently handles exponentially many trees\n- Demonstrates advantages in experimental results\n- Effective in large-scale machine translation settings\n- Includes qualitative analysis on forest usage\n- Enhances effectiveness in neural machine translation",
    "Slide Topic: Introduction to Semantic Graphs and Their Properties\nSummary:\n- Encode language structure as human knowledge representation\n- Local level: Connect specific semantic concepts through edges\n- Edges represent relations like hypernymy ('is-a') and meronymy ('is-part-of')\n- Global level: Induce regular properties in relation graphs\n- Example: WordNet illustrates semantic graph principles\n- Facilitates understanding and processing of human language\n\nSlide Topic: Local vs. Global Properties in Semantic Graphs\nSummary:\n- Local properties studied via relation prediction tasks.\n- Local methods embed synsets and relations into vector space.\n- Global aspects' structural regularity is well-documented.\n- Global properties rarely used in prediction settings.\n- Global semantic graph features can aid local tasks.\n- Study demonstrates facilitation of relation prediction using global features.\n\nSlide Topic: Exponential Random Graph Model (ERGM) and Its Limitations\nSummary:\n- ERGM estimates weights on local and global graph features.\n- Utilizes a log-linear model over networks.\n- Normalization term grows exponentially with the number of nodes.\n- Cannot decompose into smaller parts, necessitating approximations.\n- Scales to thousands of nodes at most.\n- Insufficient for large networks like WordNet (10^5 nodes).\n\nSlide Topic: Max-Margin Markov Graph Model (M3GM) for Semantic Graphs\nSummary:\n- Replaces maximum likelihood with margin-based objective.\n- Compares observed network against alternative networks.\n- Named Max-Margin Markov Graph Model (M3GM).\n- Gradient loss approximated by importance sampling.\n- Complexity linear in the number of edges.\n- Scalable to large networks, up to 10^5 nodes.\n\nSlide Topic: Feature Extraction and Motifs in Semantic Graphs\nSummary:\n- Total edge count as a basic feature.\n- Counts of cycles of length 2 and 3.\n- Nodes with exactly 1, 2, or 3 outgoing/incoming edges.\n- Nodes with at least 1, 2, or 3 outgoing/incoming edges.\n- Number of paths of length 2.\n- Transitivity: proportion of length-2 paths with an additional edge.\n\nSlide Topic: Parameter Estimation and Training of M3GM\nSummary:\n- ERGM requires normalization term over all possible graphs.\n- Approximate probability using variant of Monte Carlo Maximum Likelihood Estimation (MC-MLE).\n- Proposed margin loss objective for training.\n- Negative sample log score should be at least 1 below positive sample.\n- Motivates hinge loss for optimization.\n- L(\u0398, G; G) = 1 \u2212 log \u03c8ERGM+(G) + log \u03c8ERGM+(G) +.\n\nSlide Topic: Evaluation of M3GM on WN18RR Dataset\nSummary:\n- M3GM evaluated on relation graph edge prediction task.\n- Dataset consists of labeled edges (s, r, t).\n- Evaluation creates predictions by hiding source or target entities.\n- Predictor assessed on hidden entity prediction accuracy.\n- WN18RR dataset used, a subset curated from WordNet 3.0.\n- Contains 18 relations for about 41,000 synsets.\n\nSlide Topic: Synset Embedding Initialization Methods\nSummary:\n- Embedding representations needed for WordNet synsets in the model.\n- Two methods explored: Averaging and AutoExtend retrofitting + Mimick.\n- Averaging combines embeddings of words representing the synset.\n- AutoExtend specifically targets WordNet synset embeddings.\n- AutoExtend retrofits pre-trained word embeddings to a tripartite graph.\n- Tripartite graph connects word forms, lemmas, and synsets.\n\nSlide Topic: Analysis of M3GM's Learned Features and Their Impact\nSummary:\n- Model prefers broad scattering of targets for member meronym and has part relations.\n- Member meronym relations are flat, while has part relations are top-down hierarchical.\n- Multiple unique hypernyms are undesired, indicating a bottom-up hierarchical relation.\n- Lines 3, 6, and 7 suggest deeper interactions between different relation types.\n- Model assigns positive weights to hypernyms with derivationally-related forms.\n- Derivational equivalence classes exist in higher, more abstract levels of the hypernym hierarchy.\n\nSlide Topic: Future Directions and Applications of M3GM\nSummary:\n- M3GM extends beyond individual edge predictions.\n- Scores bundles of new edges for entity linking.\n- Maintains structural coherence in semantic graphs.\n- Plans to combine with search algorithms for graph expansion.\n- Automates knowledge graph extension with new entities.\n- Explores multilingual applications for broader utility.",
    "Slide Topic: Introduction to Chinese Spelling Check Challenges\nSummary:\n- No word delimiters exist in Chinese.\n- Chinese words can be single or multiple characters.\n- Over 13,000 Chinese characters exist.\n- Each character has its own contextual meaning.\n- Complexity exceeds English with only 26 letters.\n- Makes Chinese spelling checking highly challenging.\n\nSlide Topic: SIGHAN 2015 Bake-off Overview\nSummary:\n- Focus on Chinese spelling errors in CFL learners' essays.\n- Provided platform for developing automatic Chinese spell checkers.\n- Task involved detecting and correcting spelling errors.\n- Checkers identify all possible spelling errors in passages.\n- Errors' locations are highlighted by the checkers.\n- Suggested corrections are provided for identified errors.\n\nSlide Topic: Data Sets for Chinese Spelling Check\nSummary:\n- Corpus from TOCFL essay section\n- Errors manually annotated by native speakers\n- Corrections provided for each error\n- Essays split into three sets\n- Training, Dry run, and Test sets used\n\nSlide Topic: Evaluation Metrics for Spelling Checkers\nSummary:\n- Two levels determine correctness: detection and correction.\n- Detection level: all incorrect character locations must match the standard.\n- Correction level: all corrections must exactly match the standard.\n- Achieve high detection and correction performance.\n- Minimize false positive rate.\n- Accuracy and low false positives are key.\n\nSlide Topic: Task Description and Input-Output Format\nSummary:\n- Evaluate Chinese spelling checker capability.\n- Input: Passage with or without spelling errors.\n- Output: Locations of incorrect characters.\n- Suggest correct characters for errors.\n- Each character or punctuation mark counts as one spot.\n- Input instance has a unique passage number (pid).\n\nSlide Topic: Data Preparation and Annotation\nSummary:\n- Corpus sourced from TOCFL essay section\n- Spelling errors manually annotated\n- Native speakers provided corrections\n- Essays split into three sets\n- Training, Dry run, and Test sets created\n\nSlide Topic: Performance Metrics and Confusion Matrix\nSummary:\n- Confusion matrix includes TP, FP, TN, FN.\n- TP: Correctly identified positive cases.\n- TN: Correctly identified negative cases.\n- FP: Incorrectly identified positive cases.\n- FN: Incorrectly identified negative cases.\n- Metrics assess spelling checker performance.\n\nSlide Topic: Evaluation Results and Best Performing Systems\nSummary:\n- NCUT & NTUT achieved lowest false positive rate at 0.0509.\n- CAS system excelled in detection-level evaluations, scoring over 0.7.\n- CAS system had the best error detection, F1 score of 0.6404.\n- CAS correction accuracy was 0.6918, outperforming other teams.\n- CAS system demonstrated superior performance in both detection and correction levels.\n\nSlide Topic: Approaches and Linguistic Resources Used by Participants\nSummary:\n- CAS team proposes a unified framework for Chinese spelling correction.\n- Uses HMM-based approach for sentence segmentation and correction candidates.\n- Applies two-stage filter process for re-ranking candidates.\n- NCUT & NTTU team proposes word vector/conditional random field based error detector.\n- Utilizes error detection results to guide and speed up language model rescoring.\n\nSlide Topic: Conclusions and Future Research Directions\nSummary:\n- Overview of SIGHAN 2015 Bake-off for Chinese spelling check\n- Covered task design, data preparation, and evaluation metrics\n- Publicly available datasets and evaluation tools provided\n- Performance results and participant approaches analyzed\n- Future focus on Chinese grammatical error correction\n- Development of new language resources for improved learning techniques",
    "Slide Topic: Introduction to Out-of-Vocabulary (OOV) Word Embeddings\nSummary:\n- Word embeddings enable generalization to unseen words.\n- Embeddings capture distributional similarity in low-dimensional space.\n- Large datasets help but cannot cover entire vocabularies.\n- Out-of-vocabulary (OOV) words remain a challenge.\n- OOV words are terms not seen in training data.\n- Effective handling of OOV words improves model robustness.\n\nSlide Topic: MIMICK-RNN: A Quasi-Generative Approach for OOV Words\nSummary:\n- Addresses challenge of OOV words using quasi-generative method.\n- Learns word embeddings from written forms without prior knowledge.\n- Trains RNN on character level with embeddings as targets.\n- Predicts vectors for OOV words in downstream tasks.\n- Named MIMICK-RNN for mimicking distributional embeddings.\n- Reads word spelling to generate accurate embeddings.\n\nSlide Topic: Related Work on Embedding Rare and Unseen Words\nSummary:\n- Studies use morphological or orthographic info for word embeddings.\n- Enables unseen word embeddings based on internal structure.\n- Bojanowski and Grave (2014) sum morpheme embeddings for word vectors.\n- Luong et al. (2013) use recursive neural networks over morphological parses.\n- Bhattacharjee et al. (2016) use morpheme embeddings as prior distributions.\n- Our approach is a post-processing step, applicable to any word embeddings.\n\nSlide Topic: MIMICK Word Embeddings: Model and Training\nSummary:\n- Treats OOV embedding as a generation problem.\n- Assumes a generative protocol for word form-based embeddings.\n- Trains a model on existing vocabulary for unseen word predictions.\n- Uses Word Type Character Bi-LSTM as predictive function.\n- Training objective minimizes squared Euclidean distance.\n- Aligns predicted embeddings with pre-trained word embeddings.\n\nSlide Topic: Evaluation of MIMICK on Stanford RareWord Dataset\nSummary:\n- MIMICK captures word similarity for rare and unseen words.\n- Results shown in Table 3 highlight performance recovery.\n- MIMICK RNN recovers about half of the performance loss.\n- Original Polyglot model suffers from out-of-vocabulary words.\n- 'All pairs' condition shows significant performance improvement.\n- MIMICK outperforms VarEmbed in the evaluation.\n\nSlide Topic: Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes\nSummary:\n- Uses a model similar to Ling et al. (2015) for tagging.\n- Attaches a projection layer to the output of a sentence-level bidirectional LSTM.\n- Duplicates the projection layer for each morphosyntactic attribute type.\n- Input is the hidden state for each token from an underlying LSTM.\n- MLP projection network outputs attribute-specific probability distributions.\n- Outputs probabilities for possible values of each attribute per token.\n\nSlide Topic: Experimental Settings and Language Selection\nSummary:\n- Morphological complexity varies across languages.\n- Selected languages represent diverse morphological patterns.\n- Language family and script type influence model performance.\n- Vary parameters to account for orthography-based approach.\n- Consider recommendations from de Lhoneux & Nivre (2016).\n- Include suggestions from Schluter & Agi\u0107 (2017).\n\nSlide Topic: Evaluation Metrics and Models\nSummary:\n- Micro F1 used for evaluation\n- Each token's attribute compared separately to gold labeling\n- Correct prediction: matching non-NONE attribute/value\n- Recall and precision calculated over entire set\n- Models tested: No-Char, MIMICK, CHAR\u2192TAG, Both\n- CHAR\u2192TAG uses 20D character embeddings, 128-unit hidden layer\n\nSlide Topic: Results and Analysis: Low-Resource and Full-Resource Settings\nSummary:\n- Performance reported in both low-resource and full-resource settings.\n- Low-resource training sets created by random sampling until token limit reached.\n- Results shown for full sets and N=5000 tokens in Table 5.\n- Slavic languages show largest improvements due to free word order.\n- Chinese demonstrates significant improvement across all settings.\n- MIMICK model learns well-informed embeddings using large vocabulary dataset.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Introduced algorithm for inferring OOV word embeddings from limited-vocabulary models.\n- Method enhances performance on sequence-tagging tasks for various languages.\n- Task-agnostic approach benefits low-resource languages and tasks with scarce labeled data.\n- Utilized Bi-LSTM for tagging morphosyntactic attributes at the token level.\n- MIMICK model trained with characters as input in this study.\n- Future work may explore subword units like morphemes, phonemes, or bitmap representations.",
    "Slide Topic: Introduction to Adversarial Perturbations in Machine Learning\nSummary:\n- Adversarial perturbations alter model inputs to achieve misclassification.\n- Attacks exploit vulnerabilities in machine learning models.\n- Models show brittleness to inputs outside training data.\n- Lack of robustness poses security risks.\n- Critical for safety-sensitive applications like autonomous vehicles.\n- Insights aid in improving model reliability and security.\n\nSlide Topic: Challenges in Adversarial Attacks on Discrete Text Data\nSummary:\n- Small changes in text are noticeable.\n- Existing frameworks are not directly applicable.\n- No standard distance metric for text.\n- Semantic similarity assessment is complex.\n- Text data discreteness complicates attack methods.\n- Research is ongoing for effective evaluation metrics.\n\nSlide Topic: Proposed Criterion for Adversarial Examples in NLP\nSummary:\n- Criterion for adversarial examples in NLP proposed.\n- Focuses on meaning preservation in source.\n- Requires meaning destruction in target.\n- Applies particularly to seq2seq models.\n- Good models should produce equivalent outputs.\n- Validates model robustness against attacks.\n\nSlide Topic: A Framework for Evaluating Adversarial Attacks on Seq2Seq Models\nSummary:\n- Adversarial perturbations aim to cause model failures.\n- Evaluate by measuring target similarity between original and perturbed outputs.\n- Use score s_tgt(y, \u0177_M) for target similarity assessment.\n- Quantify discrepancy in meaning between original and adversarial inputs.\n- Source similarity score s_src(x, x') is crucial for meaningful evaluation.\n- Ensure evaluation reflects both output failure and input perturbation impact.\n\nSlide Topic: Similarity Metrics for Evaluating Adversarial Attacks\nSummary:\n- Semantic similarity scores not precisely defined.\n- Evaluating sentence similarity is an ongoing research area.\n- Decoupling adversarial example definition from similarity metrics is sensible.\n- Manual metrics are used for assessing semantic similarity.\n- Automatic metrics are also employed for the same purpose.\n- Both types of metrics are crucial for evaluating adversarial attacks.\n\nSlide Topic: Gradient-Based Adversarial Attacks and Their Constraints\nSummary:\n- Perform attacks by replacing one word to maximize adversarial loss.\n- Aim to preserve sentence meaning through modifications.\n- Restrict substitutions to semantically similar words.\n- Enhance word-level perturbations for better meaning preservation.\n- Inspired by substitution attacks for effectiveness.\n- Focus on gradient-based approach for targeted modifications.\n\nSlide Topic: Experimental Evaluation of Adversarial Attacks and Metrics\nSummary:\n- Experiments evaluate proposed framework for adversarial attacks.\n- Assess which automatic metrics align better with human judgment.\n- Compare various adversarial attacks using the evaluation framework.\n- Explicitly meaning-preserving attacks receive better assessment scores.\n- Human judgment correlation is a key focus in metric evaluation.\n- Demonstrates effectiveness of constrained adversarial attacks.\n\nSlide Topic: Adversarial Training with Meaning-Preserving Attacks\nSummary:\n- Adversarial training augments training data with adversarial examples.\n- Investigates impact of meaning-preserving attacks on robustness.\n- Aims to minimize effect on test performance compared to unconstrained attacks.\n- Char Swap attacks used to preserve meaning in adversarial examples.\n- Improves model robustness without significantly affecting test performance.\n- Outperforms training with unconstrained attacks in terms of test performance.",
    "Slide Topic: Introduction to Semantic Role Labeling (SRL) and Current Limitations\nSummary:\n- SRL captures predicate-argument relations.\n- Models focus on 'who did what to whom'.\n- High-performing models use BIO-tagging.\n- Tagging is done for one predicate at a time.\n- Models rely on gold predicates for evaluation.\n- Deployment requires error-prone predicate identification.\n\nSlide Topic: Proposed End-to-End SRL Model\nSummary:\n- Predicts all predicates and arguments in one pass.\n- Builds on recent coreference resolution model.\n- Utilizes learned, contextualized span representations.\n- Directly predicts SRL graphs over text spans.\n- Enhances efficiency and accuracy in semantic parsing.\n\nSlide Topic: Span-Graph Formulation and Its Advantages\nSummary:\n- Overcomes limitations of semi-Markov and BIO-based models.\n- Models overlapping spans across different predicates.\n- Generalizes token-level representations in BIO models.\n- Dynamically decides which spans and roles to include.\n- Eliminates the need for standard syntactic features.\n- Enhances flexibility and accuracy in output structures.\n\nSlide Topic: Model Architecture and Learning Process\nSummary:\n- Predicate space includes all input sentence tokens.\n- Argument space consists of all continuous spans.\n- Determines relations between predicate-argument pairs.\n- Considers no relation as a possible outcome.\n- Minimizes negative log likelihood of gold structure.\n- Optimizes during learning process for accurate predictions.\n\nSlide Topic: Beam Pruning for Efficiency\nSummary:\n- Overcome computational impracticality of O(n^3 |L|) relations.\n- Define beams B_a and B_p for candidate arguments and predicates.\n- Rank candidates by unary score in each beam.\n- Reduce number of relational factors evaluated.\n- Model evaluates O(n^2 |L|) factors.\n- Enhances efficiency through selective candidate consideration.\n\nSlide Topic: Neural Architecture Details\nSummary:\n- Builds contextualized representations for argument spans and predicate words\n- Uses BiLSTM outputs for representation\n- Employs feed-forward networks to compute factor scores\n- Scoring functions implemented with feed-forward networks\n- Based on predicate and argument representations\n\nSlide Topic: Experimental Setup and Results\nSummary:\n- Experiments conducted on CoNLL 2005 and 2012 benchmarks.\n- Two setups used: end-to-end and gold predicates.\n- End-to-end setup involves a joint model.\n- Joint model outperforms previous best pipeline system.\n- Performance improvement ranges from 1.3 to 6.0 F1 difference.\n- Improvement observed across all settings.\n\nSlide Topic: Analysis of Model Performance\nSummary:\n- Better at predicting distant arguments from predicates.\n- Likely due to architectural differences.\n- Achieves comparable syntactic agreement to previous syntax-based systems.\n- Makes more constraint violations than previous systems.\n- Particularly prone to predicting duplicate core arguments.\n\nSlide Topic: Comparison with Previous Systems\nSummary:\n- Supports joint predicate identification\n- Incorporates span-level features\n- Excels in long-range dependencies\n- Aligns well with syntactic boundaries\n- Assumes strong independence\n- Challenges in maintaining global consistency\n\nSlide Topic: Future Work and Potential Improvements\nSummary:\n- Incorporate higher-order inference methods.\n- Relax strong independence assumptions.\n- Combine span-based architecture with self-attention layers.\n- Enhance contextualization for better performance.",
    "Slide Topic: Introduction to Incremental Domain Adaptation in Machine Translation\nSummary:\n- Enhances statistical and neural machine translation systems.\n- Particularly improves neural machine translation (NMT) systems.\n- Applied in computer-aided translation (CAT) scenarios.\n- User and machine translation system collaborate.\n- User translations immediately adapt the machine translation system.\n- Tailors the system to specific translation tasks.\n\nSlide Topic: Motivation for Measuring Immediate Adaptation in Interactive Translation\nSummary:\n- Perceived performance is crucial for user acceptance.\n- Repeated corrections of errors cause user frustration.\n- Lexical choice errors are particularly noticeable.\n- Users expect correct translation of domain-specific terms.\n- Manual correction should lead to improved future translations.\n- Immediate adaptation enhances user satisfaction and system trust.\n\nSlide Topic: Proposed Metrics for Measuring Zero-Shot and One-Shot Vocabulary Acquisition\nSummary:\n- Three metrics proposed for vocabulary acquisition measurement.\n- One metric for one-shot acquisition.\n- One metric for zero-shot acquisition.\n- One metric for both acquisition types.\n- Metrics measure recall of target-language content words.\n- Content words are defined as non-stop words.\n\nSlide Topic: Evaluation of Different Adaptation Techniques on Online Adaptation Datasets\nSummary:\n- English to German NMT system based on Transformer architecture\n- Trained on 100M bilingual sentence pairs using in-house framework\n- Model: 6 encoder layers, 3 decoder layers, 8 attention heads per layer\n- Attention head dimensionality 256, distinct input/output embeddings\n- Vocabulary size around 40,000, generated with byte-pair encoding\n- Adaptation uses learning rate \u03b3 of 10\u22122, no dropout, no label smoothing\n\nSlide Topic: Analysis of Adaptation Techniques' Impact on Zero-Shot and One-Shot Recall\nSummary:\n- Lasso technique improves R0 over baseline.\n- Lasso shows lower one-shot recall than other methods.\n- Lasso often needs multiple exposures to acquire terms.\n- Raw subword output results are shown in Table 4.\n- Lasso's R0 slightly below baseline with raw output (-1%).\n- Findings for R1 and R0+1 remain consistent with regular metric.\n\nSlide Topic: Comparison of Traditional MT Metrics with Proposed Metrics\nSummary:\n- Proposed metrics R0, R1, and R0+1 measure immediate adaptation performance.\n- Lasso outperforms other techniques in zero-shot recall (R0) and combined recall (R0+1).\n- Full has the highest one-shot recall (R1) on two out of five test sets.\n- Full is a close runner-up to lasso in all other test sets.\n- Adaptation degrades zero-shot recall (R0).\n- One-shot recall (R1) improves with adaptation.\n\nSlide Topic: Behavior of Adaptation Methods Over Time\nSummary:\n- Plots show cumulative score differences of adapted systems vs baseline.\n- Metrics R0 and BLEU provide distinct signals for data.\n- R0 shows significant noise impact around segment 800.\n- Noise in segment 774 affects adapted systems but not baseline.\n- BLEU score dips at segment 752, showing 10% relative degradation.\n- Dip due to pathological repetition of a single character in output.",
    "Slide Topic: Introduction to Presuppositions and Adverbial Triggers\nSummary:\n- Presuppositions are shared beliefs in discourse.\n- Include definite descriptions, factive verbs, and adverbs.\n- 'Again' in \"John is going to the restaurant again\" implies prior visits.\n- Adverbial triggers like 'again', 'also', and 'still' are key.\n- These adverbs create assumptions about prior states or events.\n- Focus is on how adverbial triggers shape discourse understanding.\n\nSlide Topic: Task Definition and Dataset Creation\nSummary:\n- Two datasets created from Penn Treebank and English Gigaword corpora.\n- Extract contexts with presupposition triggers and without.\n- Form binary classification task.\n- Focus on well-known adverb triggers in linguistic literature.\n- Control for class balance and syntactic governor of triggers.\n- Ensure models understand presuppositional properties, not just correlate factors.\n\nSlide Topic: Baseline Models and Attention Mechanisms\nSummary:\n- Tested logistic regression and deep learning classifiers.\n- Deep learning methods include RNNs and CNNs.\n- Attention mechanisms weigh information from multiple context points.\n- They infer long-range dependencies in data effectively.\n- Attention-based RNNs predict article definiteness successfully.\n- Outperforms standard models in handling presupposition triggers.\n\nSlide Topic: Proposed Weighted-Pooling Attention Mechanism\nSummary:\n- Introduces weighted pooling for predicting adversarial presupposition triggers.\n- Weighs RNN hidden states based on inputs, not simple averaging.\n- Aggregates learned attention scores through a reweighting scheme.\n- Adds a new level of attention without increasing parameters.\n- Reduces computational impact compared to LSTM baseline.\n- Enhances model's focus on relevant input features.\n\nSlide Topic: Experimental Setup and Hyperparameter Tuning\nSummary:\n- 64 units in fully connected layers\n- 40 units for POS embeddings\n- Dropout probability set to 0.5\n- Mini-batch size of 64 used\n- Word embeddings initialized with 300-size word2vec\n- Adam algorithm for fine-tuning model parameters\n\nSlide Topic: Performance Comparison of Models\nSummary:\n- Our model outperforms baselines for most triggers on two datasets.\n- Achieves 82.42% accuracy predicting 'also' on Gigaword dataset.\n- Introduces task of predicting adverbial presupposition triggers.\n- Presents new datasets with a reusable data extraction method.\n- Develops new attention mechanism in RNN for better prediction.\n- Improved performance without introducing additional parameters.\n\nSlide Topic: Analysis of Model Predictions\nSummary:\n- Model correctly predicts presupposition in complex sentences.\n- Uses PTB dataset for analysis.\n- Identifies 'still' as modifying 'place' via 'continue'.\n- Demonstrates handling of complex coreference.\n- Incorrectly predicts presupposition in another case.\n- Highlights subtlety and difficulty in prediction task.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Investigated predicting adverbial presupposition triggers\n- Introduced datasets for the prediction task\n- Presented a novel weighted-pooling attention mechanism\n- Model outperforms CNN and LSTM without extra parameters\n- Standard LSTM model is the base for our improvements\n- Future work: focus on models for data imbalance scenarios",
    "Slide Topic: Introduction to Deep Learning in Question Answering\nSummary:\n- Deep learning enhances question answering across various data types.\n- Models answer questions about images, tables, and text passages.\n- Stakeholders seek to understand these models' capabilities.\n- Model goodness is measured by error rate on test sets.\n- High accuracy indicates a good model.\n- Test set must reflect real-world tasks for valid assessment.\n\nSlide Topic: Sensitivity Analysis of Deep Learning Models to Question Words\nSummary:\n- Analyzes model sensitivity to question words using attribution.\n- Generates adversarial questions to test model robustness.\n- Finds model often ignores many question words.\n- Relies heavily on images for producing answers.\n- Maintains over 50% accuracy with most words removed.\n- Retains accuracy even when only 'color' remains in questions.\n\nSlide Topic: Attribution Techniques and Integrated Gradients\nSummary:\n- Uses Integrated Gradients (IG) to identify influential question words.\n- IG ensures attributions sum to the difference in output probabilities.\n- Identifies variables as influential if changing them alters output probability.\n- Uninfluential variables receive no attribution in IG.\n- IG satisfies properties of completeness and implementation invariance.\n- Enhances interpretability of deep learning system's decision-making process.\n\nSlide Topic: Analysis of Visual Question Answering Models\nSummary:\n- Task: Answer questions about images.\n- Analyzed deep network from Kazemi and Elqursh (2017).\n- Network often ignores many question words.\n- Relies heavily on images to produce answers.\n- Maintains over 50% accuracy with most words removed.\n- Retains accuracy when only 'color' remains in questions.\n\nSlide Topic: Analysis of Question Answering on Tabular Data\nSummary:\n- Analyzes Neural Programmer (NP) system for answering questions on tabular data.\n- NP selects operation sequences akin to SQL queries to find answers.\n- Operation selections are influenced more by content-free words.\n- Content-free words include common prepositions and articles.\n- Important words like nouns and adjectives have less influence.\n- Insights reveal NP's reliance on less informative linguistic cues.\n\nSlide Topic: Analysis of Reading Comprehension Models\nSummary:\n- Task: Answer questions about text paragraphs.\n- Analyzed Yu et al. (2018) network.\n- Network often ignores important words.\n- Jia and Liang (2017) proposed sentence addition attacks.\n- Attacks succeed more when added sentences contain key question words.\n- Findings highlight model's vulnerability to irrelevant information.\n\nSlide Topic: Overstability Test for Question Answering Networks\nSummary:\n- Introduces an overstability test for QA networks.\n- Previous work showed reading comprehension networks are overly stable to semantic changes.\n- Overstability also applies to questions in QA networks.\n- Observed in visual and tabular QA networks as well.\n- Uses attributions to define a general-purpose test.\n- Measures extent of overstability across different network types.\n\nSlide Topic: Crafting Adversarial Questions Using Attribution Insights\nSummary:\n- Leverage weaknesses to perturb questions.\n- Targeted attacks enhance adversarial effectiveness.\n- Attacks succeed 50% more with top-attributed nouns.\n- Added sentences include key nouns for success.\n- Insights aid in constructing more successful attacks.\n- Improve training datasets with these findings.\n\nSlide Topic: Impact of Attributions on Improving Attack Success Rates\nSummary:\n- Insights from attributions enhance attack success rates.\n- Successful attacks modify low/zero attributed content words.\n- Success occurs when added sentences alter key question words.\n- Attacks succeed if content words in questions are absent in context.\n- Attacks fail if high-attributed content words are not in added sentences.\n\nSlide Topic: Implications and Future Directions for Question Answering Models\nSummary:\n- Attribution workflow enhances model quality iteration.\n- Unrealistic attacks reveal genuine QA product weaknesses.\n- Under-reliance on key question terms poses risks.\n- Other QA models likely share similar vulnerabilities.\n- Attribution methods assess extent of such issues.\n- Enhancements needed for safer, more reliable QA systems.",
    "Slide Topic: Introduction to Conversational Interfaces and Their Use Cases\nSummary:\n- Advances in speech and language tech boost conversational interfaces.\n- Interfaces like Alexa, Cortana, and Siri are increasingly common.\n- Users can complete specific tasks, such as playing music.\n- Users can ask questions answered by querying knowledge graphs.\n- Interfaces interact via voice or text for enhanced user experience.\n- Broad applications in homes, offices, and customer service.\n\nSlide Topic: Unified Framework for Q&A and SLU Using Executable Semantic Parsing\nSummary:\n- Proposes executable semantic parsing as a common framework.\n- Unifies Q&A and SLU under a single approach.\n- Q&A parses inputs into machine-readable logical forms.\n- SLU represents user intent and slots in machine-readable form.\n- Enhances interoperability between Q&A and SLU tasks.\n- Improves accuracy and efficiency in both use cases.\n\nSlide Topic: Neural Transition-based Parsing Model with Stack-LSTMs\nSummary:\n- Follows transition-based parsing paradigm\n- Words not consumed from buffer\n- No strict alignment between input words and tree nodes\n- Buffer encoded using Bi-LSTM\n- Stack encoded using Stack-LSTM\n- Stack-LSTM allows push and pop operations\n\nSlide Topic: Handling Rare and Unknown Words with Copy Mechanism\nSummary:\n- Data sparsity problem addressed by delexication.\n- Delexication replaces rare and unknown words with coarse categories.\n- Copy mechanism allows decoder to copy rare words from input.\n- Copy mechanism avoids generating rare words from limited vocabulary.\n- Performance compared across datasets in Table 2.\n- Copy mechanism generally outperforms delexication.\n\nSlide Topic: Transfer Learning Techniques for Low-Resource Domains\nSummary:\n- Large training data available for some domains.\n- Little training data for new domains.\n- Investigates pre-training and multi-task learning approaches.\n- Multi-task learning shares most architecture.\n- Output layers differ for each task in multi-task learning.\n- Aims to bootstrap parser for low-resource domains.\n\nSlide Topic: Data Sets and Their Characteristics for Q&A and SLU\nSummary:\n- Overnight dataset annotated with Lambda DCS\n- Covers eight domains: calendar, blocks, housing, restaurants, publications, recipes, social network, basketball\n- NLmaps dataset contains over 2,000 geographical questions\n- Questions in NLmaps retrieved from OpenStreetMap\n- Both datasets used for Q&A and SLU tasks\n- Diverse data supports varied query understanding and response generation\n\nSlide Topic: Experimental Results and Analysis on Different Data Sets\nSummary:\n- Experiments conducted on single-task semantic parsing.\n- Three data sources compared for performance differences.\n- Impact of attention mechanism on performance analyzed.\n- Del lexicalization vs. copy mechanism for data sparsity evaluated.\n- Exact match accuracy used as evaluation metric.\n- Ratio of correctly parsed sentences defines accuracy.\n\nSlide Topic: Impact of Attention Mechanism on Parsing Performance\nSummary:\n- Uses additive attention to focus on relevant buffer words.\n- Attention mechanism not consumed as in traditional parsers.\n- Ablation experiments show variable impact on different datasets.\n- Beneficial for larger datasets.\n- Potentially harmful for smaller datasets.\n- Highlights attention's conditional effectiveness in parsing.\n\nSlide Topic: Comparison of Delexicalization and Copy Mechanism for Data Sparsity\nSummary:\n- Delexicalization replaces rare words with coarse categories.\n- Copy mechanism allows the decoder to copy rare words from input.\n- Both methods address data sparsity issues.\n- Performance comparison across datasets shown in Table 2.\n- Copy mechanism generally outperforms delexicalization.\n- Results consistent across all datasets.\n\nSlide Topic: Conclusions and Future Work on Transfer Learning Across Heterogeneous Data Sets\nSummary:\n- Framed SLU as executable semantic parsing, addressing commercial SLU system limitations.\n- Demonstrated framework effectiveness for Q&A and SLU across different datasets.\n- Explored learning in low-resource domains using high-resource domains.\n- Showed system effectiveness with pre-training and MTL across various domains.\n- Preliminary results suggest potential in transfer learning across heterogeneous datasets.\n- Future work needed to further explore transfer learning in diverse data sets.",
    "Slide Topic: Introduction to Stance Classification\nSummary:\n- Identifies users' positions on specific topics.\n- Analyzes text to recognize stance towards a target.\n- Example: Sentence stance on climate change concern.\n- Traditionally, uses target-specific classifier models.\n- Classifier trained for predictions on same target.\n- Aims for automated stance recognition accuracy.\n\nSlide Topic: Traditional vs. Cross-Target Stance Classification\nSummary:\n- Traditional method trains target-specific classifiers.\n- Classifiers predict stance on the same target.\n- Cross-target classification adapts from related targets.\n- Leverages knowledge from existing targets for prediction.\n- More challenging due to potential model incompatibility.\n- Enhances flexibility but increases complexity.\n\nSlide Topic: CrossNet Model Architecture\nSummary:\n- Focuses on cross-target stance classification\n- Explores model generalization between related domains\n- Learns domain-specific aspects from source targets\n- Applies learned aspects to predict on destination targets\n- Proposes CrossNet, a novel neural model\n- Utilizes self-attention mechanism for implementation\n\nSlide Topic: Embedding and Context Encoding Layers\nSummary:\n- Embedding Layer converts words into dense vectors.\n- Context Encoding Layer captures sentence context.\n- Uses Bi-directional LSTM for encoding.\n- Models sentence dependency on target.\n- Incorporates conditional encoding for enhanced dependency modeling.\n\nSlide Topic: Aspect Attention Layer\nSummary:\n- Implements domain-specific aspect discovery\n- Focuses on cross-target stance inference\n- Key aspects reflect users' major concerns\n- These aspects are core to understanding stances\n- Multiple users mention these aspects in discussions\n- Uses self-attention to highlight core parts of stance-bearing sentences\n\nSlide Topic: Prediction Layer and Model Training\nSummary:\n- MLP predicts stance label based on domain aspect encoding.\n- Softmax outputs predicted probability for each class.\n- Model uses multi-class cross-entropy loss.\n- L2 regularization prevents overfitting.\n- Training optimizes for stance classification accuracy.\n\nSlide Topic: Experimental Setup and Datasets\nSummary:\n- Word embeddings initialized with 200d GloVe vectors from 27B Twitter corpus.\n- Embeddings remain fixed during training.\n- Model trained on 90% source target, validated on 10%.\n- Tested on a separate destination target.\n- Stratified 10-fold cross-validation conducted.\n- Results averaged across folds.\n\nSlide Topic: Classification Performance and Baseline Comparison\nSummary:\n- Table 3 shows results for Women's Rights and American Politics domains.\n- Results include in-target and cross-target performance metrics.\n- Our model outperforms two baselines in almost all cases.\n- Aspect attention mechanism benefits target-level generalization.\n- Improved generalization does not compromise in-target performance.\n- Demonstrates effectiveness across different domains.\n\nSlide Topic: Visualization of Attention Mechanism\nSummary:\n- Model visualizes self-attention on correctly classified tweets.\n- Highlights parts of sentences relevant to domain aspects.\n- Visualization shows model's focus on domain-specific information.\n- Demonstrates model's ability to capture relevant sentence parts.\n- Enhances stance inference across related targets.\n- Validates model's effectiveness in domain-specific analysis.\n\nSlide Topic: Future Work and Extensions\nSummary:\n- Select effective source targets for better destination results.\n- Generalize from multiple source targets, not just one.\n- Incorporate other related sources for knowledge transfer.\n- Evaluate model performance in multilingual scenarios.",
    "Slide Topic: Introduction to Semantic Parsing and CNLVR Dataset\nSummary:\n- Semantic parsing converts language into executable programs.\n- CNLVR dataset aids in studying spurious correlations.\n- Images in CNLVR contain boxes with objects of various attributes.\n- Each image is paired with a complex natural language statement.\n- Task involves determining the truthfulness of the statement.\n- CNLVR enhances understanding of visual reasoning through language.\n\nSlide Topic: Challenges in Weakly-Supervised Semantic Parsing\nSummary:\n- Training from denotations complicates semantic parser training.\n- Search challenge: Algorithm must navigate vast program space.\n- Spuriousness issue: Incorrect programs can yield correct denotations.\n- Learner may be misled by incorrect yet successful programs.\n- Spuriousness has received relatively less attention.\n\nSlide Topic: Abstract Representation for CNLVR\nSummary:\n- Insight: Manual construction of a small lexicon for closed worlds.\n- Lexicon clusters language tokens and program constants.\n- Reduces lexical problem in utterances and programs.\n- Creates partially abstract representation for reasoning.\n- Lexicon augmentation can occur at search time.\n- Utilizes partial programs from earlier iterations.\n\nSlide Topic: Model Architecture and Search Techniques\nSummary:\n- Based on Guu et al. (2017) semantic parser\n- Encoder-decoder architecture defines distribution p\u03b8(z|x)\n- Bidirectional LSTM encodes utterance into contextualized representations\n- Decoder uses feed-forward network with attention over encoder outputs\n- Beam search combats large program space challenge\n- Semantic typing ensures syntactically valid program construction\n\nSlide Topic: Training from Weak Supervision\nSummary:\n- Treat program Z as a latent variable.\n- Maximize p(y | x) through marginalization.\n- Use beam search to find programs in subset B.\n- Basic method fails for CNLVR due to search difficulties.\n- Abstract examples reduce issues of search and spuriousness.\n- Learn from abstract examples to improve training.\n\nSlide Topic: Learning from Abstract Examples\nSummary:\n- Abstract representation for utterances and logical forms defined.\n- Seven abstract clusters correspond to main semantic types.\n- Each cluster linked to a small lexicon of language-program token pairs.\n- Lexicon mappings show canonical expression of program constants in natural language.\n- Abstract examples enhance training of semantic parsers.\n\nSlide Topic: Experimental Evaluation and Results\nSummary:\n- W.+DISC achieves 84.0 accuracy and 65.0 consistency on public test set.\n- On hidden test set, W.+DISC scores 82.5 accuracy and 63.9 consistency.\n- W.+DISC outperforms state-of-the-art by 14.7 accuracy points.\n- RULE parser's accuracy is slightly below MAXENT's.\n- Semantic parsing approach is highly suitable for this task.\n- Results indicate significant improvement over existing methods.\n\nSlide Topic: Error Analysis and Insights\nSummary:\n- Errors linked to complex syntactic and semantic utterances.\n- Half of errors involve multiple modifying clauses or nesting.\n- Model often ignores conditions or misapplies composition operators.\n- Cardinality interpretation issues contribute to errors.\n- Conditions applied to items instead of sets in some cases.\n- Ambiguously phrased utterances also lead to errors.\n\nSlide Topic: Related Work and Future Directions\nSummary:\n- Denotations popular for scaling semantic parsers since the decade's start.\n- Current work focuses on training parsers using denotations.\n- Future work aims to automate lexicon learning.\n- Automation reduces manual effort significantly.\n- Targets scaling to larger domains with language variability.\n- Aims to handle substantial variability in language expressions.",
    "Slide Topic: Introduction to BioASQ Challenge\nSummary:\n- BioASQ is a challenge for semantic indexing, QA, and info extraction.\n- Task B focuses on biomedical question answering.\n- Fifth BioASQ challenge is currently ongoing.\n- 500 questions released in batches of 100 every two weeks.\n- Participants have 24 hours to submit results.\n\nSlide Topic: Task B, Phase B: Question Answering in Biomedical Domain\nSummary:\n- System participated in Task B, Phase B.\n- Given questions and gold-standard snippets.\n- Returns list of answer candidates.\n- Questions categorized: factoid, list, summary, yes/no.\n- Focus on factoid and list questions.\n- Operates in biomedical domain.\n\nSlide Topic: Evaluation Metrics for Factoid and List Questions\nSummary:\n- Factoid questions use ranked answer candidates.\n- Mean Reciprocal Rank (MRR) evaluates factoid responses.\n- List questions are evaluated as answer sets.\n- Precision and recall compare given to standard answers.\n- F1 score is the harmonic mean of precision and recall.\n- F1 score serves as the official evaluation measure.\n\nSlide Topic: Traditional vs. Neural Network Approaches in Biomedical QA\nSummary:\n- Traditional QA systems use a multi-step pipeline.\n- Steps include entity recognition, question classification, and scoring.\n- Requires extensive domain-specific feature engineering.\n- Neural network QA systems are trained end-to-end.\n- Neural systems simplify the architecture, reducing resource needs.\n- Offers a more streamlined approach to biomedical QA tasks.\n\nSlide Topic: Neural Architecture of the Proposed System\nSummary:\n- System uses a neural network for input processing.\n- Inputs include a question and context snippets.\n- Outputs pointers to start and end tokens in context.\n- Core architecture based on FastQA (Weissenborn et al., 2017).\n- Generalizes FastQA's output layer for enhanced functionality.\n- Capable of answering both list and factoid questions.\n\nSlide Topic: Training and Fine-Tuning Process\nSummary:\n- Two-step training process\n- Pre-training on SQuAD using Weissborn et al. (2017) method\n- Fine-tuning on BioASQ dataset\n- Adam optimizer used in both phases\n- Exponentially decaying learning rate applied\n- Initial learning rates: 10\u207b\u00b3 for pre-training, 10\u207b\u2074 for fine-tuning\n\nSlide Topic: Handling Factoid and List Questions with Neural Network\nSummary:\n- Uses sigmoid for start probability, enabling multiple answer spans.\n- Generalizes factoid QA network to handle list questions.\n- Outputs top five answer strings for factoid questions.\n- Employs probability threshold for list question answers.\n- Sigmoid function replaces softmax for better span prediction.\n- Enhances model flexibility across different question types.\n\nSlide Topic: Implementation Details and Training Environment\nSummary:\n- System implemented using TensorFlow\n- Trained on NVIDIA GPU\n- GPU model: GeForce Titan X\n\nSlide Topic: Results and Discussion on BioASQ 5B\nSummary:\n- Results for all five test batches of BioASQ 5B in Table 1.\n- Performance numbers are preliminary, awaiting synonym updates.\n- Single and ensemble systems perform similarly overall.\n- Their ranks are consistently close to each other.\n- Ensemble model outperforms slightly on average.\n- Final performance will reflect updated gold-standard answers.\n\nSlide Topic: Conclusion and Future Work\nSummary:\n- Summarized system design for BioASQ 5B submission.\n- Used neural architecture trained end-to-end on QA task.\n- Novel approach not previously applied to BioASQ questions.\n- Achieved state-of-the-art results on factoid questions.\n- Competitive results on list questions.\n- Future work to further enhance model performance.",
    "Slide Topic: Introduction to Span-Based Neural Constituency Parsing\nSummary:\n- High accuracy and simplicity attract attention.\n- Uses bidirectional RNNs for input sequence modeling.\n- Does not model output tree structure.\n- Output space is decomposable.\n- Enables efficient dynamic programming algorithms.\n- CKY algorithm is applicable for parsing.\n\nSlide Topic: Limitations of Existing Span-Based Parsers\nSummary:\n- Greedy span parsers are fast but explore only one path.\n- Greedy parsers operate in linear time.\n- Chart-based parsers achieve exact search and top accuracy.\n- Chart parsers run in cubic time, limiting their use.\n- Slow performance affects longer sentences and beyond.\n- Efficiency-accuracy trade-off remains a challenge.\n\nSlide Topic: Proposed Linear-Time Span-Based Neural Parser\nSummary:\n- Combines merits of greedy and chart-based approaches\n- Designs a linear-time span-based neural parser\n- Searches over exponentially large space efficiently\n- Uses left-to-right dynamic programming\n- Employs action-synchronous style for parsing\n- Requires (2n - 1) actions for n-word sentences\n\nSlide Topic: Dynamic Programming Score Decomposition\nSummary:\n- Decompose tree score into sum of span scores\n- Follows method by Stern et al. (2017a)\n- Enables efficient tree score calculation\n- Sum individual span scores for total tree score\n- Key property for span-based parsing\n- Enhances computational efficiency in dynamic programming\n\nSlide Topic: Graph-Structured Stack Without Extra Bookkeeping\nSummary:\n- Re-formulated as shift-reduce parser\n- Maintains step index for action-synchronous beam search\n- Eliminates need for extra bookkeeping\n- Introduces alternative implementation method\n- Proves new theorem for deeper insight\n- Enhances understanding of Graph-Structured Stack (GSS)\n\nSlide Topic: Action-Synchronous Beam Search\nSummary:\n- Reduces runtime complexity with incremental parser\n- Maintains top b parsing states at each step\n- Prunes all other states to focus on best options\n- Involves O(n) parsing actions\n- Achieves linear time complexity relative to sentence length\n- Sacrifices exact search for efficiency\n\nSlide Topic: Cube Pruning for Improved Runtime\nSummary:\n- Introduces cube pruning to beam search.\n- Candidates placed in a heap.\n- Retrieves top b states for next time-step.\n- Continues until b items are popped from heap.\n- Enhances runtime efficiency.\n- Runtime improved to O(n b log b).\n\nSlide Topic: Training Methodology and Loss Function\nSummary:\n- Uses Structured SVM for training.\n- Aims to score gold tree t* higher than any other tree t.\n- Margin \u2206(t, t*) ensures correct tree scoring.\n- Introduces loss function penalizing wrong spans crossing gold-tree spans.\n- Employs max-violation update for training.\n- Utilizes beam search for optimization.\n\nSlide Topic: Experimental Results on Penn Treebank and PTB-RST\nSummary:\n- Experiments conducted on Penn Treebank and PTB-RST.\n- Achieves state-of-the-art performance on Penn Treebank.\n- Single model end-to-end system performance highlighted.\n- Demonstrates tractability on longer sequences.\n- Effective for discourse parsing on PTB-RST.\n- Shows superior accuracy and efficiency.",
    "Slide Topic: Introduction to Machine Translation and Semantic Representations\nSummary:\n- Converts source language to fluent target language with same meaning.\n- Uses statistical techniques on parallel data for learning transformations.\n- Learns meaning equivalent representations directly from natural data.\n- Captures syntactic operations like reordering from data.\n- Simplistic view of linguistic data in commonly used methods.\n- Focuses on fluency and semantic accuracy in translation.\n\nSlide Topic: Unified Semantic Representations and AMR\nSummary:\n- Interest in unified semantic representations is growing.\n- Deep analyses with heavy normalization of morphology, syntax, and semantics.\n- Abstract Meaning Representation (AMR) is a novel semantic representation.\n- AMR normalizes sentence-level semantics for better understanding.\n- Influences various natural language understanding and generation tasks.\n- Particularly impacts machine translation accuracy and efficiency.\n\nSlide Topic: Related Work on Deep Language Representations in NLP and MT\nSummary:\n- Extensive research on deep language representations in NLP and MT.\n- Representations abstract language-specific phenomena, aligning different languages.\n- Early machine translation systems used parsing for source sentences.\n- Parsed representations were translated into target language representations.\n- Text generation was applied to produce final translations.\n- Advances continue to improve cross-linguistic understanding and translation accuracy.\n\nSlide Topic: Semantic Representation Based on Logical Form\nSummary:\n- Based on Logical Form (Vanderwende, 2015)\n- LF is a labeled directed graph\n- Nodes correspond to content words\n- Edge labels describe semantic relations\n- Nodes store linguistic info like verb frames\n- Includes definiteness, tense, and more\n\nSlide Topic: Graph-to-String Translation Models\nSummary:\n- Models for semantic-graph-to-string translation developed.\n- Models are discriminative, translation-focused.\n- Structure similar to max entropy language models.\n- Inspired by IBM Models 1, 2 and HMM translation models.\n- Translation viewed as selecting target words in order.\n- Selection based on source language and prior target words.\n\nSlide Topic: Alignment Techniques for Semantic Graph Nodes\nSummary:\n- Techniques for aligning source-side semantic graph nodes to target-side surface words explored.\n- Gibbs sampling used for alignment.\n- Generative HMM-style model attempts direct alignment of target language words to source nodes.\n- Unlike traditional HMM word alignment, likelihood of jumping between nodes depends on graph path.\n- Graph path-based approach enhances alignment accuracy over linear distance-based methods.\n\nSlide Topic: Discriminative Translation Model Implementation\nSummary:\n- Alignment is assumed given for the model.\n- Training uses parser and GIZA++ for surface word alignment.\n- Parser provides source surface token alignment at test time.\n- MT decoder supplies bilingual surface word alignment.\n- Alignments compose between graph nodes and source tokens.\n- Model leverages composed alignments for translation.\n\nSlide Topic: Feature Set for Translation Model\nSummary:\n- Rich semantic representation supports diverse features.\n- Features include lemma, part of speech, and more.\n- Extracts information from aligned graph node (n_a_i).\n- Considers previous graph node (n_a_i-1) and parent node.\n- Analyzes shortest path in semantic graph between nodes.\n- Describes path with length and relations (edges).\n\nSlide Topic: Experimental Setup and Results\nSummary:\n- Tested model in n-best re-ranking experiment.\n- Trained phrase-based MT system for English\u2192French.\n- Used 1 million parallel sentence pairs for training.\n- Produced 1000-best lists for WMT 2009, 2010, and 2013 test sets.\n- System utilized 13 commonly used features.\n- Achieved modest but consistent improvement.\n\nSlide Topic: Conclusion and Future Directions\nSummary:\n- Presented initial attempt at integrating semantic features in SMT.\n- Used discriminative training and broad feature set for model.\n- Captured morphological, syntactic, and semantic information.\n- Current gains are modest.\n- Future work: enhance feature engineering and decoder integration.\n- Potential for significant improvements with further research.",
    "Slide Topic: Overview of the BioASQ Challenge 2017\nSummary:\n- Provides an overview of data issued during the 2017 challenge.\n- Presents systems that participated in the challenge.\n- Evaluates the performance of participating systems.\n- Task 5a: Large-scale semantic indexing.\n- Task 5b: Biomedical semantic QA.\n- Task 5c: Funding information extraction.\n\nSlide Topic: Task 5a: Large-scale Semantic Indexing\nSummary:\n- Classify PubMed documents into MeSH hierarchy concepts.\n- Use unannotated new PubMed articles as test sets.\n- MEDLINE indexers do not annotate test set articles.\n- Evaluate system performance with standard retrieval measures.\n- Include hierarchical measures in performance evaluation.\n- Participants have 21 hours to provide answers per test set.\n\nSlide Topic: Task 5b: Biomedical Semantic QA\nSummary:\n- Aims to challenge systems with large-scale biomedical QA.\n- Covers yes/no, factoid, list, and summary question types.\n- Divided into two phases and five independent batches.\n- Participants receive 100 questions per batch.\n- Systems have 24 hours to submit answers.\n- Tests all stages of the question answering process.\n\nSlide Topic: Task 5c: Funding Information Extraction\nSummary:\n- Introduced in 2017.\n- Focuses on grant information from biomedical articles.\n- Extracts grant IDs and funding agencies.\n- Evaluates systems on ID extraction.\n- Assesses agency extraction accuracy.\n- Measures full-grant extraction performance.\n\nSlide Topic: Systems and Approaches for Task 5a\nSummary:\n- 'Search system' uses UIIMA-based workflow with varied search strategies.\n- 'MZ' systems apply Binary Relevance and Latent Dirichlet Allocation models.\n- 'Sequencer' from TU Darmstadt employs RNNs for sequence prediction.\n- 'DeepMESH' utilizes document-to-vector and tf-idf feature embeddings.\n- 'AUTH' team improved their system with a new ensemble method.\n- Ensemble method in 'AUTH' is based on label frequencies and multi-label stacking.\n\nSlide Topic: Systems and Approaches for Task 5b\nSummary:\n- Basic QA pipeline used MetaMap for query expansion.\n- Basic QA pipeline applied BM25 model for question-document matching.\n- Olelo system utilized NLP components on SAP HANA database.\n- USTB team combined strategies to enrich query terms.\n- Fdu systems used a language model for document retrieval.\n- UNCC team focused on concept and article retrieval using Stanford Parser.\n\nSlide Topic: Systems and Approaches for Task 5c\nSummary:\n- Fudan University team used parsing and sentence classification.\n- Identified positive sentences and scanned for grant IDs and agencies.\n- Employed regular expressions and hand-made rules for identification.\n- 'DZG' systems utilized linear SVM with tf-idf vectors of bigrams.\n- Applied various sequential learning models for task execution.\n- AUT team focused on specific text sections for extraction.\n\nSlide Topic: Evaluation and Results of Task 5a\nSummary:\n- Three batches of Task 5a evaluated independently\n- Used flat and hierarchical evaluation measures\n- DeepMeSH systems outperformed strong baselines\n- AUTH systems also showed superior performance\n- d2v and tf-idf transformations were key\n- Semantic indexing methods proved crucial\n\nSlide Topic: Evaluation and Results of Task 5b\nSummary:\n- Phase A ranked systems by MAP for document, concept, snippet, and RDF triple annotations.\n- Phase B evaluated systems on accuracy for yes/no questions.\n- Phase B used MRR for factoid questions.\n- Phase B applied mean F-measure for list questions.\n- High scores noted in yes/no questions.\n- Room for improvement in factoid and list questions.\n\nSlide Topic: Evaluation and Results of Task 5c\nSummary:\n- Micro-recall used for evaluation\n- Metrics applied to grant IDs, agencies, full-grant extraction\n- Participants outperformed baseline system\n- Overall performance was very good\n- Current techniques show improvement potential\n- Multiple methodologies enhance performance",
    "Slide Topic: Introduction to Phrase-based Statistical Machine Translation (PB-SMT)\nSummary:\n- PB-SMT is a data-driven machine translation approach.\n- Widely used for over a decade.\n- Involves training, tuning, and decoding processes.\n- Training creates core knowledge for translation.\n- Tuning optimizes translation quality.\n- Decoding generates final translated output.\n\nSlide Topic: Moses Toolkit and Its Role in PB-SMT\nSummary:\n- Developed by the Statistical Machine Translation Group at the University of Edinburgh.\n- Provides tools for training, tuning, and decoding in PB-SMT.\n- Facilitates phrase table extraction from alignments.\n- Integrates grow-diag-final-and heuristic for alignment.\n- Supports statistical machine translation processes.\n- Enhances accuracy and efficiency in translation models.\n\nSlide Topic: Word or Phrase Alignment in PB-SMT Training\nSummary:\n- Aligns words/phrases in sentence-aligned bi-corpus\n- Establishes translation relationships between languages\n- Critical for translation quality in PB-SMT\n- Involves mapping source to target language elements\n- One of the most time-consuming training steps\n- Enhances accuracy and relevance of translations\n\nSlide Topic: Probabilistic Alignment Models: IBM Models and HMM\nSummary:\n- Uses probability to find best word or phrase links in parallel sentences.\n- IBM models and HMMs are key implementations.\n- Both use the EM algorithm for alignment.\n- Focus on statistical models for alignment accuracy.\n- GIZA++ implements IBM Models for word alignment.\n- Widely used in machine translation and alignment tasks.\n\nSlide Topic: Associative Alignment Approaches: Anymalign and Cutnalign\nSummary:\n- Associative approaches avoid traditional alignment models.\n- They use statistical measures for independence.\n- Anymalign samples large sub-corpora randomly.\n- Anymalign analyzes word or phrase occurrence distributions.\n- Cutnalign performs bilingual hierarchical sub-sentential alignment.\n- Cutnalign uses recursive binary segmentation for alignment.\n\nSlide Topic: Data Used for Experiments: Chinese-Japanese JPO Patent Corpus\nSummary:\n- Data from Chinese-Japanese JPO Patent Corpus\n- Provided by WAT 2015 for patents task\n- Contains 1 million parallel sentences\n- Covers Chemistry, Electricity, Mechanical Engineering, Physics\n- Sentences are 40 words or less\n- Used for training translation models\n\nSlide Topic: Bilingual Hierarchical Sub-sentential Alignment Method: Cutnalign\nSummary:\n- Cutnalign aligns source and target sentences hierarchically.\n- Uses recursive binary segmentation of alignment matrix.\n- Measures translation link strength between word pairs.\n- Computes optimal clustering of bipartite graph.\n- Segments and aligns sentences sub-sententially.\n- Enhances understanding of translation structures.\n\nSlide Topic: Improvements to Cutnalign for Speed and Efficiency\nSummary:\n- Multi-processing in sampling-based alignment method\n- Multi-processing in hierarchical sub-sentential alignment method\n- Core re-implemented in C for faster execution\n- Approximations in N-cut computation to speed up decisions\n- Reduced search space for efficiency gains\n- Overall alignment process accelerated\n\nSlide Topic: Experimental Setup and Evaluation Metrics\nSummary:\n- Experiments used GIZA++ or MGIZA for alignment.\n- Phrase tables extracted using grow-diag-final-and heuristic.\n- Moses toolkit integrated for alignment and phrase table extraction.\n- Translation quality assessed with BLEU and RIBES metrics.\n- Different alignment methods evaluated in PB-SMT system.\n- System built using Moses, KenLM, and lexicalized reordering model.\n\nSlide Topic: Results and Comparison with GIZA++/Moses Baseline\nSummary:\n- Evaluation results show no significant differences across Moses versions.\n- Anyalign timeouts do not affect evaluation results significantly.\n- Cutalign versions have minimal impact on evaluation outcomes.\n- Training times vary greatly depending on Anyalign timeouts.\n- Fastest training time recorded was 57 minutes.\n- Training is about five times faster than with GIZA++ or MGIZA.",
    "Slide Topic: Introduction to Multi-Task Learning (MTL) in NLP\nSummary:\n- MTL transfers knowledge across tasks effectively.\n- Shares parameters across models in neural architectures.\n- Boosts performance of related tasks.\n- Proven effective in tasks like Machine Translation.\n- Enhances parsing accuracy through shared learning.\n- Maximizes model efficiency and learning.\n\nSlide Topic: Challenges in Low-Resource Language Settings\nSummary:\n- Many NLP tasks struggle with insufficient training data.\n- Data annotation is impractical for all languages.\n- Complex tasks demand extensive annotation efforts.\n- Resource limitations hinder POS tagging and other tasks.\n- Rapid NLP system development is needed for disaster response.\n- Extremely low-resource languages pose significant challenges.\n\nSlide Topic: Proposed Multi-Lingual Multi-Task Architecture\nSummary:\n- Addresses limitations of single transfer models.\n- Combines multiple transfer models in a unified architecture.\n- Utilizes two levels of parameter sharing.\n- Shares character embeddings, CNNs, and LSTM layers across models.\n- Employs different sharing strategies for various transfer schemes.\n- Enhances performance through multi-lingual and multi-task learning.\n\nSlide Topic: Basic Architecture for Sequence Labeling\nSummary:\n- Assigns categorical labels to each token in a sentence.\n- LSTM-CRF models excel in multilingual multitask learning.\n- Requires minimal feature engineering through embeddings.\n- Neural network layers abstracted as vectors for easy component sharing.\n- Character embeddings bridge morphological and semantic information.\n- Facilitates information transfer between languages with similar scripts.\n\nSlide Topic: Parameter Sharing Strategies in the Architecture\nSummary:\n- Base architecture shares character embeddings, Char CNN, and bidirectional LSTM.\n- Different parameter sharing strategies for various transfer schemes.\n- Cross-task transfer uses the same word embedding matrix across tasks.\n- Cross-lingual transfer shares linear and CRF layers among languages.\n- Strategy aims to transfer task-specific knowledge efficiently.\n- Enhances model performance and reduces training data requirements.\n\nSlide Topic: Experimental Setup and Data Sets\nSummary:\n- Name Tagging uses Dutch (NLD) and Spanish (ESP) from CoNLL 2002\n- Name Tagging uses English (ENG) from CoNLL 2003\n- Name Tagging uses Russian (RUS) from LDC2016E95\n- Name Tagging uses Chechen (CHE) from TAC KBP 2017\n- POS Tagging uses English, Dutch, Spanish, and Russian from CoNLL 2017\n\nSlide Topic: Comparison of Different Models in Low-Resource Settings\nSummary:\n- Models compared: Baseline LSTM-CNN, cross-task, and cross-lingual transfer models.\n- Tasks: Dutch, Spanish, and Chechen Name Tagging.\n- Settings: Low-resource conditions.\n- Our model outperforms the monolingual single-task baseline.\n- Visible gains over single transfer models observed.\n- Demonstrates effectiveness in resource-limited environments.\n\nSlide Topic: Qualitative Analysis of Model Performance\nSummary:\n- Baseline and our model trained with 100 main task sentences.\n- Shared character-level networks transfer morphological and semantic information.\n- Baseline fails to identify unseen Dutch word 'Palestijnen'.\n- Our model recognizes 'Palestijnen' due to shared Char CNN.\n- Model leverages similarity to English word 'Palestinians'.\n- Demonstrates improved cross-lingual word recognition.\n\nSlide Topic: Ablation Studies and Sensitivity to Auxiliary Data\nSummary:\n- Conducted ablation studies on Dutch Name Tagging.\n- Varied training sentence numbers for target task.\n- Each component addition usually enhances performance (F-score, %).\n- Impact depends on target task data size.\n- Language-specific layer slightly impairs performance with 10 training sentences.\n- Results summarized in Table 6.\n\nSlide Topic: Related Work and Future Directions\nSummary:\n- Multi-task learning applied in various NLP areas.\n- Combines cross-task and cross-lingual transfer in one architecture.\n- Transfers knowledge from multiple auxiliary tasks simultaneously.\n- Previous work includes machine translation, text classification, and more.\n- Future research: apply to event extraction and semantic role labeling.\n- Focus on tasks involving structure prediction for expansion.",
    "Slide Topic: Introduction to Query Auto-Completion (QAC) and Personalization\nSummary:\n- QAC suggests queries as users type.\n- Saves time and reduces cognitive load.\n- Example: 'mete' suggests 'meters' or 'meteorite'.\n- Personalization adjusts suggestions based on user.\n- User needs and interests influence query likelihood.\n- Enhances user experience with tailored suggestions.\n\nSlide Topic: Overview of the Most Popular Completion (MPC) Algorithm and Its Limitations\nSummary:\n- MPC bases completions on popular queries in training data.\n- It matches specified prefixes to suggest completions.\n- Improvements include using temporal information or user history.\n- Extensions enhance MPC by incorporating additional signals.\n- Standard MPC struggles with unseen query prefixes.\n- Limitations highlight the need for more adaptive algorithms.\n\nSlide Topic: Introduction to the FactorCell Model for Personalized QAC\nSummary:\n- Advances personalized QAC by integrating user personalization with language model efficiency.\n- Addresses handling of rare and unseen query prefixes effectively.\n- Learns from user past queries to adapt future query responses.\n- Utilizes FactorCell model for enhanced performance.\n- Employs embedding vector for transforming language model weights.\n- Uses low-rank matrix for additive transformation in recurrent layer.\n\nSlide Topic: Learning User Embeddings for Personalization\nSummary:\n- Embeddings learned for each user during training.\n- Embeddings capture latent demographic factors.\n- Users with fewer than 15 queries grouped as one entity.\n- User embedding matrix U k\u00d7m learned via back-propagation.\n- Embeddings updated through back-propagation on query selection.\n- Results in k distinct user embeddings for personalization.\n\nSlide Topic: Recurrent Layer Adaptation Techniques: ConcatCell vs. FactorCell\nSummary:\n- Three model architectures compared for recurrent layer adaptation.\n- Unadapted LM serves as the baseline model.\n- ConcatCell concatenates user embedding with character embedding at each step.\n- FactorCell uses a low-rank adaptation matrix to transform recurrent layer weights.\n- FactorCell overcomes bias vector adaptation limitations of previous methods.\n\nSlide Topic: Data and Implementation Details for Experiments\nSummary:\n- Used AOL query data from three months in 2006.\n- First six of ten files for training, 12 million queries.\n- 240,000 queries reserved for tuning and validation.\n- One million queries from 30,000 users for testing.\n- Models trained for six epochs with Adam optimizer.\n- Learning rate set at 10^-3.\n\nSlide Topic: Experimental Results and Analysis\nSummary:\n- Neural models outperform MPC baseline.\n- Personalized models surpass unadapted ones.\n- Factor Cell model excels in both large and small experiments.\n- Gains are primarily for seen prefixes.\n- Both Factor Cell and Concat Cell improve with more user queries.\n- Factor Cell's lead over Concat Cell increases over time.\n\nSlide Topic: Case Studies and Semantic Coherence of Adapted Models\nSummary:\n- Conducted case studies to analyze user embedding information.\n- Compared Factor Cell and Concat Cell models.\n- Factor Cell examples show higher semantic coherence.\n- Factor Cell better identifies relevant queries.\n- Identified high school student-related queries accurately.\n- Included entertainment sources popular among target demographic.\n\nSlide Topic: Comparison with Related Work and MPC Variants\nSummary:\n- Standard MPC cannot handle unseen prefixes.\n- Variants exist that can manage unseen prefixes.\n- Our focus was on personalization's impact on generative models.\n- Did not compare against specific MPC variants.\n- Prior work partitions data by time, not user.\n- User-based partitioning is essential for long-term personalization testing.\n\nSlide Topic: Conclusion and Future Directions for Personalized QAC\nSummary:\n- Personalization improves LSTM model performance.\n- Factor Cell model enhances recurrent layer adaptation.\n- Benefits of personalization grow over time.\n- System builds better user representations with incoming data.\n- Approach can include time as an additional factor.\n- Future work may extend personalization to other layers.",
    "Slide Topic: Introduction to Word Embeddings and Their Limitations\nSummary:\n- Word embeddings enhance NLP tasks like machine translation and question answering.\n- They capture semantic properties of words effectively.\n- Widely used in syntactic parsing and other language understanding tasks.\n- However, they struggle with words having multiple meanings.\n- This limitation hinders precise language processing in complex contexts.\n- Improved models are needed to address these semantic challenges.\n\nSlide Topic: Previous Approaches to Addressing Word Sense Ambiguity\nSummary:\n- Induced word senses from monolingual or bilingual data.\n- Relied solely on statistics from text corpora.\n- Did not utilize knowledge from semantic networks.\n- Induced senses are not easily interpretable.\n- Difficult to map induced senses to lexical resources.\n- Limited practical application due to these constraints.\n\nSlide Topic: Proposed SW2V Model: Integrating Text Corpora and Semantic Networks\nSummary:\n- Conventional methods post-process word embeddings to learn sense embeddings.\n- SW2V directly integrates knowledge from text corpora and semantic networks.\n- SW2V is a neural model designed for simultaneous learning.\n- It targets embeddings for both words and their senses.\n- SW2V enhances representation accuracy for each word sense.\n- This approach avoids limitations of post-processing conventional embeddings.\n\nSlide Topic: Shallow Word-Sense Connectivity Algorithm\nSummary:\n- Produces embeddings for words and senses jointly.\n- Requires corpus with words connected to senses in context.\n- Proposes an unsupervised, shallow algorithm for word-sense connectivity.\n- Algorithm is simple and effective.\n- Can be applied to almost any semantic network.\n- Linear in corpus size, ensuring efficient processing.\n\nSlide Topic: Joint Training of Words and Senses in SW2V\nSummary:\n- Aims to create a shared vector space for words and their senses.\n- Extends conventional word embedding models.\n- Integrates explicit knowledge into the model architecture.\n- Expands input and output layers to include word senses.\n- Leverages intrinsic relationships between words and senses.\n- Enhances representation by jointly training words and their meanings.\n\nSlide Topic: Analysis of Model Components and Configurations\nSummary:\n- Analyze SW2V model components\n- Nine model configurations examined\n- Use CBOW model from word2vec\n- Standard hyperparameters applied\n- 300M-word corpus from UMBE project\n- Semantic network uses BabelNet 3.0\n\nSlide Topic: Evaluation of SW2V on Word Similarity Tasks\nSummary:\n- SW2V evaluated qualitatively and quantitatively.\n- Tested in three different tasks.\n- Compared against standard word-based approaches.\n- Evaluated in word similarity task.\n- Outperforms sense-based comparison systems.\n- Performs better than original word2vec.\n\nSlide Topic: Evaluation of Sense Clustering with SW2V\nSummary:\n- Evaluated on Wikipedia sense clustering task\n- SW2V outperforms all comparison systems\n- Superior to NASAARI and SenseEmbed representations\n- Uses same setup and lexical resource\n- Confirms accurate capture of word sense semantics\n- Demonstrates effectiveness in sense-specific tasks\n\nSlide Topic: Interconnectivity of Word and Sense Embeddings\nSummary:\n- Previous experiments focused on sense embeddings' effectiveness.\n- This experiment tests word and sense embeddings' interconnectivity.\n- SW2V yields best MCS results in both datasets.\n- AutoExtend fails to accurately capture a word's dominant sense.\n- AutoExtend performs worse than random sense selection baseline.\n- Highlights challenges in linking word senses accurately.\n\nSlide Topic: Conclusion and Future Directions\nSummary:\n- Proposed SW2V: neural model for joint training of word and sense vectors.\n- Utilizes text corpora and semantic networks for training.\n- Achieves coherent vector space without post-processing.\n- Scalable to larger semantic networks like BabelNet.\n- Outperforms previous sense-based models in vector coherence.\n- Future work: integrate WSD and Entity Linking for NLP applications.",
    "Slide Topic: Introduction to Search-based Structured Prediction\nSummary:\n- Models generate natural language structures as search problems.\n- Structures include POS tags, syntax trees, translations, semantic graphs.\n- Gained research attention due to competitive accuracy and speed.\n- Performance\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u8fd0\u884c\u65f6\u95f4\u77ed\u3002\n- \u901a\u8fc7\u6a21\u4eff\u53c2\u8003\u7b56\u7565\u5b66\u4e60\u63a7\u5236\u6574\u4e2a\u641c\u7d22\u8fc7\u7a0b\u7684\u968f\u673a\u7b56\u7565\u3002\n- \u9002\u7528\u4e8e\u591a\u79cd\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4efb\u52a1\u3002\n\nSlide Topic: Challenges in Search-based Structured Prediction\nSummary:\n- Reference policy ambiguities exist.\n- Multiple actions lead to optimal structure.\n- Only one action is typically chosen for training.\n- Training and testing discrepancy occurs.\n- Learned policy enters unlearned states during testing.\n- These issues harm generalization and performance.\n\nSlide Topic: Knowledge Distillation in Search-based Structured Prediction\nSummary:\n- Proposes integrated knowledge distillation for two problems.\n- Distills single model from ensemble of baseline models.\n- Matches ensemble's output distribution on reference states.\n- Ensemble explores search space randomly.\n- Single model learns to mimic ensemble's distribution.\n- Focuses on encountered exploration states for learning.\n\nSlide Topic: Ensemble Techniques for Improving Generalization\nSummary:\n- Ensemble techniques address data ambiguities.\n- Aims to enhance model generalization.\n- Uses search-based structured prediction models.\n- Trains multiple models with varied initial weights.\n- Combines models by averaging output distributions.\n- Improves overall prediction accuracy and robustness.\n\nSlide Topic: Distillation from Reference States\nSummary:\n- Changed NLL learning objective in Algorithm 1\n- Introduced distillation loss as in Algorithm 2\n- Model learns from states produced by reference policy\n- Method named \"distillation from reference\"\n- Enhances model training efficiency and accuracy\n- Utilizes reference states for improved learning outcomes\n\nSlide Topic: Distillation from Exploration States\nSummary:\n- Sample search sequences from ensemble.\n- Learn from soft targets on sampled states.\n- Transform policy \u03c0R(s, y) into \u03c0E(s).\n- Sample action a from q(a | s) with temperature T.\n- Temperature T controls distribution sharpness.\n- Named \"distillation from exploration\" due to exploration-based training instances.\n\nSlide Topic: Combining Distillation from Reference and Exploration\nSummary:\n- Distillation from reference guides model to mimic reference policy actions.\n- Distillation from exploration teaches model on diverse states.\n- Both methods enhance generalization from different angles.\n- Combining them aims to further boost performance.\n- Use \u03c0R and \u03c0E to generate training states.\n- Learn p(a | s) on the generated states.\n\nSlide Topic: Experimental Setup and Results for Dependency Parsing\nSummary:\n- Experiments conducted on transition-based dependency parsing and neural machine translation.\n- Stack-LSTM parsing model used for classifier parameterization.\n- Model proposed by Dyer et al. (2015) is utilized.\n- Best iteration and \u03b1 determined on development set.\n- Distillation parser outperforms greedy counterparts.\n\nSlide Topic: Experimental Setup and Results for Neural Machine Translation\nSummary:\n- Experiment uses German-to-English dataset from IWSLT 2014.\n- One-layer LSTM with 256 hidden units for encoder and decoder.\n- Dataset is a small portion for focused evaluation.\n- Distillation translator outperforms baseline models.\n- Experiments confirm feasibility of distilling search-based models.\n- Success indicates potential for improved structured prediction.\n\nSlide Topic: Analysis of Distillation Method's Effectiveness\nSummary:\n- Studied ensemble's behavior on 'problematic' states to assess generalization.\n- Empirically analyzed feasibility of learning fully from distillation loss.\n- Examined effect of \u03b1 in distillation from reference settings.\n- Found learning from distillation loss less sensitive to initialization.\n- Achieved a more stable model through distillation loss learning.",
    "Slide Topic: Introduction to Complaints in Computational Linguistics\nSummary:\n- Complaints express negative mismatch between reality and expectations.\n- They target states, products, organizations, or events.\n- Understanding complaints aids linguists, psychologists, and organizations.\n- Automatic identification of complaints is crucial for NLP applications.\n- Computational approaches to studying complaints are underdeveloped.\n\nSlide Topic: Definition and Importance of Complaints\nSummary:\n- Complaints express violated expectations.\n- They are directed at a party responsible.\n- Complaints are distinct from negative sentiment.\n- They highlight a breach of favorable expectations.\n- Linguists analyze complaints as a speech act.\n- Complaints facilitate addressing and resolving issues.\n\nSlide Topic: Related Work in Linguistics and Marketing\nSummary:\n- Complaints are well-studied in linguistics and marketing.\n- Olshain and Weinbach (1987) define complaints as breached expectations.\n- Linguistic research qualitatively examines complaint types in discourse.\n- Studies analyze complaints in both elicited and naturally occurring speech.\n- V\u00e1squez (2011) analyzed 100 negative reviews on TripAdvisor.\n- Complaints in online reviews often demand reparations or compensation.\n\nSlide Topic: Data Collection and Annotation\nSummary:\n- No existing dataset for annotated complaints in linguistics.\n- Created new dataset of written utterances annotated for complaints.\n- Used Twitter as data source for high self-expression and direct user interaction.\n- Manually assembled list of 93 customer service handles.\n- Collected most recent 3,200 tweets available for download.\n- Randomly sampled tweets addressed to each customer support handle for annotation.\n\nSlide Topic: Feature Analysis for Complaint Identification\nSummary:\n- Used generic linguistic features for text classification\n- Included unigrams, LIWC, and word clusters\n- Analyzed methods for predicting complaints\n- Examined complaint-specific features\n- Conducted univariate Pearson correlation for feature sets\n- Focused on linguistic markers like negations, punctuation, and verbs\n\nSlide Topic: Predictive Models for Complaint Detection\nSummary:\n- Experiment with feature-based approaches for complaint prediction.\n- Use Logistic Regression with Elastic Net regularization.\n- Explore neural methods: MLP and LSTM networks.\n- Bag-of-words features achieve F1 of 77.5 and AUC of 0.866.\n- Combining all features boosts F1 to 78 and AUC to 0.864.\n- Text content alone used for predictive model building.\n\nSlide Topic: Distant Supervision and Domain Adaptation\nSummary:\n- Uses distant supervision to identify extra complaint data.\n- Seven hashtags identified to likely correspond to complaints.\n- Employs pooling and Easy Adapt domain adaptation techniques.\n- Combines distantly supervised data with annotated data.\n- Domain adaptation boosts F1 score to 79.\n- Domain adaptation increases ROC AUC by 0.012.\n\nSlide Topic: Domain-Specific Performance Analysis\nSummary:\n- Models trained with best methods and features assessed.\n- In-domain data used for training.\n- Out-of-domain data added to training set for pooling.\n- Easy Adapt domain adaptation combines in- and out-of-domain data.\n- Adding out-of-domain data improves predictive performance in most cases.\n- Domain adaptation benefits most domains, except one.\n\nSlide Topic: Cross-Domain Experiments and Results\nSummary:\n- Models trained on one domain, tested on others.\n- Additional models trained on all domains except the test domain.\n- Predictive performance is relatively consistent across domains.\n- 'Food & Beverage' shows consistently lower performance.\n- 'Other' domain achieves higher performance.\n- Results based on all available data from other domains.\n\nSlide Topic: Conclusions and Future Research Directions\nSummary:\n- Introduced computational approach for modeling complaints using linguistics and machine learning.\n- Created dataset of annotated English Twitter posts across nine domains.\n- Analyzed syntactic patterns and linguistic markers specific to complaints.\n- Built predictive models achieving up to 79% Macro F1 (0.885 AUC).\n- Future work: Identify complaint targets and use additional context for improved performance.\n- Explore role of complaints in personal conversations and political discourse.",
    "Slide Topic: Introduction to Emotional Analysis in Text\nSummary:\n- Human emotions are complex and unique.\n- Emotions are often described as 'feelings'.\n- Ekman proposed six basic emotions: anger, disgust, fear, guilt, joy, and sadness.\n- Emotional phrases better represent emotions than single words.\n- Identifying and classifying emotional phrases is crucial.\n- This process is vital in Natural Language Processing (NLP).\n\nSlide Topic: Related Work in Emotion Detection and Lexicon Development\nSummary:\n- Straparava and Valitutti (2004) created WORDNET-AFFECT, labeling synsets with emotional attributes.\n- Mohammad et al. (2010) analyzed annotations to understand emotion distribution across parts of speech.\n- SenticNet (Cambria et al., 2010) was inspired by earlier work (Poria et al., 2013) for emotion lexicon development.\n- Balahur et al. (2012) demonstrated that common sense knowledge approaches effectively detect emotions in texts like ISEAR corpus.\n\nSlide Topic: Data Preprocessing and Corpus Preparation\nSummary:\n- Collected emotional statements from ISEAR database\n- Statements categorized by emotion classes\n- Dataset includes 3000 respondents from 37 countries\n- Statements split into sentences and tokenized\n- Tokenized words grouped into trigrams\n- Trigram formation to analyze token context roles\n\nSlide Topic: Context Vector Formation and Affinity Score Calculation\nSummary:\n- Context Windows mapped to vectors in a vector space.\n- Affinity Score calculated for each pair of Context Vectors.\n- Score based on dot product of vectors divided by square of distance.\n- Calculation inspired by Newton's law of gravitation.\n- Used to assess role in classifying emotions.\n- Scores computed for each emotion class.\n\nSlide Topic: Feature Selection and Analysis\nSummary:\n- Sentences POS tagged using Stanford POS Tagger\n- POS tagged Context Windows (PTCW) extracted\n- POS tag sequences from PTCWs named POS Tagged Windows (PTW)\n- 'Fear' emotion class has maximum number of CWs and unique PTCWs\n- 'Anger' class contains maximum number of unique PTWs\n\nSlide Topic: TF and TF-IDF Measures for Context Windows\nSummary:\n- TF and IDF calculated for context windows per emotion class.\n- Guilt shows highest Max_TF and variance scores.\n- Anger and disgust exhibit lowest Max_TF scores.\n- TF-IDF minimum, maximum, and variance calculated per emotion.\n- Guilt emotion has highest Max_TF-IDF.\n- Disgust emotion has lowest Max_TF-IDF.\n\nSlide Topic: Ranking Score of Context Windows\nSummary:\n- Duplicate context windows within the same emotion class were removed.\n- Each context window was assigned a ranking score.\n- Words in each context window were searched in the SentiWordnet lexicon.\n- Scores were considered positive, negative, or both if applicable.\n- Sum of absolute scores for all words in a context window was calculated.\n- Scores were sorted to rank each context window in its emotion class.\n\nSlide Topic: Classification Methods and Results Analysis\nSummary:\n- Accuracies obtained using user-defined test data and 10-fold cross-validation.\n- Bayes Net Classifier achieved 100% accuracy on test data.\n- Bayes Net Classifier achieved 97.91% accuracy on cross-validation data.\n- J48 classifier accuracy: 77% on test data, 83.54% on cross-validation.\n- Naive Bayes Simple accuracy: 92.30% on test data, 27.07% on cross-validation.\n\nSlide Topic: Comparison of Distance Metrics in Classification\nSummary:\n- Bayes Net classifier achieves 100% accuracy on Test data with Chebyshev distance.\n- J48 classifier reaches 84.82% accuracy on Test data using Chebyshev distance.\n- Naive Bayes and Decision Table show 80% and 98.62% accuracy on Test data, respectively.\n- Decision Tree classifier outperforms others on affinity score data.\n- Chebyshev distance yields best results for classification accuracy.\n- Bayes Net and Decision Tree classifiers are top performers with Chebyshev distance.\n\nSlide Topic: Conclusions and Future Work\nSummary:\n- Vector formation applied to Context Windows\n- Calculated TF and TF-IDF measures\n- Affinity score inspired by Newton's law of gravitation\n- Used Bayes Net, J48, Naive Bayes Simple, and Decision Table classifiers\n- Future: Incorporate more lexicons for emotion classification\n- Plan to include associative learning for rule identification"
]