[
    "```json\n{\n  \"Introduction to Abstract Meaning Representation (AMR)\": \"Abstract Meaning Representation (AMR) is a semantic graph representation that abstracts away from the syntactic realization of a sentence, where nodes in the graph represent concepts and edges represent semantic relations between them. AMRs are graphs, rather than trees, because co-references and control structures result in nodes with multiple parents, called reentrancies. For instance, the AMR of Figure 1 (a) contains a reentrancy between finger and he, caused by the possessive pronoun his.\",\n  \"AMR-to-Text Generation and Encoder/Decoder Architectures\": \"AMR-to-text generation is the task of automatically generating natural language from AMR graphs. Attentive encoder/decoder architectures, commonly used for Neural Machine Translation (NMT), have been explored for this task. In order to use sequence-to-sequence models, Konstas et al. (2017) reduce the AMR graphs to sequences, while Song et al. (2018) and Beck et al. (2018) directly encode them as graphs. Graph encoding allows the model to explicitly encode reentrant structures present in the AMR graphs.\",\n  \"Handling Reentrancies in AMR Graphs\": \"While central to AMR, reentrancies are often hard to treat both in parsing and in generation. Previous work either removed them from the graphs, hence obtaining sequential or tree-structured data, while other work maintained them but did not analyze their impact on performance. It is not yet clear whether explicit encoding of reentrancies is beneficial for generation.\",\n  \"Input Representations for AMR Graphs\": \"Graph-structured AMRs are normally represented as rooted and directed graphs. In order to obtain tree structures, it is necessary to discard the reentrancies from the AMR graphs. Sequential AMRs follow Konstas et al. (2017), where the input sequence is a linearized and anonymized AMR graph. Linearization is used to convert the graph into a sequence, effectively losing reentrancy information.\",\n  \"Encoder Architectures for AMR-to-Text Generation\": \"Recurrent Neural Network Encoders use a bidirectional LSTM (BiLSTM) network to encode the sequential linearization. TreeLSTM Encoders assume tree-structured input, so AMR graphs must be preprocessed to respect this constraint. Graph Convolutional Network (GCN) Encoders learn embeddings of nodes in a graph by looking at its nearby nodes, which can naturally be used to encode graphs.\",\n  \"Stacking Encoders for Enhanced AMR-to-Text Generation\": \"We aimed at stacking the explicit source of structural information provided by TreeLSTMs and GCNs with the sequential information which BiLSTMs extract well. In our experiments, we found that applying the structural encoder before the RNN achieves better scores, especially for GCN-based models.\",\n  \"Experimental Setup and Evaluation Metrics\": \"We use both BLEU and Meteor as evaluation metrics. All systems are implemented in PyTorch using the framework OpenNMT-py. Hyperparameters of each model were tuned on the development set of LDC2015E86. We train with SGD with the initial learning rate set to 1 and decay to 0.8. Batch size is set to 100.\",\n  \"Impact of Reentrancies on Model Performance\": \"To test the hypothesis that graph encoders deal better with reentrancies, we evaluate the various scenarios as a function of the number of reentrancies in each example. The gap between the graph encoder and the other encoders is widest for examples with more than six reentrancies, demonstrating that explicitly encoding reentrancies is more beneficial than the overall scores suggest.\",\n  \"Handling Long-range Dependencies in AMR Graphs\": \"When we encode a long sequence, interactions between items that appear distant from each other in the sequence are difficult to capture. The graph encoder always outperforms both the sequential and the tree encoder. The gap with the sequential encoder increases for longer dependencies, indicating that longer dependencies are an important factor in improving results for both tree and graph encoders, especially for the latter.\"\n}\n```",
    "```json\n{\n  \"Introduction to Word Embeddings and Their Analysis\": \"Learned vector representations of words, known as word embeddings, have become ubiquitous throughout natural language processing (NLP) applications. As a result, analysis of embedding spaces to understand their utility as input features has emerged as an important avenue of inquiry, in order to facilitate proper use of embeddings in downstream NLP tasks. Many analyses have focused on nearest neighborhoods, as a viable proxy for semantic information.\",\n  \"Limitations of Neighborhood-Based Semantic Analysis\": \"However, neighborhood-based analysis is limited by the unreliability of nearest neighborhoods (Wendlandt et al., 2018). Further, it is intended to evaluate the semantic content of embedding spaces, as opposed to characteristics of the feature space itself.\",\n  \"Geometric Analysis of Word Embeddings\": \"Geometric analysis offers another recent angle from which to understand the properties of word embeddings, both in terms of their distribution (Mimno and Thompson, 2017) and correlation with downstream performance (Chandrahas et al., 2018). Through such geometric investigations, neighborhood-based semantic characterizations are augmented with information about the continuous feature space of an embedding.\",\n  \"Transformations Applied to Pretrained Embeddings\": \"In this work, we consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and FastText (Bojanowski et al., 2017), and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance.\",\n  \"Affine Transformations and Their Impact\": \"Affine transformations have been previously utilized for post-processing of word embeddings. We use the following affine transformations: translations; reflections over a hyperplane; rotations about a subspace; homotheties. These transformations align with prior work on analyzing and post-processing embeddings for specific tasks, and are highly interpretable with respect to the original embedding space.\",\n  \"Cosine Distance Encoding (CDE) Transformation\": \"Our cosine distance encoding transformation f CDE : R d \u2192 R |V | obfuscates the distribution of features in R d by representing a set of word vectors as a pairwise distance matrix. We use a subset consisting of the distance from each point to the embeddings of the 10K most frequent words from each embedding set, yielding f CDE : R d \u2192 R 10 4. We then use an autoencoder to map this back to R d for comparability.\",\n  \"Nearest Neighbor Encoding (NNE) Transformation\": \"Our nearest neighbor encoding transformation f NNE : R d \u2192 R |V | discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. We take the previously proposed approach of combining the output of f NNE (v) for each v \u2208 V to form a sparse adjacency matrix, which describes a directed nearest neighbor graph.\",\n  \"Evaluation of Transformed Embeddings\": \"In order to measure the contributions of each geometric aspect described in Section 3 to the utility of word embeddings as input features, we evaluate embeddings transformed using our sequence of operations on a battery of standard intrinsic evaluations, which model linguistic information directly in the vector space; and extrinsic evaluations, which use the embeddings as input to learned models for downstream applications.\",\n  \"Analysis and Discussion of Evaluation Results\": \"Comparing within the set of affine transformations, the innocuous effect of rotations, dilations, and reflections on both intrinsic and extrinsic tasks suggests that the models used are robust to simple linear transformations. Extrinsic evaluations are also relatively insensitive to translations, which can be modeled with bias terms, though the lack of learned models and reliance on cosine similarity for the intrinsic tasks makes them more sensitive to shifts relative to the origin.\",\n  \"Conclusion and Future Work\": \"Our findings suggest that methods for training and tuning embeddings, especially for downstream tasks, should explicitly focus on local geometric structure in the vector space. As embeddings are used for increasingly specialized applications, and as recent contextualized embedding methods such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) allow for dynamic generation of embeddings from specific contexts, our findings suggest that work on tuning and improving these embeddings should focus explicitly on local geometric structure in sampling and evaluation methods.\"\n}\n```",
    "```json\n{\n  \"Introduction to Geolocation Prediction in Social Media\": \"Geolocation prediction is one such analytical method that has been studied widely to predict a user location or a document location. Location information is crucially important information for analyses such as disaster analysis, disease analysis, and political analysis. Geolocation prediction has been performed for Wikipedia, Flickr, Facebook, and Twitter. Among these sources, Twitter is often preferred because of its characteristics, which are suited for geolocation prediction. Herein, we propose a neural network model to tackle geolocation prediction in Twitter.\",\n  \"Text-based Approaches for Geolocation Prediction\": \"Probability distributions of words over locations have been used to estimate the geolocations of users. Maximum likelihood estimation approaches and language modeling approaches minimizing KL-divergence have succeeded in predicting user locations using word distributions. Supervised machine learning methods with word features are also popular in text-based geolocation prediction. Multinomial Naive Bayes, logistic regression, hierarchical logistic regression, and a multilayer neural network with stacked denoising autoencoder have realized geolocation prediction from text.\",\n  \"User Network-based Approaches for Geolocation Prediction\": \"Social media often include interactions of several kinds among users. These interactions can be regarded as links that form a network among users. Several studies have used such user network information to predict geolocation. Friend and follower information in Twitter were used to predict user locations with a most frequent friend algorithm, a unified descriptive model, location-based generative models, dynamic Bayesian networks, a support vector machine, and maximum likelihood estimation. Mention information in Twitter is also used with label propagation models and an energy and social local coefficient model.\",\n  \"Metadata-based Approaches for Geolocation Prediction\": \"Metadata such as location fields are useful as effective clues to predict geolocation. Hecht et al. reported that decent accuracy of geolocation prediction can be achieved using location fields. Approaches to combine metadata with texts are also proposed to extend text-based approaches. Combinatory approaches such as a dynamically weighted ensemble method, polygon stacking, stacking, and average pooling with a neural network have strengthened geolocation prediction.\",\n  \"Proposed Neural Network Model for Geolocation Prediction\": \"Our model combines text, metadata, and user network information using a complex neural network. Neural networks have recently shown effectiveness to capture complex representations combining simpler representations from large-scale datasets. We intend to obtain unified text, metadata, and user network representations with an attention mechanism that is superior to the earlier ensemble approaches.\",\n  \"Evaluation of the Proposed Model\": \"We evaluated the models in the following four commonly used metrics in geolocation prediction: accuracy, accuracy@161, median error distance, and mean error distance. The proposed model achieved a 2.8% increase in accuracy and a 2.4% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LR-STACK on TwitterUS. On W-NUT, the proposed model achieved a 4.8% increase in accuracy and a 6.6% increase in accuracy@161 against the counterpart baseline model MADCEL-B-LR-STACK.\",\n  \"Analysis of Attention Probabilities in the Proposed Model\": \"To gain insight into the unification processes, we analyzed the states of two attention layers: Attention U and Attention UN. The model assigns higher probabilities to timeline representations than to other three representations in TwitterUS compared to W-NUT. This finding is reasonable because timelines in TwitterUS consist of more tweets and are likely to be more informative than in W-NUT.\",\n  \"Limitations and Future Directions of the Proposed Model\": \"The proposed model still contains 28-30% errors even in accuracy@161. Errors with high confidences were found in cases when a location field is incorrect due to a reason such as a house move, and when a user tweets a place name of a travel. As future works, we are planning to expand the proposed model to handle multiple locations and a temporal state to capture location changes and states like traveling.\"\n}\n```",
    "```json\n{\n  \"Introduction to Presuppositions and Adverbial Triggers\": \"In pragmatics, presuppositions are assumptions or beliefs in the common ground between discourse participants when an utterance is made. These include definite descriptions, factive verbs, and certain adverbs, among others. For example, consider the statement: 'John is going to the restaurant again.' This statement is only appropriate in the context where 'John has been to the restaurant' is held to be true because of the presence of the presupposition trigger 'again'. Our focus in this paper is on adverbial presupposition triggers such as 'again', 'also', and 'still'.\",\n  \"Task Definition and Dataset Creation\": \"We create two datasets based on the Penn Treebank corpus and the English Gigaword corpus, extracting contexts that include presupposition triggers as well as other similar contexts that do not, in order to form a binary classification task. We focus on these adverbs in our investigation because these triggers are well known in the existing linguistic literature and commonly triggering presuppositions. We control for a number of potential confounding factors, such as class balance, and the syntactic governor of the triggering adverb, so that models cannot exploit these correlating factors without any actual understanding of the presuppositional properties of the context.\",\n  \"Baseline Models and Attention Mechanisms\": \"We test a number of standard baseline classifiers on these datasets, including a logistic regression model and deep learning methods based on recurrent neural networks (RNN) and convolutional neural networks (CNN). Attention is a promising approach to this task because it allows a model to weigh information from multiple points in the previous context and infer long-range dependencies in the data. An attention-based RNN has achieved success in predicting article definiteness, which involves another class of presupposition triggers.\",\n  \"Proposed Weighted-Pooling Attention Mechanism\": \"We introduce a new weighted pooling attention mechanism designed for predicting adverbial presupposition triggers. Our attention mechanism allows for a weighted averaging of our RNN hidden states where the weights are informed by the inputs, as opposed to a simple unweighted averaging. Unlike other attention models, instead of simply averaging the scores to be weighted, our approach aggregates (learned) attention scores by learning a reweighting scheme of those scores through another level (dimension) of attention. Additionally, our mechanism does not introduce any new parameters when compared to our LSTM baseline, reducing its computational impact.\",\n  \"Experimental Setup and Hyperparameter Tuning\": \"After tuning, we found the following hyperparameters to work best: 64 units in fully connected layers and 40 units for POS embeddings. We used dropout with probability 0.5 and mini-batch size of 64. For all models, we initialize word embeddings with word2vec pretrained embeddings of size 300. Unknown words are randomly initialized to the same size as the word2vec embeddings. All neural network performance reported in this paper use fixed embeddings. Fully connected layers in the LSTM, CNN, and WP model are regularized with dropout. The model parameters for these neural networks are fine-tuned with the Adam algorithm.\",\n  \"Performance Comparison of Models\": \"Our model outperforms these baselines for most of the triggers on the two datasets, achieving 82.42% accuracy on predicting the adverb 'also' on the Gigaword dataset. The contributions of this work are as follows: 1. We introduce the task of predicting adverbial presupposition triggers. 2. We present new datasets for the task of detecting adverbial presupposition triggers, with a data extraction method that can be applied to other similar pre-processing tasks. 3. We develop a new attention mechanism in an RNN architecture that is appropriate for the prediction of adverbial presupposition triggers, and show that its use results in better prediction performance over a number of baselines without introducing additional parameters.\",\n  \"Analysis of Model Predictions\": \"Consider the following pair of samples that we randomly choose from the PTB dataset (shortened for readability): 1. ...Taped just as the market closed yesterday, it offers Ms. Farrell advising, 'We view the market here as going through a relatively normal cycle... We continue to feel that the stock market is the @@@@ place to be for long-term appreciation'. In this case, the word 'continue' suggests that adverb 'still' could be used to modify head word 'place'. Our model correctly predicts this sample as containing a presupposition, despite the complexity of the coreference across the text. In the second case, our model falsely predicts the presence of a presupposition, which underlines the subtlety and difficulty of the task at hand.\",\n  \"Conclusion and Future Work\": \"In this work, we have investigated the task of predicting adverbial presupposition triggers and introduced several datasets for the task. Additionally, we have presented a novel weighted-pooling attention mechanism which is incorporated into a recurrent neural network model for predicting the presence of an adverbial presuppositional trigger. Our results show that the model outperforms the CNN and LSTM, and does not add any additional parameters over the standard LSTM model. In future work, we would like to focus more on designing models that can deal with and be optimized for scenarios with severe data imbalance.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Role Labeling (SRL) and Current Limitations\": \"Semantic role labeling (SRL) captures predicate-argument relations, such as 'who did what to whom.' Recent high-performing SRL models are BIO-taggers, labeling argument spans for a single predicate at a time. They are typically only evaluated with gold predicates and must be pipelined with error-prone predicate identification models for deployment.\",\n  \"Proposed End-to-End SRL Model\": \"We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass. Our model builds on a recent coreference resolution model, by making central use of learned, contextualized span representations. We use these representations to predict SRL graphs directly over text spans.\",\n  \"Span-Graph Formulation and Its Advantages\": \"Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models: it can model overlapping spans across different predicates in the same output structure. The span representations also generalize the token-level representations in BIO-based models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features.\",\n  \"Model Architecture and Learning Process\": \"Our model considers the space of possible predicates to be all the tokens in the input sentence, and the space of arguments to be all continuous spans. It decides what relation exists between each predicate-argument pair (including no relation). The model minimizes the negative log likelihood of the gold structure during learning.\",\n  \"Beam Pruning for Efficiency\": \"To overcome the computational impracticality of considering O(n^3 |L|) possible relations, we define two beams B_a and B_p for storing the candidate arguments and predicates, respectively. The candidates in each beam are ranked by their unary score, reducing the overall number of relational factors evaluated by the model to O(n^2 |L|).\",\n  \"Neural Architecture Details\": \"Our model builds contextualized representations for argument spans a and predicate words p based on BiLSTM outputs and uses feedforward networks to compute the factor scores. The scoring functions are implemented with feed-forward networks based on the predicate and argument representations.\",\n  \"Experimental Setup and Results\": \"We experiment on the CoNLL 2005 and CoNLL 2012 benchmarks, using two SRL setups: end-to-end and gold predicates. In the end-to-end setup, our joint model outperforms the previous best pipeline system by an F1 difference of anywhere between 1.3 and 6.0 in every setting.\",\n  \"Analysis of Model Performance\": \"Our model is better at accurately predicting arguments that are farther away from the predicates, likely due to architectural differences. It achieves comparable syntactic agreement to previous syntax-based systems. However, it makes more constraint violations compared to previous systems, particularly in predicting duplicate core arguments.\",\n  \"Comparison with Previous Systems\": \"Compared to previous BIO systems, our new model supports joint predicate identification and is able to incorporate span-level features. It does better at long-range dependencies and agreement with syntactic boundaries, but is weaker at global consistency due to our strong independence assumption.\",\n  \"Future Work and Potential Improvements\": \"In the future, we could incorporate higher-order inference methods to relax the strong independence assumption. It would also be interesting to combine our span-based architecture with self-attention layers for more effective contextualization.\"\n}\n```",
    "```json\n{\n  \"Introduction to Machine Translation and Semantic Representations\": \"The goal of machine translation is to take source language utterances and convert them into fluent target language utterances with the same meaning. Most recent approaches learn transformations using statistical techniques on parallel data. Meaning equivalent representations of words and phrases are learned directly from natural data, as are other syntactic operations such as reordering. However, commonly used methods have a very simple view of the linguistic data.\",\n  \"Unified Semantic Representations and AMR\": \"Recently there has been a resurgence of interest in unified semantic representations: deep analyses with heavy normalization of morphology, syntax, and even semantic representations. In particular, Abstract Meaning Representation (AMR, Banarescu et al. (2013)) is a novel representation of (sentential) semantics. Such representations could influence a number of natural language understanding and generation tasks, particularly machine translation.\",\n  \"Related Work on Deep Language Representations in NLP and MT\": \"There is a large body of related work on utilizing deep language representation in NLP and MT in particular. This is not surprising considering that such representations provide abstractions of many language-specific phenomena, effectively bringing different languages closer together. A number of machine translation systems starting as early as the 1950s therefore used a form of transfer: the source sentences were parsed, and those parsed representations were translated into target representations. Finally text generation was applied.\",\n  \"Semantic Representation Based on Logical Form\": \"Our representation of sentence semantics is based on Logical Form (Vanderwende, 2015). LFs are labeled directed graphs whose nodes roughly correspond to content words in the sentence. Edge labels describe semantic relations between nodes. Additional linguistic information, such as verb subcategorization frames, definiteness, tense etc., is stored in graph nodes as bits.\",\n  \"Graph-to-String Translation Models\": \"We develop models for semantic-graph-to-string translation. These models are essentially discriminative translation models, relying on a decomposition structure similar to both maximum entropy language models and IBM Models 1, 2 (Brown et al., 1993), and the HMM translation model (Vogel et al., 1996). In particular, we see translation as a process of selecting target words in order conditioned on source language representation as well as prior target words.\",\n  \"Alignment Techniques for Semantic Graph Nodes\": \"We have experimented with a number of techniques for aligning source-side semantic graph nodes to target-side surface words. Gibbs sampling. We can attempt to directly align the target language words to the source language nodes using a generative HMM-style model. Unlike the HMM word alignment model (Vogel et al., 1996), the likelihood of jumping between nodes is based on the graph path between those nodes, rather than the linear distance.\",\n  \"Discriminative Translation Model Implementation\": \"For our discriminative model, the alignment is assumed to be given. At training time, it is the alignment produced by the parser composed with GIZA++ surface word alignment. At test time, we compose the alignment between graph nodes and source surface tokens (given by the parser) with the bilingual surface word alignment provided by the MT decoder.\",\n  \"Feature Set for Translation Model\": \"Our semantic representation enables us to use a very rich set of features, including information commonly used by both translation models and language models. We extract a significant amount of information from the graph node n_a_i aligned to the generated word: lemma, part of speech, all bits. We extract the same features from the previous graph node (n_a_i\u22121), from the parent node. We also look at the shortest path in the semantic graph from the previous node to the current one and we extract features which describe it: path length, relations (edges) along the path.\",\n  \"Experimental Setup and Results\": \"We tested our model in an n-best re-ranking experiment. We began by training a basic phrase-based MT system for English\u2192French on 1 million parallel sentence pairs and produced 1000-best lists for three test sets provided for the Workshop on Statistical Machine Translation (Bojar et al., 2013) -WMT 2009, 2010 and 2013. This system had a set of 13 commonly used features. We obtained a modest but consistent improvement.\",\n  \"Conclusion and Future Directions\": \"We have presented an initial attempt at including semantic features in a statistical machine translation system. Our approach uses discriminative training and a broad set of features to capture morphological, syntactic, and semantic information in a single model. Although our gains are not particularly large yet, we believe that additional effort on feature engineering and decoder integration could lead to more substantial gains. We have only scratched the surface of possible integrations.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Problem and Neural Models\": \"Teaching machines to answer arbitrary user-generated questions is a long-term goal of natural language processing. For a wide range of questions, existing information retrieval methods are capable of locating documents that are likely to contain the answer. However, automatically extracting the answer from those texts remains an open challenge. The recent success of neural models at answering questions given a related paragraph suggests they have the potential to be a key part of a solution to this problem.\",\n  \"Pipelined Approach for Paragraph Selection\": \"In this section we propose a pipelined QA system, where a single paragraph is selected and passed to a paragraph-level question answering model. If there is a single source document, we select the paragraph with the smallest TF-IDF cosine distance with the question. Document frequencies are computed using the individual paragraphs within the document. If there are multiple input documents, we found it beneficial to use a linear classifier that uses the same TF-IDF score, whether the paragraph was the first in its document, how many tokens preceded it, and the number of question words it includes as features.\",\n  \"Handling Noisy Labels in Distantly Supervised Data\": \"In a distantly supervised setup we label all text spans that match the answer text as being correct. This can lead to training the model to select unwanted answer spans. To handle this difficulty, we use a summed objective function similar to the one from Kadlec et al. (2016), that optimizes the negative log-likelihood of selecting any correct answer span.\",\n  \"Model Architecture and Components\": \"Our model uses a model with the following layers: Embedding, Pre-Process, Attention, Self-Attention, Prediction, and Dropout. The Embedding layer uses pretrained word vectors and character-derived word embeddings. The Pre-Process layer uses a shared bi-directional GRU to process the question and passage embeddings. The Attention layer uses the Bi-Directional Attention Flow (BiDAF) model to build a query-aware context representation. The Self-Attention layer uses a layer of residual self-attention. The Prediction layer applies a bidirectional GRU and linear layer to predict answer start and end scores. Dropout is applied to the input to all the GRUs and the input to the attention mechanisms at a rate of 0.2.\",\n  \"Confidence Method for Multi-Paragraph Settings\": \"We adapt this model to the multi-paragraph setting by using the un-normalized and un-exponentiated score given to each span as a measure of the model's confidence. At test time we run the model on each paragraph and select the answer span with the highest confidence. We experiment with four approaches to training models to produce comparable confidence scores: Shared-Normalization, Merge, No-Answer Option, and Sigmoid.\",\n  \"Training Methods for Comparable Confidence Scores\": \"We experiment with four approaches to training models to produce comparable confidence scores: Shared-Normalization, Merge, No-Answer Option, and Sigmoid. In all cases, we sample paragraphs that do not contain an answer as additional training points. The Shared-Normalization approach uses a modified objective function where span start and end scores are normalized across all paragraphs sampled from the same context. The Merge approach concatenates all paragraphs sampled from the same context together during training. The No-Answer Option approach allows the model to select a special 'no-answer' option for each paragraph. The Sigmoid approach uses the sigmoid loss objective function.\",\n  \"Experimental Setup and Datasets\": \"We evaluate our approach on four datasets: TriviaQA unfiltered, TriviaQA wiki, TriviaQA web, and SQuAD. For preprocessing, we note that for TriviaQA web we do not subsample as was done by Joshi et al. (2017), instead training on all 530k training examples. We also observe that TriviaQA documents often contain many small paragraphs, so we restructure the documents by merging consecutive paragraphs together up to a target size. We use a maximum paragraph size of 400 unless stated otherwise. Paragraph separator tokens with learned embeddings are added between merged paragraphs to preserve formatting information.\",\n  \"Results and Performance Analysis\": \"On TriviaQA web and TriviaQA wiki, our model is firmly ahead of prior work. We achieve a score of 61.55 EM and 67.61 F1 on the TriviaQA unfiltered dev set, advancing the only prior result reported for this dataset by 10 points. On SQuAD, our shared-norm approach is able to reach a peak performance of 72.37 F1 and 64.08 EM given 15 paragraphs. Our model achieves 59.14 EM and 67.34 F1 on the document-level SQuAD dataset, significantly outperforming the 49.7 EM reported by Chen et al. (2017a).\",\n  \"End-to-End QA System Demonstration\": \"We perform one final experiment that tests our model as part of an end-to-end question answering system. Given a question, we retrieve up to 10 web documents using a Bing web search of the question, and all Wikipedia articles about entities the entity linker TAGME identifies in the question. We then use our linear paragraph ranker to select the 16 most relevant paragraphs from all these documents, which are passed to our model to locate the final answer span. We test the system on questions from the TREC QA tasks, in particular a curated set of questions from Baudi\u0161 (2015). Our model achieves 53.31 accuracy on this dataset, which is a strong proof-of-concept that neural paragraph reading combined with existing document retrieval methods can advance the state-of-the-art on general question answering.\",\n  \"Discussion and Future Directions\": \"We found that models that have only been trained on answer-containing paragraphs can perform very poorly in the multi-paragraph setting. The shared-norm approach consistently outperformed the other methods, especially on SQuAD and TriviaQA unfiltered, where many paragraphs were needed to reach peak performance. Figures 3, 4, and 5 show this technique has a minimal effect on the performance when only one paragraph is used, suggesting the model's per-paragraph performance is preserved. Meanwhile, it can be seen the accuracy of the shared-norm model never drops as more paragraphs are added, showing it successfully resolves the problem of being distracted by irrelevant text. Future work could focus on advancing the sentence and paragraph level reading comprehension abilities of the model, and adding a mechanism to handle document-level coreferences.\"\n}\n```",
    "```json\n{\n  \"Introduction to Query Auto-Completion (QAC) and Personalization\": \"Query auto-completion (QAC) is a feature used by search engines that provides a list of suggested queries for the user as they are typing. For instance, if the user types the prefix 'mete' then the system might suggest 'meters' or 'meteorite' as completions. This feature can save the user time and reduce cognitive load. Personalization relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests.\",\n  \"Overview of the Most Popular Completion (MPC) Algorithm and Its Limitations\": \"Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm. MPC suggests completions based on the most popular queries in the training data that match the specified prefix. One way to improve MPC is to consider additional signals such as temporal information or information gleaned from a users' past queries. However, the standard implementation of MPC cannot handle unseen prefixes.\",\n  \"Introduction to the FactorCell Model for Personalized QAC\": \"Building on the work of Park and Chiba (2017), we consider the task of personalized QAC, advancing current methods by combining the obvious advantages of personalization with the effectiveness of a language model in handling rare and previously unseen prefixes. The model must learn how to extract information from a user's past queries and use it to adapt the generative model for that person's future queries. We leverage the recently introduced FactorCell model that uses an embedding vector to additively transform the weights of the language model's recurrent layer with a low-rank matrix.\",\n  \"Learning User Embeddings for Personalization\": \"During training, we learn an embedding for each of the users. These embeddings are thought to hold latent demographic factors for each user. Users with less than 15 queries in the training data are grouped together as a single entity, leaving k users. The user embeddings matrix U k\u00d7m is learned via back-propagation as part of the end-to-end model. Each person's user embedding is updated via back-propagation every time they select a query.\",\n  \"Recurrent Layer Adaptation Techniques: ConcatCell vs. FactorCell\": \"We consider three model architectures which differ only in the method for adapting the recurrent layer. The first is the unadapted LM, the second is the ConcatCell model, which works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer. The FactorCell model remedies the limitation of adapting just the bias vector by letting the user embedding transform the weights of the recurrent layer via the use of a low-rank adaptation matrix.\",\n  \"Data and Implementation Details for Experiments\": \"Our experiments make use of the AOL Query data collected over three months in 2006. The first six of the ten files were used for training, containing approximately 12 million queries from 173,000 users for an average of 70 queries per user. A set of 240,000 queries from those same users was reserved for tuning and validation. From the remaining files, one million queries from 30,000 users are used to test the models on a disjoint set of users. Models were trained for six epochs using the Adam optimizer with a learning rate of 10^-3.\",\n  \"Experimental Results and Analysis\": \"The neural models do better than the MPC baseline. The personalized models are both better than the unadapted one. The FactorCell model is the best overall in both the big and small sized experiments, but the gain is mainly for the seen prefixes. Both the FactorCell and the ConcatCell show continued improvement as more queries from each user are seen, and the FactorCell outperforms the ConcatCell by an increasing margin over time.\",\n  \"Case Studies and Semantic Coherence of Adapted Models\": \"We conducted case studies to better understand what information is represented in the user embeddings and what makes the FactorCell different from the ConcatCell. The FactorCell model examples are more semantically coherent than the ConcatCell examples. For instance, after searching for 'high school softball' and 'math homework help', the FactorCell model identifies queries that a high school student might make, including entertainment sources and a celebrity entertainer popular with that demographic.\",\n  \"Comparison with Related Work and MPC Variants\": \"While the standard implementation of MPC cannot handle unseen prefixes, there are variants which do have that ability. We did not compare against these specific models because our goal was to show how personalization can improve the already-proven generative neural model approach. Prior work partitions the data by time instead of by user, which is necessary in order to properly test personalization over longer time ranges.\",\n  \"Conclusion and Future Directions for Personalized QAC\": \"Our experiments show that the LSTM model can be improved using personalization. The method of adapting the recurrent layer clearly matters, and we obtained an advantage by using the FactorCell model. The benefits of personalization are immediate and increase over time as the system continues to leverage the incoming data to build better user representations. The approach can easily be extended to include time as an additional conditioning factor.\"\n}\n```",
    "```json\n{\n  \"Introduction to End-to-End Differentiable Functions in NLP\": \"Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization. Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semantically analyzed. Pipelines, which make 'hard' (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing.\",\n  \"Challenges of Integrating Pipelines with Neural Architectures\": \"Pipelines, which make 'hard' (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing. Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing, we argue that pipelines can be treated as layers in neural architectures for NLP tasks.\",\n  \"Overview of Existing Solutions for Backpropagation Through Discrete Decisions\": \"Several solutions are readily available: Reinforcement learning (most notably the REINFORCE algorithm), and structured attention (SA). These methods replace argmax with a sampling or marginalization operation. We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors. The straight-through estimator (STE) treats discrete decisions as if they were differentiable and simply passes through gradients. While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.\",\n  \"Introduction to SPIGOT: Structured Projection of Intermediate Gradients Optimization Technique\": \"This paper introduces a new method, the structured projection of intermediate gradients optimization technique (SPIGOT), which defines a proxy for the gradient of a loss function with respect to the input to argmax. Unlike STE's gradient proxy, SPIGOT aims to respect the constraints in the argmax problem. SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto. We show empirically that SPIGOT works even when the maximization and the projection are done approximately.\",\n  \"Application of SPIGOT in NLP Pipelines\": \"We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle. These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task. The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.\",\n  \"Detailed Explanation of SPIGOT's Mechanism and Implementation\": \"Our aim is to allow a (structured) argmax layer in a neural network to be treated almost like any other differentiable function. This would allow us to place, for example, a syntactic parser in the middle of a neural network, so that the forward calculation simply calls the parser and passes the parse tree to the next layer, which might derive syntactic features for the next stage of processing. The challenge is in the backward computation, which is key to learning with standard gradient-based methods. When its output is discrete as we assume here, argmax is a piecewise constant function. At every point, its gradient is either zero or undefined. So instead of using the true gradient, we will introduce a proxy for the gradient of the loss function with respect to the inputs to argmax, allowing backpropagation to proceed through the argmax layer.\",\n  \"Experiments and Results: Syntactic-then-Semantic Parsing\": \"In this experiment we consider an intermediate syntactic parsing task, followed by semantic dependency parsing as the end task. We empirically evaluate our method with two sets of experiments: using syntactic tree structures in semantic dependency parsing, and using semantic dependency graphs in sentiment classification. Table 1 compares the semantic dependency parsing performance of SPIGOT to all five baselines. Both STE and SPIGOT use hard syntactic features. By allowing backpropagation into the intermediate syntactic parser, they both consistently outperform PIPELINE. On the other hand, when marginal syntactic tree structures are used, SA outperforms PIPELINE only on the out-of-domain PSD test set, and improvements under other cases are not observed. Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain.\",\n  \"Experiments and Results: Semantic Dependencies for Sentiment Classification\": \"Our second experiment uses semantic dependency graphs to improve sentiment classification performance. We are not aware of any efficient algorithm that solves marginal inference for semantic dependency graphs under determinism constraints, so we do not include a comparison to SA. Table 2 compares our SPIGOT method to three baselines. Pipelined semantic dependency predictions bring 0.9% absolute improvement in classification accuracy, and SPIGOT outperforms all baselines. In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE.\",\n  \"Analysis of Intermediate Model Behavior Under End-Task Training Signal\": \"We examine here how the intermediate model is affected by the end-task training signal. Is the end-task signal able to 'overrule' intermediate predictions? We use the syntactic-then-semantic parsing model as a case study. Table 3 compares a pipelined system to one jointly trained using SPIGOT. We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems' syntactic predictions agree (SAME), or not (DIFF). The second group includes sentences with much lower syntactic parsing accuracy (91.3 vs. 97.4 UAS), and SPIGOT further reduces this to 89.6. Even though these changes hurt syntactic parsing accuracy, they lead to a 1.1% absolute gain in labeled F1 for semantic parsing.\",\n  \"Comparison with Existing Methods and Discussion of SPIGOT's Advantages\": \"Compared to STE, SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain. For PSD, SPIGOT achieves similar performance to STE on in-domain test set, but has a 0.5% absolute labeled F1 improvement on out-of-domain data, where syntactic parsing is less accurate. In this task STE achieves slightly worse performance than a fixed pre-trained PIPELINE. SPIGOT achieves stronger performance than baselines under both settings, and outperforms state-of-the-art systems on semantic dependency parsing.\"\n}\n```",
    "```json\n{\n  \"Introduction to Similarity Search and Semantic Hashing\": \"The problem of similarity search, also called nearest-neighbor search, involves finding documents from a large collection that are most similar to a query document. Semantic hashing is an effective approach for fast similarity search, where every document is represented as a similarity-preserving discrete (binary) hashing code. By calculating pairwise Hamming distances between hashing codes, the similarity between two documents can be evaluated efficiently.\",\n  \"Challenges in Existing Text Hashing Approaches\": \"Existing text hashing approaches typically require two-stage training procedures, which can result in suboptimal local optima because different modules within the model are optimized separately. Additionally, binary constraints are often handled ad hoc by truncation, leading to the loss of information contained in continuous representations during the separate binarization process.\",\n  \"Proposed Neural Variational Inference Framework for Text Hashing\": \"We propose a neural variational inference framework for text hashing that learns binary latent codes for documents in an end-to-end manner. This framework represents binary (hashing) codes as either deterministic or stochastic Bernoulli latent variables, optimizing the inference and generative networks jointly by maximizing a variational lower bound to the marginal distribution of input documents.\",\n  \"Training with Binary Latent Variables\": \"To estimate the parameters of the encoder and decoder networks, we maximize a variational lower bound, which encourages the approximate posterior distribution to be close to the multivariate Bernoulli prior. The gradients for binary latent variables are estimated using the straight-through (ST) estimator, allowing the first loss term to be backpropagated into the encoder network to fine-tune the hash function.\",\n  \"Injecting Data-dependent Noise to Latent Variables\": \"Inspired by rate-distortion theory, we propose to inject data-dependent noise into the latent codes during the decoding stage. This approach adaptively accounts for the tradeoff between minimizing rate (number of bits used) and distortion (reconstruction error) during training, leading to stronger retrieval performance.\",\n  \"Supervised Hashing Extension of NASH\": \"The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing by learning a mapping from latent variable z to labels y. A joint training objective is employed to balance between maximizing the variational lower bound and minimizing the discriminative loss.\",\n  \"Experimental Setup and Datasets\": \"We use three standard publicly available datasets for training and evaluation: Reuters21578, 20Newsgroups, and TMC. The model is trained using Adam with a learning rate of 1 \u00d7 10\u22123, and dropout is employed on the output of encoder networks. The dimension of z is set to 8, 16, 32, 64, or 128.\",\n  \"Experimental Results and Analysis\": \"Our NASH models consistently outperform baseline methods on all datasets, indicating that our model effectively assigns similar hashing codes to documents with the same label. The injection of noise into the decoder networks improves the robustness of learned binary representations, with data-dependent noise achieving even better results.\",\n  \"Ablation Study and Effect of Stochastic Sampling\": \"An ablation study shows that stochastic sampling of latent variables improves the learned representations. Increasing the non-linearity of the encoder network leads to better empirical results, while a non-linear decoder network can lead to less meaningful binary representations.\",\n  \"Qualitative Analysis of Learned Semantic Information\": \"The learned matrix E in our NASH model effectively groups semantically similar words together, demonstrating that the proposed generative framework can abstract useful semantic information from documents despite the binary/discrete constraint.\"\n}\n```",
    "```json\n{\n  \"Introduction to Cross-Lingual Transfer Learning\": \"Cross-lingual transfer learning is a technique which can compensate for the dearth of data, by transferring knowledge from high-to low-resource languages. Most methods proposed for cross-lingual transfer rely on a single source language, which limits the transferable knowledge to only one source. However, to the best of our knowledge, none of these approaches adequately account for the quality of transfer, but rather 'weight' the contribution of each language uniformly.\",\n  \"Challenges in Cross-Lingual Transfer Learning\": \"Cross-lingual transfer does not work out of the box, especially when using large numbers of source languages, and distantly related target languages. Simple methods such as uniform ensembling do not work well in an NER setting using a collection of 41 languages.\",\n  \"Proposed Zero-Shot Multilingual Transfer Method (BEA uns)\": \"We propose a novel method for zero-shot multilingual transfer, inspired by research in truth inference in crowd-sourcing, a related problem, in which the 'ground truth' must be inferred from the outputs of several unreliable annotators. Our proposed model adapts these ideas to a multilingual transfer setting, whereby we learn the quality of transfer, and language-specific transfer errors, in order to infer the best labelling in the target language, as part of a Bayesian graphical model.\",\n  \"Handling Few-Shot Transfer with RaRe\": \"We propose an alternative way of exploiting the limited annotations, RaRe, which first ranks the systems, and then uses the top ranked models' outputs alongside the gold data to retrain a model on the target language. This process is implemented as minibatch scheduling, where the labels for each minibatch are randomly sampled from transfer model h with probability \u03c9 h. Finally, the model is fine-tuned using the small supervised dataset, in order to correct for phenomena that are not captured from model transfer.\",\n  \"Experimental Setup and Data\": \"Our primarily evaluation is over a subset of the Wikiann NER corpus, using 41 out of 282 languages, where the languages were chosen based on their overlap with multilingual word embedding resources. For language-independent word embedding features we use fastText 300 dimensional Wikipedia embeddings, and map them to the English embedding space using character-identical words as the seed for the Procrustes rotation method for learning bilingual embedding spaces from MUSE.\",\n  \"Results and Analysis of Direct Transfer\": \"Using a single model for direct transfer (English: en) is often a terrible choice. The oracle choice of source language model does much better, however it is not always a closely related language. The role of script appears to be more important than language family, as seen for Slavic languages where direct transfer works well between pairs languages using the same alphabet, but much more poorly when there is an alphabet mismatch.\",\n  \"Impact of Source Language Choice on Transfer Performance\": \"An important question is how the other models, particularly the unsupervised variants, are affected by the number and choice of source languages. MV ent, BEA ent sup, and RaRe use a small labeled dataset to rank the source models. BEA ent uns, oracle has the access to the perfect ranking of source models based on their real F1 on the test set. BEA uns\u00d72 is completely unsupervised in that it uses its own estimates to rank all source models.\",\n  \"Comparison with Related Work\": \"Two main approaches for cross-lingual transfer are representation and annotation projection. Representation projection learns a model in a high-resource source language using representations that are cross-linguistically transferable, and then directly applies the model to data in the target language. Annotation projection has been applied to POS tagging, and transfer from multiple source languages has been shown to improve over single source transfer, however, multi-source transfer might be noisy as a result of divergence in script, phonology, morphology, syntax, and semantics between the source languages, and the target language.\",\n  \"Conclusion and Future Work\": \"We proposed two new multilingual transfer models (RaRe and BEA), based on unsupervised transfer, or a supervised transfer setting with a small 100 sentence labelled dataset in the target language. Our unsupervised method, BEA uns, provides a fast and simple way of annotating data in the target language, which is capable of reasoning under noisy annotations, and outperforms several competitive baselines. Light supervision improves performance further, and our second approach, RaRe, based on ranking transfer models and then retraining on the target language, results in further and more consistent performance improvements.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Parsing and Surface Realization\": \"The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, e.g. Abstract Meaning Representation (AMR; Banarescu et al., 2013), Elementary Dependency Structure (EDS; Oepen and L\u00f8nning, 2006), and Dependency-based Minimal Recursion Semantics (DMRS; Copestake, 2009). Still underexploited is a formal framework for manipulating graphs that parallels automata, tranducers, or formal grammars for strings and trees.\",\n  \"Graph Grammar and DAG Automata in NLP\": \"Two such formalisms have recently been proposed and applied for NLP. One is graph grammar, e.g., Hyperedge Replacement Grammar (HRG; Ehrig et al., 1999). The other is DAG automata, originally studied by Kamimura and Slutzki (1982) and extended by Chiang et al. (2018). In this paper, we study DAG transducers in depth, with the goal of building accurate, efficient yet robust natural language generation (NLG) systems.\",\n  \"Type-Logical Semantic Graphs and Their Characteristics\": \"The meaning representation studied in this work is what we call type-logical semantic graphs, i.e., semantic graphs grounded under type-logical semantics (Carpenter, 1997), one dominant theoretical framework for modeling natural language semantics. In this framework, adjuncts, such as adjective and adverbal phrases, are analyzed as (higher-order) functors, the function of which is to consume complex arguments (Kratzer and Heim, 1998). This makes the typological structure quite flat rather than hierarchical, which is an essential distinction between natural language semantics and syntax.\",\n  \"Challenges with Existing DAG Transducers\": \"To the best of our knowledge, the only existing DAG transducer for NLG is the one proposed by Quernheim and Knight (2012). Quernheim and Knight introduced a DAG-to-tree transducer that can be applied to AMR-to-text generation. This transducer is designed to handle hierarchical structures with limited reentrencies, and it is unsuitable for meaning graphs transformed from type-logical semantics. Furthermore, Quernheim and Knight did not describe how to acquire graph recognition and transduction rules from linguistic data, and reported no result of practical generation.\",\n  \"Proposed DAG Transducer Design\": \"We borrow ideas from declarative programming, another programming paradigm, which describes what a program must accomplish, rather than how to accomplish it. We propose a novel DAG transducer to perform graph-to-program transformation. The input of our transducer is a semantic graph, while the output is a program licensed by a declarative programming language rather than linguistic structures. By executing such a program, we can easily get a surface string.\",\n  \"Declarative Programming Language for String Concatenation\": \"The syntax in the BNF format of our declarative programming language, denoted as Lc, for string calculation is: \u27e8program\u27e9 ::= \u27e8statement\u27e9* \u27e8statement\u27e9 ::= \u27e8variable\u27e9 = \u27e8expr\u27e9 \u27e8expr\u27e9 ::= \u27e8variable\u27e9 | \u27e8string\u27e9 | \u27e8expr\u27e9 + \u27e8expr\u27e9 Here a string is a sequence of characters selected from an alphabet (denoted as \u03a3out) and can be empty (denoted as \u03f5). The semantics of '=' is value assignment, while the semantics of '+' is string concatenation. The value of variables are strings.\",\n  \"Transduction Process: Recognition and Instantiation\": \"The transduction consists of two steps: Recognition and Instantiation. The goal of the Recognition step is to find an edge labeling function \u03c1: E \u2192 Q which satisfies that for every node v, \u03c1(in(v)) \u2113(v) \u2212\u2212\u2192 \u03c1(out(v)) matches the recognition part of a rule in R. The Instantiation step involves using rule(v) to denote the rule used on node v. Assume s is the generation part of rule(v). For every edge e_i adjacent to v, assume \u03c1(e_i) = l(n, d). We replace L with \u03c8(\u2113(v)) and replace every occurrence of v_l(j,d) in s with a new variable x_ij (1 \u2264 j \u2264 n). Then we get a newly generated expression for v.\",\n  \"DAG Transduction-based NLG for Different Languages\": \"Different languages exhibit different morphosyntactic and syntactico-semantic properties. For example, Russian and Arabic are morphologically-rich languages and heavily utilize grammatical markers to indicate grammatical as well as semantic functions. On the contrary, Chinese, as an analytic language, encodes grammatical and semantic information in a highly configurational rather than either inflectional or derivational way. Such differences affect NLG significantly. Considering generating Chinese sentences, it seems sufficient to employ our DAG transducer to obtain a sequence of lemmas, since no morphological production is needed. But for morphologically-rich languages, we do need to model complex morphological changes.\",\n  \"Inducing Transduction Rules from EDS Graphs\": \"We present an empirical study on the feasibility of DAG transduction-based NLG. We focus on variable-free MRS representations, namely EDS (Oepen and L\u00f8nning, 2006). The data set used in this work is DeepBank 1.1 (Flickinger et al., 2012). EDS-specific constraints must be kept during transduction, such as ensuring that for a rule I \u03c3 \u2212\u2192 \u27e8O, E\u27e9, a state with direction u in I or a state with direction r in O is called head state and its variables are called head variables. There is at most one head state in a rule and only head variables or S can be the left sides of statements.\",\n  \"Evaluation and Performance Analysis\": \"We use DeepBank 1.1 (Flickinger et al., 2012), i.e., gold-standard ERS annotations, as our main experimental data set to train a DAG transducer as well as a sequence-to-sequence morpholyzer, and wikiwoods (Flickinger et al., 2010), i.e., automatically-generated ERS annotations by ERG, as additional data set to enhance the sequence-to-sequence morpholyzer. The training, development, and test data sets are from DeepBank and split according to DeepBank's recommendation. There are 34,505, 1,758, and 1,444 sentences (all disconnected graphs as well as their associated sentences are removed) in the training, development, and test data sets. The remarkable accuracy, efficiency, and robustness demonstrate the feasibility of applying a DAG transducer to resolve NLG, as well as the effectiveness of our transducer design.\"\n}\n```",
    "```json\n{\n  \"Introduction to Social Sensors and Twitter-based Influenza Surveillance\": \"The increased use of social media platforms has led to wide sharing of personal information. Especially Twitter, a micro-blogging platform that enables users to communicate by updating their status using 140 or fewer characters, has attracted great attention of researchers and service developers because Twitter can be a valuable personal information resource. Among the applications mentioned above, this study particularly examines detection of seasonal influenza epidemics because the influenza detection is a popular application of Twitter. To date, more than 30 Twitter-based influenza detection and prediction systems have been developed worldwide.\",\n  \"Dataset Collection and Preprocessing\": \"We collected 7.7 million influenza related tweets, starting from August 2012 to January 2016, via Twitter API. Then, we filtered noises (removed retweets including the word, RT, and tweets linked to other web pages including the word, http from the collected tweet data). To analyze a word, we applied a Japanese morphological parser (JUMAN) and obtained the stem forms. As a result, 27,588 words were extracted. Then, we investigated the word frequency per day to build a word matrix (days \u00d7 words).\",\n  \"Identification of Forecasting Words and Time Lag Estimation\": \"We assume that a word, 'fever' presents a clue to an upcoming influenza outbreak. Inferring that people are frequently afflicted by symptoms such as 'fever' and 'headache' immediately before the onset and diagnosis of influenza, we designate such words as forecasting words. The problems to be solved are two-fold: (1) estimating the optimal time lag for each forecasting word and (2) incorporating these time lags into the model. For the first problem, the suitable time lag for each word is measured by calculating the cross correlation between the word frequency and the patient number.\",\n  \"Construction of Time-shifted Word Matrix\": \"Given the IDSC reports and wider range of tweets, Cross Correlation is used to search for the most suitable time shift width for each word frequency. After time shifts for all words, a shifted word matrix X is constructed. Figure 2a presents the initial (original) word matrix (\u03c4 = 0 for all words) of 50 words (randomly selected). This matrix includes several low-correlated words, making several vertically irregular lines. In contrast, the time shift operation arranges the irregular words to match the IDSC reports, producing a beautiful horizontal line.\",\n  \"Nowcasting Model: Parameter Estimation and Performance\": \"To construct the linear model (called nowcasting model), the parameter \u03b2 is estimated as minimizing the squared error. In the case of vocabulary size |V|, which is much larger order than sample size T, it has been observed empirically that the prediction performance of l1-penalized regression, the Lasso is dominated by the l2-penalized one. Therefore, we employ the Elastic Net, which combines the l1-penalty and l2-penalty. In the experimentally obtained results, the proposed model achieved the best nowcasting performance to date (correlation ratio 0.93).\",\n  \"Forecasting Model: Extension and Evaluation\": \"Our nowcasting model can be extended naturally to forecasting model. To predict the number of future patients \u2206f days after, we force to shift the word frequency at least \u2206f days. Results of forecasting accuracy are presented in Figure 4. In both models, the accuracy was superior to the baseline until around 3 weeks into the future. In addition, the accuracy for prediction one week into the future was almost identical to that in the case of \u03c4min = 0.\",\n  \"Discussion on Seasonal Variability and Model Limitations\": \"In general, the proposed approach (time shift operation) fitted the IDSC reports, demonstrating the basic feasibility. However, exceptions were apparent, as for the model trained in Season 3. One reason is that a gap exists in the suitable time shift widths between the train (Season 3) and the other (Seasons 1 and 2). This inconsistency of time shifts also affected the forecasting performance directly. In spite of the word 'shot' is the largest weighted feature in the case of \u03c4min = 21 and Train in Season 3, these word correlations were 0.310 in Season 1 and 0.03 in Season 2. Consequently, it caused a considerable decrease of the forecasting accuracy.\",\n  \"Comparison with Related Work and Future Directions\": \"This study proposed a novel social media based influenza surveillance system using forecasting words that appear in Twitter usage before main epidemics occur. This report is the first of the relevant literature describing a model that enables prediction of future epidemics. Furthermore, the model has much room for potential application to prediction of other events. An important difference is that this study handles one more parameter for each word: time shift (days). To handle many parameters, we first ascertain the best time shift widths. Then we explore weight parameters using L1 or elastic net.\"\n}\n```",
    "```json\n{\n  \"Introduction to Satire and Its Characteristics\": \"Satire is a form of art used to criticize in an entertaining manner. It makes use of different stylistic devices, e.g., humor, irony, sarcasm, exaggerations, parody or caricature. The occurrence of harsh, offensive or banal and funny words is typical. Satirical news are written with the aim of mimicking regular news in diction.\",\n  \"The Task of Satire Detection and Its Challenges\": \"The task of satire detection is to automatically distinguish satirical news from regular news. This is relevant, for instance, for downstream applications, such that satirical articles can be ignored in knowledge base population. Solving this problem computationally is challenging. Even human readers are sometimes not able to precisely recognize satire.\",\n  \"Limitations of Previous Work in Satire Detection\": \"Previous work mostly builds on top of corpora of news articles which have been labeled automatically based on the publication source. We hypothesize that such distant labeling approach leads to the model mostly representing characteristics of the publishers instead of actual satire. This has two main issues: First, interpretation of the model to obtain a better understanding of concepts of satire would be misleading, and second, generalization of the model to unseen publication sources would be harmed.\",\n  \"Proposed Model with Adversarial Training\": \"We propose a new model with adversarial training to control for the confounding variable of publication sources, i.e., we debias the model. Our experiments and analysis show that (1) the satire detection performance stays comparable when the adversarial component is included, and (2) that adversarial training is crucial for the model to pay attention to satire instead of publication characteristics.\",\n  \"Dataset Creation and Its Unique Features\": \"We publish a large German data set for satire detection which is a) the first data set in German, b) the first data set including publication sources, enabling the experiments at hand, and c) the largest resource for satire detection so far. The corpus contains articles published between January 1st, 2000 and May 1st, 2018. Each publication has individual typical phrases and different most common words.\",\n  \"Model Architecture and Training Details\": \"We propose to train the publication identifier as an adversary. The feature extractor uses word embeddings and a bidirectional LSTM, followed by a self-attention layer. The satire detector feeds the representation from the feature extractor into a softmax layer and performs a binary classification task. The publication identifier performs a multi-class classification task with 15 publication sources.\",\n  \"Research Questions and Baseline Model\": \"We discuss two questions. RQ1: How does a decrease in publication classification performance through adversarial training affect the satire classification performance? RQ2: Is adversarial training effective for avoiding that the model pays most attention to the characteristics of publication source rather than actual satire? As a baseline model, we train the satire detector part on the satire task and then freeze the weights of the feature extractor to train the publication classifier on top of it.\",\n  \"Evaluation Metrics and Hyperparameter Tuning\": \"For evaluating satire detection, we use precision, recall and F1 score of the satire class. For publication identification, we calculate a weighted macro precision, recall and F1 score. We use Adam with an initial learning rate of 0.0001 and a decay rate of 10^-6. We use mini-batch gradient descent training with a batch size of 32 and alternating batches of the two branches of our model.\",\n  \"Results and Analysis of Adversarial Training Impact\": \"The model with \u03bb = 0.2 achieves a comparable satire classification performance while the publication identification performance drops, especially the precision declines from 44.2% to 30.8%. This suggests that there are overlapping features (cues) for both satire and publication classification. The model with adversarial training is able to find cues for satire, being humor in this example.\",\n  \"Conclusion and Future Directions\": \"We presented evidence that simple neural networks for satire detection learn to recognize characteristics of publication sources rather than satire and proposed a model that uses adversarial training to control for this effect. Future work could investigate the effect of other potential confounding variables in satire detection, such as the distribution of time and region of the articles.\"\n}\n```",
    "```json\n{\n  \"Introduction to Deep Learning in Question Answering\": \"Recently, deep learning has been applied to a variety of question answering tasks. For instance, to answer questions about images, tabular data, and passages of text. Developers, end-users, and reviewers (in academia) would all like to understand the capabilities of these models. The standard way of measuring the goodness of a system is to evaluate its error on a test set. High accuracy is indicative of a good model only if the test set is representative of the underlying realworld task.\",\n  \"Sensitivity Analysis of Deep Learning Models to Question Words\": \"In this paper, we propose techniques to analyze the sensitivity of a deep learning model to question words. We do this by applying attribution (as discussed in section 3), and generating adversarial questions. We find that the network ignores many question words, relying largely on the image to produce answers. For instance, we show that the model retains more than 50% of its original accuracy even when every word that is not 'color' is deleted from all questions in the validation set.\",\n  \"Attribution Techniques and Integrated Gradients\": \"We employ an attribution technique called Integrated Gradients (IG) (Sundararajan et al., 2017) to isolate question words that a deep learning system uses to produce an answer. IG satisfies the condition that the attributions sum to the difference between the probabilities at the input and the baseline. We call a variable uninfluential if all else fixed, varying it does not change the output probability. IG satisfies the property that uninfluential variables do not get any attribution.\",\n  \"Analysis of Visual Question Answering Models\": \"The task is to answer questions about images. We analyze the deep network in Kazemi and Elqursh (2017). We find that the network ignores many question words, relying largely on the image to produce answers. For instance, we show that the model retains more than 50% of its original accuracy even when every word that is not 'color' is deleted from all questions in the validation set.\",\n  \"Analysis of Question Answering on Tabular Data\": \"We analyze a system called Neural Programmer (henceforth, NP) (Neelakantan et al., 2017) that answers questions on tabular data. NP determines the answer to a question by selecting a sequence of operations to apply on the accompanying table (akin to an SQL query; details in section 5). We find that these operation selections are more influenced by content-free words (e.g., 'in', 'at', 'the', etc.) in questions than important words such as nouns or adjectives.\",\n  \"Analysis of Reading Comprehension Models\": \"The task is to answer questions about paragraphs of text. We analyze the network by Yu et al. (2018). Again, we find that the network often ignores words that should be important. Jia and Liang (2017) proposed attacks wherein sentences are added to paragraphs that ought not to change the network's answers, but sometimes do. Our main finding is that these attacks are more likely to succeed when an added sentence includes all the question words that the model found important (for the original paragraph).\",\n  \"Overstability Test for Question Answering Networks\": \"A key contribution of this work is an overstability test for question answering networks. Jia and Liang (2017) showed that reading comprehension networks are overly stable to semantics-altering edits to the passage. In this work, we find that such overstability also applies to questions. Furthermore, this behavior can be seen in visual and tabular question answering networks as well. We use attributions to a define a general-purpose test for measuring the extent of the overstability (sections 4.3 and 5.3).\",\n  \"Crafting Adversarial Questions Using Attribution Insights\": \"We leverage this weakness and perturb questions to craft targeted attacks. For instance, we find that attacks are 50% more likely to be successful when the added sentence includes top-attributed nouns in the question. This insight should allow the construction of more successful attacks and better training data sets.\",\n  \"Impact of Attributions on Improving Attack Success Rates\": \"We use insights derived from attributions to improve the attack success rate of Jia and Liang (2017). For instance, we find that an adversarial sentence is successful in fooling the model in two cases: First, a contentful word in the question gets low/zero attribution and the adversarially added sentence modifies that word. Second, a contentful word in the question that is not present in the context. On the flip side, an adversarial sentence is unsuccessful when a contentful word in the question having high attribution is not present in the added sentence.\",\n  \"Implications and Future Directions for Question Answering Models\": \"We believe that a workflow that uses attributions can aid the developer in iterating on model quality more effectively. While the attacks in this paper may seem unrealistic, they do expose real weaknesses that affect the usage of a QA product. Under-reliance on important question terms is not safe. We also believe that other QA models may share these weaknesses. Our attribution-based methods can be directly used to gauge the extent of such problems.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Network Mappings in Cross-Modal Retrieval\": \"Neural network mappings are widely used to bridge modalities or spaces in cross-modal retrieval, zero-shot learning, building multimodal representations, or in word translation. Typically, a neural network is trained to predict the distributed vectors of one modality from the other. At test time, operations such as retrieval or labeling are performed based on the nearest neighbors of the predicted vectors. The success of such systems relies entirely on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure.\",\n  \"The Phenomenon of Input Vector Preservation in Mapped Vectors\": \"Recent work evidences that considerable information about the input modality propagates into the predicted modality. The mapped vectors often resemble the input vectors more than the target vectors in semantic terms, which goes against the goal of a cross-modal map. This phenomenon is more closely tied to the continuity and topology-preserving properties of neural networks, ensuring that points that are close together are mapped close together.\",\n  \"Proposed Measure for Quantifying Semantic Similarity\": \"To measure the similarity between the neighborhood structure of two sets of paired vectors, we propose the mean nearest neighbor overlap measure (mNNO K (V, Z)). This measure quantifies how semantically similar two sets of paired vectors are, providing a more realistic estimate of the semantic similarity between predicted and target vectors.\",\n  \"Experimental Setup and Datasets\": \"We perform extensive experiments in three benchmarks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki). We use a rich variety of state-of-the-art text and image features and loss functions to learn image-to-text and text-to-image neural net mappings. The datasets include ImageNet, IAPR TC-12, and Wiki, with features such as GloVe, word2vec, and bi-GRU for text, and VGG-128 and ResNet for images.\",\n  \"Results of Experiment 1: Neighborhood Structure Analysis\": \"Our results reveal that the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the target vectors. This is evident in Table 1, where the test mean nearest neighbor overlap (mNNO) between the input vectors and the mapped vectors is higher than that between the target vectors and the mapped vectors.\",\n  \"Results of Experiment 2: Semantic Structure Preservation with Untrained Networks\": \"In a second experiment, we show that the semantic structure of the input vectors is preserved after mapping them with an untrained network. This further evidences that feed-forward nets naturally preserve semantic information about the input, as shown in Table 2, where the performance of mapped and input vectors is compared in concept similarity tasks.\",\n  \"Discussion on the Implications of the Findings\": \"We uncover and raise awareness of a largely ignored phenomenon relevant to a wide range of cross-modal / cross-space applications such as retrieval, zero-shot learning, or image annotation. The findings suggest that neural net cross-modal mappings can produce mapped vectors more akin to the input vectors than the target vectors, in terms of semantic structure.\",\n  \"Advocacy for Semantic-Based Evaluation Criteria\": \"Ultimately, this paper aims to encourage the development of better architectures to bridge modalities / spaces and advocate for the use of semantic-based criteria to evaluate the quality of predicted vectors, such as the neighborhood-based measure proposed here, instead of purely geometric measures such as mean squared error (MSE).\"\n}\n```",
    "```json\n{\n  \"Introduction to the Scientific Publication Lifecycle and Research Challenges\": \"The Scientific Publication Lifecycle comprises of different activities carried out by researchers. Of all these activities, the three main activities are literature review, actual research work and dissemination of results through conferences and journals. These three activities in themselves cover multiple sub-activities that require specific expertise and experience. Prior studies have shown researchers with low experience, face difficulties in completing research related activities. These researchers rely on assistance from supervisors, experts and librarians for learning the required skills to pursue such activities.\",\n  \"Overview of Academic Assistive Systems and Their Role in Research\": \"Apart from human assistance, academic assistive systems have been built for alleviating the expertise gap between experts and novices in terms of research execution. Some of these interventions include search systems with faceted user interfaces for better display of search results, bibliometric tools for visualizing citation networks, and scientific paper recommender systems, to name a few. In the area of manuscript writing, techniques have been proposed to recommend articles for citation contexts in manuscripts. In the context of manuscript publication, prior studies have tried to recommend prospective conference venues most suited for the research in hand.\",\n  \"Rec4LRW System: A Tool for Literature Review and Manuscript Preparation\": \"The Rec4LRW system has been built as a tool aimed to help researchers in two main tasks of literature review and one manuscript preparatory task. The three tasks are (i) Building an initial reading list of research papers, (ii) Finding similar papers based on a set of papers, and (iii) Shortlisting papers from the final reading list for inclusion in manuscript based on article-type choice. The recommendation mechanisms of the three tasks are based on seven features/criteria that represent the characteristics of the bibliography and its relationship with the parent research paper.\",\n  \"Dataset and User Interface Features of Rec4LRW\": \"A snapshot of the ACM Digital Library (ACM DL) is used as the dataset for the system. Papers from proceedings and journals for the period 1951 to 2011 form the dataset. The papers from the dataset have been shortlisted based on full text and metadata availability in the dataset, to form the sample set/corpus for the system. The sample set contains a total of 103,739 articles and corresponding 2,320,345 references. The system displays unique UI features such as author-specified keywords, references count, and short summary of the paper (if the abstract of the paper is missing).\",\n  \"Technique for Shortlisting Papers Based on Article-Type Preference\": \"The objective of this task is to help researchers in identifying important (based on citation counts) and unique papers from the final reading list. These papers are to be considered as potential candidates for citation in the manuscript. For this task, the Girvan-Newman algorithm was used for identifying the clusters in the citations network. The specific goal of clustering is to identify the communities within the citation network. From the identified clusters, the top cited papers are shortlisted. The algorithm is implemented as the EdgeBetweennessClusterer in JUNG library.\",\n  \"User Evaluation Study Design and Participants\": \"An online pre-screening survey was conducted to identify the potential participants. Participants needed to have experience in writing conference or journal paper(s) as a qualification for taking part in the study. All the participants were required to evaluate the three tasks and the overall system. The study was conducted between November 2015 and January 2016. Out of the eligible 230 participants, 116 participants signed the consent form and completed the whole study inclusive of the three tasks in the system. 57 participants were Ph.D./Masters students while 59 were research staff, academic staff and librarians.\",\n  \"Agreement Percentages and Quality Measures from User Evaluation\": \"The agreement percentages (AP) for the seven measures by the participant groups are shown in Figure 2. In the current study, an agreement percentage above 75% is considered as an indication of higher agreement from the participants. As expected, the AP of students was consistently higher than the staff with the biggest difference found for the measures Usefulness (82.00% for students, 64.15% for staff) and Good_List (76.00% for students, 62.26% for staff). The quality measures Importance (85.96% for students, 77.97% for staff) and Shortlisting_Feature (84.21% for students, 74.58% for staff) had the highest APs.\",\n  \"Qualitative Feedback Analysis: Preferred and Critical Aspects\": \"In Table 2, the top five categories of the preferred aspects and critical aspects are listed. Preferred Aspects. Out of the total 116 participants, 68 participants chose to give feedback about the features that they found to be useful. 24% of the participants felt that the feature of the shortlisting papers based on article-type preference was quite preferable and would help them in completing their tasks in a faster and efficient manner. They also felt that the quality of the shortlisting papers was satisfactory. Critical Aspects. Out of the 116 participants, 41 participants gave critical comments about the task and features of the system catering to the task. Around 16% of the participants felt that the study procedure of adding 30 papers to the reading list as a precursor for running the task was uninteresting.\",\n  \"Conclusion and Future Directions for Rec4LRW System\": \"For literature review and manuscript preparatory related tasks, the gap between novices and experts in terms of task knowledge and execution skills is well-known. With the Rec4LRW system, we have attempted to address the aforementioned gap with a novel task for shortlisting articles from researcher's reading list, for inclusion in manuscript. The shortlisting task makes use of a popular community detection algorithm for identifying communities of papers generated from the citations network of the papers from the reading list. Additionally, we have also tried to vary shortlisted papers count by taking the article-type choice into consideration. As hypothesized before the start of the study, students evaluated the task favorably for all measures. There was high level of agreement among all participants on the availability of important papers among the shortlisted papers. This finding validates the aim of the task in identifying the papers that manuscript reviewers would expected to be cited.\"\n}\n```",
    "```json\n{\n  \"Introduction to Deep Learning for Sentence Representations\": \"Deep learning has brought the possibility of automatically learning continuous representations of sentences. These representations can be geared towards particular tasks such as classifying the sentence in various aspects or relating the sentence to other sentences. Alternatively, we can aim at 'universal' sentence representations that perform reasonably well in a range of such tasks. The representations can be learned either in an unsupervised way or supervised, relying on manually constructed training sets of sentences equipped with annotations of the appropriate type.\",\n  \"Overview of Sentence Representation Learning Methods\": \"In the first works on RNN sequence-to-sequence models, researchers provided visualizations of the phrase and sentence embedding spaces and observed that they reflect semantic and syntactic structure to some extent. Hill et al. (2016) perform a systematic evaluation of sentence representation in different models, including NMT, by applying them to various sentence classification tasks and by relating semantic similarity to closeness in the representation space. Shi et al. (2016) investigate the syntactic properties of representations learned by NMT systems by predicting sentence-and word-level syntactic labels and by generating syntax trees from these representations.\",\n  \"Proposed Compound Attention Model Architecture\": \"Our compound attention model incorporates attention in both the encoder and the decoder. In the encoder, we process the input sequence using a bidirectional recurrent network with gated recurrent units (GRU). We then use inner attention to compute several weighted averages of the encoder states, which become the rows of a new matrix M. In the decoder, we propose to leverage the structured sentence embedding by applying attention to its components, which is no different from the classical attention mechanism used in NMT, except that it acts on this fixed-size representation instead of the sequence of encoder states.\",\n  \"Evaluation of Sentence Representations\": \"Continuous sentence representations can be evaluated in many ways. We evaluate our learned representations with classification and similarity tasks from SentEval and by examining clusters of sentence paraphrase representations. SentEval performs evaluation on 10 classification and 7 similarity tasks using the SentEval evaluation tool. Paraphrases are evaluated using two paraphrase sources: COCO and HyTER Networks, and we compute metrics such as cluster classification accuracy, nearest-neighbor paraphrase retrieval accuracy, and inverse Davies-Bouldin index.\",\n  \"Experimental Setup and Datasets\": \"We trained English-to-German and English-to-Czech NMT models using Neural Monkey. The de models were trained on the Multi30K multilingual image caption dataset, extended by Helcl and Libovick\u00fd, which contains 410k sentence pairs. The cs models were trained on CzEng 1.7. We also evaluate the InferSent model pre-trained on the natural language inference (NLI) task and a bag-of-words baseline (GloVe-BOW) obtained by averaging GloVe word vectors.\",\n  \"Translation Quality Results\": \"We estimate translation quality of the various models using single-reference case-sensitive BLEU. The best performing is the ATTN Bahdanau et al. model, followed by Transformer (de only) and our ATTN-ATTN (compound attention). The non-attentive FINAL Cho et al. is the worst, except cs-MAXPOOL. For 5 selected cs models, we also performed the WMT-style 5-way manual ranking on 200 sentence pairs, which confirms the automatic evaluation results.\",\n  \"SentEval Task Performance\": \"Due to the large number of SentEval tasks, we present the results abridged in two different ways: by reporting averages and by showing only the best models in comparison with other methods. The supervised InferSent clearly outperforms all other models in all tasks except for MRPC and TREC. Our models are outperformed even by GloVe-BOW, except for the NLI tasks (SICK-E and SNLI) where cs-FINAL-CTX is better.\",\n  \"Paraphrase Evaluation\": \"We evaluate the representation of paraphrases using two paraphrase sources: COCO and HyTER Networks. For every model, we encode each dataset to obtain a set of sentence embeddings with cluster labels. We then compute metrics such as cluster classification accuracy, nearest-neighbor paraphrase retrieval accuracy, and inverse Davies-Bouldin index.\",\n  \"Correlation Analysis Between BLEU and Representation Metrics\": \"To assess the relation between the various measures of sentence representations and translation quality as estimated by BLEU, we plot a heatmap of Pearson correlations. On the cs dataset, most metrics of representation are positively correlated, but most representation metrics correlate with BLEU negatively. The negative correlation between the number of attention heads and the representation metrics can be partly explained by the observation that the heads tend to 'divide' the sentence into segments, which is less fit for representing paraphrases where the subject and object swap their position.\",\n  \"Discussion on the Impact of Attention Heads on Representation Quality\": \"The better the BLEU score, the worse the meaning representation. This observation is important for representation learning where bilingual MT now seems less likely to provide useful data, but perhaps more so for MT itself, where the struggle towards a high single-reference BLEU score leads to systems that refuse to consider the meaning of the sentence. The negative correlation between the number of attention heads and the representation metrics can be partly explained by the observation that the heads tend to 'divide' the sentence into segments, which is less fit for representing paraphrases where the subject and object swap their position.\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentence Embeddings\": \"Neural vector representations have become ubiquitous in all subfields of natural language processing. Sentence embedding methods attempt to encode a variable-length input sentence into a fixed length vector. A number of such sentence embedding methods have been proposed in recent years. Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments of semantic relatedness, typically with respect to the SemEval Semantic Textual Similarity competitions.\",\n  \"Evaluation of Sentence Embeddings\": \"Sentence embeddings have mainly been evaluated in terms of how well their cosine similarities mirror human judgments of semantic relatedness, typically with respect to the SemEval Semantic Textual Similarity competitions. The SICK dataset was created to better benchmark the effectiveness of different models across a broad range of challenging lexical, syntactic, and semantic phenomena, in terms of both similarities and the ability to be predictive of entailment.\",\n  \"Sentence Modification Schemes\": \"We rely on simple transformations to generate several different sets of sentence triplets. Not-Negation involves inserting the negation marker not before the first verb of the original sentence. Quantifier-Negation prepends the quantifier expression there is no to original sentences. Synonym Substitution substitutes the verb in the original sentence with an appropriate synonym. Embedded Clause Extraction extracts the clauses for sentences containing verbs such as say, think. Passivization changes sentences from active to passive voice. Argument Reordering swaps the subject and object of the original sentence. Fixed Point Inversion selects a word in the sentence as the pivot and inverts the order of words before and after the pivot.\",\n  \"Sentence Triplet Generation\": \"Given the above forms of modified sentences, we induce five evaluation datasets, consisting of triplets of sentences. Negation Detection explores how well sentence embeddings can distinguish sentences with similar structure and opposite meaning. Negation Variants investigates how well the sentence embeddings reflect negation quantifiers. Clause Relatedness explores whether the similarity between a sentence and its embedded clause is higher than between a sentence and its negation. Argument Sensitivity ascertains whether the sentence embeddings succeed in distinguishing semantic information from structural information. Fixed Point Reorder explores how well the sentence embeddings account for shifts in meaning due to the word order in a sentence.\",\n  \"Experimental Setup and Datasets\": \"We create the evaluation datasets listed in Table 1, drawing on source sentences from SICK, Penn Treebank WSJ, and MSR Paraphrase corpus. Although the process to modify the sentences is automatic, we rely on human annotators to double-check the results for grammaticality and semantics. This is particularly important for synonym substitution, for which we relied on WordNet.\",\n  \"Embedding Methods Compared\": \"In our experiments, we compare three prominent sentence embedding methods: GloVe Averaging, which takes the average of the word vectors for all words in a sentence; Concatenated P-Mean Embeddings, which concatenates different p-means of multiple kinds of word vectors; Sent2Vec, which learns word and n-gram embeddings such that the average of all words and n-grams in a sentence can serve as a high-quality sentence vector.\",\n  \"Negation Detection Results\": \"Table 2 lists the results for the Negation Detection dataset, where S, S + , S * refer to the original, Synonym Substitution, and Not-Negation versions of the sentences, respectively. For each of the considered embedding methods, we first report the average cosine similarity scores between all relevant sorts of pairings of two sentences. On this dataset, we observe that GloVe Avg. is more often than not misled by the introduction of synonyms, although the corresponding word vector typically has a high cosine similarity with the original word's embedding. In contrast, both InferSent and SkipThought succeed in distinguishing unnegated sentences from negated ones.\",\n  \"Negation Variants Analysis\": \"In Table 3, S, S + , S * refer to the original, Not-Negation, and Quantifier-Negation versions of a sentence, respectively. Accuracy in this problem is defined as the percentage of sentence triples whose similarity between S+ and S * is higher than similarity between S and S+ and S + and S *. The results of both averaging of word embeddings and SkipThought are dismal in terms of the accuracy. InferSent, in contrast, appears to have acquired a better understanding of negation quantifiers, as these are commonplace in many NLI datasets.\",\n  \"Clause Relatedness Evaluation\": \"In Table 4, S, S + , S * refer to original, Embedded Clause Extraction, and Not-Negation, respectively. Although not particularly more accurate than random guessing, among the considered approaches, Sent2vec fares best in distinguishing the embedded clause of a sentence from a negation of said sentence. For a detailed analysis, we can divide the sentence triplets in this dataset into two categories. It seems plausible that SkipThought is more sensitive to the word order due to the recurrent architecture. Infersent also achieved better performance on sentences resembling a) compared with sentences resembling b).\",\n  \"Argument Sensitivity Analysis\": \"In Table 5, S, S + , S * refer to the original sentence, its Passivization form, and the Argument Reordering version, respectively. Although recurrent architectures are able to consider the order of words, unfortunately, none of the analyzed approaches prove adept at distinguishing the semantic information from structural information in this case.\",\n  \"Fixed Point Reorder Evaluation\": \"In Table 6, S, S + , S * refer to the original sentence, its semantically equivalent one, and Fixed Point Inversion Version. As Table 6 indicates, sentence embeddings based on means (GloVe averages), weighted means (Sent2Vec), or concatenation of p-mean embeddings (P-Means) are unable to distinguish the fixed point inverted sentence from the semantically equivalent one, as they do not encode sufficient word order information into the sentence embeddings. Sent2Vec does consider n-grams but these do not affect the results sufficiently. SkipThought and InferSent did well when the original sentence and its semantically equivalent share similar structure.\",\n  \"Conclusion and Comparative Analysis\": \"This paper proposes a simple method to inspect sentence embeddings with respect to their semantic properties, analyzing three popular embedding methods. We find that both SkipThought and InferSent distinguish negation of a sentence from synonymy. InferSent fares better at identifying semantic equivalence regardless of the order of words and copes better with quantifiers. SkipThought is more suitable for tasks in which the semantics of the sentence corresponds to its structure, but it often fails to identify sentences with different word order yet similar meaning. In almost all cases, dedicated sentence embeddings from hidden states of a neural network outperform a simple averaging of word embeddings.\"\n}\n```",
    "```json\n{\n  \"Introduction to Knowledge Bases and Semantic Parsing\": \"Organizing the world's facts and storing them in a structured database, large-scale knowledge bases (KB) like DBPedia and Freebase have become important resources for supporting open-domain question answering (QA). Most state-of-the-art approaches to KB-QA are based on semantic parsing, where a question (utterance) is mapped to its formal meaning representation (e.g., logical form) and then translated to a KB query. The answers to the question can then be retrieved simply by executing the query.\",\n  \"Challenges in Traditional Semantic Parsing\": \"However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB. Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem.\",\n  \"Proposed Semantic Parsing Framework\": \"Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in \u03bb-calculus and is semantically closely related to \u03bb-DCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions.\",\n  \"Query Graph Design and Its Relation to Knowledge Base\": \"Our query graph consists of four types of nodes: grounded entity (rounded rectangle), existential variable (circle), lambda variable (shaded circle), aggregation function (diamond). Grounded entities are existing entities in the knowledge base K. Existential variables and lambda variables are un-grounded entities. In particular, we would like to retrieve all the entities that can map to the lambda variables in the end as the answers. Aggregation function is designed to operate on a specific entity, which typically captures some numerical properties.\",\n  \"Staged Query Graph Generation Process\": \"Given a question, we formalize the query graph generation process as a search problem, with staged states and actions. Let S = {\u03c6, S e , S p , S c } be the set of states, where each state could be an empty graph (\u03c6), a single-node graph with the topic entity (S e ), a core inferential chain (S p ), or a more complex query graph with additional constraints (S c ). Let A = {A e , A p , A c , A a } be the set of actions. An action grows a given graph by adding some edges and nodes.\",\n  \"Deep Convolutional Neural Networks for Semantic Matching\": \"To handle the huge variety of the semantically equivalent ways of stating the same question, as well as the mismatch of the natural language utterances and predicates in the knowledge base, we propose using Siamese neural networks for identifying the core inferential chain. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine.\",\n  \"Augmenting Constraints and Aggregations in Query Graphs\": \"A graph with just the inferential chain forms the simplest legitimate query graph and can be executed against the knowledge base K to retrieve the answers. To further restrict the set of answer entities, the graph with only the core inferential chain can be expanded by two types of actions: A c and A a . A c is the set of possible ways to attach an entity to a variable node, where the edge denotes one of the valid predicates that can link the variable to the entity. Sometimes, the constraints are described over the entire answer set through the aggregation function, such as the word 'first' in our example question q ex .\",\n  \"Learning the Reward Function for Query Graph Generation\": \"Given a state s, the reward function \u03b3(s) basically judges whether the query graph represented by s is the correct semantic parse of the input question q. We use a log-linear model to learn the reward function. The features we designed essentially match specific portions of the graph to the question, and generally correspond to the staged actions described previously, including: Topic Entity, Core Inferential Chain, Constraints & Aggregations, and Overall.\",\n  \"Experimental Results and Performance Analysis\": \"Tab. 1 shows the results of our system, STAGG (Staged query graph generation), compared to existing work. As can be seen from the table, our system outperforms the previous state-of-the-art method by a large margin -7.2% absolute gain. Given the staged design of our approach, it is thus interesting to examine the contributions of each component. Because topic entity linking is the very first stage, the quality of the entities found in the questions, both in precision and recall, affects the final results significantly.\",\n  \"Error Analysis and Future Work\": \"Although our approach substantially outperforms existing methods, the room for improvement seems big. After all, the accuracy for the intended application, question answering, is still low and only slightly above 50%. We randomly sampled 100 questions that our system did not generate the completely correct query graphs, and categorized the errors. About one third of errors are in fact due to label issues and are not real mistakes. This includes label error (2%), incomplete labels (17%, e.g., only one song is labeled as the answer to 'What songs did Bob Dylan write?') and acceptable answers (15%, e.g., 'Time in China' vs. 'UTC+8').\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentence Acceptability and Grammaticality\": \"Sentence acceptability is defined as the extent to which a sentence is well formed or natural to native speakers of a language. It encompasses semantic, syntactic and pragmatic plausibility and other non-linguistic factors such as memory limitation. Grammaticality, by contrast, is the syntactic well-formedness of a sentence. In this research, we focus on predicting acceptability judgements rather than grammaticality.\",\n  \"Research Objectives and Hypotheses\": \"Our work is guided by 3 hypotheses: H1: Document context boosts sentence acceptability judgements. H2: Document context helps language models to model acceptability. H3: A language model predicts acceptability more accurately when it is tested on sentences within document context than when it is tested on the sentences alone.\",\n  \"Dataset Construction and Annotation Methodology\": \"To construct our dataset, we extracted 100 random articles from the English Wikipedia and sampled a sentence from each article. We used the Moses MT system to translate each sentence from English to 4 target languages and back to English, generating sentences of varying degrees of well-formedness. We used Amazon Mechanical Turk to gather acceptability judgements, presenting sentences both with and without their document context.\",\n  \"Impact of Document Context on Acceptability Ratings\": \"We found a strong correlation of Pearson's r = 0.80 between acceptability ratings with and without context. Adding context generally improves acceptability, especially for ill-formed sentences, but it diminishes the acceptability of well-formed sentences. This suggests that context boosts sentence ratings most for less well-formed sentences.\",\n  \"Language Models for Predicting Sentence Acceptability\": \"We experimented with two types of language models: a standard LSTM language model and a topically-driven model (tdlm). The tdlm incorporates document context as a conditioning variable during training. Both models were tested with and without context at test time to predict sentence acceptability.\",\n  \"Evaluation of Acceptability Measures Against Human Ratings\": \"We evaluated the models against crowd-sourced annotated sentences judged both in context and out of context. We found that using context at test time improves model performance, and tdlm consistently outperforms lstm over both types of human ratings and test input variants.\",\n  \"Comparison of LSTM and Topically-Driven Language Models\": \"We compared the performance of lstm and tdlm models with and without context at test time. The results showed that tdlm outperforms lstm, and incorporating context during training or testing improves model performance.\",\n  \"Discussion on the Effect of Context on Model Performance\": \"We observed that context positively influences acceptability, particularly for ill-formed sentences, but it also has the reverse effect for well-formed sentences. Incorporating context during training or testing improves model performance, but prediction performance declines when tested on judgements collected with context.\",\n  \"Related Work and Future Research Directions\": \"Previous work has focused on automatic detection of grammatical errors and acceptability prediction without considering context. Future work will investigate why context reduces acceptability for well-formed sentences and experiment with a wider range of models, including sentence embedding methodologies.\"\n}\n```",
    "```json\n{\n  \"Introduction to Taylor's Law and Its Application in Natural Language\": \"Taylor's law characterizes how the variance of the number of events for a given time and space grows with respect to the mean, forming a power law. It is a quantification method for the clustering behavior of a system. Despite such diverse application across domains, there has been little analysis based on Taylor's law in studying natural language. For the work described in this article, we applied Taylor's law for texts, in a manner close to the original concept. We considered lexical fluctuation within texts, which involves the cooccurrence and burstiness of word alignment.\",\n  \"Methodology for Measuring Taylor's Exponent in Texts\": \"Given a set of elements W (words), let X = X1, X2, ..., XN be a discrete time series of length N, where Xi \u2208 W for all i = 1, 2, ..., N, i.e., each Xi represents a word. For a given segment length \u2206t \u2208 N (a positive integer), a data sample X is segmented by the length \u2206t. The number of occurrences of a specific word wk \u2208 W is counted for every segment, and the mean \u00b5k and standard deviation \u03c3k across segments are obtained. Doing this for all word kinds w1, ..., w|W| \u2208 W gives the distribution of \u03c3 with respect to \u00b5. Experimentally, the Taylor exponent \u03b1 is known to take a value within the range of 0.5 \u2264 \u03b1 \u2264 1.0 across a wide variety of domains as reported in (Eisler, Bartos, and Kert\u00e9sz, 2007), including finance, meteorology, agriculture, and biology.\",\n  \"Analysis of Taylor Exponents Across Different Languages and Text Types\": \"The Taylor exponent depended only slightly on the data size. Figure 2 shows this dependency for the two largest data sets used, The New York Times (NYT, 1.5 billion words) and The Mainichi (24 years) newspapers. When the data size was increased, the exponent exhibited a slight tendency to decrease. The median and quantiles of the Taylor exponent were calculated for the different kinds of data listed in Table 1. The first two boxes show results with an exponent of 0.50. These results were each obtained from 10 random samples of the randomized sequences. The remaining boxes show results for real data. The exponents for texts from Project Gutenberg ranged from 0.53 to 0.68.\",\n  \"Comparison of Taylor Exponents Between Natural Language and Other Data Sources\": \"Analysis of different kinds of data showed how the Taylor exponent differed according to the data source. Figure 3 shows plots for samples from enwiki8 (tagged Wikipedia), the child-directed speech of Thomas (taken from CHILDES), programming language data sets, and music. The distributions appear different from those for the natural language texts, and the exponents were significantly larger. This means that these data sets contained expressions with fixed forms much more frequently than did the natural language texts.\",\n  \"Impact of Data Size and Segment Size on Taylor Exponents\": \"The Taylor exponent also differed according to \u2206t, and Figure 6 shows the dependence of\u03b1 on \u2206t. For each kind of data shown in Figure 4, the mean exponent is plotted for various \u2206t. As reported in (Eisler, Bartos, and Kert\u00e9sz, 2007), the exponent is known to grow when the segment size gets larger. The reason is that words occur in a bursty, clustered manner at all length scales: no matter how large the segment size becomes, a segment will include either many or few instances of a given word, leading to larger variance growth.\",\n  \"Evaluation of Machine-Generated Text Using Taylor's Exponent\": \"The main contribution of this paper is the findings of Taylor's law behavior for real texts as presented thus far. This section explains the applicability of these findings, through results obtained with baseline language models. As mentioned previously, i.i.d. mathematical processes have a Taylor exponent of 0.50. We show here that, even if a process is not trivially i.i.d., the exponent often takes a value of 0.50 for random processes, including texts produced by standard language models such as n-gram based models. Figure 8 shows two results produced by neural language models. Figure 8a shows the result for a sample of 2 million characters produced by a standard (three-layer) stacked character-based LSTM unit that learned the complete works of Shakespeare. The Taylor exponent of the generated text was 0.50. This indicates that the character-level language model could not capture or reproduce the word-level clustering behavior in text.\",\n  \"Implications of Taylor's Law for Understanding Linguistic Complexity\": \"The Taylor exponent is another way to quantify the complexity of natural language: it allows for continuous quantification based on lexical fluctuation. Since the Taylor exponent can quantify and characterize one aspect of natural language, our findings are applicable in computational linguistics to assess language models. The Taylor exponent is initially 0.5 when the segment size is very small. This can be analytically explained as follows (Eisler, Bartos, and Kert\u00e9sz, 2007). Consider the case of \u2206t=1. Let n be the frequency of a particular word in a segment. We have \u27e8n\u27e9 \u226a 1.0, because the possibility of a specific word appearing in a segment becomes very small. Because \u27e8n\u27e92 \u2248 0, \u03c32 = \u27e8n2\u27e9 \u2212 \u27e8n\u27e92 \u2248 \u27e8n2\u27e9. Because n = 1 or 0 (with \u2206t=1), \u27e8n2\u27e9 = \u27e8n\u27e9 = \u00b5. Thus, \u03c32 \u2248 \u00b5. Overall, the results show the possibility of applying Taylor's exponent to quantify the complexity underlying coherence among words.\"\n}\n```",
    "```json\n{\n  \"Introduction to Language Identification and Its Applications\": \"The task of identifying the language of text or utterances has a number of applications in natural language processing. Font Llitj\u00f3s and Black (2001) show that language identification can improve the accuracy of letter-to-phoneme conversion. Li et al. (2007) use language identification in a transliteration system to account for different semantic transliteration rules between languages when the target language is Chinese. Huang (2005) improves the accuracy of machine transliteration by clustering his training data according to the source language.\",\n  \"Traditional N-gram Approaches for Language Identification\": \"N-gram approaches have proven very popular for language identification in general. Cavnar and Trenkle (1994) apply n-gram language models to general text categorization. They construct character-level language models using n-grams up to a certain maximum length from each class in their training corpora. To classify new text, they generate an n-gram frequency profile from the text and then assign it to the class having the most similar language model, which is determined by summing the differences in n-gram ranks.\",\n  \"SVMs for Language Identification of Short Texts\": \"Rather than using language models to determine the language of a name, we propose to count character n-gram occurrences in the given name, for n up to some maximum length, and use these counts as the features in an SVM. We choose SVMs because they can take a large number of features and learn to weigh them appropriately. In addition to n-gram counts, we also include word length as a feature.\",\n  \"Experimental Setup and Data Sets\": \"We used two corpora to test our SVM-based approach: the Transfermarkt corpus of soccer player names, and the Chinese-English-Japanese (CEJ) corpus of first names and surnames. For all data sets, we held out 10% of the data as the test set. We then found optimal parameters for each kernel type using 10-fold cross-validation on the remaining training set.\",\n  \"Evaluation of SVMs on Transfermarkt Corpus\": \"SVMs clearly outperform using language models on the Transfermarkt corpus; in fact, SVMs yield better accuracy on last names than language models on full names. Differences between kernels are not statistically significant.\",\n  \"Evaluation of SVMs on CEJ Corpus\": \"We found that the RBF and sigmoid kernels were very slow-presumably due to the large size of the corpus-so we tested only the linear kernel. Table 2 shows our results in comparison to those of language models reported in (Li et al., 2007); we reduce the error rate by over 50%.\",\n  \"Application of Language Identification to Machine Transliteration\": \"Machine transliteration is one of the primary potential applications of language identification because the language of a word often determines its pronunciation. We therefore tested language identification to see if results could indeed be improved by using language identification as a pre-processing step.\",\n  \"Results and Analysis of Transliteration Experiment\": \"Splitting the data and training two separate models yielded a combined top-1 accuracy of 46.0%, as compared to 47.0% achieved by a single transliteration model trained over the full data; this difference is not statistically significant. We believe that the principal reason for this performance decrease is the reduction in the amount of data available for the training of the separate models.\",\n  \"Conclusion and Future Work\": \"We have proposed a novel approach to the task of language identification of names. We have shown that applying SVMs with n-gram counts as features outperforms the predominant approach based on language models. In the future, we plan to investigate other methods of incorporating language identification in machine transliteration.\"\n}\n```",
    "```json\n{\n  \"Introduction to RBMT and SMT Challenges\": \"Recently, statistical machine translation (SMT) has been broadly developed and successfully used in the portion of practicable systems. However, it is costly to make a large volume of parallel corpora in a wide range of domains for commercial use. For this reason, we have developed rule based machine translation (RBMT) system using a monolingual corpus in the target language.\",\n  \"Development of RBMT Using Monolingual Corpus\": \"For example, target word selection is possible based on co-occurrence relationship extracted from a monolingual corpus (Suzuki et al., 2005). Furthermore, we have developed a word sense disambiguation based on a monolingual corpus in the target domain, and it has been applied to Japanese-Korean and Korean-Japanese translation systems (Kumano 2013).\",\n  \"Application of Statistical Post Editing (SPE) to RBMT\": \"By using the parallel corpora, we have confirmed advantages which apply statistical post editing (SPE) to RBMT in domain adaptation (Suzuki, 2011). In the last workshop (Nakazawa et al., 2014), we participated in Japanese-English and Japanese-Chinese tasks with SPE approach and obtained higher evaluation results than RBMT.\",\n  \"Overview of Toshiba's Machine Translation System\": \"Our RBMT system is basically a transfer-based machine translation (Izuha et al., 2008). The core framework consists of morphological analysis, syntactic/semantic analysis, target word selection, structural transfer, syntactic generation and morphological generation. Furthermore, huge amount of rules as translation knowledge including word dictionaries can realize both high translation performance and flexibility of customization.\",\n  \"System Combination Using RNNLM\": \"We realized a system combination between SPE and SMT as n-best reranking using a RNNLM. The n-best reranking can be achieved using both basic features and RNNLM score. In tuning, we combined 100-best candidates of both SPE and SMT for dev-set, and ran MERT tuning by adding the RNNLM score to the basic features.\",\n  \"Technical Term Dictionaries and Pre/Postprocessing\": \"As the preparation for each task, we selected technical term dictionaries by the same principle in the last workshop (Sonoh el al., 2014). For JPOzh-ja, we used an additional patent dictionary, which is extracted from JPO Chinese-Japanese dictionary. Furthermore, for JPOko-ja, we used n-gram probability dictionary, which was made from monolingual patent resources, in order to resolve word sense disambiguation.\",\n  \"Experimental Results and Performance Analysis\": \"Table 1 and 2 show the overall BLEU and RIBES scores for 'scientific papers subtask' and 'patents subtask', respectively. COMB means results of the system combination and Rerank means results of reranking using RNNLM (100best for SMT and SPE, 200-best for COMB). In all tasks, SPE improves translation results of RBMT on the BLEU and RIBES.\",\n  \"Official Results and Human Evaluation\": \"We basically submitted two results, one is SPE and the other is the system combination between SPE and SMT. Furthermore, top two systems on the BLEU scores were evaluated by the crowdsourcing. In WAT2015 results, we note that Toshiba systems were ranked as one of the top three systems in human evaluation in ja-en, ja-zh and JPOzh-ja.\",\n  \"Discussion on Evaluation Differences and System Characteristics\": \"On receiving the crowdsourcing results, we analyzed differences between our system and Online A, which obtained the highest HUMAN score in JPOko-ja. We hypothesize that the significant difference between the crowdsourcing and the inner evaluators occurs from the evaluation of the number expressions, such as '\u30b7\u30b9\u30c6\u30e0(100)' and '\u30b7\u30b9\u30c6\u30e0 100'.\",\n  \"Conclusion and Future Work\": \"SPE and reranking with RNNLM achieved higher BLEU than phrase-based SMT in most language pairs. Furthermore, the system combination between SPE and SMT improved BLEU score in Japanese-English pair and Japanese-Chinese pair. In the other hand, a straightforward correlation between automatic evaluation and human evaluation is not confirmed in our system. We need to establish the combination of multi-systems for practical use purpose, taking advantage of their characteristics and qualities.\"\n}\n```",
    "```json\n{\n  \"Introduction to Phrase-based Statistical Machine Translation (PB-SMT)\": \"Phrase-based Statistical Machine Translation (PB-SMT) is a data-oriented approach to machine translation that has been widely used for over a decade. It involves three main processes: training, tuning, and decoding. The training step is crucial as it creates the core knowledge used in machine translation.\",\n  \"Moses Toolkit and Its Role in PB-SMT\": \"The Moses open source statistical machine translation toolkit was developed by the Statistical Machine Translation Group at the University of Edinburgh. It plays a significant role in PB-SMT by providing tools for training, tuning, and decoding. Phrase tables are extracted from alignments using the grow-diag-final-and heuristic integrated in the Moses toolkit.\",\n  \"Word or Phrase Alignment in PB-SMT Training\": \"Word or phrase alignment in the training step of PB-SMT allows obtaining translation relationships among the words or phrases in a sentence-aligned bi-corpus. This step is critical as it affects the quality of translation and is one of the most time-consuming processes.\",\n  \"Probabilistic Alignment Models: IBM Models and HMM\": \"The probabilistic approach in alignment attempts to determine the best set of alignment links between source and target words or phrases in parallel sentences. IBM models and HMM alignment models are typical implementations of the EM algorithm and are widely used in this category. GIZA++ implements IBM Models and aligns words based on statistical models.\",\n  \"Associative Alignment Approaches: Anymalign and Cutnalign\": \"Associative approaches, such as Anymalign and Cutnalign, do not rely on an alignment model but on independence statistical measures. Anymalign samples large numbers of sub-corpora randomly to obtain source and target word or phrase occurrence distributions, while Cutnalign is a bilingual hierarchical sub-sentential alignment method based on a recursive binary segmentation process.\",\n  \"Data Used for Experiments: Chinese-Japanese JPO Patent Corpus\": \"The data used in the experiments are the Chinese-Japanese JPO Patent Corpus provided by WAT 2015 for the patents subtask. It contains 1 million Chinese-Japanese parallel sentences in four domains: Chemistry, Electricity, Mechanical engineering, and Physics. Sentences of 40 words or less were used for training the translation models.\",\n  \"Bilingual Hierarchical Sub-sentential Alignment Method: Cutnalign\": \"Cutnalign is a bilingual hierarchical sub-sentential alignment method based on a recursive binary segmentation process of the alignment matrix between a source sentence and its translation. It measures the strength of the translation link between any source and target pair of words, computes the optimal joint clustering of a bipartite graph, and segments and aligns a pair of sentences.\",\n  \"Improvements to Cutnalign for Speed and Efficiency\": \"Improvements to Cutnalign include multi-processing in both the sampling-based alignment method and hierarchical sub-sentential alignment method to accelerate the overall alignment process. The core of Cutnalign was re-implemented in C, and approximations in the computation of N cut were introduced to accelerate some decisions and reduce the search space.\",\n  \"Experimental Setup and Evaluation Metrics\": \"Experiments were performed with GIZA++ or MGIZA, and the phrase tables were extracted from the alignments obtained using the grow-diag-final-and heuristic integrated in the Moses toolkit. Translations were evaluated using BLEU and RIBES metrics. Different alignment methods were evaluated within a PB-SMT system built using the Moses toolkit, the Ken Language Modeling toolkit, and a lexicalized reordering model.\",\n  \"Results and Comparison with GIZA++/Moses Baseline\": \"The results show that there are no significant differences among the evaluation results based on different versions of Moses, different Anymalign timeouts, or different versions of Cutnalign. However, the training times changed considerably depending on the timeouts for Anymalign. The fastest training time was 57 minutes, about one fifth of the time used by GIZA++ or MGIZA.\"\n}\n```",
    "```json\n{\n  \"Introduction to NMT and Tree-based Systems\": \"NMT has witnessed promising improvements recently. Depending on the types of input and output, these efforts can be divided into three categories: string-to-string systems; tree-to-string systems; and string-to-tree systems. Compared with string-to-string systems, tree-to-string and string-to-tree systems (henceforth, tree-based systems) offer some attractive features. They can use more syntactic information and can conveniently incorporate prior knowledge.\",\n  \"Advantages of Tree-based NMT Methods\": \"Because of these advantages, tree-based methods become the focus of many researches of NMT nowadays. Based on how to represent trees, there are two main categories of tree-based NMT methods: representing trees by a tree-structured neural network, and representing trees by linearization. Compared with the former, the latter method has a relatively simple model structure, so that a larger corpus can be used for training and the model can be trained within reasonable time, hence is preferred from the viewpoint of computation.\",\n  \"Linearization Methods for Tree-based NMT\": \"Regarding the linearization adopted for tree-to-string NMT (i.e., linearization of the source side), Sennrich and Haddow (2016) encoded the sequence of dependency labels and the sequence of words simultaneously, partially utilizing the syntax information, while traversed the constituent tree of the source sentence and combined this with the word sequence, utilizing the syntax information completely. Regarding the linearization used for string-to-tree NMT (i.e., linearization of the target side), Nadejde et al. (2017) used a CCG supertag sequence as the target sequence, while Aharoni and Goldberg (2017) applied a linearization method in a top-down manner, generating a sequence ensemble for the annotated tree in the Penn Treebank (Marcus et al., 1993).\",\n  \"Packed Forest Representation\": \"The packed forest gives a representation of exponentially many parsing trees, and can compactly encode many more candidates than the n-best list. Formally, a packed forest is a pair V, E, where V is the set of nodes and E is the set of hyperedges. Each v \u2208 V can be represented as X i,j, where X is a constituent label and i, j \u2208 [0, n] are indices of words, showing that the node spans the words ranging from i (inclusive) to j (exclusive).\",\n  \"Proposed Forest-based NMT Approach\": \"Inspired by the tree-based NMT methods based on linearization, we propose an efficient forest-based NMT approach, which can encode the syntactic information of a packed forest on the basis of a novel weighted linearization method for a packed forest, and can decode the linearized packed forest under the simple sequence-to-sequence framework.\",\n  \"Forest Linearization Algorithm\": \"To address the above two problems, we propose a novel linearization algorithm for a packed forest (Algorithm 1). The algorithm linearizes the packed forest from the root node to leaf nodes by calling the EXPANDSEQ procedure recursively, while preserving the word order in the sentence. In this way, word sequential information is preserved. Within the EXPANDSEQ procedure, once a hyperedge is linearized, the tails are also linearized immediately. In this way, parent-child information is preserved.\",\n  \"Encoding the Linearized Forest\": \"The linearized packed forest forms the input of the encoder, which has two major differences from the input of a sequence-to-sequence NMT system. First, the input sequence of the encoder consists of two parts: the symbol sequence and the score sequence. Second, each symbol in the symbol sequence consists of several parts (words and constituent labels), which are combined by certain operators (c, \u2297, \u2295, or ).\",\n  \"Experimental Setup and Results\": \"We evaluate the effectiveness of our forest-based NMT systems on English-to-Chinese and English-to-Japanese translation tasks. The experimental results demonstrate the effectiveness of our framework. For both English-Chinese and English-Japanese, compared with the s2s baseline system, both the 1-best and forest-based configurations yield better results. This indicates syntactic information contained in the constituent trees or forests is indeed useful for machine translation.\",\n  \"Qualitative Analysis of Translation Results\": \"For the s2s case, no syntactic information is utilized, and therefore the output of the system is not a grammatical Chinese sentence. The attributive phrase of 'Czech border region' is a complete sentence. However, the attributive is not allowed to be a complete sentence in Chinese. For the case of using 1-best constituent tree, the output is a grammatical Chinese sentence. However, the phrase 'adjacent to neighboring Slovakia' is completely ignored in the translation result. After analyzing the constituent tree, we found that this phrase was incorrectly parsed as an 'adverb phrase', so that the NMT system paid little attention to it, because of the low confidence given by the parser. In contrast, for the case of the packed forest, we can see this phrase was not ignored and was translated correctly.\",\n  \"Comparison with Related Work\": \"Compared with these related studies, our framework utilizes a linearized packed forest, meaning the encoder can encode exponentially many trees in an efficient manner. The experimental results demonstrated these advantages. Our proposed method is effective in a large-scale MT setting, and we present qualitative analysis regarding the effectiveness of using forests in NMT.\"\n}\n```",
    "```json\n{\n  \"Introduction to Metaphor in Language\": \"Metaphor enriches language, playing a significant role in communication, cognition, and decision making. Relevant statistics illustrate that about one third of sentences in typical corpora contain metaphor expressions. Linguistically, metaphor is defined as a language expression that uses one or several words to represent another concept, rather than taking their literal meanings of the given words in the context.\",\n  \"Challenges in Computational Metaphor Processing\": \"Computational metaphor processing refers to modelling non-literal expressions (e.g., metaphor, metonymy, and personification) and is useful for improving many NLP tasks such as Machine Translation (MT) and Sentiment Analysis. However, existing methods often fail to identify metaphors when important contextual words are excluded, leading to literal interpretations of phrases that are actually metaphoric within a sentence.\",\n  \"Proposed Unsupervised Metaphor Processing Model\": \"In this paper, we propose an unsupervised metaphor processing model which can identify and interpret linguistic metaphors at the word-level. Our model is built upon word embedding methods and uses WordNet for lexical relation acquisition. It does not rely on any labelled data for model training and captures metaphor in an unsupervised, data-driven manner.\",\n  \"Methodology and Technical Details\": \"Our metaphor identification framework is built upon word embedding, which is based on Continuous Bag of Words (CBOW) and Skip-gram. The workflow involves training word embeddings, separating the target word and its context, constructing a candidate word set, identifying the best fit word, and computing the cosine similarity between the target word and the best fit word to determine if the target word is literal or metaphorical.\",\n  \"Evaluation and Comparison with Baselines\": \"We evaluate our model against three strong baselines on the task of metaphor identification. Our model significantly outperforms the unsupervised learning baselines on both phrase and sentence evaluation, and achieves equivalent performance to the state-of-the-art deep learning baseline on phrase-level evaluation.\",\n  \"Metaphor Processing for Machine Translation\": \"We conducted experiments to evaluate how metaphor processing can be used to support English-Chinese machine translation. By paraphrasing the metaphorically used target word with the best fit word, the performance of translation engines can be improved. Our model improves the metaphoric translation significantly, by testing on Google Translate and Bing Translator.\",\n  \"Experimental Results and Analysis\": \"The experimental results show that our model outperforms the unsupervised baselines in both sentence and phrase evaluations. The interpretation of the identified metaphorical words given by our model also contributes to Google and Bing translation systems with 11% and 9% accuracy improvements, respectively.\",\n  \"Future Work and Implications\": \"Future work will introduce weighted CBOW and Skip-gram to learn positional information within sentences. Additionally, we believe that compared with simply identifying metaphors, metaphor processing applied in practical tasks can be more valuable in the real world.\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentiment-to-Sentiment Translation\": \"Sentiment-to-sentiment 'translation' requires the system to change the underlying sentiment of a sentence while preserving its non-emotional semantic content as much as possible. It can be regarded as a special style transfer task that is important in Natural Language Processing (NLP). It has broad applications, including review sentiment transformation, news rewriting, etc. Yet the lack of parallel training data poses a great obstacle to a satisfactory performance.\",\n  \"Challenges in Sentiment-to-Sentiment Translation\": \"The reason is that these methods attempt to implicitly separate the emotional information from the semantic information in the same dense hidden vector, where all information is mixed together in an uninterpretable way. Due to the lack of supervised parallel data, it is hard to only modify the underlying sentiment without any loss of the nonemotional semantic information.\",\n  \"Proposed Cycled Reinforcement Learning Approach\": \"To tackle the problem of lacking parallel data, we propose a cycled reinforcement learning approach that contains two parts: a neutralization module and an emotionalization module. The neutralization module is responsible for extracting non-emotional semantic information by explicitly filtering out emotional words. The emotionalization module is responsible for adding sentiment to the neutralized semantic content for sentiment-to-sentiment translation.\",\n  \"Neutralization Module\": \"The neutralization module first identifies non-emotional words and then feeds them into the emotionalization module. We use a single Longshort Term Memory Network (LSTM) to generate the probability of being neutral or being polar for every word in a sentence. The neutralization module is taught to extract non-emotional semantic content better, which further enhances the emotionalization module.\",\n  \"Emotionalization Module\": \"The emotionalization module E \u03c6 is responsible for adding sentiment to the neutralized semantic content. In our work, we use a bi-decoder based encoder-decoder framework, which contains one encoder and two decoders. One decoder adds the positive sentiment and the other adds the negative sentiment. The input sentiment signal determines which decoder to use.\",\n  \"Cycled Reinforcement Learning Mechanism\": \"Two modules are trained by the proposed cycled method. The neutralization module first neutralizes an emotional input to semantic content and then the emotionalization module is forced to reconstruct the original sentence based on the source sentiment and the semantic content. Because of the discrete choice of neutral words, the loss is no longer differentiable over the neutralization module. Therefore, we formulate it as a reinforcement learning problem and use policy gradient to train the neutralization module.\",\n  \"Experimental Setup and Datasets\": \"We conduct experiments on two review datasets that contain user ratings associated with each review. Following previous work, we consider reviews with rating above three as positive reviews and reviews below three as negative reviews. The positive and negative reviews are not paired. We process the two datasets with the following steps: first, we filter out the reviews that exceed 20 words. Second, we construct text-sentiment pairs by extracting the first sentence in a review associated with its sentiment label.\",\n  \"Evaluation Metrics and Results\": \"We conduct two evaluations in this work, including an automatic evaluation and a human evaluation. The automatic evaluation measures sentiment transformation by evaluating the accuracy of generating designated sentiment and uses the BLEU score to evaluate the content preservation performance. The human evaluation is performed on the test set, where annotators score the transformed text in terms of sentiment and semantic similarity.\",\n  \"Human Evaluation and Comparison with Baselines\": \"Table 2 shows the human evaluation results. It can be clearly seen that the proposed method obviously improves semantic preservation. The semantic score is increased from 3.87 to 5.08 on the Yelp dataset, and from 3.22 to 4.67 on the Amazon dataset. In general, our proposed model achieves the best overall performance.\",\n  \"Error Analysis and Future Work\": \"Although the proposed method outperforms the state-of-the-art systems, we also observe several failure cases, such as sentiment-conflicted sentences (e.g., 'Outstanding and bad service'), neutral sentences (e.g., 'Our first time here'). Sentiment-conflicted sentences indicate that the original sentiment is not removed completely. This problem occurs when the input contains emotional words that are unseen in the training data, or the sentiment is implicitly expressed. Handling complex sentiment expressions is an important problem for future work.\"\n}\n```",
    "```json\n{\n  \"Introduction to Deep Neural Networks and Domain Adaptation Challenges\": \"Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks such as dependency parsing, named entity recognition, and semantic role labeling. In contrast, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics which do not generalize to other tasks. Other approaches that are in theory more general only evaluate on proprietary datasets or on a single benchmark, which carries the risk of overfitting to the task.\",\n  \"Evaluation of Traditional Bootstrapping Algorithms in Neural NLP Models\": \"In this work, we re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily-with a few additions-with the current generation of NLP models. We re-evaluate three traditional bootstrapping methods, self-training, tri-training, and tri-training with disagreement for neural network-based approaches on two NLP tasks with different characteristics, namely, a sequence prediction and a classification task (POS tagging and sentiment analysis).\",\n  \"Self-Training Method and Its Variants for Neural Networks\": \"Self-training (Yarowsky, 1995) is one of the earliest and simplest bootstrapping approaches. In essence, it leverages the model's own predictions on unlabeled data to obtain additional information that can be used during training. Typically the most confident predictions are taken at face value. We introduce our novel multi-task tri-training method in \u00a72.3. Self-training has shown mixed success. Its main downside is that the model is not able to correct its own mistakes and errors are amplified, an effect that is increased under domain shift.\",\n  \"Tri-Training and Its Variants for Domain Adaptation\": \"Tri-training (Zhou and Li, 2005) is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. Tri-training with disagreement (S\u00f8gaard, 2010) is based on the intuition that a model should only be strengthened in its weak points and that the labeled data should not be skewed by easy data points. Tri-training with disagreement is more data-efficient than tri-training and has achieved competitive results on part-of-speech tagging.\",\n  \"Multi-Task Tri-Training: A Novel Approach to Reduce Complexity\": \"In order to reduce both the time and space complexity of tri-training, we propose Multi-task Tri-training (MT-Tri). MT-Tri leverages insights from multi-task learning (MTL) to share knowledge across models and accelerate training. Rather than storing and training each model separately, we propose to share the parameters of the models and train them jointly using MTL. The output softmax layers are model-specific and are only updated for the input of the respective model. We show the model in Figure 1 (as instantiated for POS tagging).\",\n  \"Experimental Setup and Datasets for POS Tagging and Sentiment Analysis\": \"In order to ascertain which methods are robust across different domains, we evaluate on two widely used unsupervised domain adaptation datasets for two tasks, a sequence labeling and a classification task, cf. Table 1 for data statistics. For sentiment analysis, we evaluate on the Amazon reviews dataset (Blitzer et al., 2006). For POS tagging, we use the SANCL 2012 shared task dataset (Petrov and McDonald, 2012).\",\n  \"Results and Analysis of Sentiment Analysis Experiments\": \"For sentiment analysis, we show results for all 12 domain adaptation scenarios in Figure 2. Tri-training achieves the best average results on two target domains and clearly outperforms the state of the art on average. MT-Tri outperforms the state of the art on 3/4 domains, and even slightly traditional tri-training, resulting in the overall best method. This improvement is mainly due to the B->E and D->E scenarios, on which tri-training struggles.\",\n  \"Results and Analysis of POS Tagging Experiments\": \"Results for tagging in the low-data regime (10% of WSJ) are given in Table 3. In the low-data setup, tri-training with disagreement works best, reaching an overall average accuracy of 89.70, closely followed by classic tri-training, and significantly outperforming the baseline on 4/5 domains. For the high-data setup (Table 4), classic tri-training is the best method. In particular, traditional tri-training is complementary to word embedding initialization, pushing the non-pre-trained baseline to the level of SRC with Glove initialization.\",\n  \"Comparison with State-of-the-Art Methods and Baselines\": \"We compare to the best results published in the literature. We make the somewhat surprising observation that classic tri-training outperforms task-agnostic state-of-the-art semi-supervised learning and recent neural adaptation approaches. For sentiment analysis, we found tri-training and our MT-Tri model to outperform DANN. For POS tagging, classic tri-training is superior, performing especially well on OOVs and low frequency tokens, which suggests it is less affected by error propagation.\",\n  \"Conclusions and Future Work on Bootstrapping Algorithms in NLP\": \"We re-evaluate a range of traditional general-purpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift. For the two examined NLP tasks, classic tri-training works the best and even outperforms a recent state-of-the-art method. The drawback of tri-training is its time and space complexity. We therefore propose a more efficient multi-task tri-training model, which outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis. Overall, we emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs.\"\n}\n```",
    "```json\n{\n  \"Introduction to Stance Classification\": \"Stance classification is the task of automatically identifying users' positions about a specific target from text. Table 1 shows an example of this task, where the stance of the sentence is recognized as favorable on the target climate change is concern. Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest.\",\n  \"Traditional vs. Cross-Target Stance Classification\": \"Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest. An alternative to this approach is to conduct a cross-target classification, where the classifier is adapted from different but related targets, which allows benefiting from the knowledge of existing targets. Cross-target stance classification is a more challenging task simply because the language models may not be compatible between different targets.\",\n  \"CrossNet Model Architecture\": \"In this paper, we focus on cross-target stance classification and explore the limits of generalizing models between different but domain-related targets. The basic idea is to learn a set of domain-specific aspects from a source target, and then apply them to prediction on a destination target. To this end, we propose CrossNet, a novel neural model that implements the above idea based on the self-attention mechanism.\",\n  \"Embedding and Context Encoding Layers\": \"In the Embedding Layer, we use word embeddings to represent each word in the input as a dense vector. In the Context Encoding Layer, we encode the contextual information in the input sentence and target using a bi-directional Long Short-Term Memory Network (BiLSTM). We also borrow the idea of conditional encoding to model the dependency of the sentence on the target.\",\n  \"Aspect Attention Layer\": \"In this layer, we implement the idea of discovering domain-specific aspects for cross-target stance inference. The key observation is that the domain aspects that reflect users' major concerns are usually the core of understanding their stances and could be mentioned by multiple users in a discussion. We utilize self-attention to signal the core parts of a stance-bearing sentence.\",\n  \"Prediction Layer and Model Training\": \"We predict the stance label of the sentence based on its domain aspect encoding using a multilayer perceptron (MLP) and apply the softmax to get the predicted probability for each of the C classes. For model training, we use multi-class cross-entropy loss with L2-regularization.\",\n  \"Experimental Setup and Datasets\": \"The word embeddings are initialized with the pretrained 200d GloVe word vectors on the 27B Twitter corpus and fixed during training. The model is trained (90%) and validated (10%) on a source target, and tested on a destination target. Stratified 10-fold cross-validation is conducted to produce averaged results.\",\n  \"Classification Performance and Baseline Comparison\": \"Table 3 shows the results (in-target and cross-target) on the two domains: Women's Rights and American Politics. Our model is shown to achieve better results than the two baselines in almost all cases, which implies that the aspect attention mechanism adopted in our model could benefit target-level generalization while it does not hurt the in-target performance.\",\n  \"Visualization of Attention Mechanism\": \"To show that our model can select sentence parts that are related to domain aspects, we visualize the self-attention results on some tweet examples that are correctly classified by our model. The most highlighted parts in each example are relevant to the respective domain, demonstrating that our model could benefit stance inference across related targets through capturing domain-specific information.\",\n  \"Future Work and Extensions\": \"In the future, there are several ways of extending our model. First, selecting the effective source targets to generalize from is crucial for achieving satisfying results on the destination targets. Second, our current model for identifying users' stances on mining projects only generalizes from one source target. It could be useful to also consider other related sources for knowledge transfer. Finally, it would be interesting to evaluate our model in a multilingual scenario.\"\n}\n```",
    "```json\n{\n  \"Introduction to Domain-Specific Sentiment Analysis\": \"The choice of the words to express an opinion depends on the domain as users often use domain-specific words. For example, entertaining and boring are frequently used in the movie domain to express an opinion; however, finding these words in the electronics domain is rare. Moreover, there are words which are likely to be used across domains in the same proportion, but may change their polarity orientation from one domain to another. Such a polarity changing word should be assigned positive orientation in the movie domain and negative orientation in the automobile domain.\",\n  \"Challenges in Cross-Domain Sentiment Analysis\": \"Due to these differences across domains, a supervised algorithm trained on a labeled source domain, does not generalize well on an unlabeled target domain and the cross-domain performance degrades. Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using the manually annotated review corpus. This is not practical as there are numerous domains and getting manually annotated data for every new domain is an expensive and time consuming task.\",\n  \"Significance of Consistent Polarity Words (SCP) Across Domains\": \"The words which are equally significant with a consistent polarity across domains represent the usable information for cross-domain sentiment analysis. In this paper, we present a novel technique based on \u03c7 2 test and cosine-similarity between context vector of words to identify Significant Consistent Polarity (SCP) words across domains.\",\n  \"Identification of SCP Words Using \u03c7\u00b2 Test and Cosine-Similarity\": \"To identify SCP words, we use \u03c7 2 test to find the significance of a word in the corpus for sentiment classification in the labeled source domain. For the unlabeled target domain, we use cosine-similarity between context vector of words to assign polarity to words. Essentially, if a word w gives a higher cosine-similarity with the PosPivot (Positive-pivot) than the NegPivot (Negative-pivot), the decision method assigns the positive polarity to the word w, else negative polarity to the word w.\",\n  \"Ensemble-Based Adaptation Algorithm for Cross-Domain Sentiment Classification\": \"The proposed cross-domain adaptation approach attempts to learn such domain specific features from the target domain using a classifier trained on SCP words in the source domain. An ensemble of the classifiers trained on the SCP features (transferred from the source) and domain specific features (learned within the target) further enhances the cross-domain performance.\",\n  \"Experimental Setup and Dataset\": \"In this paper, we show comparison between SCP-based domain adaptation (our approach) and SCL-based domain adaptation approach proposed by Bhatt et al. (2015) using four domains, viz., Electronics (E), Kitchen (K), Books (B), and DVD. Data in each domain is divided into three parts, viz., train (60%), validation (20%) and test (20%).\",\n  \"Performance Comparison with Existing Methods\": \"Table 5 depicts that the system-3 is better than system-1 and system-2 for all pairs, except K to B and B to D. For these two pairs, system-2 performs better than system-3, though the difference in accuracy is very low (below 1%). On the other hand, SCL-based system-5 performs better than the common-unigrams based system-4.\",\n  \"Error Analysis and Domain Similarity Impact\": \"The pairs of domains which share a greater number of domain-specific words, result in a higher accuracy cross-domain classifier. For example, Electronics (E) and Kitchen (K) domains share many domain-specific words, hence pairing of such similar domains as the source and the target results into a higher accuracy classifier in the target domain.\",\n  \"Conclusion and Future Work\": \"In this paper, we proposed that the Significant Consistent Polarity (SCP) words represent the transferable information from the labeled source domain to the unlabeled target domain for cross-domain sentiment classification. We showed a strong positive correlation of 0.78 between the SCP words identified by our approach and the sentiment classification accuracy achieved in the unlabeled target domain. Furthermore, we show that an ensemble of the classifiers trained on the SCP features and target specific features overcomes the errors of the individual classifiers.\"\n}\n```",
    "```json\n{\n  \"Introduction to Span-Based Neural Constituency Parsing\": \"Span-based neural constituency parsing has attracted attention due to its high accuracy and extreme simplicity. Unlike other recent neural constituency parsers which use neural networks to model tree structures, the span-based framework is considerably simpler, only using bidirectional RNNs to model the input sequence and not the output tree. Because of this factorization, the output space is decomposable which enables efficient dynamic programming algorithms such as CKY.\",\n  \"Limitations of Existing Span-Based Parsers\": \"Existing span-based parsers suffer from a crucial limitation in terms of search: on the one hand, a greedy span parser is fast (linear-time) but only explores one single path in the exponentially large search space, and on the other hand, a chart-based span parser performs exact search and achieves state-of-the-art accuracy, but in cubic time, which is too slow for longer sentences and for applications that go beyond sentence boundaries.\",\n  \"Proposed Linear-Time Span-Based Neural Parser\": \"We propose to combine the merits of both greedy and chart-based approaches and design a linear-time span-based neural parser that searches over exponentially large space. Following Huang and Sagae (2010), we perform left-to-right dynamic programming in an action-synchronous style, with (2n \u2212 1) actions (i.e., steps) for a sentence of n words.\",\n  \"Dynamic Programming Score Decomposition\": \"Like Stern et al. (2017a), we also decompose the score of a tree t to be the sum of the span scores. This decomposition allows us to efficiently calculate the score of a tree by summing up the scores of individual spans, which is a key property of span-based parsing.\",\n  \"Graph-Structured Stack Without Extra Bookkeeping\": \"We reformulate this DP parser as a shift-reduce parser and maintain a step index to perform action-synchronous beam search. This new formulation suggests an alternative implementation that does not require any extra bookkeeping, proving a new theorem that gives deep insight into GSS.\",\n  \"Action-Synchronous Beam Search\": \"The incremental nature of our parser allows us to further lower the runtime complexity at the cost of inexact search. At each time step, we maintain the top b parsing states, pruning off the rest. With O(n) parsing actions, our time complexity becomes linear in the length of the sentence.\",\n  \"Cube Pruning for Improved Runtime\": \"To improve runtime, we introduce cube pruning to our beam search, where we put candidate actions into a heap and retrieve the top b states to be considered in the next time-step. This process finishes when we pop b items from the heap, improving the runtime to O(nb log b).\",\n  \"Training Methodology and Loss Function\": \"We use a Structured SVM approach for training, where we want the model to score the gold tree t* higher than any other tree t by at least a margin \u2206(t, t*). We also introduce a new loss function that penalizes wrong spans that cross gold-tree spans, and employ max-violation update to train this parser with structured SVM and beam search.\",\n  \"Experimental Results on Penn Treebank and PTB-RST\": \"We present experiments on the Penn Treebank and the PTB-RST discourse treebank. Our system achieves state-of-the-art performances on the Penn Treebank among single model end-to-end systems, and demonstrates tractability on longer sequences for discourse parsing.\"\n}\n```",
    "```json\n{\n  \"Introduction to Diversity in Paraphrasing\": \"Inducing diversity in the task of paraphrasing is an important problem in NLP with applications in data augmentation and conversational agents. Diversity in paraphrasing models finds applications in text simplification, document summarization, QA systems, data augmentation, conversational agents, and information retrieval. Previous paraphrasing approaches have mainly focused on the issue of generating semantically similar paraphrases, while paying little attention towards diversity.\",\n  \"Challenges in Existing Paraphrasing Approaches\": \"Most of the current paraphrasing models rely solely on top-k beam search sequences to obtain a set of paraphrases. The resulting set, however, contains many structurally similar sentences with only minor, word-level changes. Although some prior works address the notion of diversity in NLP, including in sequence learning frameworks, they are restrictive in terms of retaining fidelity with respect to the source sentence.\",\n  \"Monotone Submodular Function Maximization for Paraphrasing\": \"Addressing the task of diverse paraphrasing through the lens of monotone submodular function maximization alleviates the problem of retaining fidelity while maximizing diversity. The submodular objective offers better flexibility in terms of controlling diversity as well as fidelity, and there exists a simple greedy algorithm for solving monotone submodular function maximization, which guarantees the diverse solution to be almost as good as the optimal solution.\",\n  \"Diverse Paraphraser using Submodularity (DiPS) Methodology\": \"We introduce Diverse Paraphraser using Submodularity (DiPS). DiPS maximizes a novel submodular objective function specifically targeted towards paraphrasing. Our approach is built upon the SEQ2SEQ framework, where we incorporate a submodular objective to obtain high-quality paraphrases during the decoding phase.\",\n  \"Fidelity and Diversity Objectives in DiPS\": \"To achieve high fidelity, we define two complementary notions of sentence similarity: syntactic and semantic. For diversity, we propose a function that rewards sequences with a higher number of distinct n-grams and another based on edit distance to ensure structural variations. Our goal is to strike a balance between these two to obtain high-quality generations.\",\n  \"Experiments and Evaluation of DiPS\": \"We evaluate our method against recent paraphrasing models and multiple diversity-inducing schemes. DiPS outperforms these baseline models in terms of fidelity metrics such as BLEU, METEOR, and TERp. Additionally, we evaluate the efficacy of our method by using the generated paraphrases as augmented samples in the task of paraphrase recognition on the Quora-PR dataset.\",\n  \"Data Augmentation Using DiPS\": \"We evaluate the importance of using high-quality paraphrases in two downstream classification tasks: intent classification and question classification. Our experiments show that DiPS provides generation with high structural variations without compromising on fidelity, leading to substantial gains in accuracy scores on both types of classification models.\",\n  \"Conclusion and Future Work\": \"In this paper, we have proposed DiPS, a model which generates high-quality paraphrases by maximizing a novel submodular objective function designed specifically for paraphrasing. Through extensive experiments on multiple standard datasets, we have demonstrated the effectiveness of our approach over numerous baselines. We observe that the diverse paraphrases generated are not only interesting and meaning-preserving but are also helpful in data augmentation.\"\n}\n```",
    "```json\n{\n  \"Introduction to Automatic Machine Translation Evaluation Metrics\": \"Automatic Machine Translation (MT) evaluation metrics have been criticized for a variety of reasons. However, the relatively consistent correlation of higher BLEU scores and better human judgements in major machine translation shared tasks has led to the conventional wisdom that translations with significantly higher BLEU scores generally suggest a better translation than its lower scoring counterparts.\",\n  \"BLEU Metric and Its Limitations\": \"Papineni et al. (2002) originally define BLEU n-gram precision p n by summing the n-gram matches for every hypothesis sentence S in the test corpus C. BLEU is a precision based metric; to emulate recall, the brevity penalty (BP) is introduced to compensate for the possibility of high precision translation that are too short. The resulting system BLEU score is calculated as follows: BLEU = BP \u00d7 exp( N n=1 w n log p n ). A BLEU score can range from 0 to 1 and the closer to 1 indicates that a hypothesis translation is closer to the reference translation.\",\n  \"Real-World Example of BLEU Score Discrepancy\": \"In this paper, we demonstrate a real-world example of machine translation that yielded high automatic evaluation scores but failed to obtain a good score on manual evaluation in an MT shared task submission. The unigram, bigram, trigrams and fourgrams (p 1, p 2, p 3, p 4) precision of the hypothesis translation are 90.0, 78.9, 66.7 and 52.9 respectively. The p n score for the hypothesis sentence precision score for the reference is 70.75. When considering the brevity penalty of 0.905, the overall BLEU is 64.03. Comparatively, the n-gram precisions for the baseline translations are p 1=84.2, p 2=66.7, p 3=47.1 and p 4=25.0 and the overall BLEU is 43.29 with a BP of 0.854. In this respect, one would consider the baseline translation inferior to the hypothesis with a >10 BLEU difference. However, there is only a subtle difference between the hypothesis and the baseline translation (\u305d \u308c \u305e \u308cvs \u592b\u3005). This is an actual example from the 2nd Workshop on Asian Translation (WAT 2015) MT shared task evaluation, and five crowd-sourced evaluators consider the baseline translation a better translation.\",\n  \"RIBES Metric and Its Limitations\": \"To overcome reordering, the RIBES score was introduced by adding a rank correlation coefficient prior to unigram matches without the need for higher order n-gram matches. The example above shows the marginal effectiveness of RIBES when penalizing wrongly ordered phrases in the hypothesis. The baseline translation accurately translates the meaning of the sentence with a minor partial translation of the technical variables. However, the hypothesis translation made serious adequacy errors when inverting the values of the technical variables but the hypothesis translation was minimally penalized in RIBES and also BLEU. The RIBES score for the hypothesis and baseline translations are 94.04 and 86.33 respectively whereas their BLEU scores are 53.3 and 58.8. In the WAT 2015 evaluation, five evaluators unanimously voted in favor for the baseline translation. Although the RIBES score presents a wider difference between the hypothesis and baseline translation than BLEU, it is insufficient to account for the arrant error that the hypothesis translation made.\",\n  \"Alternative Approaches to BLEU and RIBES\": \"There are other automatic MT evaluation metrics that depend on the same precision-based score with primary differences in how the Count match (ngram) is measured. Banerjee and Lavie (2005) introduced the METEOR metric that allows hypotheses' n-grams to match paraphrases and stems instead of just the surface strings. Lin and Och (2004) presented the ROUGE-S metrics that uses skip-grams matches. More recently, pre-trained regression models based on semantic textual similarity and neural network-based similarity measures trained on skip-grams are applied to replace the n-gram matching. While enriching the surface n-gram matching allows the automatic evaluation metric to handle variant translations, it does not resolve the 'prominent crudeness' of BLEU involving (i) the omission of content-bearing materials not being penalized, and (ii) the inability to calculate recall despite the brevity penalty.\",\n  \"Experimental Setup for WAT 2015 Shared Task\": \"We describe our system submission to the WAT 2015 shared task for Korean to Japanese patent translation. The Japan Patent Office (JPO) Patent Corpus is the official resource provided for the shared task. The training dataset is made up of 1 million sentences. We used the phrase-based SMT implemented in the Moses toolkit with the following vanilla Moses experimental settings: MGIZA++ implementation of IBM word alignment model 4, bi-directional lexicalized reordering model, compression of the phrasetable and lexical reordering model, language modeling using KenLM with 5-grams, and Minimum Error Rate Training (MERT) to tune the decoding parameters.\",\n  \"Human Evaluation Methodology\": \"The human judgment scores for the WAT evaluations were acquired using the Lancers crowdsourcing platform. Human evaluators were randomly assigned documents from the test set. They were shown the source document, the hypothesis translation, and a baseline translation generated by the baseline phrase-based MT system. Five evaluators were asked to judge each document. The crowdsourced evaluators were non-experts, thus their judgments were not necessarily precise, especially for patent translations. The evaluators were asked to judge whether the hypothesis or the baseline translation was better, or they were tied. The translation that was judged better constituted a win and the other a loss. The final human judgment score, HUMAN, is calculated as follows: HUMAN = 100 \u00d7 W \u2212 L W + L + T. By definition, the HUMAN score ranges from \u2212100 to +100, where higher is better.\",\n  \"Results and Analysis of the WAT 2015 Submission\": \"Moses' default parameter tuning method, MERT, is non-deterministic, and hence it is advisable to tune the phrase-based model more than once. We repeated the tuning step and submitted the system translations that achieved the higher BLEU score for manual evaluation. We expect this system to score close to zero. We submitted a total of three sets of output to the WAT 2015 shared task, two of which underwent manual evaluation. Table 2 presents the BLEU scores achieved by our phrase-based MT system in contrast to the organizers' baseline phrase-based system. The difference in BLEU between the organizers' system and ours may be due to our inclusion of the second development set in building our language model and the inclusion of more training data by allowing a maximum of 80 tokens per document as compared to 40.\",\n  \"Segment Level Meta-Evaluation of BLEU and RIBES\": \"We perform a segment level meta-evaluation by calculating the BLEU and RIBES score difference for each hypothesis-baseline translation. Figures 1 and 2 show the correlations of the BLEU and RIBES score difference against the positive and negative human judgments score for every sentence. Figure 1 presents the considerable incongruity between our system's high BLEU improvements (>+60 BLEU) being rated marginally better than the baseline translation. There were even translations from our system with >+40 BLEU improvements that tied with the organizer's baseline translations. However, Figure 2 presents the awkward disparity where many segments with BLEU improvements were rated strongly as poorer translations when compared against the baseline. Many segments with high BLEU improvements were tied with the baseline translations, indicated by the grey bubbles across the positive BLEU scores.\",\n  \"Conclusion and Future Directions\": \"In this paper, we have demonstrated a real-world case where high BLEU and RIBES scores do not correlate with better human judgment. Using our system's submission for the WAT 2015 patent shared task, we presented several factors that might contribute to the poor correlation, and also performed a segment level meta-evaluation to identify segments where our system's high BLEU / RIBES improvements were deemed substantially worse than the baseline translations. We hope our results and analysis will lead to improvements in automatic translation evaluation metrics.\"\n}\n```",
    "```json\n{\n  \"Introduction to Search-based Structured Prediction\": \"Search-based structured prediction models the generation of natural language structure (part-of-speech tags, syntax tree, translations, semantic graphs, etc.) as a search problem. It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy.\",\n  \"Challenges in Search-based Structured Prediction\": \"Such imitation process can sometimes be problematic. One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance. Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned. All these problems harm the generalization ability of search-based structured prediction and lead to poor performance.\",\n  \"Knowledge Distillation in Search-based Structured Prediction\": \"In this paper, we propose to consider these two problems in an integrated knowledge distillation manner. We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble's output distribution on the reference states. We also let the ensemble randomly explore the search space and learn the single model to mimic ensemble's distribution on the encountered exploration states.\",\n  \"Ensemble Techniques for Improving Generalization\": \"To overcome the ambiguities in data, techniques like ensemble are often adopted. In this paper, we consider to use ensemble technique to improve the generalization ability of our search-based structured prediction model. In practice, we train M search-based structured prediction models with different initialized weights and ensemble them by the average of their output distribution.\",\n  \"Distillation from Reference States\": \"We started from changing the NLL learning objective in Algorithm 1 into the distillation loss as shown in Algorithm 2. Since such method learns the model on the states produced by the reference policy, we name it as distillation from reference.\",\n  \"Distillation from Exploration States\": \"One way to accomplish the imitation can be sampling search sequence from the ensemble and learn from the soft target on the sampled states. More concretely, we change \u03c0 R (s, y) into a policy \u03c0 E (s) which samples an action a from q(a | s) 1 T , where T is the temperature that controls the sharpness of the distribution. Since such distillation generate training instances from exploration, we name it as distillation from exploration.\",\n  \"Combining Distillation from Reference and Exploration\": \"Distillation from reference can encourage the model to predict the action made by the reference policy and distillation from exploration learns the model on arbitrary states. They transfer the generalization ability of the ensemble from different aspects. Hopefully combining them can further improve the performance. In this paper, we combine distillation from reference and exploration with the following manner: we use \u03c0 R and \u03c0 E to generate a set of training states. Then, we learn p(a | s) on the generated states.\",\n  \"Experimental Setup and Results for Dependency Parsing\": \"We perform experiments on two tasks: transition-based dependency parsing and neural machine translation. For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier. The best iteration and \u03b1 is determined on the development set. Our distillation parser outperforms all these greedy counterparts.\",\n  \"Experimental Setup and Results for Neural Machine Translation\": \"We conduct our experiments on a small machine translation dataset, which is the German-to-English portion of the IWSLT 2014 machine translation evaluation campaign. One-layer LSTM for both encoder and decoder with 256 hidden units are used. Our distillation translator outperforms these models. Both the parsing and machine translation experiments confirm that it's feasible to distill a reasonable search-based structured prediction model by just exploring the search space.\",\n  \"Analysis of Distillation Method's Effectiveness\": \"In this section, we first study the ensemble's behavior on 'problematic' states to show its generalization ability. Then, we empirically study the feasibility of fully learning from the distillation loss by studying the effect of \u03b1 in the distillation from reference setting. Finally, we show that learning from distillation loss is less sensitive to initialization and achieves a more stable model.\"\n}\n```",
    "```json\n{\n  \"Introduction to Adversarial Perturbations in Machine Learning\": \"Attacking a machine learning model with adversarial perturbations is the process of making changes to its input to maximize an adversarial goal, such as mis-classification or mis-translation. These attacks provide insight into the vulnerabilities of machine learning models and their brittleness to samples outside the training distribution. Lack of robustness to these attacks poses security concerns to safety-critical applications, e.g., self-driving cars.\",\n  \"Challenges in Adversarial Attacks on Discrete Text Data\": \"In discrete spaces such as natural language sentences, the situation is more problematic; even a flip of a single word or character is generally perceptible by a human reader. Thus, most of the mathematical framework in previous work is not directly applicable to discrete text data. Moreover, there is no canonical distance metric for textual data like the p norm in real-valued vector spaces such as images, and evaluating the level of semantic similarity between two sentences is a field of research of its own.\",\n  \"Proposed Criterion for Adversarial Examples in NLP\": \"We propose a simple but natural criterion for adversarial examples in NLP, particularly untargeted attacks on seq2seq models: adversarial examples should be meaning-preserving on the source side, but meaning-destroying on the target side. This feature is extremely important; given two sentences with equivalent meaning, we would expect a good model to produce two outputs with equivalent meaning.\",\n  \"A Framework for Evaluating Adversarial Attacks on Seq2Seq Models\": \"The goal of adversarial perturbations is to produce failure cases for the model M. Hence, the evaluation must include some measure of the target similarity between y and y M, which we will denote s tgt(y,\u0177 M). However, it is crucial, when evaluating adversarial attacks on MT models, that the discrepancy between the original and adversarial input sentence be quantified in a way that is sensitive to meaning. Let us denote such a source similarity score s src(x,x).\",\n  \"Similarity Metrics for Evaluating Adversarial Attacks\": \"Throughout \u00a72.1, we have not given an exact description of the semantic similarity scores s src and s tgt. Indeed, automatically evaluating the semantic similarity between two sentences is an open area of research and it makes sense to decouple the definition of adversarial examples from the specific method used to measure this similarity. In this section, we will discuss manual and automatic metrics that may be used to calculate it.\",\n  \"Gradient-Based Adversarial Attacks and Their Constraints\": \"We perform gradient-based attacks that replace one word in the sentence so as to maximize an adversarial loss function L adv, similar to the substitution attacks proposed in (Ebrahimi et al., 2018b). We propose simple modifications of the approach presented in \u00a73.1 to create adversarial perturbations at the word level that are more likely to preserve meaning. The basic idea is to restrict the possible word substitutions to similar words.\",\n  \"Experimental Evaluation of Adversarial Attacks and Metrics\": \"Our experiments serve two purposes. First, we examine our proposed framework of evaluating adversarial attacks (\u00a72), and also elucidate which automatic metrics correlate better with human judgment for the purpose of evaluating adversarial attacks (\u00a74.2). Second, we use this evaluation framework to compare various adversarial attacks and demonstrate that adversarial attacks that are explicitly constrained to preserve meaning receive better assessment scores (\u00a74.3).\",\n  \"Adversarial Training with Meaning-Preserving Attacks\": \"Adversarial training (Goodfellow et al., 2014) augments the training data with adversarial examples. We investigate whether adversarial training with meaning-preserving attacks can improve robustness while not impacting test performance as much as unconstrained attacks. We find that adversarial training with CharSwap attacks improves robustness while not impacting test performance as much as unconstrained attacks.\"\n}\n```",
    "```json\n{\n  \"Introduction to Discourse-Aware Machine Translation\": \"Handling discourse phenomena in translation is crucial, especially in ambiguous cases and ensuring coherent translations. Traditional machine translation systems often ignore these phenomena, focusing on translating sentences in isolation. However, recent research has shown that using context-aware neural machine translation (NMT) models can yield performance improvements. Our work aims to enhance understanding of how selected discourse phenomena are modeled in NMT, focusing on a simple discourse-aware model that demonstrates improvements over a discourse-agnostic baseline on an English-Russian subtitles dataset.\",\n  \"Traditional Approaches vs. Context-Aware NMT\": \"Earlier research on discourse phenomena in translation focused on specific phenomena like pronoun translation, discourse connectives, verb tense, and topic adaptation, with special-purpose features engineered to model these phenomena. With the advent of neural machine translation (NMT) models, an alternative approach is to provide additional context to an NMT system at training time, allowing it to induce relevant predictive features. While context-aware NMT models have shown performance improvements, it is still unclear which discourse phenomena are successfully handled and how they are modeled.\",\n  \"Proposed Context-Aware Model Architecture\": \"Our model is based on the Transformer architecture, which is modified to handle additional context. A source sentence and a context sentence are first encoded independently, and then a single attention layer, combined with a gating function, is used to produce a context-aware representation of the source sentence. This architecture ensures that the contextual information flows through the attention layer, making it more accurate and interpretable compared to simply concatenating input sentences.\",\n  \"Experimental Setup and Data\": \"We use the OpenSubtitles2018 corpus for English and Russian, applying data cleaning and selecting 2 million training instances. For development and testing, we randomly select two subsets of 10,000 instances from movies not encountered in training. Sentences were encoded using byte-pair encoding, with source and target vocabularies of about 32,000 tokens. We generally used the same parameters and optimizer as in the original Transformer.\",\n  \"Overall Performance and Analysis\": \"Our best model, using the previous sentence as context, achieves a 0.7 BLEU improvement over the discourse-agnostic model. The concatenation baseline appears less accurate than our introduced context-aware model, suggesting that our model is not only more amendable to analysis but also potentially more effective. Evaluating our model on a test set with shuffled context sentences shows a significant drop in performance, confirming that the model relies on context information to improve translation quality.\",\n  \"Analysis of Contextual Information Utilization\": \"We analyze the distribution of attention to context and find that the model mostly attends to <bos> and <eos> context tokens, with attention to words only when it wants to pass some contextual information. The model heavily attends to contextual history for the translation of some pronouns, indicating that context is especially helpful at the beginning of a sentence and for shorter sentences.\",\n  \"Impact of Context on Pronoun Translation\": \"Our analysis shows that the model attends heavily to the contextual history for the translation of some pronouns, leading to empirical improvements in translation. We observe larger improvements in performance when the antecedent for pronouns under consideration contains a noun, suggesting that the model has learned to attend to context for their translation.\",\n  \"Latent Anaphora Resolution through Attention Mechanism\": \"The results suggest that the context-aware model exploits information about the antecedent of an ambiguous pronoun. We hypothesize that the model's attention mechanism can be interpreted as a latent anaphora resolution. Experiments show that the attention weights are in much better agreement with the coreference system than any of the heuristics, indicating that the model is indeed performing anaphora resolution.\",\n  \"Comparison with External Coreference Resolution Systems\": \"Our experiments show that a context-aware neural machine translation system can implicitly learn coreference phenomena without any feature engineering. While the agreement between our model and the ground truth is 72%, which is 5% below the coreference system, it is significantly higher than the best heuristic, confirming that our model performs latent anaphora resolution.\"\n}\n```",
    "```json\n{\n  \"Introduction to Natural Language Inference (NLI) Task\": \"In this paper, we focus on the task of Natural Language Inference (NLI), which is known as a significant yet challenging task for natural language understanding. In this task, we are given two sentences which are respectively called premise and hypothesis. The goal is to determine whether the logical relationship between them is entailment, neutral, or contradiction. Recently, performance on NLI has been significantly boosted since the release of some high quality large-scale benchmark datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2017).\",\n  \"Discourse Marker Prediction (DMP) and Its Role in NLI\": \"People usually use some certain set of words to express the discourse relation between two sentences. These words, such as 'but' or 'and', are denoted as discourse markers. These discourse markers have deep connections with the intrinsic relations of two sentences and intuitively correspond to the intent of NLI, such as 'but' to 'contradiction', 'so' to 'entailment', etc. Very few NLI works utilize this information revealed by discourse markers.\",\n  \"Sentence Encoder Model for Discourse Marker Prediction\": \"Following , we use BookCorpus as our training data for discourse marker prediction, which is a dataset of text from unpublished novels, and it is large enough to avoid bias towards any particular domain or application. After preprocessing, we obtain a dataset with the form (S1, S2, m), which means the first half sentence, the last half sentence, and the discourse marker that connected them in the original text. Our goal is to predict the m given S1 and S2.\",\n  \"Discourse Marker Augmented Network Architecture\": \"As presented in Figure 1, we show how our Discourse Marker Augmented Network incorporates the learned encoder into the NLI model. Encoding Layer We denote the premise as P and the hypothesis as H. To encode the words, we use the concatenation of following parts: Word Embedding, Character Embedding, POS and NER tags, and Exact Match. For encoding, we pass all sequences of vectors into a bi-directional LSTM and obtain the results.\",\n  \"Integration of Discourse Marker Knowledge into NLI Model\": \"We combine the representations of the sentences computed above with the representations learned from DMP to obtain the final prediction. We use a linear function with softmax function to predict the logical relation from the given set: entailment, neutral or contradiction.\",\n  \"Reinforcement Learning for Label Confidence Optimization\": \"We propose a new objective function that combines both the log probabilities of the ground-truth label and a reward defined by the property of the datasets for the reinforcement learning. We use the REINFORCE algorithm to minimize the negative expected reward, which is defined to measure the distance to all the ideas of the annotators.\",\n  \"Experimental Setup and Datasets\": \"We use the Stanford CoreNLP toolkit to tokenize the words and generate POS and NER tags. The word embeddings are initialized by 300d Glove, the dimensions of POS and NER embeddings are 30 and 10. We apply Tensorflow r1.3 as our neural network framework. We set the hidden size as 300 for all the LSTM layers and apply dropout between layers with an initial ratio of 0.9, the decay rate as 0.97 for every 5000 step.\",\n  \"Performance Evaluation and Comparison with State-of-the-Art Models\": \"We present the ensemble results on both datasets in the bottom part of the table. We build an ensemble model which consists of 10 single models with the same architecture but initialized with different parameters. The performance of our model achieves 89.6% on SNLI, 80.3% on matched MultiNLI and 79.4% on mismatched MultiNLI, which are all state-of-the-art results.\",\n  \"Ablation Analysis and Contribution of Model Components\": \"We conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model. Firstly we only use the results of the sentence encoder model to predict the answer, in other words, we represent each sentence by a single vector and use dot product with a linear function to do the classification. The result is obviously not satisfactory, which indicates that only using sentence embedding from discourse markers to predict the answer is not ideal in large-scale datasets.\",\n  \"Semantic Analysis and Visualization of Model Performance\": \"In Figure 2, we show the performance on the three relation labels when the model is pre-trained on different discourse markers sets. We observe that some discourse markers such as 'if' or 'before' contribute much less than other words which have strong logical hints, although they actually improve the performance of the model. Compared to the other two categories, the 'contradiction' label examples seem to benefit the most from the pre-trained sentence encoder.\"\n}\n```",
    "```json\n{\n  \"Introduction to Simultaneous Interpretation and Cognitive Challenges\": \"Simultaneous Interpretation (SI) is an inherently difficult task that carries significant cognitive and attentional burdens. The role of the simultaneous interpreter is to accurately render the source speech in a given target language in a timely and precise manner. Interpreters employ a range of strategies, including generalization and summarization, to convey the source message as efficiently and reliably as possible. Unfortunately, the interpreter is pitched against the limits of human memory and stamina, and after only minutes of interpreting, the number of errors made by an interpreter begins to increase exponentially.\",\n  \"Role of Computer-Assisted Interpretation (CAI) in Supporting Interpreters\": \"The role of Computer-Assisted Interpretation (CAI) is to provide live and interactive terminology support to interpreters, which can improve the quality of interpreter output. However, there is a danger that these systems will provide too much information and increase the cognitive load imposed upon the interpreter. The ideal level of support depends on current interpreter performance, and the system can minimize distraction by providing assistance only when an interpreter is struggling.\",\n  \"Quality Estimation (QE) for Machine Translation and Its Adaptation for Simultaneous Interpretation\": \"Quality Estimation (QE) for machine translation (MT) systems takes in the source sentence and MT-generated outputs and estimates a measure of quality. The default, out-of-the-box, sentence-level feature set for QuEst++ includes seventeen features such as number of tokens in source/target utterances, average token length, n-gram frequency, etc. While this feature set is effective for evaluation of MT output, SI output is inherently different-full of pauses, hesitations, paraphrases, re-orderings and repetitions.\",\n  \"Interpretation-Specific Features for Enhancing QE Accuracy\": \"To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter. These include the ratio of pauses/hesitations/incomplete words, ratio of non-specific words, ratio of 'quasi-'cognates, and ratio of number of words. These features are designed to capture the unique aspects of SI output, such as the use of pauses as a cognitive strategy and the compression of output by replacing or omitting common nouns.\",\n  \"Evaluation Metrics for Simultaneous Interpretation Quality\": \"Novice interpreters are assessed for accuracy on the number of omissions, additions, and inaccurate renditions of lexical items and longer phrases. While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve. We use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and content-function word distinctions, and thus should be better equipped to deal with the disparity between MT and SI.\",\n  \"Data Sources and Corpus Details for Training and Testing QE Models\": \"For our EN-JA language data, we train the pipeline on combined data from seven TED Talks taken from the NAIST TED SI corpus, which provides human transcribed SI output from three interpreters of low, intermediate, and high levels of proficiency. Our EN-FR and EN-IT data are drawn from the EPTIC corpus, which provides source and interpreter transcripts for speeches from the European Parliament, as well as translations of transcripts of the source speech.\",\n  \"Experiments and Results: Comparing Baseline, Trimmed, and Proposed Feature Sets\": \"To evaluate the quality of our QE system, we use the Pearson's r correlation between the predicted and true METEOR for each language pair. We compare the baseline, trimmed, and proposed feature sets. Our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank) (+0.104), achieving its highest accuracy on the EN-FR dataset. Over all datasets, the gain of the proposed model is statistically significant at p < 0.05 by the pairwise bootstrap.\",\n  \"Analysis of Results and Insights into Feature Contributions\": \"We further present two analyses: ablation on the full feature set and a qualitative comparison. Relative difference in utterance length appears to aid Japanese and French above other languages. Cognates are particularly useful in EN-FR and EN-IT; this may be indicative of the corpus domain or of cognate frequency in those languages. While pauses and hesitations seem to aid the model in EN-FR and EN-IT, they appear to hinder EN-JA.\",\n  \"Qualitative Example of QE Model Performance\": \"Below is a qualitative EN-IT example with a METEOR score of 0.079 (being substantially lower than the average METEOR score across all datasets; 0.262). The baseline model prediction of its score was 0.127, and our proposed model, 0.066. This is an example in which a CAI system might offer assistance to an interpreter struggling to produce an accurate rendition.\",\n  \"Conclusion and Future Directions for Improving Interpreter Performance Evaluation\": \"We introduce a novel and effective application of QE to evaluate interpreter output, which could be immediately applied to allow CAI systems to selectively offer assistance to struggling interpreters. This work uses METEOR to evaluate interpreter output, but creation of fine-grained measures to evaluate various aspects of interpreter performance is an interesting avenue for future work.\"\n}\n```",
    "```json\n{\n  \"Introduction to Zero-Shot Learning in Text Classification\": \"Zero-shot learning (ZSL) for text classification aims to classify documents of classes which are absent from the learning stage. This scenario is particularly relevant in dynamic environments where new classes can emerge after the learning stage, such as on social media, where the number of topics is growing rapidly. In such cases, classification models are required to recognize the text of new topics using only general information, since labeled training instances are unfeasible to obtain for each new topic.\",\n  \"Challenges and Existing Approaches in Zero-Shot Learning\": \"Despite the importance of zero-shot learning, there are few studies that exploit one of the three main types of semantic knowledge (semantic attributes, concept ontology, and semantic word embeddings) in text classification. Moreover, some previous works used different datasets to train and test, but there is similarity between classes in the training and testing set, which does not work under a strict zero-shot scenario.\",\n  \"Proposed Two-Phase Framework for Zero-Shot Text Classification\": \"To tackle the zero-shot text classification problem, this paper proposes a novel two-phase framework together with data augmentation and feature augmentation. The framework consists of coarse-grained classification, which predicts whether an input document comes from seen or unseen classes, and fine-grained classification, which finally decides its class. Both phases are based on convolutional neural networks and do not require labeled data of unseen classes.\",\n  \"Data Augmentation Technique: Topic Translation\": \"In the first phase, a data augmentation technique called topic translation is applied to strengthen the capability of the framework to detect documents from unseen classes effectively. This technique translates a document from its original seen class to a new unseen class using analogy, creating augmented documents for the unseen classes by topic-translation from the documents of seen classes in the training dataset.\",\n  \"Feature Augmentation Using Integrated Semantic Knowledge\": \"In the second phase, feature augmentation based on semantic knowledge is used to provide additional information which relates the document and the unseen classes to generalize the zero-shot reasoning. The relationship vector shows the degree of relatedness between the word and the class according to semantic knowledge, and it is constructed using ConceptNet, a general knowledge graph, to create the relationship vectors.\",\n  \"Implementation Details and Dataset Information\": \"The experiments used two textual datasets: the DBpedia ontology dataset and the 20newsgroups dataset. The vocabulary size of each dataset was limited to the 20,000 most frequent words, and all numbers were excluded. The structure of the CNN layer in Phase 1 had three filter sizes [3, 4, 5] with 400 filters for each filter size, and the subsequent dense layer had 300 units. In Phase 2, the CNN layer had three filter sizes [2, 4, 8] with 600 filters for each filter size, and the two intermediate dense layers had 400 and 100 units respectively.\",\n  \"Experimental Results and Performance Analysis\": \"The evaluation of Phase 1 showed that our framework outperformed DOC on both datasets, and the augmented data improved the accuracy of detecting documents from unseen classes. The zero-shot classifier in Phase 2 achieved the highest accuracy when fed by the combination of all three types of inputs [v w ; v c ; v w,c ], asserting that the integration of semantic knowledge is an effective means for knowledge transfer from seen to unseen classes in the zero-shot scenario.\",\n  \"Comparison with Baseline Models and Other Approaches\": \"The proposed two-phase framework achieved the highest overall accuracy on both datasets and both unseen rates compared with four baselines: a count-based model, label similarity, RNN Autoencoder, and CNN+FC. The framework maintained the accuracy on seen classes while achieving competitive prediction accuracy on unseen classes.\",\n  \"Discussion on the Effectiveness of Data and Feature Augmentation\": \"The experiments demonstrated that data augmentation by topic translation improved the accuracy in detecting instances from unseen classes, while feature augmentation enabled knowledge transfer from seen to unseen classes for zero-shot learning. The framework and the integrated semantic knowledge achieved the highest overall accuracy compared with all the baselines and recent approaches in all settings.\",\n  \"Future Work and Potential Extensions\": \"In the future, the framework will be extended to do multi-label classification with a larger amount of data. Additionally, the study will explore how semantic units defined by linguists can be used in the zero-shot scenario.\"\n}\n```",
    "```json\n{\n  \"Introduction to Crisis Response through Social Media Analysis\": \"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters. In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks. Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic damage.\",\n  \"Challenges in Real-Time Tweet Classification During Crises\": \"In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time. Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining. However, the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).\",\n  \"Proposed Deep Learning Framework for Domain Adaptation and Semi-Supervised Learning\": \"In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework. In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks -one for semi-supervised learning and the other for domain adaptation.\",\n  \"Convolutional Neural Network Architecture for Feature Extraction\": \"The input to the network is a tweet t = (w 1 , . . . , w n ) containing words that come from a finite vocabulary V defined from the training set. The first layer of the network maps each of these words into a distributed representation R d by looking up a shared embedding matrix E \u2208 R |V|\u00d7d . We initialize the embedding matrix E in our network with word embeddings that are pretrained on a large crisis dataset. The output of the look-up layer is a matrix X \u2208 R n\u00d7d , which is passed through a number of convolution and pooling layers to learn higher-level feature representations.\",\n  \"Semi-Supervised Learning Component Using Graph-Based Embedding\": \"The semi-supervised branch (shown at the middle in Figure 1) induces structural similarity between training instances (labeled or unlabeled) in the source and target events. We adopt the recently proposed graph-based semi-supervised deep learning framework, which shows impressive gains over existing semi-supervised methods on multiple datasets. In this framework, a 'similarity' graph G first encodes relations between training instances, which is then used by the network to learn internal representations (i.e., embeddings).\",\n  \"Domain Adversarial Training for Invariant Feature Learning\": \"We achieve this by domain adversarial training of neural networks. We put a domain discriminator, another branch in the network, that takes the shared internal representation z as input, and tries to discriminate between the domains of the input -in our case, whether the input tweet is from D S or from D T . The domain discriminator is defined by a sigmoid function. We use the negative log-probability as the discrimination loss.\",\n  \"Training Algorithm and Hyperparameter Settings\": \"We use AdaDelta adaptive update to update the model parameters. In each iteration, we do three kinds of gradient updates to account for the three different loss components. The main challenge in adversarial training is to balance the competing components of the network. In our experiments, the weight of the domain adversary loss \u03bb d was fixed to 1e \u2212 8, and the weight of the semi-supervised loss \u03bb g was fixed to 1e \u2212 2.\",\n  \"Experimental Setup and Datasets\": \"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL). These datasets are comprised of millions of tweets collected through the Twitter streaming API using event-specific keywords/hashtags. To obtain the labeled examples for our task, we employed paid workers from the Crowdflower crowdsourcing platform. The annotation consists of two classes relevant and non-relevant.\",\n  \"Results and Discussion on Semi-Supervised Learning\": \"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets. It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines -supervised and self-training based semi-supervised. Specifically, the graph-based approach shows 4% to 13% absolute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively.\",\n  \"Results and Discussion on Domain Adaptation\": \"The results with domain adversarial training show improvements across both events -from 1.8% to 4.1% absolute gains in F1. These results attest that adversarial training is an effective approach to induce domain invariant features in the internal representation. Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.\"\n}\n```",
    "```json\n{\n  \"Introduction to Human-Computer Conversation\": \"Human-computer conversation is a critical and challenging task in AI and NLP. There have been two major streams of research in this direction, namely task oriented dialog and general purpose dialog (i.e., chit-chat). Task oriented dialog aims to help people complete specific tasks such as buying tickets or shopping, while general purpose dialog attempts to produce natural and meaningful conversations with people regarding a wide range of topics in open domains.\",\n  \"Challenges in General Purpose Dialog\": \"General purpose dialog is intrinsically different from machine translation. In machine translation, since every sentence and its translation are semantically equivalent, there exists a 1-to-1 relationship between them. However, in general purpose dialog, a general response (e.g., 'I don't know') could correspond to a large variety of input utterances. Previous Seq2Seq models, which treat all the utterance-response pairs uniformly and employ a single model to learn the relationship between them, will inevitably favor such general responses with high frequency.\",\n  \"Previous Approaches to Addressing General Responses\": \"There have been a few efforts attempting to address this issue in literature. Li et al. (2016a) proposed to use the Maximum Mutual Information (MMI) as the objective to penalize general responses. Xing et al. (2017) pre-defined a set of topics from an external corpus to guide the generation of the Seq2Seq model. However, it is difficult to ensure that the topics learned from the external corpus are consistent with that in the conversation corpus, leading to the introduction of additional noises.\",\n  \"Proposed Specificity Controlled Seq2Seq Model (SC-Seq2Seq)\": \"In our work, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. The specificity control variable then interacts with the usage representation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels.\",\n  \"Encoder-Decoder Framework in SC-Seq2Seq\": \"The encoder is to map the input utterance X into a compact vector that can capture its essential topics. The decoder is to generate a response Y given the hidden representations of the input utterance X under some specificity level denoted by the control variable s. Specifically, at step t, we define the probability of generating any target word y t by a 'mixture' of probabilities: p(y t ) = \u03b2p M (y t ) + \u03b3p S (y t ), where p M (y t ) denotes the semantic-based generation probability, p S (y t ) denotes the specificity-based generation probability, \u03b2 and \u03b3 are the coefficients.\",\n  \"Distant Supervision Techniques for Training SC-Seq2Seq\": \"We train our SC-Seq2Seq model by maximizing the log likelihood of generating responses over the training set D. Note here since s is an explicit control variable in our model, we need the triples (X, Y, s) for training. We introduce two ways of distant supervision on the specificity control variable s, namely Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF).\",\n  \"Experimental Setup and Baseline Methods\": \"We conduct our experiments on the public Short Text Conversation (STC) dataset released in NTCIR-13. We compare our proposed SC-Seq2Seq model against several state-of-the-art baselines: (1) Seq2Seq-att: the standard Seq2Seq model with the attention mechanism; (2) MMI-bidi: the Seq2Seq model using Maximum Mutual Information (MMI) as the objective function to reorder the generated responses; (3) MARM: the Seq2Seq model with a probabilistic framework to model the latent responding mechanisms; (4) Seq2Seq+IDF: an extension of Seq2Seq-att by optimizing specificity under the reinforcement learning framework.\",\n  \"Evaluation Metrics and Results\": \"For evaluation, we follow the existing work and employ both automatic and human evaluations: (1) distinct-1 & distinct-2: we count numbers of distinct unigrams and bigrams in the generated responses, and divide the numbers by total number of generated unigrams and bigrams. (2) BLEU: BLEU has been proved strongly correlated with human evaluations. BLEU-n measures the average n-gram precision on a set of reference sentences. (3) Average & Extrema: Average and Extrema projects the generated response and the ground truth response into two separate vectors by taking the mean over the word embeddings or taking the extremum of each dimension respectively, and then computes the cosine similarity between them.\",\n  \"Human Evaluation and Case Studies\": \"Table 4 shows the human evaluation results. We can observe that: (1) SC-Seq2Seq NIWF,s=1 generates the most informative responses and interesting (labeled as '+2') and the least general responses than all the baseline models. Meanwhile, SC-Seq2Seq NIWF,s=0 generates the most general responses (labeled as '+1'); (2) MARM generates the most bad responses (labeled as '+0'), which indicates the drawbacks of the unknown latent responding mechanisms; (3) The kappa values of our models are all larger than 0.4, considered as 'moderate agreement' regarding quality of responses.\",\n  \"Analysis of Usage Representations\": \"We also conduct some analysis to understand the usage representations of words introduced in our model. We randomly sample 500 words from our SC-Seq2Seq NIWF and apply t-SNE to visualize both usage and semantic embeddings. As shown in Figure 3, we can see that the two distributions are significantly different. In the usage space, words like '\u8102 \u80aa \u809d(fatty liver)' and '\u4e45 \u5750(outsit)' lie closely which are both specific words, and both are far from the general words like '\u80d6(fat)'. On the contrary, in the semantic space, '\u8102 \u80aa \u809d(fatty liver)' is close to '\u80d6(fat)' since they are semantically related, and both are far from the word '\u4e45\u5750(outsit)'.\"\n}\n```",
    "```json\n{\n  \"Introduction to QA-SRL and Its Challenges\": \"QA-SRL is a method for predicting the predicate-argument structures of a sentence using question-answer pairs. It is appealing because it is intuitive to non-experts and has been shown to closely match traditional predicate-argument structure annotation schemes. However, achieving high recall is challenging as it is relatively easy to gather answerable questions but difficult to ensure that every possible question is labeled for every verb.\",\n  \"Crowdsourcing QA-SRL Annotations\": \"We introduce a new, scalable approach for crowdsourcing QA-SRL annotations. This approach includes a streamlined web interface with an autosuggest mechanism and automatic quality control to boost recall, and a validation stage to ensure high precision. With this method, we produced QA-SRL Bank 2.0, a dataset with 133,479 verbs from 64,018 sentences across 3 domains, totaling 265,140 question-answer pairs, in just 9 days.\",\n  \"Annotation Pipeline and Interface Design\": \"Our crowdsourcing pipeline consists of a generation and validation step. In the generation step, a sentence with one of its verbs marked is shown to a single worker, who must write QA-SRL questions for the verb and highlight their answers in the sentence. The questions are passed to the validation step, where n workers answer each question or mark it as invalid. We provide an autocomplete drop-down to streamline question writing, which significantly reduces the number of keystrokes required to enter new questions after the first one.\",\n  \"Quality Control and Validation\": \"Quality judgments of question validity had moderate agreement, with about 89.5% of validator judgments rating a question as valid, and the agreement rate between judgments of the same question on whether the question is invalid is 90.9%. This gives a Fleiss's Kappa of 0.51. In the higher-density re-run, validators were primed to be more critical, resulting in 76.5% of judgments considering a question valid, and agreement was at 83.7%, giving a Fleiss's Kappa of 0.55.\",\n  \"Models for QA-SRL Parsing\": \"Our proposed parsers construct QA-SRL tuples in a three-step pipeline: identifying verbal predicates, unlabeled span detection, and question generation. We explore two models for span detection: a sequence-tagging model with BIO encoding and a span-based model which assigns a probability to every possible span. For question generation, we introduce two models: a local model that predicts the words for each slot independently and a sequence model that uses the machinery of an RNN to share information between slots.\",\n  \"Span Detection Models\": \"We explore two models for span detection: a sequence-tagging model with BIO encoding and a span-based model which assigns a probability to every possible span. The span-based model significantly improves over the BIO model in both precision and recall, although the difference is less pronounced under IOU matching.\",\n  \"Question Generation Models\": \"For question generation, we introduce two models: a local model that predicts the words for each slot independently and a sequence model that uses the machinery of an RNN to share information between slots. The sequential model's exact match accuracy is significantly higher, while word-level accuracy is roughly comparable, reflecting the fact that the local model learns the slot-level posteriors.\",\n  \"Evaluation Metrics and Results\": \"We evaluate span detection using a modified notion of precision and recall, and for question generation, we report the rate at which the predicted question exactly matches the gold question, as well as a relaxed match where we only count the question word (WH), subject (SBJ), object (OBJ) and Miscellaneous (Misc) slots. The sequential model's exact match accuracy is significantly higher, while word-level accuracy is roughly comparable.\",\n  \"Data Expansion and Retraining\": \"By detecting spans at a low probability cutoff, we over-generate QA pairs for already-annotated sentences. We pass these questions to the validation step of our crowdsourcing pipeline with n = 3 validators, resulting in new labels. This process increased the number of valid questions in the train and dev partitions by 20%. We retrained our final model on the training set extended with the new valid questions, yielding modest improvements on both span detection and question generation in the development set.\",\n  \"Final Human Evaluation and System Output\": \"We use the crowdsourced validation step to do a final human evaluation of our models. The final model (span-based span detection and sequential question generation) trained with the expanded data showed a question accuracy of 82.64% and span accuracy of 77.61% when generating a similar number of questions per sentence as were labeled in the original data annotation (2 questions / verb).\"\n}\n```",
    "```json\n{\n  \"Introduction to Constituency Parsing and Its Importance\": \"Devising fast and accurate constituency parsing algorithms is an important, long-standing problem in natural language processing. Parsing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection, and more recently, natural language inference and machine translation.\",\n  \"Overview of Existing Parsing Approaches and Their Limitations\": \"Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing. Transition-based models decompose the structured prediction problem into a sequence of local decisions, enabling fast greedy decoding but also leading to compounding errors. Chart-based models can incorporate structured loss functions during training and benefit from exact inference via the CYK algorithm but suffer from higher computational cost during decoding.\",\n  \"Proposed Model: Syntactic Distances for Constituency Parsing\": \"In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the concept of 'syntactic distance'. Our model is trained to reproduce the ordering between split points induced by the ground-truth distances by means of a margin rank loss. Crucially, our model works in parallel, which allows for an easy parallelization in modern parallel computing architectures for deep learning, such as GPUs.\",\n  \"Algorithm for Converting Parse Trees to Syntactic Distances\": \"We propose two algorithms, one to convert a parse tree into a compact representation based on distances between consecutive words. The syntactic distances of a parse tree are defined for each possible split point in the sentence, and the order induced by the syntactic distances fully specifies the order in which the sentence needs to be recursively split into smaller constituents.\",\n  \"Algorithm for Reconstructing Parse Trees from Syntactic Distances\": \"Algorithm 2 can reconstruct a unique binary tree from the output of the model (d,\u0109,t). The algorithm simply chooses the split point i with the maximumd i, and assigns to the span the predicted label\u0109 i. This makes the running time of our algorithm to be in O(n log n).\",\n  \"Model Architecture for Predicting Syntactic Distances and Constituent Labels\": \"We use a neural network architecture that stacks recurrent (LSTM) and convolutional layers. Words and tags are first mapped to sequences of embeddings, then concatenated together as inputs for a stack of bidirectional LSTM layers. To predict the constituent labels for each word, we pass the hidden states representations through a 2-layer network with softmax output. To calculate the syntactic distances for each position, the vectors are transformed through a 2-layer feed-forward network with a single output unit.\",\n  \"Training Objective and Loss Functions\": \"The training objective is the sum of the prediction losses of syntactic distances and constituent labels. We use a standard softmax classifier with a cross-entropy loss for constituent labels. For syntactic distances, we propose to minimize a pair-wise learning-to-rank loss, similar to those proposed in (Burges et al., 2005). This loss encourages the model to reproduce the full ranking order induced by the ground-truth distances.\",\n  \"Experimental Results and Performance Evaluation\": \"We evaluate our model on the standard Wall Street Journal (WSJ) part of the Penn Treebank (PTB) dataset and the Chinese Treebank (CTB) dataset. Our model achieves good performance for single-model constituency parsing trained without external data, achieving 91.8 F1 on PTB and 86.5 F1 on CTB, matching and sometimes outperforming recent chart-based and transition-based parsing models.\",\n  \"Ablation Study and Impact of Model Components\": \"We perform an ablation study by removing components from a network trained with the best set of hyperparameters and re-train the ablated version from scratch. This gives an idea of the relative contributions of each of the components in the model. The model trained with MSE loss underperforms considerably compared to a model trained with the rank loss.\",\n  \"Parsing Speed and Efficiency\": \"The prediction of syntactic distances can be batched in modern GPU architectures. The distance to tree conversion is a O(n log n) divide-and-conquer algorithm. In our setting, we use an NVIDIA TITAN Xp graphics card for running the neural network part, and the distance to tree inference is run on an Intel Core i7-6850K CPU, with 3.60GHz clock speed. Our model achieves 111.1 sentences per second, significantly more efficient than previous models.\"\n}\n```",
    "```json\n{\n  \"Introduction to Bilingual Lexicon Induction\": \"Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts.\",\n  \"Challenges and Advancements in Low-Resource Language Pairs\": \"Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. Many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint, such as using document-aligned data or a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework.\",\n  \"Embedding-Based Methods for Bilingual Lexicon Induction\": \"One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces.\",\n  \"Incorporating Linguistic Features: Orthographic Information\": \"These features can help identify words with common ancestry (such as the English-Italian pair agile-agile) and borrowed words (macaronimaccheroni). The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods.\",\n  \"Orthographic Extension of Word Embeddings\": \"This method augments the embeddings for all words in both languages before using them in the self-learning framework. To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the spelling of the word.\",\n  \"Orthographic Similarity Adjustment\": \"This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework, which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words.\",\n  \"Experimental Setup and Datasets\": \"We use the datasets used by Artetxe et al. (2017), consisting of three language pairs: English-Italian, English-German, and English-Finnish. Each dataset includes monolingual word embeddings for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations.\",\n  \"Results and Discussion\": \"For our experiments with orthographic extension of word embeddings, each embedding was extended by the size of the union of the alphabets of both languages. The size of this union was 199 for English-Italian, 200 for English-German, and 287 for English-Finnish. Table 1 compares our methods against the system of Artetxe et al. (2017), using scaling factors selected based on development data results.\",\n  \"Conclusion and Future Work\": \"In this work, we presented two techniques (which can be combined) for improving embedding-based bilingual lexicon induction for related languages using orthographic information and no parallel data, allowing their use with low-resource language pairs. These methods increased accuracy in our experiments, with both the combined and embedding extension methods providing significant gains over the baseline system. In the future, we want to extend this work to related languages with different alphabets and to extend other unsupervised bilingual lexicon induction systems.\"\n}\n```",
    "```json\n{\n  \"Introduction to Extractive Summarization\": \"We study extractive summarization in this work where salient word sequences are extracted from the source document and concatenated to form a summary. Existing supervised approaches to extractive summarization frequently use human abstracts to create annotations for extraction units. E.g., a source word is labelled 1 if it appears in the abstract, 0 otherwise.\",\n  \"Challenges with Existing Supervised Approaches\": \"Despite the usefulness, there are two issues with this scheme. First, a vast majority of the source words are tagged 0s, only a small portion are 1s. This is due to the fact that human abstracts are short and concise; they often contain words not present in the source. Second, not all labels are accurate. Source words that are labelled 0 may be paraphrases, generalizations, or otherwise related to words in the abstracts. These source words are often mislabelled.\",\n  \"Proposed Training Paradigm Using Cloze-Style Questions\": \"We explore a new training paradigm for extractive summarization. We convert human abstracts to a set of Cloze-style comprehension questions, where the question body is a sentence of the abstract with a blank, and the answer is an entity or a keyword. Because the questions cannot be answered by applying general world knowledge, system summaries are encouraged to preserve salient source content that is relevant to the questions (\u2248 human abstract) such that the summaries can work as a document surrogate to predict correct answers.\",\n  \"Attention Mechanism for Question-Answering\": \"An attention mechanism is used to locate parts of the summary that are relevant to Q k. We define \u03b1 k,i \u221d exp(q k W a h S i ) to represent the importance of the i-th summary word (h S i ) to answering the k-th question (q k ), characterized by a bilinear term. A context vector c k is constructed as a weighted sum of all summary words relevant to the k-th question, and it is used to predict the answer.\",\n  \"Reinforcement Learning for Extractive Summarization\": \"This study extends the work of (Lei et al., 2016) to use reinforcement learning to explore the space of extractive summaries. While the original work focuses on generating rationales to support supervised classification, the goal of our study is to produce fluent, generic document summaries. The question-answering (QA) task is designed to fulfill this goal and the QA performance is only secondary.\",\n  \"Reward Function Components\": \"The final reward function R(Y ) is a linear interpolation of all the components; \u03b3, \u03b1, \u03b2 are coefficients and we describe their parameter tuning in \u00a74. R(Y )=R a (Y )+\u03b3R b (Y )\u2212\u03b1R f (Y )\u2212\u03b2R s (Y ).\",\n  \"Comparison of Entity and Keyword-Based QA Pairs\": \"We experiment with two variants of our approach. 'EntityQ' uses QA pairs whose answers are named entities. 'KeywordQ' uses pairs whose answers are sentence root words. According to the R-1, R-2, and R-L scores presented in Table 2, both methods are superior to the baseline systems on the benchmark dataset, yielding 11.5 and 11.6 R-2 F-scores, respectively.\",\n  \"Experimental Setup and Results\": \"All training, validation, and testing was performed using the CNN dataset containing news articles paired with human-written highlights (i.e., abstracts). We compare our methods with state-of-the-art published systems, including both extractive and abstractive approaches. According to the R-1, R-2, and R-L scores presented in Table 2, both methods are superior to the baseline systems on the benchmark dataset, yielding 11.5 and 11.6 R-2 F-scores, respectively.\",\n  \"Future Work and Usability Studies\": \"Future work. We are interested in investigating approaches that automatically group selected summary segments into clusters. Each cluster can capture a unique aspect of the document, and clusters of text segments can be color-highlighted. Inspired by the recent work of Narayan et al. (2018), we are also interested in conducting the usability study to test how well the summary highlights can help users quickly answer key questions about the documents. This will provide an alternative strategy for evaluating our proposed method against both extractive and abstractive baselines.\"\n}\n```",
    "```json\n{\n  \"Introduction to Chinese Spelling Check Challenges\": \"Chinese spelling checkers are relatively difficult to develop, partly because no word delimiters exist among Chinese words and a Chinese word can contain only a single character or multiple characters. Furthermore, there are more than 13 thousand Chinese characters, instead of only 26 letters in English, and each with its own context to constitute a meaningful Chinese word. All these make Chinese spell checking a challengeable task.\",\n  \"SIGHAN 2015 Bake-off Overview\": \"Due to the greater challenge in detecting and correcting spelling errors in CFL leaners' written essays, SIGHAN 2015 Bake-off, again features a Chinese Spelling Check task, providing an evaluation platform for the development and implementation of automatic Chinese spelling checkers. Given a passage composed of several sentences, the checker is expected to identify all possible spelling errors, highlight their locations, and suggest possible corrections.\",\n  \"Data Sets for Chinese Spelling Check\": \"The learner corpus used in our task was collected from the essay section of the computer-based Test of Chinese as a Foreign Language (TOCFL), administered in Taiwan. The spelling errors were manually annotated by trained native Chinese speakers, who also provided corrections corresponding to each error. The essays were then split into three sets: Training Set, Dryrun Set, and Test Set.\",\n  \"Evaluation Metrics for Spelling Checkers\": \"The criteria for judging correctness are determined at two levels: (1) Detection level: all locations of incorrect characters in a given passage should be completely identical with the gold standard. (2) Correction level: all locations and corresponding corrections of incorrect characters should be completely identical with the gold standard. In addition to achieving satisfactory detection/correction performance, reducing the false positive rate is also important.\",\n  \"Task Description and Input-Output Format\": \"The goal of this task is to evaluate the capability of a Chinese spelling checker. A passage consisting of several sentences with/without spelling errors is given as the input. The checker should return the locations of incorrect characters and suggest the correct characters. Each character or punctuation mark occupies 1 spot for counting location. The input instance is given a unique passage number pid.\",\n  \"Data Preparation and Annotation\": \"The learner corpus used in our task was collected from the essay section of the computer-based Test of Chinese as a Foreign Language (TOCFL), administered in Taiwan. The spelling errors were manually annotated by trained native Chinese speakers, who also provided corrections corresponding to each error. The essays were then split into three sets: Training Set, Dryrun Set, and Test Set.\",\n  \"Performance Metrics and Confusion Matrix\": \"The confusion matrix used for performance evaluation includes TP (True Positive), FP (False Positive), TN (True Negative), and FN (False Negative). The criteria for judging correctness are determined at two levels: detection and correction. The evaluation tool uses these metrics to assess the performance of the spelling checkers.\",\n  \"Evaluation Results and Best Performing Systems\": \"The research team NCTU&NTUT achieved the lowest false positive rate at 0.0509. For the detection-level evaluations, the system result submitted by CAS achieved promising performance exceeding 0.7. The CAS system provided the best error detection results, achieving a high F1 score of 0.6404. For correction-level evaluations, the correction accuracy provided by the CAS system (0.6918) significantly outperformed the other teams.\",\n  \"Approaches and Linguistic Resources Used by Participants\": \"The CAS team proposes a unified framework for Chinese spelling correction. They used HMM-based approach to segment sentences and generate correction candidates. Then, a two-stage filter process is applied to re-ranking the candidates for choosing the most promising candidates. The NCTU&NTUT team proposes a word vector/conditional random field based spelling error detector. They utilize the error detection results to guide and speed up the time-consuming language model rescoring procedure.\",\n  \"Conclusions and Future Research Directions\": \"This paper provides an overview of SIGHAN 2015 Bake-off for Chinese spelling check, including task design, data preparation, evaluation metrics, performance evaluation results, and the approaches used by the participant teams. All data sets with gold standards and evaluation tool are made publicly available. The future direction focuses on the development of Chinese grammatical error correction and building new language resources to help improve existing techniques for computer-aided Chinese language learning.\"\n}\n```",
    "```json\n{\n  \"Introduction to the ACL Anthology\": \"The ACL Anthology is a service by the Association for Computational Linguistics (ACL) providing open access to proceedings of all ACL sponsored conferences and journal articles. It also hosts third-party computational linguistics literature from sister organizations and their national venues. The Anthology offers text and faceted search of indexed papers, author-specific pages, and can incorporate third-party metadata and services embedded within pages. As of the document, it hosts over 43,000 computational linguistics and natural language processing papers, along with their metadata, serving over 4,500 daily requests.\",\n  \"Technical Infrastructure and Hosting\": \"The Anthology's technology stack includes Ruby on Rails for the framework, Solr for the search engine, PostgreSQL for the database, and Nginx/Jetty for the web server. The operating system is Debian GNU-Linux. Hosting and bandwidth have historically been provided by universities free of charge, with the Anthology currently hosted at Saarland University. Future plans include hosting under the umbrella of the ACL itself, unifying all services under https://www.aclweb.org/portal/.\",\n  \"Paper Ingestion and Metadata Management\": \"The most important task is the importing, indexing, and provisioning of newly accepted papers from recent conference proceedings and journal issues. The original Anthology defined an XML format for simple bibliographic metadata, which has been extended to support recent features like associated software, posters, videos, and datasets. The process of importing papers is semi-automated, largely integrated with the mechanisms for managing ACL conference submissions and printed proceedings.\",\n  \"Community Involvement and Volunteer Management\": \"The Anthology is maintained through volunteer efforts coordinated by the Anthology editor. Volunteers join and leave the team in unpredictable patterns, making knowledge preservation about the Anthology's operational workflow a key challenge. The Anthology editor ensures continuity, but there is a concerted effort to improve documentation for all tasks related to maintaining the Anthology to reduce the learning curve for new volunteers.\",\n  \"Documentation and Future-Proofing Efforts\": \"All code, documentation, bug reports, and feature requests are hosted on GitHub. Documentation has been expanded based on problems and questions encountered during the setup and update process of a new Anthology instance. This effort aims to reduce the learning curve for new volunteers and make it easier to migrate the Anthology to new servers or create mirrors.\",\n  \"Security and Dependency Management\": \"Security is a major concern, with older dependencies increasing exposure to unpatched bugs. To tackle issues with outdated software, the Anthology volunteer group is working on making the entire Anthology available via a Docker image. Docker provides a virtualized environment that abstracts away the complex server setup and makes it easier to tackle dependency problems independently from future mirror deployments.\",\n  \"Docker Integration for Enhanced Robustness\": \"The Anthology volunteer group is working on making the entire Anthology available via a Docker image. Docker provides a virtualized environment that abstracts away the relatively complex server setup and makes it easier to tackle dependency problems independently from future mirror deployments. This makes it possible for member institutions and even interested individual members to easily provide a mirror or experiment with the data in the Anthology.\",\n  \"Challenges and Future Extensions\": \"Maintaining community buy-in for the Anthology is necessary to ensure its future. This is best assured by extending the Anthology with useful capabilities that align with research efforts. The Anthology is currently stable and supports its current, intended use, but challenges include hosting mirrors, adding and indexing new publications, maintaining and updating the code, and extending the Anthology's capabilities.\",\n  \"Anonymous Pre-prints and Community Control\": \"A current challenge is the increasing popularity of pre-prints and their role in promoting scientific progress. However, such pre-print systems are not anonymous, interfering with the gains that author-blinded publications help in combating bias. One solution would be the use of anonymous pre-prints as an option for authors, which could be implemented through a collaboration with an existing pre-print service or by hosting pre-prints directly within the Anthology.\",\n  \"Reviewer Matching Using Anthology Data\": \"The Anthology data can be leveraged to support conference organizers in the assignment of potential peer reviewers. Data for judging the fit between a reviewer and submitted papers are available in the Anthology, such as a reviewer's interests and expertise as encoded in their previous publications. Mining and representing such information directly from the Anthology makes it unnecessary to upload papers to an external platform, mitigating current low response rates.\"\n}\n```",
    "```json\n{\n  \"Introduction to Conversational Interfaces and Their Use Cases\": \"Due to recent advances in speech recognition and language understanding, conversational interfaces such as Alexa, Cortana, and Siri are becoming more common. They currently have two large uses cases. First, a user can use them to complete a specific task, such as playing music. Second, a user can use them to ask questions where the questions are answered by querying knowledge graph or database back-end.\",\n  \"Unified Framework for Q&A and SLU Using Executable Semantic Parsing\": \"In this work, we propose executable semantic parsing as a common framework for both use cases by framing SLU as executable semantic parsing that unifies the two use cases. For Q&A, the input utterances are parsed into logical forms that represent the machine-readable representation of the question, while in SLU, they represent the machine-readable representation of the user intent and slots.\",\n  \"Neural Transition-based Parsing Model with Stack-LSTMs\": \"In this paper, we consider the neural executable semantic parser of Cheng et al. (2017), which follows the transition-based parsing paradigm. Its transition system differs from traditional systems as the words are not consumed from the buffer because in executable semantic parsing, there are no strict alignments between words in the input and nodes in the tree. The neural architecture encodes the buffer using a Bi-LSTM and the stack as a Stack-LSTM, a recurrent network that allows for push and pop operations.\",\n  \"Handling Rare and Unknown Words with Copy Mechanism\": \"A popular way to deal with the data sparsity problem is to delexicalize the data, that is replacing rare and unknown words with coarse categories. Alternatively, it is possible to use a copy mechanism to enable the decoder to copy rare words from the input rather than generating them from its limited vocabulary. We compare the two solutions across all data sets on the right side of Table 2. Regardless of the data set, the copy mechanism generally outperforms delexicalization.\",\n  \"Transfer Learning Techniques for Low-Resource Domains\": \"We consider the scenario where large training corpora are available for some domains and we want to bootstrap a parser for a new domain where little training data is available. We investigate the use of two transfer learning approaches: pre-training and multi-task learning. For MTL, the different tasks share most of the architecture and only the output layers, which are responsible for predicting the output tokens, are separate for each task.\",\n  \"Data Sets and Their Characteristics for Q&A and SLU\": \"For Q&A, we consider Overnight (Wang et al., 2015b) and NLmaps (Lawrence and Riezler, 2016). Overnight It contains sentences annotated with Lambda DCS (Liang, 2013). The sentences are divided into eight domains: calendar, blocks, housing, restaurants, publications, recipes, socialnetwork, and basketball. NLmaps It contains more than two thousand questions about geographical facts, retrieved from OpenStreetMap (Haklay and Weber, 2008).\",\n  \"Experimental Results and Analysis on Different Data Sets\": \"We first run experiments on single-task semantic parsing to observe the differences among the three different data sources discussed in Section 4. Specifically, we explore the impact of an attention mechanism on the performance as well as the comparison between delexicalization and a copy mechanism for dealing with data sparsity. The metric used to evaluate parsers is the exact match accuracy, defined as the ratio of sentences correctly parsed.\",\n  \"Impact of Attention Mechanism on Parsing Performance\": \"Because the buffer is not consumed as in traditional transition-based parsers, Cheng et al. (2017) use an additive attention mechanism (Bahdanau et al., 2015) to focus on the more relevant words in the buffer for the current state of the stack. In order to find the impact of attention on the different data sets, we run ablation experiments, as shown in Table 2 (left side). We found that attention between stack and buffer is not always beneficial: it appears to be helpful for larger data sets while harmful for smaller data sets.\",\n  \"Comparison of Delexicalization and Copy Mechanism for Data Sparsity\": \"A popular way to deal with the data sparsity problem is to delexicalize the data, that is replacing rare and unknown words with coarse categories. Alternatively, it is possible to use a copy mechanism to enable the decoder to copy rare words from the input rather than generating them from its limited vocabulary. We compare the two solutions across all data sets on the right side of Table 2. Regardless of the data set, the copy mechanism generally outperforms delexicalization.\",\n  \"Conclusions and Future Work on Transfer Learning Across Heterogeneous Data Sets\": \"We framed SLU as an executable semantic parsing task, which addresses a limitation of current commercial SLU systems. By applying our framework to different data sets, we demonstrate that the framework is effective for Q&A as well as for SLU. We explored a typical scenario where it is necessary to learn a semantic parser for a new domain with little data, but other high-resource domains are available. We show the effectiveness of our system and both pre-training and MTL on different domains and data sets. Preliminary experiment results on transfer learning across domains belonging to heterogeneous data sets suggest future work in this area.\"\n}\n```",
    "```json\n{\n  \"Introduction to Cross-Language Plagiarism\": \"Plagiarism is a significant issue, especially in higher education. While monolingual plagiarism is well-addressed, the expansion of the Internet and machine translation tools have facilitated cross-language plagiarism, where a text is plagiarized and translated into another language. Detecting this type of plagiarism is challenging because the suspicious document is no longer in the same language as its source.\",\n  \"Challenges in Cross-Language Plagiarism Detection\": \"The challenge in detecting cross-language plagiarism lies in the fact that the suspicious document is no longer in the same language as its source. This makes it difficult to identify similarities using traditional methods that rely on lexical and syntactic features in a single language.\",\n  \"Use of Word Embeddings for Plagiarism Detection\": \"Word embeddings, which represent words in a continuous space based on their context, can be used to calculate similarity between sentences in the same or different languages. This is appealing for plagiarism detection as they capture synonymy and morphological closeness intrinsically. We use the MultiVec toolkit for computing and managing these continuous representations.\",\n  \"State-of-the-Art Methods for Cross-Language Textual Similarity\": \"Several state-of-the-art methods are used for cross-language textual similarity detection, including Cross-Language Character N-Gram (CL-CnG), Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS), Cross-Language Alignment-based Similarity Analysis (CL-ASA), Cross-Language Explicit Semantic Analysis (CL-ESA), and Translation + Monolingual Analysis (T+MA).\",\n  \"Evaluation Protocol and Dataset\": \"The evaluation protocol involves building a distance matrix of size N x M, where each textual unit is compared to itself and to M-1 other units randomly selected from the sub-corpus. The dataset used is multilingual, containing French, English, and Spanish texts, and is based on both parallel and comparable corpora, including Wikipedia, conference papers, product reviews, Europarl, and JRC.\",\n  \"Proposed Methods: CL-CTS-WE and CL-WES\": \"We propose two new methods: CL-CTS-WE, which replaces the lexical resource used in CL-CTS with distributed representations of words, and CL-WES, which performs a direct comparison between two sentences in different languages using word embeddings. CL-WES builds bilingual common representation vectors and applies cosine similarity between them.\",\n  \"Syntax Weighting in Distributed Representations (CL-WESS)\": \"CL-WESS improves CL-WES by introducing a syntax flavor. It syntactically tags the textual units, assigns weights to each type of tag, and optimizes these weights to improve the F1 score. The syntactic aggregation formula uses the weight of each part-of-speech tag to compute the final vector representation of the unit.\",\n  \"Combining Multiple Methods for Improved Detection\": \"We combine multiple methods to improve cross-language similarity detection performance. This includes weighted fusion, where we assign weights to the similarity scores of each method, and decision tree fusion, which uses the C4.5 algorithm to determine the optimal decision tree for combining methods.\",\n  \"Results and Discussion on Method Performance\": \"The use of word embeddings improves CL-CTS, with CL-CTS-WE showing an overall performance gain of +3.83% on chunks and +3.19% on sentences. CL-WESS, the syntactically weighted version of CL-WES, is significantly better than CL-C3G and is the best single method evaluated so far on our corpus. Decision tree fusion leads to an overall F1 score of 89.15% at chunk level and 88.50% at sentence level.\",\n  \"Conclusion and Future Work\": \"Our work has shown that augmenting baseline approaches with word embeddings can significantly improve cross-language textual similarity detection. The most promising approach is CL-WESS, which beats the previous best state-of-the-art method. Future work will focus on improving CL-WESS by analyzing syntactic weights and adapting them according to the plagiarist's stylometry.\"\n}\n```",
    "```json\n{\n  \"Introduction to Mental State Tracking in Stories\": \"Understanding a story requires reasoning about the causal links between the events in the story and the mental states of the characters, even when those relationships are not explicitly stated. This reasoning is remarkably hard for both statistical and neural machine readers -despite being trivial for humans. In this paper, we construct a new annotation formalism to densely label commonsense short stories in terms of the mental states of the characters.\",\n  \"Challenges in Understanding Commonsense Stories\": \"Unique Challenges While there have been a variety of annotated resources developed on the related topics of sentiment analysis, entity tracking, and story understanding, our study is the first to annotate the full chains of mental state effects for story characters. This poses several unique challenges as annotations require (1) interpreting discourse (2) understanding implicit causal effects, and (3) understanding formal psychology theory categories.\",\n  \"Annotation Formalism for Mental States\": \"The dataset provides a fully-specified chain of motivations and emotional reactions for each story character as preand post-conditions of events. Second, the annotations include state changes for entities even when they are not mentioned directly in a sentence, thereby capturing implied effects unstated in the story. Finally, the annotations encompass both formal labels from multiple theories of psychology as well as open text descriptions of motivations and emotions, providing a comprehensive mapping between open text explanations and label categories.\",\n  \"Theoretical Foundations: Motivation and Emotion Theories\": \"We draw from these prior works to derive a set of categorical labels for annotating the step-by-step causal dynamics between the mental states of story characters and the events they experience. We use two popular theories of motivation: the 'hierarchy of needs' of Maslow and the 'basic motives' of Reiss to compile 5 coarse-grained and 19 fine-grained motivation categories. Among several theories of emotion, we work with the 'wheel of emotions' of Plutchik, as it has been a common choice in prior literature on emotion categorization.\",\n  \"Annotation Framework and Pipeline\": \"We introduce a new annotation framework that pipelines a set of smaller isolated tasks as illustrated in Figure 3. The tasks include entity resolution, action resolution, affect resolution, motivation annotation, and emotional reaction annotation. Each task is designed to be completed by crowdsourced workers from Amazon Mechanical Turk.\",\n  \"Dataset Statistics and Insights\": \"Our dataset to date includes a total of 300k low-level annotations for motivation and emotion across 15,000 stories. It covers over 150,000 character-line pairs, in which 56k character-line pairs have an annotated motivation and 105k have an annotated change in emotion. The dataset is split into training, development, and test sets, with the categorization sets being the hardest and most expensive in the pipeline.\",\n  \"New Tasks and Baseline Models\": \"The multiple modes covered by the annotations in this new dataset allow for multiple new tasks to be explored. We outline three task types below, covering a total of eight tasks on which to evaluate: state classification, annotation classification, and explanation generation. Baseline models include encoders based on word embeddings, common neural network architectures, or memory networks.\",\n  \"Experimental Setup and Results\": \"We show results on the test set for categorizing Maslow, Reiss, and Plutchik states. Despite the difficulty of the task, all models outperform the random baseline. The best performing models in each task are most effective at predicting Maslow physiological needs, Reiss food motives, and Plutchik reactions of joy.\",\n  \"Impact of Context and Pretraining on Model Performance\": \"The performance boost from adding entity-specific contextual information (i.e., not ablating h c ) indicates that the models learn to condition on a character's previous experience to classify its mental state at the current time step. Pretraining encoder parameters using the free response annotations from the training set offers a clear performance boost for all models on all three prediction tasks.\",\n  \"Related Work and Future Directions\": \"Incorporating emotion theories into NLP tasks has been explored in previous projects. Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories. We view our dataset as a future testbed for evaluating models trained on any number of resources for learning common sense about emotional reactions and motivations.\"\n}\n```",
    "{\n  \"Introduction to the Study of Language Use and Occupational Class\": \"The growth of online social networks provides the opportunity to analyse user text in a broader context. Social status reflected through a person's occupation is a factor which influences language use. Therefore, our hypothesis is that language use in social media can be indicative of a user's occupational class. Focusing on the microblogging platform of Twitter, we explore our hypothesis by studying the task of predicting a user's occupational class given platform-related attributes and generated content, i.e. tweets. This has direct applicability in a broad range of areas from sociological studies, which analyse the behaviour of different occupations, to recruiting companies that target people for new job opportunities.\",\n  \"Standard Occupational Classification (SOC) Taxonomy\": \"To enable the user occupation study, we adopt a standardised job classification taxonomy for mapping Twitter users to occupations. The Standard Occupational Classification (SOC) is a UK government system developed by the Office of National Statistics for classifying occupations. Jobs are categorised hierarchically based on skill requirements and content. The SOC scheme includes nine major groups coded with a digit from 1 to 9. Each major group is divided into sub-major groups coded with 2 digits, where the first digit indicates the major group. Each sub-major group is further divided into minor groups coded with 3 digits and finally, minor groups are divided into unit groups, coded with 4 digits. The unit groups are the leaves of the hierarchy and represent specific jobs related to the group.\",\n  \"Creation of the Twitter Data Set for Occupational Class Prediction\": \"To the best of our knowledge there are no publicly available data sets suitable for the task we aim to investigate. Thus, we have created a new one consisting of Twitter users mapped to their occupation, together with their profile information and historical tweets. We use the account's profile information to capture users with self-disclosed occupations. We queried Twitter's Search API to retrieve for each job title a maximum of 200 accounts which best matched occupation keywords. This process produced a total number of 5,191 users from 55 minor groups (22 sub-major groups), spread across all nine major SOC groups. The final data set consists of 10,796,836 tweets collected around 5 August 2014 and is openly available.\",\n  \"Feature Extraction for User Occupational Class Prediction\": \"In this section, we overview the features used in the occupational class prediction task. They are divided into two types: (1) user level features, (2) textual features. User Level Features (UserLevel) include general user information or aggregated statistics about the tweets. Textual Features are derived from the aggregated set of user's tweets. We use our reference corpus to represent each user as a distribution over these features. We ignore the bio field from building textual features to avoid introducing biases from our data collection method. SVD Word Embeddings (SVD-E) and NPMI Clusters (SVD-C) are used to create abstract representations of words and clusters of words, respectively. Neural Embeddings (W2V-E) and Neural Clusters (W2V-C) are also used to obtain clusters of related words, i.e. 'topics'.\",\n  \"Classification with Gaussian Processes\": \"In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for using this method. GPs formulate a Bayesian non-parametric machine learning framework which defines a prior on functions. The properties of the functions are given by a kernel which models the covariance in the response values as a function of its inputs. Binary classification using GPs 'squashes' the real valued latent function f(x) output through a logistic function. The object of the GP inference is the distribution of the latent variable corresponding to a test case. Although we could use multi-class classification methods, in order to provide insight, we perform a separate one-vs-all classification for each class and then determine a label through the occupational class that has the highest likelihood.\",\n  \"Experimental Results and Predictive Accuracy\": \"We assign users to one of nine possible classes. The accuracy of our classifiers is measured on held-out data. Our data set is divided into stratified training (80%), validation (10%) and testing (10%) sets. The validation set was used to learn the LR and SVM hyperparameters, while the GP did not use this set at all. We report results using all three methods and all feature sets. We first observe that user level features are not useful for predicting the job class. This finding indicates that general social behaviour or user impact are likely to be spread evenly across classes. It also highlights the difficulty of the task and motivates the use of deeper textual features. The textual features improve performance as compared to the most frequent class baseline. GPs usually outperform SVMs by a small margin.\",\n  \"Error Analysis of Classification Results\": \"To illustrate the errors made by our classifiers, Figure 1 shows the confusion matrix of the classification results. First, we observe that class 4 is many times classified as class 2 or 3. This can be explained by the fact that classes 2, 3 and 4 contain similar types of occupations, e.g. doctors and nurses or accountants and assistant accountants. However, with very few exceptions, we notice that only adjacent classes get misclassified, suggesting that our model captures the general user skill level.\",\n  \"Qualitative Analysis of Word Clusters and Topic Usage\": \"The word clusters that were built from a reference corpus and then used as features in the GP classifier, give us the opportunity to extract some qualitative derivations from our predictive task. For the rest of the section we use the best performing model of this type (W2V-C-200) in order to analyse the results. Our main assumption is that there might be a divergence of language and topic usage across occupational classes following previous studies in sociology. Knowing that the inferred GP lengthscale hyperparameters are inversely proportional to feature (i.e. topic) relevance, we can use them to rank the topic importance and give answers to our hypothesis. Table 4 shows 10 of the most informative topics (represented by the top 10 most central and frequent words) sorted by their ARD lengthscale Mean Reciprocal Rank (MRR) across the nine classifiers.\",\n  \"Comparison with Related Work\": \"Occupational class prediction has been studied in the past in the areas of psychology and economics. French (1959) investigated the relation between various measures on 232 undergraduate students and their future occupations. This study concluded that occupational membership can be predicted from variables such as the ability of subjects in using mathematical and verbal symbols, their family economic status, body-build and personality components. Sociolinguistic and sociology studies deduct that social status is an important factor in determining the use of language. However, a large scale investigation of this hypothesis has never been attempted. Relevant to our task is a relation extraction approach proposed by Li et al. (2014) aiming to extract user profile information on Twitter. Nonetheless, the information relevant to the job attribute regards the employer of a user (i.e. the name of a company) rather than the type of occupation.\",\n  \"Conclusions and Future Directions\": \"Our paper presents the first large-scale systematic study on language use on social media as a factor for inferring a user's occupational class. To address this problem, we have also introduced an extensive labelled data set extracted from Twitter. We have framed prediction as a classification task and, to this end, we used the powerful, non-linear GP framework that combines strong predictive performance with feature interpretability. Results show that we can achieve a good predictive accuracy, highlighting that the occupation of a user influences text use. Through a qualitative analysis, we have shown that the derived topics capture both occupation specific interests as well as general class-based behaviours. We acknowledge that the derivations of this study, similarly to other studies in the field, are reflecting the Twitter population and may experience a bias introduced by users self-mentioning their occupations. However, the magnitude, occupational diversity and face validity of our conclusions suggest that the presented approach is useful for future downstream applications.\"\n}",
    "```json\n{\n  \"Introduction to BabbleLabble Framework\": \"The BabbleLabble framework converts natural language explanations and unlabeled data into a noisily-labeled training set. There are three key components: a semantic parser, a filter bank, and a label aggregator. The semantic parser converts natural language explanations into a set of logical forms representing labeling functions (LFs). The filter bank removes as many incorrect LFs as possible without requiring ground truth labels. The remaining LFs are applied to unlabeled examples to produce a matrix of labels. This label matrix is passed into the label aggregator, which combines these potentially conflicting and overlapping labels into one label for each example. The resulting labeled examples are then used to train an arbitrary discriminative model.\",\n  \"Semantic Parser for Natural Language Explanations\": \"The semantic parser takes a natural language explanation and returns a set of LFs (logical forms or labeling functions) of the form f_i: X \u2192 {\u22121, 0, 1} in a binary classification setting, with 0 representing abstention. We choose a simple rule-based semantic parser that can be used without any training. The parser uses a set of rules of the form \u03b1 \u2192 \u03b2, where \u03b1 can be replaced by the token(s) in \u03b2. To identify candidate LFs, we recursively construct a set of valid parses for each span of the explanation, based on the substitutions defined by the grammar rules. At the end, the parser returns all valid parses (LFs in our case) corresponding to the entire explanation.\",\n  \"Filter Bank for Improving LF Quality\": \"The filter bank removes as many incorrect LFs as possible without requiring additional labels. It consists of two classes of filters: semantic and pragmatic. The semantic filter checks for LFs that are inconsistent with their corresponding example. The pragmatic filters remove LFs that are constant, redundant, or correlated. As we show in Section 4, over three tasks, the filter bank removes 86% of incorrect parses, and the incorrect ones that remain have average end-task accuracy within 2.5% of the corresponding correct parses.\",\n  \"Label Aggregator for Combining LF Outputs\": \"The label aggregator combines multiple (potentially conflicting) suggested labels from the LFs and combines them into a single probabilistic label per example. Concretely, if m LFs pass the filter bank and are applied to n examples, the label aggregator implements a function f: {\u22121, 0, 1} m\u00d7n \u2192 [0, 1] n. A naive solution would be to use a simple majority vote, but this fails to account for the fact that LFs can vary widely in accuracy and coverage. Instead, we use data programming, which models the relationship between the true labels and the output of the labeling functions as a factor graph.\",\n  \"Discriminative Model for Final Classification\": \"The noisily-labeled training set that the label aggregator outputs is used to train an arbitrary discriminative model. One advantage of training a discriminative model on the task instead of using the label aggregator as a classifier directly is that the label aggregator only takes into account those signals included in the LFs. A discriminative model, on the other hand, can incorporate features that were not identified by the user but are nevertheless informative. For the results reported in this paper, our discriminative model is a simple logistic regression classifier with generic features defined over dependency paths.\",\n  \"Experimental Setup and Datasets\": \"We evaluate the accuracy of BabbleLabble on three relation extraction tasks, which we refer to as Spouse, Disease, and Protein. The goal of each task is to train a classifier for predicting whether the two entities in an example are participating in the relationship of interest. Datasets statistics for each dataset are reported in Table 2, with one example and one explanation for each given in Figure 4 and additional explanations shown in Appendix B. We use existing implementations of the label aggregator, feature library, and discriminative classifier described in Sections 2.4-2.5 provided by the open-source project Snorkel.\",\n  \"High Bandwidth Supervision with BabbleLabble\": \"In Table 3 we report the average F1 score of a classifier trained with BabbleLabble using 30 explanations or traditional supervision with the indicated number of labels. On average, it took the same amount of time to collect 30 explanations as 60 labels. We observe that in all three tasks, BabbleLabble achieves a given F1 score with far fewer user inputs than traditional supervision, by as much as 100 times in the case of the Spouse task. Because explanations are applied to many unlabeled examples, each individual input from the user can implicitly contribute many (noisy) labels to the learning algorithm.\",\n  \"Utility of Incorrect Parses and Filter Bank Effectiveness\": \"In Table 4, we report LF summary statistics before and after filtering. LF correctness is based on exact match with a manually generated parse for each explanation. Surprisingly, the simple heuristic-based filter bank successfully removes over 95% of incorrect LFs in all three tasks, resulting in final LF sets that are 86% correct on average. Furthermore, among those LFs that pass through the filter bank, we found that the average difference in end-task accuracy between correct and incorrect parses is less than 2.5%.\",\n  \"Comparison of Using LFs as Functions vs. Features\": \"Once we have relevant logical forms from user-provided explanations, we have multiple options for how to use them. Srivastava et al. (2017) propose using these logical forms as features in a linear classifier. We choose instead to use them as functions for weakly supervising the creation of a larger training set via data programming. In Table 6, we compare the two approaches directly, finding that the data programming approach outperforms a feature-based one by 9.5 F1 points with the rule-based parser, and by 4.5 points with a perfect parser. We attribute this difference primarily to the ability of data programming to utilize unlabeled data.\",\n  \"Related Work and Discussion on Weak Supervision\": \"Our work has two themes: modeling natural language explanations/instructions and learning from weak supervision. The closest body of work is on 'learning from natural language.' As mentioned earlier, Srivastava et al. (2017) convert natural language explanations into classifier features (whereas we convert them into labeling functions). The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction. In distant supervision and multi-instance learning, an existing knowledge base is used to (probabilistically) impute a training set. Various extensions have focused on aggregating a variety of supervision sources by learning generative models from noisy labels.\"\n}\n```",
    "```json\n{\n  \"Introduction to Copy & Paste in NLP Archives\": \"Everything starts with a copy & paste and, of course the flood of documents that we see today could not exist without the practical ease of copy & paste. This is not new but what is new is that the availability of archives allows us to study a vast amount of papers in our domain (i.e. Natural Language Processing, NLP, both for written and spoken materials) and to figure out the level of reuse and plagiarism in this area.\",\n  \"Context and Previous Studies on Reuse and Plagiarism\": \"Our work comes after the various studies initiated in the Workshop entitled: 'Rediscovering 50 Years of Discoveries in Natural Language Processing' on the occasion of ACL's 50th anniversary in 2012 [Radev et al 2013] where a group of researchers studied the content of the corpus recorded in the ACL Anthology [Bird et al 2008]. Among these studies, one was devoted to reuse and it is worth quoting Gupta and Rosso [Gupta et al 2012]: 'It becomes essential to check the authenticity and the novelty of the submitted text before the acceptance. It becomes nearly impossible for a human judge (reviewer) to discover the source of the submitted work, if any, unless the source is already known. Automatic plagiarism detection applications identify such potential sources for the submitted work and based on it a human judge can easily take the decision'. Let's add that this subject is a specific and active domain ruled yearly by the PAN international plagiarism detection competition 1. On our side, we also conducted a specific study of reuse and plagiarism in the papers published at the Language Resources and Evaluation conference (LREC), from 1998 to 2014 [Francopoulo et al 2016].\",\n  \"Objectives of the Study\": \"Our aim is not to present the state-of-art or to compare the various metrics and algorithms for reuse and plagiarism detection, see [Hoad et al 2003] [HaCohen-Kerner et al 2010] for instance. We position our work as an extrinsic detection, the aim of which is to find near-matches between texts, as opposed to intrinsic detection whose aim is to show that different parts of a presumably single-author text could not have been written by the In contrast, our main objective is to deal with the entry level of the detection. The main question is: Is there a meaningful difference in taking the verbatim raw strings compared with the result of a linguistic parsing? A secondary objective is to present and study a series of ascertainments about the practices of our specific field.\",\n  \"The NLP4NLP Corpus and Its Preprocessing\": \"The corpus is a large content of our own research field, i.e. NLP, covering both written and spoken language processing sub-domains and extended to a limited number of corpora, for which Information Retrieval and NLP activities intersect. This corpus was collected at IMMI-CNRS and LIMSI-CNRS (France) and is named NLP4NLP 2. It currently contains 65,003 documents coming from various conferences and journals with either public or restricted access. This is a large part of the existing published articles in our field, apart from the workshop proceedings and the published books. The time period spans 50 years from 1965 to 2015. A phase of preprocessing has been applied to represent the various sources in a common format. This format follows the organization of the ACL Anthology with two parts in parallel for each document: the metadata and the content. Each document is labeled with a unique identifier, for instance 'lrec2000_1' is reified on the hard disk as two files: 'lrec2000_1.bib' and 'lrec2000_1.pdf'. For the metadata, we faced four different types of sources with different flavors and character encodings: BibTeX (e.g. ACL Anthology), custom XML (e.g. TALN), database downloads (e.g. IEEE) or HTML program of the conference (e.g. TREC). We wrote a series of small Java programs to transform these metadata into a common BibTeX format under UTF8. Each file comprises the author names and the title. The file is located in a directory which designates the year and the corpus. Concerning the content, we faced different formats possibly for the same corpus, and the amount of documents being huge, we cannot designate the file type by hand individually. To deal with this, we wrote a program to self-detect the type and sub-type as follows: A small amount of texts are in raw text: we keep them in this format. The vast majority of the documents are in PDF format of different sub-types. First, we used PDFBox 7 to determine the sub-type of the PDF content: when the content is a textual content, we use PDFBox 3 http://aclweb.org/anthology 4 www.isca-speech.org/iscaweb/index.php/archive/online-archive 5 https://www.ieee.org/index.html 6 In the case of a joint conference, the papers are counted twice. This number reduces to 65,003, if we count only once duplicated papers. Similarly, the number of venues is 577 when all venues are counted, but this number reduces to 558 when the 19 joint conferences are counted only once. again to extract the text, possibly with the use of the 'Legion of the Bouncy Castle' 8 to extract the encrypted content. When the PDF is a text under the form of an image, we use PDFBox to extract the images and then Tesseract OCR 9 to transform the images into a textual content. Then, and after some experiments, two filters are applied to avoid getting rubbish content: The content should be at least 900 characters. The content should be of good quality. In order to evaluate this quality, the content is analyzed by the morphological module of TagParser [Francopoulo 2007], a deep industrial parser based on a broad English lexicon and Global Atlas (a knowledge base containing more than one million words from 18 Wikipedias) [Francopoulo et al. 2013 ] to detect out-of-the-vocabulary (OOV) words. Based on the hypothesis that rubbish strings are OOV words, we retain a text when the ratio OOV / number of words is less than 9%. We then apply a set of symbolic rules to split the abstract, body and reference section. The file is recorded in XML. It should be noted that we made some experiments with other strategies, given the fact that we are able to compare them with respect to a quantitative evaluation of the quality, as explained before. The first experiment was to use ParsCit 10 [Councill et al. 2008 ] but the evaluation of the quality was bad, specially when the content is not pure ASCII. The result on accentuated Latin strings, or Arabic and Russian contents was awful. We also tried Grobid 11 but we did not succeed to run it correctly on Windows. A semi-automatic cleaning process was applied on the metadata in order to avoid false duplicates concerning middle names (for X Y Z, is Y a second given name or the first part of the family name?) and for this purpose, we use the specific BibTex format where the given name is separated from the family name with a comma. Then typographic variants (e.g. 'Jean-Luc' versus 'Jean Luc' or 'Herve' versus 'Herv\u00e9') were searched in a tedious process and false duplicates were normalized in order to be merged. The resulting number of different authors is 48,894. for more details about the extraction process as well as the solutions for some tricky problems like joint conferences management or abstract / body / reference sections detection. The majority (90%) of the documents come from conferences, the rest coming from journals. The overall number of words is roughly 270M. Initially, the texts are in four languages: English, French, German and Russian. The number of texts in German and Russian is less than 0.5%. They are detected automatically and are ignored. The texts in French are a little bit more numerous (3%), and are kept with the same status as the English ones. This is not a problem as our tool is able to process English and French. The corpus is a collection of documents of a single technical domain, which is NLP in the broad sense, and of course, some conferences are specialized in certain topics like written language processing, spoken language processing, including signal processing, information retrieval or machine translation.\",\n  \"Definitions of Reuse and Plagiarism\": \"As the terminology is fuzzy and contradictory among the scientific literature, we need first to define four important terms in order to avoid any misunderstanding. The term 'self-reuse' is used for a copy & paste when the source of the copy has an author who belongs to the group of authors of the text of the paste and when the source is cited. The term 'self-plagiarism' is used for a copy & paste when the source of the copy has similarly an author who belongs to the group of authors of the text of the paste, but when the source is not cited. The term 'reuse' is used for a copy & paste when the source of the copy has no author in the group of authors of the paste and when the source is cited. The term 'plagiarism' is used for a copy & paste when the source of the copy has no author in the group of the paste and when the source is not cited. Said in other words, the terms 'self-reuse' and 'reuse' qualify a situation with a proper source citation, on the contrary of 'self-plagiarism' and 'plagiarism'. Let's note that in spite of the fact that the term 'self-plagiarism' seems to be contradictory as authors should be free to use their own wordings, we use this term because it is the usual habit within the community of plagiarism detection some authors also use the term 'recycling', for instance [HaCohen-Kerner et al 2010].\",\n  \"Algorithm for Detection of Copy & Paste\": \"Algorithm Comparison of word sequences has proven to be an effective method for detection of copy For each document of the focus (the source corpus), all the sliding windows 12 of lemmas (typically 5 to 7, excluding punctuations) are built and recorded under the form of a character string key in an index locally to a document. An index gathering all these local indexes is built and is called the 'focus index'. For each document apart from the focus (i.e. outside the source corpus), all the sliding windows are built and only the windows contained in the focus index are recorded in an index locally to this document. This filtering operation is done to optimize the comparison phase, as there is no need to compare the windows out of the focus index. Then, the keys are compared to compute a similarity overlapping score [Lyon et al 2001] between documents D1 and D2, with the Jaccard distance: score(D1,D2) = shared windows# / union# (D1 windows, D2 windows). The pairs of documents D1 / D2 are then filtered according to a threshold in order to retain only significant similarity scoring situations.\",\n  \"Algorithm Comparison: Raw Text vs. Linguistic Parsing\": \"In a first implementation, we compared the raw character strings with a segmentation based on space and punctuation. But, due to the fact that the input is the result of PDF formatting, the texts may contain variable caesura for line endings or some little textual variations. Our objective is to compare at a higher level than hyphen variation (there are different sorts of hyphens), caesura (the sequence X/-/endOfLine/Y needs to match an entry XY in the lexicon to distinguish from an hyphen binding a composition), upper/lower case variation, plural, orthographic variation ('normalise' versus 'normalize'), spellchecking (particularly useful when the PDF is an image and when the extraction is of low quality) and abbreviation ('NP' versus 'Noun Phrase' or 'HMM' versus 'Hidden Markov Model'). Some rubbish sequence of characters (e.g. a series of hyphens) were also detected and cleaned. Given that a parser takes all these variations and cleanings into account, we decided to apply a full linguistic parsing, as a second strategy. The syntactic structures and relations are ignored. Then a module for entity linking is called in order to bind different names referring to the same entity, a process often labeled as 'entity linking'. Thus 'British National Corpus' is considered as possibly abbreviated to 'BNC', as well as less regular names like 'ItalWordNet' possibly abbreviated to 'IWN'. Each entry of the Knowledge Base has a canonical form, possibly associated with different variants: the aim is to normalize into a canonical form to neutralize proper noun obfuscations based on variant substitutions. After this processing, only the sentences with at least a verb are considered. We examined the differences between those two strategies concerning all types of copy & paste situations above the threshold, choosing the LREC source as the focus. The results are presented in Table 2, with the last column adding the two other columns without the duplicates produced by the couples of the same year. The strategy based on linguistic processing provides more pairs (+158) and we examined these differences. Among these pairs, the vast majority (80%) concerns caesura: this is normal because most conferences demand a double column format, so the authors frequently use caesura to save place 13. The other differences (20%) are mainly caused by lexical variations and spellchecking. Thus, the results show that using raw texts gives a more 'silent' system. The drawback is that the computation is much longer 14, but we think that it is worth the value.\",\n  \"Tuning Parameters for the Detection Algorithm\": \"There are three parameters that had to be tuned: the window size, the distance function and the threshold. The main problem we had was that we did not have any gold standard to evaluate the quality specifically on our corpus and the burden to annotate a corpus was too heavy. We therefore decided to start from the parameters presented in the articles related to the PAN contest. We then computed the results, picked a random selection of pairs that we examined and tuned the parameters accordingly. All experiments were conducted with LREC as the focus and NLP4NLP as the search space. In the PAN related articles, different window sizes are used. A window of five is the most frequent one [Kasprzak et al 2010], but our results show that a lot of common sequences like 'the linguistic unit is the' overload the pairwise score. After some trials, we decided to select a size of seven tokens, in agreement with [Citron and Ginsparg 2014]. Concerning the distance function, the Jaccard distance is frequently used but let's note that other formulas are applicable and documented in the literature. For instance, some authors use an approximation with the following formula: score(D1,D2) = shared windows# / min(D1 windows#, D2 windows#) [Clough et al 2009], which is faster to compute, because there is no need to compute the union. Given that computation time is not a problem for us, we kept the most used function which is the Jaccard distance. Concerning the threshold, we tried thresholds of 0.03 and 0.04 (3 to 4%) and we compared the results. The last value gave more significant results, as it reduced noise, while still allowing to detect meaningful pairs of similar papers. After running the first trials, we discovered that using the Jaccard distance resulted in considering as similar a set of two papers, one of them being of small content. This may be the case for invited talks, for example, when the author only provide a short abstract. In this case, a simple acknowledgement to the same institution may produce a similarity score higher than the threshold. The same happens for some eldest papers when the OCR produced a truncated document. In order to solve this problem, we added a second threshold on the minimum number of shared windows that we set at 50 after considering the corresponding erroneous cases.\",\n  \"Results and Analysis of Self-Reuse and Self-Plagiarism\": \"Table 3 provides the results of merging self-reuse (authors reusing their own text while quoting the source paper) and self-plagiarism (authors reusing their own text without quoting the source paper). As we see, it is a rather frequent phenomenon, with a total of 12,493 documents (i.e. 18% of the 67,937 documents!). In 61% of the cases (7,650 self-plagiarisms over 12,493), the authors do not quote the source paper. We found that 205 papers have exactly the same title, and that 130 papers have both the same title and the same list of authors! Also 3,560 papers have exactly the same list of authors. Given the large number of documents, it is impossible to conduct a manual checking of all the couples. We see that the most used sources are the large conferences: ISCA, IEEE-ICASSP, ACL, COLING, HLT, EMNLP and LREC. The most using sources are not only those large conferences, but also the journals: IEEE-Transactions on Acoustics, Speech and Language Processing (and its various avatars) (TASLP), Computer Speech and Language (CSAL), Computational Linguistics (CL) and Speech Com. If we consider the balance between the using and the used sources, we clearly see that the flow of papers goes from conferences to journals. The largest flows of self-reuse and self-plagiarism concern ISCA and ICASSP, in both directions, but especially from ISCA to ICASSP, ICASSP and ISCA to TASLP (also in the reverse direction) and to CSAL, ISCA to Speech Com, ACL to Computational Linguistics, ISCA to LREC and EMNLP to ACL. If we want to study the influence a given conference (or journal) has on another one, we must however recall that these figures are raw figures in terms of number of documents, and we must not forget that some conferences (or journals) are much bigger than others. For instance, LREC is a conference with more than 4,500 documents compared to LRE which is a journal with only 308 documents. If we relate the number of published papers that reuse another paper to the total number of published papers, we may see that 17% of the LRE papers (52 over 308) use content coming from the LREC conferences, without quoting them in 66% of the cases. Also the frequency of the conferences (annual or biennial) and the calendar (date of the conference and of the submission deadline) may influence the flow of papers between the sources. The similarity scores range from 4% to 97% (Fig. 1). We see that about 4,500 couples of papers have a similarity score equal or superior to 10%; about 900 (1.3% of the total number of papers) have a score superior or equal to 30%. Looking at the ones with the largest similarity score, we found a few examples of important variants in the spelling of the same authors' names, and cases of republishing the corrigendum of a previously published paper or of republishing a paper with a small difference in the title and one missing author in the authors' list. In one case, the same research center is described by the same author in two different conferences with an overlapping of 90%. In another case, the difference of the two papers is primarily in the name of the systems being presented, funded by the same project agency in two different contracts, while the description has a 45% overlap!\",\n  \"Results and Analysis of Reuse and Plagiarism\": \"Table 4 provides the results of merging reuse (authors reusing fragments of the texts of other authors while quoting the source paper) and plagiarism (authors reusing fragments of the texts of other authors without quoting the source paper). As we see, there are very few cases altogether. Only 261 papers (i.e. less than 0.4% of the 67,937 documents) reuse a fragment of papers written by other authors that they quote. In 60% of the cases (156 plagiarisms over 261), the authors do not quote the source paper, but these possible cases of plagiarism only represent 0.23% of the total number of papers. Given those small numbers, we were able to conduct a manual checking of those couples. Among the couple papers placed in the 'Reuse' category, it appeared that 12 have a least one author in common, but with a somehow different spelling and should therefore be placed in the 'Self-reuse' category. Among the couples of papers placed in the 'Plagiarism' category, 25 have a least one author in common, but with a somehow different spelling and should therefore be placed in the 'Self-plagiarism' category and 14 correctly quote the source paper, but with variants in the spelling of the authors' names, of the paper's title or of the conference or journal source or forgetting to place the source paper in the references and should therefore be placed in the 'Reuse' category. It therefore resulted in 107 cases of 'reuse' and 117 possible cases of plagiarism (0.17% of the papers) that we studied more closely. We found the following explanations: The paper cites another reference from the same authors of the source paper (typically a previous reference, or a paper published in a Journal) (46 cases) Both papers use extracts of a third paper that they both cite (31 cases) The authors of the two papers are different, but from the same laboratory (typically in industrial laboratories or funding agencies) (11 cases) The authors previously co-authored papers (typically as supervisor and PhD student or postdoc) but are now in a different laboratory (11 cases) The authors of the papers are different, but collaborated in the same project which is presented in the two papers (2 cases) The two papers present the same short example, result or definition coming from another source (13 cases) If we exclude those cases, only 3 cases of possible plagiarism remain that correspond to the same paper which appears as a patchwork of 3 other papers, while sharing several references with them. The similarity scores range from 4% to 42% (Fig. 2). Only 34 couples of papers have a similarity score equal or higher than 10%. For example, the couple showing the highest similarity score comprises a paper published in 1998 and a paper published in 2000 which both describe Chart parsing using the words of the initial paper published 20 years earlier in 1980, that they both properly quote. Among the three remaining possible cases of plagiarism, the highest similarity score is 10%, with a shared window of 200 tokens.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multimodal Social Media and Name Tagging\": \"Social platforms, like Snapchat, Twitter, Instagram and Pinterest, have become part of our lives and play an important role in making communication easier and accessible. Once text-centric, social media platforms are becoming increasingly multimodal, with users combining images, videos, audios, and texts for better expressiveness. In this context, we study the task of Name Tagging for social media containing both image and textual contents. Name tagging is a key task for language understanding, and provides input to several other tasks such as Question Answering, Summarization, Searching and Recommendation.\",\n  \"Challenges in Name Tagging for Social Media\": \"The textual components of these messages are often very short, which limits context around names. Moreover, there are linguistic variations, slangs, typos and colloquial language extremely common, such as using 'looooove' for 'love', 'LosAngeles' for 'Los Angeles', and '#Chicago #Bull' for 'Chicago Bulls'. These characteristics of social media data clearly illustrate the higher difficulty of this task, if compared to traditional newswire name tagging.\",\n  \"Proposed Model: Visual Attention and Modulation Gate\": \"We propose a visual attention model specifically for name tagging in multimodal social media data. The proposed end-to-end model only uses image-sentence pairs as input without any human designed features, and a Visual Attention component that helps understand the decision making of the model. We propose to use a gate to combine textual features extracted by a Bidirectional Long Short Term Memory (BLSTM) and extracted visual features, before feeding them into a Conditional Random Fields (CRF) layer for tag prediction. The proposed gate architecture plays the role to modulate word-level multimodal features.\",\n  \"BLSTM-CRF Sequence Labeling Model\": \"We model name tagging as a sequence labeling problem. Given a sequence of words: S = {s 1 , s 2 , ..., s n }, we aim to predict a sequence of labels: L = {l 1 , l 2 , ..., l n }, where l i \u2208 L and L is a pre-defined label set. Bidirectional LSTM. Long Short-term Memory Networks (LSTMs) are variants of Recurrent Neural Networks (RNNs) designed to capture long-range dependencies of input. The equations of a LSTM cell are provided. Name Tagging benefits from both of the past (left) and the future (right) contexts, thus we implement the Bidirectional LSTM by concatenating the left and right context representations, h t = [ \u2212 \u2192 h t , \u2190 \u2212 h t ], for each word.\",\n  \"Visual Feature Representation and Attention Mechanism\": \"We use Convolutional Neural Networks (CNNs) to obtain the representations of images. Particularly, we use Residual Net (ResNet) to extract visual features for regional areas as well as for the whole image. The global visual representation is a reasonable representation of the whole input image, but not the best. Sometimes only parts of the image are related to the associated sentence. In this work we utilize visual attention mechanism to combat the problem, which has been proven effective for vision-language related tasks such as Image Captioning and Visual Question Answering.\",\n  \"Datasets for Multimodal Name Tagging\": \"We evaluate our model on two multimodal datasets, which are collected from Twitter and Snapchat respectively. Both datasets contain four types of named entities: Location, Person, Organization and Miscellaneous. Each data instance contains a pair of sentence and image, and the names in sentences are manually tagged by three expert labelers. The Twitter dataset contains pairs of tweets and their associated images, while the Snap dataset consists of caption and image pairs exclusively extracted from snaps submitted to public and live stories.\",\n  \"Experimental Setup and Training Details\": \"To tokenize the sentences, we use the same rules as (Owoputi et al., 2013), except we separate the hashtag '#' with the words after. We use the 100-dimensional GloVe embeddings trained on 2 billions tweets to initialize the lookup table and do fine-tuning during training. The models are optimized with finetuning on both the word-embeddings and the pretrained ResNet. We use mini-batch stochastic gradient descent (SGD) with batch size 20 and momentum 0.9 on both datasets. We set an initial learning rate of \u03b7 0 = 0.03 with decay rate of \u03c1 = 0.01. We use a gradient clipping of 5.0 to reduce the effects of gradient exploding.\",\n  \"Results and Performance Analysis\": \"Table 3 shows the performance of the baseline, which is BLSTM-CRF with sentences as input only, and our proposed models on both datasets. Our final model BLSTM-CRF + VISUAL ATTENTION + GATE, which has visual attention component and modulation gate, obtains the best F1 scores on both datasets. Visual features successfully play a role of validating entity types. For example, when there is a person in the image, it is more likely to include a person name in the associated sentence, but when there is a soccer field in the image, it is more likely to include a sports team name.\",\n  \"Attention Visualization and Error Analysis\": \"Figure 5 shows some good examples of the attention visualization and their corresponding name tagging results. The model can successfully focus on appropriate regions when the images are well aligned with the associated sentences. Figure 6 shows some failed examples that are categorized into three types: (1) bad alignments between visual and textual information; (2) blur images; (3) wrong attention made by the model. Name tagging greatly benefits from visual features when the sentences are well aligned with the associated image, but it is not always the case in social media.\",\n  \"Related Work and Future Directions\": \"In recent years, several neural network architectures for named tagging that outperform traditional explicit features based methods have been proposed. However, these methods were mainly developed for newswire and paid little attention to social media. Our model is most similar to (Lample et al., 2016), but we designed a new visual attention component and a modulation control gate. We plan to expand our model to tasks such as fine-grained Name Tagging or Entity Liking in the future.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Problem and Model\": \"An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals. For example, consider the environment and instructions in Figure 1, in which a user describes moving chemicals between beakers and mixing chemicals together. To execute the second instruction, the agent needs to resolve sixth beaker and last one to objects in the environment. The third instruction requires resolving it to the rightmost beaker mentioned in the second instruction, and reasoning about the set of actions required to mix the colors in the beaker to brown. In this paper, we describe a model and learning approach to map sequences of instructions to actions.\",\n  \"Challenges in Executing Sequences of Instructions\": \"A key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world. Instructions often refer to previously mentioned objects (e.g., it in Figure 1) or actions (e.g., do it again). The world state provides the set of objects the instruction may refer to, and implicitly determines the available actions. For example, liquid cannot be removed from an empty beaker. Both types of contexts continuously change during an interaction. As new instructions are given, the instruction history expands, and as the agent acts the world state changes.\",\n  \"Model Architecture and Attention Mechanism\": \"We generate continuous vector representations for all inputs. Each input is represented as a set of vectors that are then processed with an attention function to generate a single vector representation. We use a single bidirectional RNN with a long short-term memory (LSTM) recurrence to encode the instructions. We use a decoder to generate actions. At each time step k, we compute an input representation using the attention function, update the decoder state, and compute the next action to execute. Attention is first computed over the vectors of the current instruction, which is then used to attend over the other inputs.\",\n  \"Learning Approach and SESTRA Algorithm\": \"We estimate the policy parameters \u03b8 using an exploration-based learning algorithm that maximizes the immediate expected reward. Broadly speaking, during learning, we observe the agent behavior given the current policy, and for each visited state compute the expected immediate reward by observing rewards for all actions. Algorithm 1 shows the Single-step Reward Observation (SESTRA) learning algorithm. We iterate over the training data T times. For each example j and turn i, we first perform a rollout by sampling an execution\u0113 from \u03c0\u03b8 with at most M actions. Given the sampled states visited, we compute the entropy and observe the immediate reward for all actions for each step. Entropy and rewards are used to accumulate the gradient, which is applied to the parameters using RMSPROP.\",\n  \"Reward Function and Objective\": \"The reward R (j) i : S \u00d7 S \u00d7 A \u2192 R is defined for each example j and instruction i: R (j) i (s, a, s ) = P (j) i (s, a, s ) + \u03c6 (j) i (s ) \u2212 \u03c6 (j) i (s), where s is a source state, a is an action, and s is a target state. P (j) i and negative for stopping in an incorrect state or taking an invalid action. We use a potential-based shaping term \u03c6 (Ng et al., 1999), where \u03c6 (j) i (s ) \u2212 \u03c6 (j) i (s) computes the edit distance between the state s and the goal, measured over the objects in each state. The shaping term densifies the reward, providing a meaningful signal for learning in nonterminal states. Objective We maximize the immediate expected reward over all actions and use entropy regularization.\",\n  \"SCONE Domains and Data\": \"SCONE has three domains: ALCHEMY, SCENE, and TANGRAMS. Each interaction contains five instructions. Table 1 shows data statistics. Table 2 shows discourse reference analysis. State encodings are detailed in the Supplementary Material. ALCHEMY Each environment in ALCHEMY contains seven numbered beakers, each containing up to four colored chemicals in order. Instructions describe pouring chemicals between and out of beakers, and mixing beakers. We treat all beakers as stacks. There are two action types: PUSH and POP. POP takes a beaker index, and removes the top color. PUSH takes a beaker index and a color, and adds the color at the top of the beaker. To encode a state, we encode each beaker with an RNN, and concatenate the last output with the beaker index embedding. The set of vectors is the state embedding.\",\n  \"Experimental Setup and Evaluation\": \"Evaluation Following Long et al. (2016), we evaluate task completion accuracy using exact match between the final state and the annotated goal state. We report accuracy for complete interactions (5utts), the first three utterances of each interaction (3utts), and single instructions (Inst). For single instructions, execution starts from the annotated start state of the instruction. Systems We report performance of ablations and two baseline systems: POLICYGRADIENT: policy gradient with cumulative episodic reward without a baseline, and CONTEXTUALBANDIT: the contextual bandit approach of Misra et al. (2017). Both systems use the reward with the shaping term and our model. We also report supervised learning results (SUPERVISED) by heuristically generating correct executions and computing maximum-likelihood estimate using context-action demonstration pairs.\",\n  \"Ablation Studies and Model Performance\": \"Table 4 shows development results, including model ablation studies. Removing previous instructions (-previous instructions) or both states (-current and initial state) reduces performance across all domains. Removing only the initial state (-initial state) or the current state (-current state) shows mixed results across the domains. Providing access to both initial and current states increases performance for ALCHEMY, but reduces performance on the other domains. We hypothesize that this is due to the increase in the number of parameters outweighing what is relatively marginal information for these domains.\",\n  \"Error Analysis and Model Limitations\": \"We analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction. We study fifty examples in each domain to identify the type of failures. Table 5 shows the counts of major error categories. We consider multiple reference resolution errors. State reference errors indicate a failure to resolve a reference to the world state. For example, in ALCHEMY, the phrase leftmost red beaker specifies a beaker in the environment. If the model picked the correct action, but the wrong beaker, we count it as a state reference. We distinguish between multi-turn reference errors that should be feasible, and these that that are impossible to solve without access to states before executing previous utterances, which are not provided to our model.\"\n}\n```",
    "```json\n{\n  \"Introduction to Temporal and Causal Relations\": \"Understanding events is an important component of natural language understanding. An essential step in this process is identifying relations between events, which are needed in order to support applications such as story completion, summarization, and timeline construction. Among the many relation types that could exist between events, this paper focuses on the joint extraction of temporal and causal relations.\",\n  \"Joint Extraction of Temporal and Causal Relations\": \"It is well known that temporal and causal relations interact with each other and in many cases, the decision of one relation is made primarily based on evidence from the other. In Example 1, identifying the temporal relation between e1:died and e2:exploded is in fact a very hard case: There are no explicit temporal markers (e.g., 'before', 'after', or 'since'); the events are in separate sentences so their syntactic connection is weak; although the occurrence time of e2:exploded is given (i.e., Friday) in text, it is not given for e1:died. However, given the causal relation, e2:exploded caused e1:died, it is clear that e2:exploded happened before e1:died. The temporal relation is dictated by the causal relation.\",\n  \"Challenges in Jointly Studying Temporal and Causal Relations\": \"A key obstacle in jointly studying temporal and causal relations lies in the absence of jointly annotated data. The second contribution of this work is the development of such a jointly annotated dataset which we did by augmenting the Event-Causality dataset (Do et al., 2011) with dense temporal annotations. This dataset allows us to show statistically significant improvements on both relations via the proposed joint framework.\",\n  \"Proposed Joint Framework: Temporal and Causal Reasoning (TCR)\": \"The first contribution of this work is proposing a joint framework for Temporal and Causal Reasoning (TCR), inspired by these examples. Assuming the availability of a temporal extraction system and a causal extraction system, the proposed joint framework combines these two using a constrained conditional model (CCM) (Chang et al., 2012) framework, with an integer linear programing (ILP) objective (Roth and Yih, 2004) that enforces declarative constraints during the inference phase.\",\n  \"Temporal Component of TCR\": \"To start with, we focus on introducing the temporal component, and clarify how to design the transitivity constraints and how to enforce other readily available prior knowledge to improve its performance. With this temporal component already explained, we further incorporate causal relations and complete the TCR joint inference framework. Finally, we transform the joint problem into an ILP so that it can be solved using off-the-shelf packages.\",\n  \"Incorporating Causal Relations into TCR\": \"Full Model with Causal Relations Now we have presented the joint inference framework for temporal relations in Eq. (1). It is easier to explain our complete TCR framework on top of it. Let W be the vectorization of all causal relations and add the scores from the scoring function for causality s c (\u00b7) to Eq. (1). Specifically, the full inference formulation is now: Y ,\u0174 = arg max Y \u2208Y,W \u2208W Y \u2211 i\u2208EE s ee {i \u2192 Y i } (3) + \u2211 j\u2208ET s et {j \u2192 Y j } + \u2211 k\u2208EE s c {k \u2192 W k } where W Y is the search space for W . W Y depends on the temporal labels Y in the sense that W Y = {W \u2208 R m C |\u2200i, j \u2208 E, if W (i,j) = c, (4) then W (j,i) =c, and Y (i,j) = b} where m is the dimension of W (i.e., the total number of causal pairs), R C = {c,c, null} is the label set for causal relations (i.e., 'causes', 'caused by', and 'no relation'), and W (i,j) is the causal label for pair (i, j). The constraint represented by W Y means that if a pair of events i and j are labeled to be 'causes', then the causal relation between j and i must be 'caused by', and the temporal relation between i and j must be 'before'.\",\n  \"Scoring Functions and ILP Formulation\": \"To get s ee (\u00b7) and s et (\u00b7), we trained classifiers using the averaged perceptron algorithm (Freund and Schapire, 1998) and the same set of features used in (Do et al., 2012; Ning et al., 2017), and then used the soft-max scores in those scoring functions. For example, that means s ee {i \u2192 r} = w T r \u03d5(i) \u2211 r \u2032 \u2208RT w T r \u2032 \u03d5(i), i \u2208 EE, r \u2208 R T, where {w r } is the learned weight vector for relation r \u2208 R T and \u03d5(i) is the feature vector for pair i \u2208 EE. Given a pair of ordered events, we need s c (\u00b7) to estimate the scores of them being 'causes' or 'caused by'. Since this scoring function has the same nature as s ee (\u00b7), we can reuse the features from s ee (\u00b7) and learn an averaged perceptron for s c (\u00b7). In addition to these existing features, we also use prior statistics retrieved using our temporal system from a large corpus, so as to know probabilistically which event happens before another event.\",\n  \"Experiments and Results on TB-Dense\": \"The standard train/dev/test split of TB-Dense was used and parameters were tuned to optimize the F 1 performance on dev. Gold events and time expressions were also used as in existing systems. The contributions of each proposed information sources are analyzed in the ablation study shown in Table 2, where we can see the F 1 score was improved step-by-step as new sources of information were added. Recall that Y 1 represents transitivity constraints, ET represents taking event-timex pairs into consideration, and Y 2 represents rules from CAEVO (Chambers et al., 2014). System 1 is the baseline we are comparing to, which is a local method predicting temporal relations one at a time. System 2 only applied Y 1 via ILP on top of all EE pairs by removing the 2nd term in Eq. (1); for fair comparison with System 1, we added the same ET predictions from System 1. System 3 incorporated ET into the ILP and mainly contributed to an increase in precision (from 42.9 to 44.3); we think that there could be more gain if more time expressions existed in the testset. With the help of additional high-precision rules (Y 2), the temporal performance can further be improved, as shown by System 4. Finally, using the causal extraction obtained via (Do et al., 2011) in the joint framework, the proposed method achieved the best precision, recall, and F 1 scores in our ablation study (Systems 1-5).\",\n  \"Joint Performance on New Dataset\": \"Result on our new dataset jointly annotated with both temporal and causal relations is shown in Table 4. We split the new dataset into 20 documents for training and 5 documents for testing. In the training phase, the training parameters were tuned via 5-fold cross validation on the training set. Table 4 demonstrates the improvement of the joint framework over individual components. The 'temporal only' baseline is the improved temporal extraction system for which the joint inference with causal links has NOT been applied. The 'causal only' baseline is to use s c (\u00b7) alone for the prediction of each pair. That is, for a pair i, if s c {i \u2192 causes} > s c {i \u2192 caused by}, we then assign 'causes' to pair i; otherwise, we assign 'caused by' to pair i. Note that the 'causal accuracy' column in Table 4 was evaluated only on gold causal pairs. In the proposed joint system, the temporal and causal scores were added up for all event pairs. The temporal performance got strictly better in precision, recall, and F 1, and the causal performance also got improved by a large margin from 70.5% to 77.3%, indicating that temporal signals and causal signals are helpful to each other.\",\n  \"Discussion and Future Work\": \"We have consistently observed that on the TB-Dense dataset, if automatically tuned to optimize its F 1 score, a system is very likely to have low precisions and high recall (e.g., Table 2). We notice that our system often predicts non-vague relations when the TB-Dense gold is vague, resulting in lower precision. However, on our new dataset, the same algorithm can achieve a more balanced precision and recall. This is an interesting phenomenon, possibly due to the annotation scheme difference which needs further investigation. The temporal improvements in both Table 2 and Table 4 are relatively small (although statistically significant). This is actually not surprising because C-Links are much fewer than T-Links in newswires which focus more on the temporal development of stories. As a result, many T-Links are not accompanied with C-Links and the improvements are diluted. But for those event pairs having both T-Links and C-Links, the proposed joint framework is an important scheme to synthesize both signals and improve both. The comparison between Line 5 and Line 3 in Table 4 is a showcase of the effectiveness. We think that a deeper reason for the improvement achieved via a joint framework is that causality often encodes humans prior knowledge as global information (e.g., 'death' is caused by 'explosion' rather than causes 'explosion', regardless of the local context), while temporality often focuses more on the local context. From this standpoint, temporal information and causal information are complementary and helpful to each other.\"\n}\n```",
    "```json\n{\n  \"Introduction to Emotional Analysis in Text\": \"Human emotions are the most complex and unique features to be described. If we ask someone regarding emotion, he or she will reply simply that it is a 'feeling'. Ekman (1980) proposed six basic emotions (anger, disgust, fear, guilt, joy and sadness) that have a shared meaning on the level of facial expressions across cultures. A phrase that contains more than one word can be a better way of representing emotions than a single word. Thus, the emotional phrase identification and their classification from text have great importance in Natural Language Processing (NLP).\",\n  \"Related Work in Emotion Detection and Lexicon Development\": \"Strapparava and Valitutti (2004) developed the WORDNET-AFFECT, a lexical resource that assigns one or more affective labels such as emotion, mood, trait, cognitive state, physical state, behavior, attitude and sensation etc to a number of WORDNET synsets. Mohammad, et al., (2010) has performed an extensive analysis of the annotations to better understand the distribution of emotions evoked by terms of different parts of speech. The development of SenticNet (Cambria et al., 2010) was inspired later by (Poria et al., 2013). Balahur et al., (2012) had shown that the task of emotion detection from texts such as the one in the ISEAR corpus can be best tackled using approaches based on commonsense knowledge.\",\n  \"Data Preprocessing and Corpus Preparation\": \"The emotional statements were collected from the ISEAR (International Survey on Emotion Antecedents and Reactions) database. Each of the emotion classes contains the emotional statements given by the respondents as answers based on some predefined questions. The final data set contains reports of 3000 respondents from 37 countries. The statements were split in sentences and tokenized into words and the statistics were presented in Table 1. The tokenized words were grouped to form trigrams in order to grasp the roles of the previous and next tokens with respect to the target token.\",\n  \"Context Vector Formation and Affinity Score Calculation\": \"In order to identify whether the Context Windows (CWs) play any significant role in classifying emotions or not, we have mapped the Context Windows in a Vector space by representing them as vectors. An Affinity Score was calculated for each pair of Context Vectors (p u ,q v ) where u = {1,2,3,.........n} and v = {1,2,3,.......n} for n number of vectors with respect to each of the emotion classes. The Score of any two context vectors p and q of an emotion class is the dot product of the vectors divided by the square of distance (dist) between p and q. This score was inspired by Newton's law of gravitation.\",\n  \"Feature Selection and Analysis\": \"The sentences were POS tagged using the Stanford POS Tagger and the POS tagged Context Windows were extracted and termed as PTCW. Similarly, the POS tag sequence from each of the PTCWs were extracted and named each as POS Tagged Window (PTW). It is observed that 'fear' emotion class has the maximum number of CWs and unique PTCWs whereas the 'anger' class contains the maximum number of unique PTWs.\",\n  \"TF and TF-IDF Measures for Context Windows\": \"The Term Frequencies (TFs) and the Inverse Document Frequencies (IDFs) of the CWs for each of the emotion classes were calculated. It was observed that guilt has the maximum scores for Max_TF and variance whereas the emotions like anger and disgust have the lowest scores for Max_TF. Similarly, the minimum, maximum and variance of the TF-IDF values were calculated for each emotion class, separately. Again, it is found that the guilt emotion has the highest Max_TF-IDF and disgust emotion has the lowest Max_TF-IDF.\",\n  \"Ranking Score of Context Windows\": \"It was found that some of the Context Windows appear more than one time in the same emotion class. Thus, they were removed and a ranking score was calculated for each of the context windows. Each of the words in a context window was searched in the SentiWordnet lexicon and if found, we considered either positive or negative or both scores. The summation of the absolute scores of all the words in a Context Window is returned. The returned scores were sorted so that, in turn, each of the context windows obtains a rank in its corresponding emotion class.\",\n  \"Classification Methods and Results Analysis\": \"The accuracies of the classifiers were obtained by employing user defined test data and data for 10 fold cross validation. It is observed that when Euclidean distance was considered, the BayesNet Classifier gives 100% accuracy on the Test data and gives 97.91% of accuracy on 10-fold cross validation data. On the other hand, J48 classifier achieves 77% accuracy on Test data and 83.54% on 10-fold cross validation data whereas the NaiveBayesSimple classifier obtains 92.30% accuracy on Test data and 27.07% accuracy on 10-fold cross validation data.\",\n  \"Comparison of Distance Metrics in Classification\": \"When the score values are found by using Chebyshev distance, the BayesNet classifier obtains 100% accuracy on Test data and 97.57% accuracy on 10-fold cross validation data. Similarly, J48 achieves 84.82% accuracy on the Test data and 82.75% accuracy on 10-fold cross validation data whereas NaiveBayes and DecisionTable achieve 80%, 29.85% and 98.62%, 96.93% accuracies on the Test data and 10-fold cross validation data, respectively. It has to be mentioned based on Figure 7 that the DecisionTree classifier performs better in comparison with all other classifiers and achieves the best result among the rest of the classifiers on affinity score data prepared based on the Chebyshev distance only.\",\n  \"Conclusions and Future Work\": \"In this paper, vector formation was done for each of the Context Windows; TF and TF-IDF measures were calculated. The calculated affinity score, depending on the distance values was inspired from Newton's law of gravitation. To classify these CWs, BayesNet, J48, NaivebayesSimple and DecisionTable classifiers were used. In future, we would like to incorporate more number of lexicons to identify and classify emotional expressions. Moreover, we are planning to include associative learning process to identify some important rules for classification.\"\n}\n```",
    "```json\n{\n  \"Introduction to Controversial Content and Its Detection\": \"Controversial content - that which attracts both positive and negative feedback - is not necessarily a bad thing; for instance, bringing up a point that warrants spirited debate can improve community health. Detecting potentially controversial content can be useful for both community members and community moderators. Moderators could be alerted that the discussion ensuing from some content might need monitoring. We consider the controversiality of a piece of content in the context of the community in which it is shared, because what is controversial to some audiences may not be so to others.\",\n  \"Community-Specific Controversiality and Its Challenges\": \"We identify 'break up' as a controversial concept in the relationships subreddit, but the same topic is associated with a lack of controversy in the AskWomen subreddit. Similarly, topics that are controversial in one community may simply not be discussed in another: our analysis identifies 'crossfit', a type of workout, as one of the most controversial concepts in the subreddit Fitness. However, while controversial topics may be community-specific, community moderators still may not be able to determine a priori which posts will attract controversy.\",\n  \"Proposed Early-Detection Approach for Controversial Content\": \"We propose an early-detection approach that uses not just the content of the initiating post, but also the content and structure of the initial responding comments. In doing so, we unite streams of heretofore mostly disjoint research programs. Working with over 15,000 discussion trees across six subreddits, we find that incorporating structural and textual features of budding comment trees improves predictive performance relatively quickly.\",\n  \"Dataset and Methodology for Controversy Labeling\": \"Given our interest in community-specific controversiality, we draw data from reddit.com, which hosts several thousand discussion subcommunities (subreddits) covering a variety of interests. Our dataset, which attempts to cover all public posts and comments from Reddit's inception in 2007 until Feb. 2014, is derived from a combination of Jason Baumgartner's posts and comments sets and our own scraping efforts to fill in dataset gaps. We focus on six text-based subreddits ranging over a variety of styles and topics.\",\n  \"Quantitative and Qualitative Validation of Controversy Labels\": \"We assign binary controversy labels (i.e., relatively controversial vs. relatively non-controversial) to posts according to the following process: first, we discard posts where the observed variability across 10 API queries for percent-upvoted exceeds 5%. Next, we discard posts where neither the observed upvote ratio nor the observed score vary at all. Finally, we sort each community's surviving posts by upvote percentage, and discard the small number of posts with percent-upvoted below 50%. The top quartile of posts according to this ranking (i.e., posts with mostly only upvotes) are labeled 'non-controversial.' The bottom quartile of posts, where the number of downvotes cannot exceed but may approach the number of upvotes, are labeled as 'controversial.'\",\n  \"Analysis of Early Discussion Threads for Controversy Prediction\": \"We analyze comments posted in early discussion threads for controversial vs. noncontroversial posts. In this section, we focus on comments posted within one hour of the original submission, although we consider a wider range of times in later experiments. We introduce two sets of features: C-RATE features, which encode the rate of commenting/number of comments; and C-TREE features, which encode structural aspects of discussion trees. Using binary logistic regression, after controlling for C-RATE, C-TREE features extracted from comments made within one hour of the original post improve model fit in all cases except for personalfinance (p < .05, LL-Ratio test).\",\n  \"Comparison of Text Models for Controversy Classification\": \"The goal of this section is to compare text-only models for classifying controversial vs. noncontroversial posts. Algorithms are given access to the full post titles and bodies, unless stated otherwise. In general, the best performing models are based on the BERT features, though HAND+W2V performs well, too. However, no performance gain is achieved when adding hand designed features to BERT. Based on the results of this experiment, we adopt BERT-MP-512 to represent text in experiments for the rest of this work.\",\n  \"Impact of Post-Time Metadata on Controversy Prediction\": \"We incorporate post-time metadata features on top of TEXT to see if they improve performance. While incorporating TIME features on top of TEXT results in consistent improvements across all communities, incorporating author features on top of TIME+TEXT does not. We adopt our highest performing models, TEXT+TIME, as a strong post-time baseline.\",\n  \"Incorporation of Early Discussion Features for Improved Prediction\": \"We augment the post-time features with early-discussion feature sets by giving our algorithms access to comments from increasing observation periods. Specifically, we train linear classifiers by combining our best post-time feature set (TEXT+TIME) with features derived from comment trees available after t minutes, and sweep t from t = 15 to t = 180 minutes in 15 minute intervals. For five of the six communities we consider, the performance of the comment feature classifier significantly (p < .05) exceeds the performance of the post-time baseline in less than three hours of observation.\",\n  \"Domain Transfer Experiments and Feature Transferability\": \"We conduct experiments where we train models on one subreddit and test them on another. The results of this experiment suggest that while text features are quite strong in-domain, they are brittle and community specific. Conversely, while rate and structural comment tree features do not carry as much in-domain predictive capacity on their own, they generally transfer better between communities.\"\n}\n```",
    "```json\n{\n  \"Introduction to Weblio Pre-reordering SMT System\": \"In this paper, we describe the details of Weblio Pre-reordering Statistical Machine Translation (SMT) System, experiments, and some issues we faced. For this SMT system, we applied the pre-reordering method proposed in (Zhu et al., 2014). In particular, this method automatically learns pre-reordering model from word alignments and parse trees. Statistical language model is integrated in the pre-reordering model in order to reorder each node layer in parse trees. In the 1st Workshop on Asian Translation (WAT2014) (Nakazawa et al., 2014), we mainly applied this method in English-Japanese translation subtask.\",\n  \"Head-restructured CFG Parse Tree (HRCFG Tree) for Reordering\": \"In order to reorder SVO (subject-verb-object) order into SOV (subject-object-verb) order, correctly reordering long-distance words those play important roles in a sentence is crucial. Thus, the reordering model is required to capture the reordering patterns for those words. The approach in (Zhu et al., 2014) addresses this problem by injecting sentence-level dependencies into CFG parse trees. Local grammatical structures are still kept unchanged in the parse tree. This new parse tree is called 'Head-restructured CFG parse tree' in the original paper. In this paper, we use 'HRCFG tree' in short to represent it. An example of HRCFG tree is shown in Figure 1. In Figure 1, a normal CFG parse tree is shown in the left, and the corresponding HRCFG tree in the right. In this example, tree components explicitly show subject, object, and verb parts in the sentence. This structure with explicit annotations makes the reordering model easier to capture long-distance reordering patterns.\",\n  \"Reordering Model Integrated with Language Model\": \"The reordering model we used in our MT systems follows the same fashion of the model in (Zhu et al., 2014). A language model is integrated to identify the best order of a node layer according to the order of target language. With a bilingual training data and automatically learned word alignments given by GIZA++ (Och and Ney, 2004), we find the best order for each node layer in all parse trees, making them fit the order of aligned parts in the target sentence. Specifically, for tree nodes n = (n1, n2, ..., nk), terminal nodes beneath ni is defined as ti. Let wi represent a set of words in target side which are aligned with any terminal node in ti. In this step, for each node layer n, we redetermine the order of n according to the average position of aligned words wi for each node ni. Then we export a sequence of reordered nonterminal tags. For some kinds of node, nonterminal tags in the sequences are replaced by head word. After we trained a language model on them, the language model can be used to estimate the likelihood that a tag sequence follows the order of target language.\",\n  \"N-best Reordering Implementation\": \"In the reordering model we described above, the best order for the whole sentence is actually comprised by all 1-best orders of every node layers in the parse tree. Although the language model usually works perfectly to give the best reordering result, in some cases, the best reordering result is unclear until the translation phase. We give an example here, for the sentence 'The rocket is launched by NASA', two plausible reordering results are shown in Table 1. The first reordering result in Table 1 is preferred by the reordering model as 'nsubjpass auxpass launched prep by' is usually reordered into 'nsubjpass prep by launched auxpass'. Unfortunately, Table 1, it's hard to find out a best order before translation. Considering N-best reordering results is necessary in order to obtain the best translation result. In our MT system, we implement this feature simply by collecting N-best reordering results for all node layers, and finally rank the reordering results by accumulated language model score.\",\n  \"Experimental Setup and Methodology\": \"For our baseline system, we use 1-best parse trees for training and test. Stanford tokenizer and Berkeley parser (Petrov et al., 2006) are selected in the pre-processing phase in order to produce CFG parse trees. Then we obtain dependency parse trees by applying Stanford rules (Klein and Manning, 2002) to CFG parse trees. HRCFG trees are built upon these two kinds of parse trees. For the Japanese text, we use Kytea (Neubig, Nakata, and Mori, 2011) to tokenize it. Due to the limitation of computational resource, we are only able to train our reordering model on 1.5M bilingual text (with relatively high scores in ASPEC parallel corpus) for English-to-Japanese translation task. We used this trained reordering model to reorder all training data in the source side. We use conventional Phrase-based model implemented in Moses toolkit to finish remaining SMT pipe line. Distortion limit is set to 6 in all our experiments.\",\n  \"Experiment Results and Analysis\": \"We carried out several experiments combining the use of N-best parse trees and N-best reordering results. A list of automatic evaluation scores for different system settings are listed in Table 2. In particular, for systems marked with 'N-best parse', 30 parse trees with highest parsing scores are used. For systems marked with 'N-best reorder', 10 reordering results with highest reordering scores are accepted for each parse tree. That is, for System 4, a maximum of 300 reordering results are generated for one sentence. In WAT2014, we submitted System 3 and System 4 to human evaluation. Note that in Table 2, our in-house automatic evaluation scores are slightly different from that on the score board of WAT2014 due to different automatic evaluation pipe line we used. Official evaluation scores are listed in Table 3. Where 'BASELINE' refers to Phrase-based SMT system (Koehn, Och, and Marcu, 2003) as the official baseline for human evaluation. Our experiment results shown in Table 2 show that incorporating N-best parse tree and reordering results gained improvements for both BLEU and RIBES metrics.\",\n  \"Evaluation of Pre-reordering Performance\": \"In this section, we evaluate the performance of pre-reordering. Follows the method described in (Isozaki et al., 2010), we estimate Kendall's \u03c4 from word alignments. A comparison of Kendall's \u03c4 distribution upon first 1.5M sentences of ASPEC corpus is shown in Figure 5. Average Kendall's \u03c4 of natural order and adjusted order is 0.30 and 0.71 respectively. Note that in (Isozaki et al., 2010), the algorithm for estimating Kendall's \u03c4 does not take the words with multiple alignments into account. Hence, the graph of Kendall's \u03c4 only gives a rough idea of the performance of pre-reordering. In particular, the algorithm skipped 20.30% aligned words for corpus in natural order and 14.06% aligned words for pre-reordered corpus. However, the distribution of Kendall's \u03c4 in Figure 5 gives an intuitive picture of the improvements of word order. Sentences which are fully identical in word order increased from 1.8% to 15% after pre-reordering (labeled with '=1.0' in Figure 5).\",\n  \"Error Analysis of Pre-reordering\": \"Although our pre-reordering SMT system is able to produce relatively better translation results compared to baseline SMT systems. In many cases, the translation results still suffer from the defect of the reordering model. As the reordering model described in Section 3 is actually a language model built on sequences mixed with nonterminal tags and words. Involving words in the model makes the reordering more flexible, but also makes the model sparse. For some rare or unknown words, the reordering model usually fails to reorder sentences correctly. In Table 4, we show 2 reordering samples. In Sample 1, the sentence is correctly reordered. The word 'were' in English side should be placed in the end of the reordered sentence, which is expected to be translated to ' ' in Japanese. In Sample 2, we replace the verb 'confirmed' in Sample 1 to 'observed', then the reordering model the changes were observed \u2192 the changes va nsubjpass were observed fails to place the word 'were' into the rightmost position. The errors like what we show in Table 4 are actually widespread in the reordering results for the ASPEC test corpus.\",\n  \"Context-aware Machine Translation Efforts\": \"In this section, we describe some efforts for utilizing context information during the translation. We made an attempt to tackle the phrase selection problem for English-Japanese translation. In Japanese, many English words have multiple translations. Especially Japanese words in the form of Katakana usually also have corresponding expressions comprised of Chinese characters. For instance, the phrase 'remote control' can be translated to both 'ENKAKUSEIGYO' ( ) and 'RIMOKON' ( ). We show the distribution of these two translations across different domains in Figure 6. Figure 6, it's reasonable to think the phrase 'RIMOKON' is more preferred in domain J, P, Q, and R. While in domain N, the two phrases appear almost the same times. A simple solution is to make language model more domain-specific. We carried out experiments that simply interpolate general language model and in-domain language model. The experiment results for first three domains are shown in Figure 7. Figure 7, we show the language model perplexity achieved on domain-specific test data using different settings. Different interpolation weights for the in-domain language model are tried. We can see the interpolated language model generally achieves best perplexities when the weight for in-domain language model is set to 0.5. Applying these interpolated language models for translation tasks in corresponding domains should help improving the quality of translation.\",\n  \"Conclusion and Future Research Directions\": \"In this paper, we described the reordering model we applied in Weblio Pre-reordering SMT system, and also some efforts to utilize N-best parse trees and N-best reordering results. According to our in-house experiment results, the automatic evaluation scores are generally improved when multiple candidates of parse tree and reordering result are considered. However, in the human evaluation, incorporating N-best parse trees did not gain improvements. As we demonstrated in Section 5.1, the reordering model is still unstable, and fails to work correctly even for some simple cases. Further improvement is required to enable the reordering model to deal with general cases correctly. Then, in Section 5.2, we show interpolating general and in-domain language models can be a quick solution to improve translation quality when domain information is given as context. For future research, we still plan to explore the performance limit of pre-reordering models. With a complex reordering model considers multiple factors of the language, it's still plausible for this approach to grow in performance. Also, as the pre-reordering model used in this paper is independent of specific language pair, more experiments can be conducted on different language pairs.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Machine Translation (NMT) and Its Advantages Over SMT\": \"Neural Machine Translation (NMT) is a brand-new approach that samples translation results directly from Recurrent Neural Networks (RNNs). Most published models involve an encoder and a decoder in the network architecture, called the Encoder-Decoder approach. This approach has shown significant improvements over traditional Statistical Machine Translation (SMT) methods, especially when large training data is available. For instance, stacked Long Short-Term Memory (LSTM) units in both encoder and decoder have outperformed phrase-based SMT baselines in various language pairs.\",\n  \"Evaluation of NMT Models in English-Japanese Translation\": \"In this paper, we evaluate NMT models in the English-Japanese translation task. We use the Japanese-English Scientific Paper Abstract Corpus (ASPEC-JE) as training data and focus on evaluating the models for English-Japanese translation. Our experiments show that even simple NMT models can produce better translations compared with all SMT baselines, demonstrating the effectiveness of NMT in this language pair.\",\n  \"Impact of Different Recurrent Units on NMT Performance\": \"We tested various network architectures with different recurrent units in our experiments. The changing perplexities of the models show that soft-attention models with LSTM units constantly outperform multi-layer Encoder-Decoder models. However, the performance of the soft-attention model with GRU units is significantly lower than that with LSTM units, indicating that the choice of recurrent units can significantly impact NMT performance.\",\n  \"Effect of Pre-reordering Training Data on NMT Models\": \"We examined the effect of using pre-reordered data for training NMT models. Surprisingly, using pre-reordered data to train soft-attention models does not benefit the perplexity but degrades the performance by a small margin. This observation is consistent when measuring translation performance directly, suggesting that pre-reordering may not be beneficial for NMT in English-Japanese translation.\",\n  \"Recovering Unknown Words in NMT Translations\": \"A critical practical problem of NMT is the fixed vocabulary size in the output layer. We propose a simple workaround to recover unknown words with a back-off system. This method involves translating the input sentence using both a NMT system and a baseline SMT system, then replacing the unknown words in the NMT result with the rarest word in the SMT result that is missing in the NMT output.\",\n  \"Qualitative Analysis of NMT Translation Errors\": \"We performed a qualitative analysis on a proportion of held-out development data and found that NMT models tend to make perfect translations by omitting some information, leading to a loss of meaning. This pattern of errors suggests that there is room for further improvements in NMT models to better preserve the complete meaning of the input sentences.\",\n  \"Soft-Attention Mechanism in NMT\": \"Soft-attention models in NMT allow the recurrent network to focus on a small portion of the input at each step, making the computational graph differentiable and thus able to be trained with standard backpropagation. This mechanism helps to mitigate the performance degradation when the input sentence gets longer, as seen in the basic Encoder-Decoder approach.\",\n  \"Experimental Setup and Model Architectures\": \"In our experiments, we evaluated four kinds of models with recurrent layers containing 1024 neurons each and a word embedding size of 1000. The models were trained using SGD with learning rate decay, a batch size of 60, and an initial learning rate of 1. Gradients were clipped to ensure an L2 norm lower than 3. During translation, we set the beam size to 20, with a fallback to 1000 if no valid translation was obtained.\",\n  \"Evaluation Metrics and Results\": \"We evaluated our models mainly by measuring perplexity on valid data, which showed a strong correlation with translation performance. Our submitted systems achieved significant improvements over SMT baselines, with our second submitted system (S2) gaining 0.24 BLEU by adding pre-reordered LSTM Search models to the ensemble. Human evaluation results further confirmed the effectiveness of our NMT models in the English-Japanese translation task.\"\n}\n```",
    "```json\n{\n  \"Introduction to Word Embeddings and Their Limitations\": \"Word embeddings have proved to be beneficial in many Natural Language Processing tasks, such as Machine Translation, syntactic parsing, and Question Answering. Despite their success in capturing semantic properties of words, these representations are generally hampered by an important limitation: the inability to discriminate among different meanings of the same word.\",\n  \"Previous Approaches to Addressing Word Sense Ambiguity\": \"Previous works have addressed this limitation by automatically inducing word senses from monolingual corpora or bilingual parallel data. However, these approaches learn solely on the basis of statistics extracted from text corpora and do not exploit knowledge from semantic networks. Additionally, their induced senses are neither readily interpretable nor easily mappable to lexical resources, which limits their application.\",\n  \"Proposed SW2V Model: Integrating Text Corpora and Semantic Networks\": \"In order to obtain a representation for each sense of a word, a number of approaches have leveraged lexical resources to learn sense embeddings as a result of post-processing conventional word embeddings. Instead, we propose SW2V (Senses and Words to Vectors), a neural model that exploits knowledge from both text corpora and semantic networks in order to simultaneously learn embeddings for both words and senses.\",\n  \"Shallow Word-Sense Connectivity Algorithm\": \"In order to jointly produce embeddings for words and senses, SW2V needs as input a corpus where words are connected to senses in each given context. We propose a simple yet effective unsupervised shallow word-sense connectivity algorithm, which can be applied to virtually any given semantic network and is linear on the corpus size.\",\n  \"Joint Training of Words and Senses in SW2V\": \"The goal of our approach is to obtain a shared vector space of words and senses. To this end, our model extends conventional word embedding models by integrating explicit knowledge into its architecture. Our model extends the input and output layers of the neural network with word senses by exploiting the intrinsic relationship between words and senses.\",\n  \"Analysis of Model Components and Configurations\": \"In this section we analyze the different components of SW2V, including the nine model configurations and the algorithm which generates the connections between words and senses in context. We use the CBOW model of word2vec with standard hyperparameters and a 300M-words corpus from the UMBC project as semantic network we use BabelNet 3.0.\",\n  \"Evaluation of SW2V on Word Similarity Tasks\": \"We perform a qualitative and quantitative evaluation of important features of SW2V in three different tasks. First, in order to compare our model against standard word-based approaches, we evaluate our system in the word similarity task. SW2V consistently outperforms all sense-based comparison systems using the same corpus, and clearly performs better than the original word2vec trained on the same corpus.\",\n  \"Evaluation of Sense Clustering with SW2V\": \"In this section we evaluate our synset representations on the Wikipedia sense clustering task. SW2V outperforms all comparison systems according to both measures, including the sense representations of NASARI and SensEmbed using the same setup and the same underlying lexical resource. This confirms the capability of our system to accurately capture the semantics of word senses on this sense-specific task.\",\n  \"Interconnectivity of Word and Sense Embeddings\": \"In the previous experiments we evaluated the effectiveness of the sense embeddings. In contrast, this experiment aims at testing the interconnectivity between word and sense embeddings in the vector space. SW2V provides the best MCS results in both datasets. In general, AutoExtend does not accurately capture the predominant sense of a word and performs worse than a baseline that selects the intended sense randomly from the set of all possible senses of the target word.\",\n  \"Conclusion and Future Directions\": \"In this paper we proposed SW2V (Senses and Words to Vectors), a neural model which learns vector representations for words and senses in a joint training phase by exploiting both text corpora and knowledge from semantic networks. Unlike previous sense-based models which require post-processing steps and use WordNet as sense inventory, our model achieves a semantically coherent vector space of both words and senses as an emerging feature of a single training phase and is easily scalable to larger semantic networks like BabelNet. As future work we plan to integrate a WSD and Entity Linking system for applying our model on downstream NLP applications.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multi-Task Learning (MTL) in NLP\": \"Multi-task Learning (MTL) is one of the most effective solutions for knowledge transfer across tasks. In the context of neural network architectures, we usually perform MTL by sharing parameters across models. Previous studies have proven that MTL is an effective approach to boost the performance of related tasks such as Machine Translation (MT) and parsing.\",\n  \"Challenges in Low-Resource Language Settings\": \"Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages. It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements. Furthermore, some special applications (e.g., disaster response and recovery) require rapid development of NLP systems for extremely low-resource languages.\",\n  \"Proposed Multi-Lingual Multi-Task Architecture\": \"To tackle the issue of limited improvement from a single transfer model, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing. In the first level, we share character embeddings, character-level convolutional neural networks, and word-level long-short term memory layer across all models. In the second level, we adopt different sharing strategies for different transfer schemes.\",\n  \"Basic Architecture for Sequence Labeling\": \"The goal of sequence labeling is to assign a categorical label to each token in a given sentence. LSTM-CRFs-based models are well-suited for multi-lingual multi-task learning for three reasons: (1) They learn features from word and character embeddings and therefore require little feature engineering; (2) As the input and output of each layer in a neural network are abstracted as vectors, it is fairly straightforward to share components between neural models; (3) Character embeddings can serve as a bridge to transfer morphological and semantic information between languages with identical or similar scripts.\",\n  \"Parameter Sharing Strategies in the Architecture\": \"On the first level, we construct the basis of the architecture by sharing character embeddings, CharCNN and bidirectional LSTM among all models. On top of this basis, we adopt different parameter sharing strategies for different transfer schemes. For cross-task transfer, we use the same word embedding matrix across tasks. For cross-lingual transfer, we share the linear layer and CRFs layer among languages to transfer task-specific knowledge.\",\n  \"Experimental Setup and Data Sets\": \"For Name Tagging, we use the following data sets: Dutch (NLD) and Spanish (ESP) data from the CoNLL 2002 shared task, English (ENG) data from the CoNLL 2003 shared task, Russian (RUS) data from LDC2016E95, and Chechen (CHE) data from TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus. For POS Tagging, we use English, Dutch, Spanish, and Russian data from the CoNLL 2017 shared task.\",\n  \"Comparison of Different Models in Low-Resource Settings\": \"In Figure 3, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model (denoted as baseline), cross-task transfer model, and cross-lingual transfer model in low-resource settings with Dutch, Spanish, and Chechen Name Tagging as the main task respectively. We can see that our model substantially outperforms the mono-lingual single-task baseline model and obtains visible gains over single transfer models.\",\n  \"Qualitative Analysis of Model Performance\": \"In Table 4, we compare Name Tagging results from the baseline model and our model, both trained with 100 main task sentences. The first three examples show that shared character-level networks can transfer different levels of morphological and semantic information. For example, in sentence #1, the baseline model fails to identify 'Palestijnen', an unseen word in the Dutch data, while our model can recognize it because the shared CharCNN represents it in a way similar to its corresponding English word 'Palestinians'.\",\n  \"Ablation Studies and Sensitivity to Auxiliary Data\": \"In order to quantify the contributions of individual components, we conduct ablation studies on Dutch Name Tagging with different numbers of training sentences for the target task. As Table 6 shows, adding each component usually enhances the performance (F-score, %), while the impact also depends on the size of the target task data. For example, the language-specific layer slightly impairs the performance with only 10 training sentences.\",\n  \"Related Work and Future Directions\": \"Multi-task Learning has been applied in different NLP areas, such as machine translation, text classification, dependency parsing, textual entailment, text summarization, and sequence labeling. By contrast, we combine cross-task transfer and cross-lingual transfer within a unified architecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously. The next step of this research is to apply this architecture to other types of tasks, such as Event Extract and Semantic Role Labeling that involve structure prediction.\"\n}\n```",
    "```json\n{\n  \"Introduction to Event Detection (ED)\": \"Event Detection (ED) is an important subtask of event extraction, focusing on identifying event triggers and their types from sentences. For example, in the sentence 'Jane and John are married', an ED system should identify 'married' as a trigger of the 'Marry' event. However, identifying events from isolated sentences can be challenging due to context-dependent meanings of event triggers.\",\n  \"Challenges in Event Detection\": \"It may be difficult to identify events from isolated sentences because the same event trigger might represent different event types in different contexts. For instance, the word 'leave' in a sentence could indicate either a 'Transport' event or an 'End-Position' event, depending on the surrounding context. Document-level information is crucial for accurate event detection, as sentences within the same document are often thematically related.\",\n  \"Existing Methods for Event Detection\": \"Existing ED methods can be categorized into feature-based and representation-based methods. Feature-based methods rely on hand-designed features, while representation-based methods use distributed representations to capture semantic information. However, most existing methods focus on sentence-level contextual information, neglecting the importance of document-level information, which can be critical for accurate event detection.\",\n  \"Proposed DEEB-RNN Model\": \"We propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for event detection at the sentence level. This model learns document embeddings through a hierarchical and supervised attention mechanism, which pays attention to event triggers at the word level and sentences containing events at the sentence level. The learned document embeddings are then used to enhance event detection in individual sentences.\",\n  \"EDODEL Module: Document Embedding Learning\": \"The EDODEL module in the DEEB-RNN model learns document embeddings by applying a hierarchical and supervised attention mechanism. This involves a word-level Bi-GRU encoder with attention on event triggers and a sentence-level Bi-GRU encoder with attention on sentences containing events. The model constructs gold attention signals at both word and sentence levels to supervise the learning process.\",\n  \"DEED Module: Document-level Enhanced Event Detection\": \"The DEED module uses the learned document embeddings to tag each trigger candidate with an event type. It employs a Bi-GRU encoder and a softmax output layer to handle event triggers with multiple words, providing a probability vector for each word indicating the likelihood of it being a trigger for a specific event type.\",\n  \"Joint Training of DEEB-RNN Model\": \"The DEEB-RNN model is trained jointly, integrating the losses from the document embedding learning and event detection modules. The joint loss function includes the cross-entropy error for event type prediction and the square error for the attention at both word and sentence levels, with hyper-parameters to balance these components.\",\n  \"Experimental Setup and Baseline Models\": \"We validate the DEEB-RNN model on the ACE-2005 dataset, comparing it with state-of-the-art methods. The dataset is divided into training, validation, and test sets. Baseline models include feature-based and representation-based methods, such as Sentence-level, Joint Local, JRNN, Skip-CNN, ANN-S2, Cross-event, PSL, and DLRNN.\",\n  \"Impact of Different Attention Strategies\": \"Experiments on the ACE-2005 dataset show that different attention strategies significantly impact the performance of the DEEB-RNN model. Using gold attention signals at both word and sentence levels (DEEB-RNN3) results in the best performance, indicating the importance of supervised attention for event detection.\",\n  \"Performance Comparison and Analysis\": \"The DEEB-RNN model outperforms existing state-of-the-art methods in terms of recall and F1-measure. This is attributed to its ability to exploit document-level information through a hierarchical and supervised attention mechanism, automatically capture event types without manual rules, and learn event detection-oriented document embeddings.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Sequence-to-Sequence Models and Their Limitations\": \"Neural sequence-to-sequence models (SEQ2SEQ) perform impressively well in several natural language processing tasks, such as machine translation and syntactic constituency parsing. However, they require massive parallel training datasets. Consequently, there has been extensive work on utilizing non-parallel corpora to boost the performance of SEQ2SEQ models. Current models, however, barely reach lead-N baselines, and/or are non-differentiable, thus relying on reinforcement learning, which is unstable and inefficient.\",\n  \"Proposed SEQ 3 Model: A Sequence-to-Sequence-to-Sequence Autoencoder\": \"By contrast, we propose a sequence-to-sequence-to-sequence autoencoder, dubbed SEQ 3, that can be trained end-to-end via gradient-based optimization. SEQ 3 employs differentiable approximations for sampling from categorical distributions, which have been shown to outperform reinforcement learning. Therefore it is a generic framework which can be easily extended to other tasks, e.g., machine translation and semantic parsing via task-specific losses. In this work, as a first step, we apply SEQ 3 to unsupervised abstractive sentence compression.\",\n  \"Compressor Component of SEQ 3\": \"The compressor C receives an input text x = \u27e8x 1 , . . . , x N \u27e9 of N words, and generates a summary y = \u27e8y 1 , . . . , y M \u27e9 of M words (M<N), y being a latent variable. An embedding layer projects the source sequence x to the word embeddings e s = \u27e8e s 1 , . . . , e s N \u27e9, which are then encoded by a bidirectional RNN, producing h s = \u27e8h s 1 , . . . , h s N \u27e9. Each h s t is the concatenation of the corresponding left-to-right and right-to-left states of the bi-RNN.\",\n  \"Differentiable Word Sampling Techniques in SEQ 3\": \"To generate the summary, we need to sample its words y t from the categorical distributions p(y t |y <t , x), which is a non-differentiable process. We use the Gumbel-Softmax (GS) reparametrization trick as a low variance approximation of sampling from categorical distributions. This allows us to perform sampling, which has the benefit of adding stochasticity and facilitating exploration of the parameter space.\",\n  \"Reconstructor Component of SEQ 3\": \"The reconstructor (R) works like the compressor, but its encoder operates on the embeddings e c 1 , . . . , e c M of the words y 1 , . . . , y M of the summary. The reconstructor tries to reproduce the input from the summary, and communicates only through the discrete words of the summary. The reconstructor is initialized similarly to the compressor, using a transformation of the concatenation of the last hidden states of its bidirectional encoder and a length vector.\",\n  \"Loss Functions in SEQ 3\": \"SEQ 3 uses multiple loss functions to ensure the quality of the generated summaries. Reconstruction Loss L R (x,x) is the (negative) log-likelihood assigned by the reconstructor to the input words. LM Prior Loss ensures that the summaries are readable by pretraining an RNN language model. Topic Loss rewards summaries with similar topic-indicating words as the input text. A Length Penalty helps the compressor to predict the end-of-sequence token at the target summary length.\",\n  \"Experimental Setup and Datasets\": \"We train SEQ 3 on the Gigaword sentence compression dataset, which consists of pairs containing the first sentence of a news article and the article's headline. We also test SEQ 3 on DUC-2003 and DUC-2004 shared tasks, containing news articles paired with reference summaries. We compare SEQ 3 to other unsupervised sentence compression models and report the average F1 of ROUGE-1, ROUGE-2, ROUGE-L.\",\n  \"Experimental Results and Analysis\": \"SEQ 3 outperforms the unsupervised Pretrained Generator across all metrics by a large margin on the Gigaword dataset. It also surpasses the LEAD-8 baseline. The model's performance drops when the LM prior is removed, indicating the importance of the language model in generating coherent summaries. SEQ 3 behaves similarly on DUC-2003 and DUC-2004, although it does not surpass the PREFIX baseline in DUC-2003.\",\n  \"Model Limitations and Future Work\": \"The model tends to copy the first words of the input sentence in the compressed text, which may be due to the autoregressive nature of the reconstructor. A possible workaround might be to modify SEQ 3 so that the first encoder-decoder pair would turn the inputs to longer sequences, and the second encoder-decoder would compress them trying to reconstruct the original inputs. In future work, we plan to explore the potential of SEQ 3 in other tasks, such as unsupervised machine translation and caption generation.\"\n}\n```",
    "```json\n{\n  \"Introduction to Out-of-Vocabulary (OOV) Word Embeddings\": \"One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space. These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts. However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications. The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods.\",\n  \"MIMICK-RNN: A Quasi-Generative Approach for OOV Words\": \"We approach this challenge from a quasigenerative perspective. Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter. We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word's spelling and mimick its distributional embedding.\",\n  \"Related Work on Embedding Rare and Unseen Words\": \"Several studies make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure. Botha and Blunsom (2014) compute word embeddings by summing over embeddings of the morphemes; Luong et al. (2013) construct a recursive neural network over each word's morphological parse; Bhatia et al. (2016) use morpheme embeddings as a prior distribution over probabilistic word embeddings. In contrast, our approach is a post-processing step that can be applied to any set of word embeddings, regardless of how they were trained.\",\n  \"MIMICK Word Embeddings: Model and Training\": \"We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordform-based protocol for creating these embeddings. By training a model over the existing vocabulary, we can later use that model for predicting the embedding of an unseen word. Our predictive function of choice is a Word Type Character Bi-LSTM. The training objective is similar to that of Yin and Sch\u00fctze (2016). We match the predicted embeddings f(w_k) to the pre-trained word embeddings e_w_k, by minimizing the squared Euclidean distance.\",\n  \"Evaluation of MIMICK on Stanford RareWord Dataset\": \"Quantitative evaluation on the Stanford RareWord dataset (Luong et al., 2013) provides more evidence that these character-based embeddings capture word similarity for rare and unseen words. The results, shown in Table 3, demonstrate that the MIMICK RNN recovers about half of the loss in performance incurred by the original Polyglot training model due to out-of-vocabulary words in the 'All pairs' condition. MIMICK also outperforms VarEmbed.\",\n  \"Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes\": \"Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model of Ling et al. (2015), who attach a projection layer to the output of a sentence-level bidirectional LSTM. We extend this approach to morphosyntactic tagging by duplicating this projection layer for each attribute type. The input to our multilayer perceptron (MLP) projection network is the hidden state produced for each token in the sentence by an underlying LSTM, and the output is attribute-specific probability distributions over the possible values for each attribute on each token in the sequence.\",\n  \"Experimental Settings and Language Selection\": \"The morphological complexity and compositionality of words varies greatly across languages. To see whether this property is influential on our MIMICK model and its performance in the downstream tagging task, we select languages that comprise a sample of multiple morphological patterns. Language family and script type are other potentially influential factors in an orthography-based approach such as ours, and so we vary along these parameters as well. We also considered language selection recommendations from de Lhoneux and Nivre (2016) and Schluter and Agi\u0107 (2017).\",\n  \"Evaluation Metrics and Models\": \"We report micro F1: each token's value for each attribute is compared separately with the gold labeling, where a correct prediction is a matching non-NONE attribute/value assignment. Recall and precision are calculated over the entire set, with F1 defined as their harmonic mean. We implement and test the following models: No-Char, MIMICK, CHAR\u2192TAG, and Both. The CHAR\u2192TAG models use 20-dimension character embeddings and a single hidden layer of size 128.\",\n  \"Results and Analysis: Low-Resource and Full-Resource Settings\": \"We report performance in both low-resource and full-resource settings. Low-resource training sets were obtained by randomly sampling training sentences, without replacement, until a predefined token limit was reached. We report the results on the full sets and on N = 5000 tokens in Table 5 (part-of-speech tagging accuracy) and POS and morphosyntactic tagging. For POS, the largest margins are in the Slavic languages (Russian, Czech, Bulgarian), where word order is relatively free and thus rich word representations are imperative. Chinese also exhibits impressive improvement across all settings, perhaps due to the large character inventory (> 12,000), for which a model such as MIMICK can learn well-informed embeddings using the large Polyglot vocabulary dataset, overcoming both word-and character-level sparsity in the UD corpus.\",\n  \"Conclusion and Future Work\": \"We present a straightforward algorithm to infer OOV word embedding vectors from pre-trained, limited-vocabulary models, without need to access the originating corpus. This method is particularly useful for low-resource languages and tasks with little labeled data available, and in fact is task-agnostic. Our method improves performance over word-based models on annotated sequence-tagging tasks for a large variety of languages across dimensions of family, orthography, and morphology. In addition, we present a Bi-LSTM approach for tagging morphosyntactic attributes at the token level. In this paper, the MIMICK model was trained using characters as input, but future work may consider the use of other subword units, such as morphemes, phonemes, or even bitmap representations of ideographic characters.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multilingual Corpora and Their Applications\": \"Multilingual corpora with different levels of comparability are useful for a range of natural language processing (NLP) tasks. They have been used for extracting parallel lexicons, feeding statistical machine translation (SMT) systems, and in multilingual retrieval models. SMT systems estimate the statistical models from bilingual texts, and having a corpus of the right domain is important to achieve high coverage. Wikipedia, an on-line community-curated encyclopaedia with editions in multiple languages, has been used as a source of data for these purposes.\",\n  \"Challenges in Extracting Comparable Corpora from Wikipedia\": \"Extracting a comparable corpus on a specific domain from Wikipedia is not straightforward. The categories in Wikipedia compose a densely-connected graph with highly overlapping categories, cycles, etc. Many articles are not associated to the categories they should belong to, and there is a phenomenon of over-categorization. This complexity makes the extraction process challenging.\",\n  \"Proposed Platform for Extracting Comparable Corpora from Wikipedia\": \"We propose a simple and effective platform for the extraction of comparable corpora from Wikipedia. The input to the pipeline is the top category of the domain (e.g., Sport). The process involves defining the domain vocabulary from the Wikipedia itself and performing a breadth-first search to explore the category graph, looking for categories that more likely belong to the required domain.\",\n  \"Graph Exploration Strategy for Domain-Specific Corpora Extraction\": \"The graph exploration strategy starts from the top category of a domain and explores the Wikipedia category graph using a breadth-first search. The exploration is controlled by two constraints: avoiding loops and exploring already traversed paths, and setting a stopping criterion to avoid exploring the whole categories graph. The stopping criterion is based on scoring the explored categories to determine if they belong to the domain.\",\n  \"Parallel Sentence Extraction from Comparable Corpora\": \"Given a pair of articles related by an interlanguage link, we estimate the similarity between all their pairs of cross-language sentences with different text similarity measures. The quality of the similarity measures affects the purity of the parallel corpus and, therefore, the quality of the translator. We compute similarities between pairs of sentences using cosine and length factor measures.\",\n  \"Evaluation of Extracted Corpora in Statistical Machine Translation\": \"We validate the quality of the obtained corpora by studying its impact on statistical machine translation. We train SMT systems with Wikipedia and Europarl to translate domain-specific texts and out-of-domain texts. The results show that the Wikipedia translators always outperform the baseline with EP, confirming that a corpus automatically extracted with an F1 smaller than 0.5 is still useful for SMT.\",\n  \"Impact of Domain-Specific Corpora on Translation Quality\": \"Training systems with Wikipedia or Europarl for domain-specific translation shows significant improvements with respect to the general Europarl translator. The most complete system (the union of the four representatives) doubles the BLEU score for all the domains with an impressive improvement of 30 points. This confirms that a corpus automatically extracted with an F1 smaller than 0.5 is still useful for SMT.\",\n  \"Comparison of Different Similarity Measures for Parallel Sentence Extraction\": \"We compare different similarity measures for parallel sentence extraction, including character n-grams, pseudo-cognates, word 1-grams, and length factor. The performance with character 3-grams is the best one, comparable to that of mono, with an F1 of 0.36. This suggests that a translator is not mandatory for performing the sentences selection.\",\n  \"Conclusions and Future Work on Improving Parallel Sentence Identification\": \"In this paper, we presented a model for the automatic extraction of in-domain comparable corpora from Wikipedia. The resulting corpus can be exploited for multiple natural language processing tasks. Our current efforts intend to generate a more robust model for parallel sentences identification and the design of other indirect evaluation schemes to validate the model performance.\"\n}\n```",
    "```json\n{\n  \"Introduction to Abstractive Sentence Summarization\": \"The exponentially growing online information has necessitated the development of effective automatic summarization systems. In this paper, we focus on an increasingly intriguing task, i.e., abstractive sentence summarization, which generates a shorter version of a given sentence while attempting to preserve its original meaning. It can be used to design or refine appealing headlines.\",\n  \"Challenges with Pure Seq2Seq Models\": \"Most previous seq2seq models purely depend on the source text to generate summaries. However, as reported in many studies, the performance of a seq2seq model deteriorates quickly with the increase of the length of generation. Our experiments also show that seq2seq models tend to 'lose control' sometimes. For example, 3% of summaries contain less than 3 words, while there are 4 summaries repeating a word for even 99 times. These results largely reduce the informativeness and readability of the generated summaries.\",\n  \"Template-Based Summarization and Its Limitations\": \"Template based summarization is a traditional approach to abstractive summarization. In general, a template is an incomplete sentence which can be filled with the input text using the manually defined rules. Since the templates are written by humans, the produced summaries are usually fluent and informative. However, the construction of templates is extremely time-consuming and requires a plenty of domain knowledge. Moreover, it is impossible to develop all templates for summaries in various domains.\",\n  \"Proposed Re 3 Sum System\": \"Inspired by retrieve-based conversation systems, we assume the golden summaries of the similar sentences can provide a reference point to guide the input sentence summarization process. We call these existing summaries soft templates since no actual rules are needed to build new summaries from them. Due to the strong rewriting ability of the seq2seq framework, in this paper, we propose to combine the seq2seq and template based summarization approaches. We call our summarization system Re 3 Sum, which consists of three modules: Retrieve, Rerank and Rewrite.\",\n  \"Retrieve Module: Finding Candidate Templates\": \"The purpose of this module is to find out candidate templates from the training corpus. We assume that similar sentences should hold similar summary patterns. Therefore, given a sentence x, we find out its analogies in the corpus and pick their summaries as the candidate templates. Since the size of our dataset is quite large (over 3M), we leverage the widely-used Information Retrieve (IR) system Lucene to index and search efficiently. We keep the default settings of Lucene to build the IR system. For each input sentence, we select top 30 searching results as candidate templates.\",\n  \"Joint Rerank and Rewrite Modules\": \"To conduct template-aware seq2seq generation (rewriting), it is a necessary step to encode both the source sentence x and soft template r into hidden states. Considering that the matching networks based on hidden states have demonstrated the strong ability to measure the relevance of two pieces of texts, we propose to jointly conduct reranking and rewriting through a shared encoding step. Specifically, we employ a bidirectional Recurrent Neural Network (BiRNN) encoder to read x and r. The BiRNN consists of a forward RNN and a backward RNN. The entire representation for the source sentence is [h x 1 ; \u00b7 \u00b7 \u00b7 ; h x \u22121 ]. Since a soft template r can also be regarded as a readable concise sentence, we use the same BiRNN encoder to convert it into hidden states [h r 1 ; \u00b7 \u00b7 \u00b7 ; h r \u22121 ].\",\n  \"Evaluation Metrics and Baselines\": \"We adopt ROUGE (Lin, 2004) for automatic evaluation. ROUGE has been the standard evaluation metric for DUC shared tasks since 2004. It measures the quality of summary by computing the overlapping lexical units between the candidate summary and actual summaries, such as unigram, bi-gram and longest common subsequence (LCS). Following the common practice, we report ROUGE-1 (uni-gram), ROUGE-2 (bi-gram) and ROUGE-L (LCS) F1 scores in the following experiments. We also measure the actual saliency of a candidate template r with its combined ROUGE scores given the actual summary y * .\",\n  \"Experimental Results and Analysis\": \"Experiments show that, in terms of informativeness, Re 3 Sum significantly outperforms the state-of-the-art seq2seq models, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries. We also measure the linguistic quality of generated summaries from various aspects. As can be seen from the rows 'LEN DIF' and 'LESS 3', the performance of Re 3 Sum is almost the same as that of soft templates. The soft templates indeed well guide the summary generation.\",\n  \"Effect of Soft Templates on Model Performance\": \"In this section, we investigate how soft templates affect our model. At the beginning, we feed different types of soft templates into the Rewriting module of Re 3 Sum. As illustrated, the more high-quality templates are provided, the higher ROUGE scores are achieved. It is interesting to see that, while the ROUGE-2 score of Random templates is zero, our model can still generate acceptable summaries with Random templates. It seems that Re 3 Sum can automatically judge whether the soft templates are trustworthy and ignore the seriously irrelevant ones. We believe that the joint learning with the Rerank model plays a vital role here.\",\n  \"Comparison with State-of-the-Art Models\": \"We compare our proposed model with the following state-of-the-art neural summarization systems: OpenNMT, which is the standard attentional seq2seq model with OpenNMT. All the settings are the same as our system. It is noted that OpenNMT officially examined the Gigaword dataset. We distinguish the official result and our experimental result with suffixes 'O' and 'I' respectively. FTSum, which encoded the facts extracted from the source sentence to improve both the faithfulness and informativeness of generated summaries. In addition, to evaluate the effectiveness of our joint learning framework, we develop a baseline named 'PIPELINE'. Its architecture is identical to Re 3 Sum. However, it trains the Rerank module and Rewrite module in pipeline.\"\n}\n```",
    "```json\n{\n  \"Introduction to Automatic Evaluation in Machine Translation\": \"To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one. Manual/human evaluation can be costly and time consuming, and so an automatic evaluation metric, given that it sufficiently correlates with manual evaluation, can be useful in developmental cycles. In studies involving hyperparameter tuning or architecture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large. As objective, reproducible quantities, metrics can also facilitate cross-paper comparisons.\",\n  \"WMT Metrics Shared Task Setup and Objectives\": \"In the setup of our Metrics Shared Task, an automatic metric compares an MT system's output translations with manual reference translations to produce: either (a) system-level score, i.e. a single overall score for the given MT system, or (b) segment-level scores for each of the output translations, or both. This year we teamed up with the organizers of the QE Task and hosted 'QE as a Metric' as a joint task. In the setup of the Quality Estimation Task, no human-produced translations are provided to estimate the quality of output translations. Quality estimation (QE) methods are built to assess MT output based on the source or based on the translation itself.\",\n  \"Manual Quality Assessment Methods and Golden Truth\": \"Direct Assessment (DA) was employed as the source of the 'golden truth' to evaluate metrics again this year. The basis of DA is to collect a large number of quality assessments (a number on a scale of 1-100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator. For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system's performance. For segment-level evaluation, converting DA judgements to golden truth expressed as relative rankings is necessary.\",\n  \"Baseline Metrics and Their Implementation\": \"The baselines we report are: BLEU and NIST, TER, WER, PER and CDER, chrF and chrF+, sacreBLEU-BLEU and sacreBLEU-chrF. BLEU, TER, WER, PER, CDER, sacreBLEU-BLEU and sacreBLEU-chrF for system-level only; sentBLEU for segment-level only and chrF for both. Chinese word segmentation is unfortunately not supported by the tokenization scripts mentioned above. For scoring Chinese with baseline metrics, we thus pre-processed MT outputs and reference translations with a script that separates Chinese characters from each other and also from non-Chinese parts.\",\n  \"Submitted Metrics and Their Descriptions\": \"The rest of this section provides a brief summary of all the metrics that participated. BEER is a trained evaluation metric with a linear model that combines sub-word feature indicators and global word order features to achieve a language agnostic and fast to compute evaluation metric. BERTr uses contextual word embeddings to compare the MT output with the reference translation. CharacTER is a character-level metric inspired by the commonly applied translation edit rate (TER). EED is a character-based metric, which builds upon CDER. ESIM is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. hLEPORb_baseline, hLEPORa_baseline are metrics based on the factor combination of length penalty, precision, recall, and position difference penalty. PReP is a method for filtering pseudo-references to achieve a good match with a gold reference. WMDO is a metric based on distance between distributions in the semantic vector space. YiSi is a unified semantic MT quality evaluation and estimation metric for languages with different levels of available resources.\",\n  \"System-Level Evaluation Results and Analysis\": \"We discuss system-level results for news task systems in Section 5.1. The system-level correlations of metrics evaluating translation of newstest2019 are provided in Tables 3, 4, and 5. The underlying texts are part of the WMT19 News Translation test set and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task. We employ Williams significance test to identify differences in correlation that are statistically significant. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 3, 4, and 5.\",\n  \"Segment-Level Evaluation Results and Analysis\": \"Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, we convert pairs of DA scores for competing translations to daRR better/worse preferences. We measure the quality of metrics' segment-level scores against the daRR golden truth using a Kendall's Tau-like formulation. The key differences between the evaluation used in WMT14-WMT16 and evaluation used in WMT17-WMT19 were (1) the move from RR to daRR and (2) the treatment of ties. Results of the segment-level human evaluation for translations sampled from the News Translation Task are shown in Tables 6 and 7.\",\n  \"Discussion on Metric Performance Across Different MT Systems\": \"The observed performance of metrics depends on the underlying texts and systems that participate in the News Translation Task. For the strongest MT systems, distinguishing which system outputs are better is hard, even for human assessors. On the other hand, if the systems are spread across a wide performance range, it will be easier for metrics to correlate with human judgements. To provide a more reliable view, we created plots of Pearson correlation when the underlying set of MT systems is reduced to top n ones. One sample such plot is in Figure 5, all language pairs and most of the metrics are in Appendix A.\",\n  \"QE Systems as Metrics: Performance and Challenges\": \"Generally, correlations for the standard reference-based metrics are obviously better than those in 'QE as a Metric' track, both when using monolingual and bilingual golden truth. In system-level evaluation, correlations for 'QE as a Metric' range from 0.028 to 0.947 across all language pairs and all metrics but they are very unstable. Even for a single metric, take UNI for example, the correlations range from 0.028 to 0.930 across language pairs. In segment-level evaluation, correlations for QE metrics range from -0.153 to 0.351 across all language pairs and show the same instability across language pairs for a given metric.\",\n  \"Future Directions and Insights for MT Evaluation Metrics\": \"Future investigations are needed to understand when BLEU applies well, and why BLEU is not effective for output from our state of the art models. Metrics and QE systems such as BERTr, ESIM, YiSi that perform well at judging our best systems often use more semantic features compared to our n-gram/char-gram based baselines. Future metrics may want to explore a) whether semantic features such as contextual word embeddings are achieving semantic understanding and b) whether semantic understanding is the true source of a metric's performance gains.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Machine Translation (NMT) and Coverage Issues\": \"Neural machine translation (NMT) emerged in the last few years as a very successful paradigm. While NMT is generally more fluent than previous statistical systems, adequacy is still a major concern: common mistakes include dropping source words and repeating words in the generated translation.\",\n  \"Previous Approaches to Mitigate Coverage Problems in NMT\": \"Previous work has attempted to mitigate this problem in various ways. Wu et al. (2016) incorporate coverage and length penalties during beam search-a simple yet limited solution, since it only affects the scores of translation hypotheses that are already in the beam. Other approaches involve architectural changes: providing coverage vectors to track the attention history, using gating architectures and adaptive attention to control the amount of source context provided, or adding a reconstruction loss.\",\n  \"Proposed Solution: Constrained Sparsemax Transformation\": \"In this paper, we propose a different solution that does not change the overall architecture, but only the attention transformation. Namely, we replace the traditional softmax by other recently proposed transformations that either promote attention sparsity or upper bound the amount of attention a word can receive. The bounds are determined by the fertility values of the source words. We combine these two ideas and propose a novel attention transformation, constrained sparsemax, which produces both sparse and bounded attention weights, yielding a compact and interpretable set of alignments.\",\n  \"Sparsemax Transformation Details\": \"The sparsemax transformation is defined as the Euclidean projection of the scores z onto the probability simplex. These projections tend to hit the boundary of the simplex, yielding a sparse probability distribution. This allows the decoder to attend only to a few words in the source, assigning zero probability mass to all other words. Martins and Astudillo (2016) have shown that the sparsemax can be evaluated in O(J) time and gradient backpropagation takes sublinear time.\",\n  \"Constrained Softmax Transformation Details\": \"The constrained softmax transformation was recently proposed by Martins and Kreutzer (2017) in the context of easy-first sequence tagging. It returns the distribution closest to softmax(z) whose attention probabilities are bounded by u. To use this transformation in the attention mechanism, we make use of the idea of fertility. Each source word j gets a credit of f j units of attention, which are consumed along the decoding process. If all the credit is exhausted, it receives zero attention from then on.\",\n  \"Constrained Sparsemax Transformation: Definition and Properties\": \"In this work, we propose a novel transformation which shares the two properties above: it provides both sparse and bounded probabilities. It is defined as the solution to a constrained optimization problem that minimizes the Euclidean distance between the scores and the probability distribution, subject to the constraints that the distribution is bounded by the fertility values. The solution takes the form \u03b1 j = max{0, min{u j , z j \u2212 \u03c4 }}, where \u03c4 is a normalization constant.\",\n  \"Fertility Bounds and Implementation Details\": \"We experiment with three ways of setting the fertility of the source words: CONSTANT, GUIDED, and PREDICTED. With CONSTANT, we set the fertilities of all source words to a fixed integer value f. With GUIDED, we train a word aligner based on IBM Model 2 and, for each word in the vocabulary, we set the fertilities to the maximal observed value in the training data. At test time, we use the expected fertilities according to our model. We append an additional <SINK> token to the end of the source sentence, to which we assign unbounded fertility.\",\n  \"Experimental Setup and Datasets\": \"We evaluated our attention transformations on three language pairs: DE-EN, JA-EN, and RO-EN. The training sets have 153,326, 329,882, and 560,767 parallel sentences, respectively. We tokenized the data using the Moses scripts and preprocessed it with subword units with a joint vocabulary and 32k merge operations. Our implementation was done on a fork of the OpenNMT-py toolkit with the default parameters.\",\n  \"Evaluation Metrics and Results\": \"As evaluation metrics, we report tokenized BLEU, METEOR, as well as two new metrics: REP-score and DROP-score. The sparse models (csparsemax as well as sparsemax combined with coverage models) have higher scores on both BLEU and METEOR. Generally, they also obtain better REP and DROP scores than csoftmax and softmax, which suggests that sparse attention alleviates the problem of coverage to some extent.\",\n  \"Conclusion and Future Work\": \"We proposed a new approach to address the coverage problem in NMT, by replacing the softmax attentional transformation by sparse and constrained alternatives: sparsemax, constrained softmax, and the newly proposed constrained sparsemax. By incorporating a model for fertility prediction, our attention transformations led to sparse alignments, avoiding repeated words in the translation.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Split-and-Rephrase Task\": \"Processing long, complex sentences is challenging. An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing. A recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it 'Split-and-Rephrase'.\",\n  \"Challenges in Processing Complex Sentences\": \"Processing long, complex sentences is challenging. This is true either for humans in various circumstances or in NLP tasks like parsing and machine translation. An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing.\",\n  \"Dataset and Evaluation Methodology\": \"The dataset includes 1,066,115 instances mapping a single complex sentence to a sequence of sentences that express the same meaning, together with RDF triples that describe their semantics. For evaluation we follow Narayan et al. (2017) and compute the averaged individual multi-reference BLEU score for each prediction.\",\n  \"Initial SEQ2SEQ Model Performance and Limitations\": \"We begin with vanilla SEQ2SEQ models with attention and reach an accuracy of 77.5 BLEU, substantially outperforming the text-to-text baseline of Narayan et al. (2017) and approaching their best RDF-aware method. However, manual inspection reveal many cases of unwanted behaviors in the resulting outputs.\",\n  \"Analysis of Model Behavior and Memorization\": \"Manual inspection reveal many cases of unwanted behaviors in the resulting outputs: (1) many resulting sentences are unsupported by the input: they contain correct facts about relevant entities, but these facts were not mentioned in the input sentence; (2) some facts are repeated-the same fact is mentioned in multiple output sentences; and (3) some facts are missing-mentioned in the input but omitted in the output. The model learned to memorize entity-fact pairs instead of learning to split and rephrase.\",\n  \"Proposed Data Split for Better Generalization\": \"We construct a new train-development-test split to better reflect our expected behavior from a split-and-rephrase model. We split the data into train, development and test sets by randomly dividing the 5,554 distinct complex sentences across the sets, while using the provided RDF information to ensure that: 1. Every possible RDF relation (e.g., BORNIN, LOCATEDIN) is represented in the training set (and may appear also in the other sets). 2. Every RDF triplet (a complete fact) is represented only in one of the splits.\",\n  \"Copy-Augmented SEQ2SEQ Model\": \"To better suit the split-and-rephrase task, we augment the SEQ2SEQ models with a copy mechanism. Such mechanisms have proven to be beneficial in similar tasks like abstractive summarization and language modeling. We hypothesize that biasing the model towards copying will improve performance, as many of the words in the simple sentences (mostly corresponding to entities) appear in the complex sentence.\",\n  \"Experimental Results and Analysis\": \"On the original split, our models outperform the count unique RDF entities of Narayan et al. (2017) by up to 8.68 BLEU, without using the RDF triples. On the new split, the vanilla SEQ2SEQ models break completely, while the copy-augmented models perform better. We also present the macro-average number of simple sentences in each prediction, and the average number of tokens for each simple sentence.\",\n  \"Conclusions and Future Research Directions\": \"We demonstrated that a SEQ2SEQ model can obtain high scores on the original split-and-rephrase task while not actually learning to split-and-rephrase. We propose a new and more challenging data-split to remedy this, and demonstrate that the cheating SEQ2SEQ models fail miserably on the new split. Augmenting the SEQ2SEQ models with a copy-mechanism improves performance on both data splits, establishing a new competitive baseline for the task. Yet, the split-and-rephrase task (on the new split) is still far from being solved.\"\n}\n```",
    "```json\n{\n  \"Increasing Retraction Rates and Their Impact on Scientific Literature\": \"Studies on retracted articles show that the amount of retracted articles has increased in relative measure to the overall increase in scientific publications. Although retracting articles helps purge the scientific literature of erroneous or unethical research, citations to such research present a real challenge. Citing articles that were retracted especially due to plagiarism, data falsification, or any other unethical practices interferes with the process of eliminating such studies from the literature and research.\",\n  \"Challenges Posed by Citations of Retracted Articles\": \"Citing articles that were retracted especially due to plagiarism, data falsification, or any other unethical practices interferes with the process of eliminating such studies from the literature and research. Both types of citations put the scientific process in jeopardy, especially when they are cited as legitimate references to previous work. Some studies on retracted articles have shown that retracted articles that received a high number of citations preretraction are more likely to occur additional citations post-retraction.\",\n  \"Types of Retraction Citations and Their Implications\": \"There are two types of retraction citations; citations that a retracted article received prior to its retraction and citations that are received post-retraction and despite retraction notices. Both types of citations put the scientific process in jeopardy, especially when they are cited as legitimate references to previous work. Some studies on retracted articles have shown that retracted articles that received a high number of citations preretraction are more likely to occur additional citations post-retraction.\",\n  \"Case Study: Scott S. Reuben and the Persistence of Citations Post-Retraction\": \"A good example is described in a study by [6] who studied the case of Scott S. Reuben who was convicted of fabricating data in 25 of his studies which resulted in mass retractions of his articles. The authors of the study have shown that the popularity of Reuben's articles did not diminish post-retraction even 5 years after the retractions have been made.\",\n  \"Methodology for Identifying and Analyzing Post-Retraction Citations\": \"ScienceDirect, Elsevier's full text database was accessed in October 2014. The database was queried for the term 'RETRACTED' in the article title and its retraction notice. In ScienceDirect, each retracted article is preceded with the word 'RETRACTED'. In addition, each Elsevier journal incorporates a retraction notice which explains who retracted the article and the reason for retraction. This allowed us to manually code each article in our dataset with an additional field 'retracted by' that represented the person/s requesting the retraction.\",\n  \"Analysis of Citations for a Retracted Article in Cell\": \"This article was published in 2010 in Cell and retracted in 2014 due to irregularities in graphs and data misrepresentation in the images. Although the graphs and images did not have any bearing on the validity of the results, according to the retraction notice, the editors stated that '...the level of care in figure preparation in Donmez et al. falls well below the standard that we expect, and we are therefore retracting the paper'. We conducted an individual content analysis of the most recent 36 citations which were tracked in 2015 and 2016. Our results show that the citations are mostly positive.\",\n  \"The S\u00e9ralini Affair: A Controversial Case of Retraction and Its Aftermath\": \"This article, published in 2012, was the subject of a debate surrounding the validity of the findings, use of animals, and even accusations of fraud. Its publication and retraction process have resulted in the 'S\u00e9ralini affair' which became a big media news item. The article described a 2-year study of rats which were fed genetically modified (GM) crops and showed increased tumors. The study, which was also scrutinized by government agencies, received major media attention that resulted in the creation of a social movement against GM food. The demand to label all GM foods is still underway.\",\n  \"Republishing Retracted Articles and Its Impact on Scientific Credibility\": \"This article was republished in 2014 by Environmental Sciences Europe. The republication of the study stirred another controversial discussion in the scientific community with several scientists writing letters expressing their concerns regarding the appearance of the same study in another journal. The republished article received 17 citations in 2015 and 2016. The vast majority of them being positive mentions. In addition, some criticism towards the peer-review practices of the retracting editors were also detected.\",\n  \"Citations of a Retracted Article on Resveratrol and Data Manipulation\": \"This article was retracted in 2014 due to serious data manipulation and falsification. In the retraction notice of this article, the editors of the journal went to great lengths to examine and re-examine the statistical claims made by the authors using the services of three separate methodologists. Following the methodologists' findings of irregularities in the reported data and falsification of results, and the authors' lack of proper response to their findings, the article was retracted from the journal. However, the article continued to be cited despite the lengthy and detailed retraction notice.\",\n  \"Recommendations for Handling Citations of Retracted Articles\": \"We recommend that publishers use reference checks to all submitted articles to detect citations of retracted articles and remove them or at least request an explanation from the authors for citing a retracted paper in a positive or neutral manner. This explanation should clearly appear in the paper. In addition, we would recommend the deletion of retracted articles from publishers' websites. Currently, at least for the major publishers: Elsevier, Springer Nature, and Wiley, but possibly a general practice, retracted articles are not only available on the publishers' site, but they are freely available, without the need for a subscription or for a one-time payment.\"\n}\n```",
    "```json\n{\n  \"Introduction to Speech-to-Text Translation (ST) and Its Challenges\": \"Speech-to-text Translation (ST) has many potential applications for low-resource languages, such as in language documentation and crisis relief. Traditional ST is a pipeline of automatic speech recognition (ASR) and machine translation (MT), requiring transcribed source audio and parallel text, which are often unavailable for low-resource languages. In these scenarios, end-to-end ST is appealing, but it requires substantial resources for training, making it challenging in low-resource settings.\",\n  \"End-to-End ST in Low-Resource Settings\": \"End-to-end ST is appealing in scenarios where there may be some source language audio paired with target language text translations, but it requires substantial resources for training. For example, a system trained on only 20 hours of data for Spanish-English ST achieved a BLEU score of 5.3, indicating low accuracy in low-resource settings.\",\n  \"Leveraging High-Resource Languages for Low-Resource ST\": \"To improve end-to-end ST in low-resource settings, we can leverage data from high-resource languages. Training a single model on multiple languages can be effective for ASR, and transfer learning has been very effective for MT. Inspired by these successes, we show that low-resource ST can leverage transcribed audio in a high-resource target language, or even a different language altogether, by pre-training a model for the high-resource ASR task and then transferring and fine-tuning its parameters for low-resource ST.\",\n  \"Model Architecture and Preprocessing Techniques\": \"For both ASR and ST, we use an encoder-decoder model with attention. The speech encoder uses a stack of two CNN layers followed by a three-layer bi-directional LSTM. The text decoder uses a fully-connected layer and a three-layer LSTM. We preprocess speech by converting raw speech input to 13-dimensional MFCCs and perform speaker-level mean and variance normalization. Text is preprocessed using byte pair encoding (BPE) to segment each word into subwords, resulting in a model that is open-vocabulary and can be trained quickly.\",\n  \"Experimental Setup and Data Sets\": \"We use the Switchboard Telephone speech corpus for English ASR, the French speech corpus from the GlobalPhone collection for French ASR, and the Fisher Spanish speech corpus for Spanish-English ST. For Mboshi-French ST, we use the Mboshi-French parallel corpus. The Spanish-English data set contains 1.5M word tokens and 17K word types, while the Mboshi-French data set consists of around 4 hours of Mboshi speech.\",\n  \"Evaluation Metrics and Baseline Models\": \"We report BLEU scores for all our models and also report word-level unigram precision and recall, taking into account stem, synonym, and paraphrase matches using METEOR. We include evaluation scores for a naive baseline model that predicts the K most frequent words of the training set as a bag of words for each test utterance.\",\n  \"Impact of ASR Pre-Training on Spanish-English ST\": \"Fine-tuning an English ASR model on Spanish-English ST data improves the BLEU score from 10.8 to 19.9 for a model trained on 20 hours of data. Transfer learning also substantially improves precision and recall, with gains of between 10 and 20 points. In very low-resource scenarios, even a model trained on only 2.5 hours of ST data with transfer learning can achieve a precision/recall of around 30%.\",\n  \"Analysis of Transfer Learning Effects\": \"Transferring all parameters of the English ASR model to the Spanish-English ST model is most effective, with the speech encoder parameters accounting for most of the gains. Transferring only decoder parameters does not improve accuracy, suggesting that the dependency in parameter training order might explain this. Models make strong gains early on in the training when using transfer learning, reaching higher BLEU scores after just 5 epochs compared to models without transfer learning.\",\n  \"Impact of Code-Switching and Cross-Linguistic Pre-Training\": \"Using the English ASR model without any fine-tuning to translate Spanish audio to English text achieves a BLEU score of 1.1, indicating that the model is matching some 4-grams in the reference due to code-switching in the Fisher-Spanish speech data set. However, using French ASR data for a Spanish-English translation task also improves performance, suggesting that ASR pre-training on a completely distinct third language can help low-resource ST.\",\n  \"Application to Mboshi-French ST\": \"For Mboshi-French ST, we achieve the best evaluation scores on all metrics by transferring the encoding parameters from the English ASR model and the decoding parameters from the French ASR model. This configuration highlights the flexibility of our framework and shows that even with only 4 hours of parallel data, we can outperform baseline models to obtain a BLEU score of 7.1 and precision/recall of about 25%.\"\n}\n```",
    "```json\n{\n  \"Historical Context of Translation\": \"Translation has played a major role in human civilization since the rise of law, religion, and trade in multilingual societies. Evidence of scribe translations goes as far back as four millennia ago, to the time of Hammurabi; this practice is also mentioned in the Bible (Esther 1:22; 8:9). For thousands of years, translators have tried to remain invisible, setting a standard according to which the act of translation should be seamless, and its product should look as if it were written originally in the target language.\",\n  \"Translation Universals and Interference\": \"Despite the continuous efforts of translators, translations are known to feature unique characteristics that set them apart from non-translated texts, referred to as originals here. This is not the result of poor translation, but rather a statistical phenomenon: various features distribute differently in originals than in translations. Such universals include simplification: the tendency to make complex source structures simpler in the target; standardization: the tendency to over-conform to target language standards; and explicitation: the tendency to render implicit source structures more explicit in the target language. In contrast to translation universals, interference reflects the 'fingerprints' of the source language on the translation product.\",\n  \"Methodology and Dataset\": \"This corpus-based study uses Europarl, the proceedings of the European Parliament and their translations into all the official European Union (EU) languages. Europarl is one of the most popular parallel resources in natural language processing, and has been used extensively in machine translation. We use a version of Europarl spanning the years 1999 through 2011, in which the direction of translation has been established through a comprehensive cross-lingual validation of the speakers' original language. All parliament speeches were translated from the original language into all other EU languages (21 at the time) using English as an intermediate, pivot language.\",\n  \"Feature Selection for Translation Analysis\": \"We represented both original and translated texts as feature vectors, where the choice of features determines the extent to which we expect source-language interference to be present in the translation product. Crucially, the features abstract away from the contents of the texts and focus on their structure, reflecting, among other things, morphological and syntactic patterns. We use the following feature sets: 1. The top-1,000 most frequent POS trigrams, reflecting shallow syntactic structure. 2. Function words (FW), words known to reflect grammar of texts in numerous classification tasks, as they include non-content words such as articles, prepositions, etc. 3. Cohesive markers; these words and phrases are assumed to be overrepresented in translated texts, where, for example, an implicit contrast in the original is made explicit in the target text with words such as 'but' or 'however'.\",\n  \"Evaluation of Phylogenetic Trees\": \"To evaluate the quality of the trees we generate, we compute their similarity to the gold standard via two metrics: unweighted, assessing only structural (topological) similarity, and weighted, estimating similarity based on both structure and branching length. We opted for a simple yet powerful adaptation of the L2-norm to leaf-pair distance, inherently suitable for both unweighted and weighted evaluation. Given a tree of N leaves, l i , i \u2208 [1..N ], the weighted distance between two leaves l i , l j in a tree \u03c4, denoted D \u03c4 (l i , l j ), is the sum of the weights of all edges on the shortest path between l i and l j .\",\n  \"Identification of Source Language\": \"Identifying the source language of translated texts is a task in which machines clearly outperform humans. We extended this experiment to 14 languages representing 3 language families (the number of languages was limited by the amount of data available). We extracted 100 chunks of 1,000 tokens each from each source language and classified the translated English (and, separately, French) texts into 14 classes using the best performing POS-trigrams feature set. Cross-validation evaluation yielded an accuracy of 75.61% on English translations (note that the baseline is 100/14 = 7.14%).\",\n  \"Reconstruction of Phylogenetic Language Trees\": \"Inspired by the results reported in Section 4.2, we generated phylogenetic language trees from both English and French texts translated from the other European languages. We hypothesized that interference from the source language was present in the translation product to an extent that would facilitate the construction of a tree sufficiently similar to the gold IE tree. The best trees, those closest to the gold standard, were generated using POS-trigrams: these are the features that are most closely associated with source-language interference.\",\n  \"Analysis of Linguistic Phenomena in Translations\": \"We demonstrated that source-language traces are dominant in translation products to an extent that facilitates reconstruction of the history of the source languages. We now inspect some of these phenomena in more detail to better understand the prominent characteristics of interference. For each phenomenon, we computed the frequencies of patterns that reflect it in texts translated to English from each individual language, and averaged the measures over each language family (Germanic, Romance, and Balto-Slavic).\",\n  \"Conclusion and Future Research Directions\": \"Translations may be considered distortions of the original text, but this distortion is far from random. It depicts a very clear picture, reflecting language typology to the extent that disregarding the sources altogether, a phylogenetic tree can be reconstructed from a monolingual corpus consisting of multiple translations. This holds for the product of highly professional translators, who conform to a common standard, and whose products are edited by native speakers, like themselves. It even holds after two phases of translations. We are presently trying to extend these results to translations in a different domain (literary texts) into a very different language (Hebrew).\"\n}\n```",
    "```json\n{\n  \"Introduction to Text Coherence\": \"Coherence plays an important role in a text because it enables a text to convey its communication purpose and meaning to its readers. Coherence also decreases reading time as a more coherent text is easier to read with less reader's cognitive load. While there is no single agreed definition of coherence, we can compile several definitions of coherence and note its important aspects.\",\n  \"Definitions and Aspects of Coherence\": \"First, a text is coherent if it can convey its communication purpose and meaning to its readers. Second, a text needs to be integrated as a whole, rather than a series of independent sentences. It means that sentences in the text are centralised around a certain theme or topic, and are arranged in a particular order in terms of logical, spatial, and temporal relations. Third, every sentence in a coherent text has relation(s) to each other. It suggests that a text exhibits discourse/rhetorical relation and cohesion. Fourth, text coherence is greatly influenced by the presence of a certain organisation in the text. The organisation helps readers to anticipate the upcoming textual information. Fifth, it is easier to read a coherent text than its less coherent counterpart.\",\n  \"Existing Computational Models of Text Coherence\": \"There are studies on computational modelling of text coherence based on the supervised learning approach, such as the Entity Grid model. The Entity Grid model has been further extended into the Role Matrix model. However, these models have a few drawbacks. First, the Entity Grid using co-reference resolution has a bias towards the original ordering of text when comparing a text with its permutated counterparts. The co-reference resolution module is trained on well-formed texts; thus it does not perform very well for ill-organised texts. The methods utilising a discourse parser for modelling text coherence have the same problem. Second, the supervised model often suffers from data sparsity, domain dependence, and computational cost for training. To alleviate these problems in the supervised model, Guinaudeau and Strube (2013) proposed an unsupervised coherence model known as the Entity Graph model.\",\n  \"Proposed Unsupervised Text Coherence Model\": \"To tackle these limitations of the past research, we present an unsupervised text coherence model that captures the contribution of related-yet-not-identical entities. We encode a text into a graph G(V, E), where V is a set of vertices and E is a set of edges in the graph. The vertex v_i \u2208 V represents the i-th sentence s_i in the text, and the weighted directed edge e_i,j \u2208 E represents a semantic relation from the i-th to the j-th sentences.\",\n  \"Graph Construction Methods for Semantic Similarity\": \"We propose three methods for constructing a graph from a text based on semantic similarity between sentence pairs in the text. The first method, Preceding adjacent vertex (PAV), establishes edges based on the similarity of the preceding adjacent sentence. The second method, Single similar vertex (SSV), allows edges to the following vertices as well as preceding vertices based on semantic similarity. The third method, Multiple similar vertex (MSV), allows multiple outgoing edges for every vertex based on a certain threshold in terms of cosine similarity.\",\n  \"Text Coherence Measure Calculation\": \"From a constructed graph by one of the three methods, text coherence measure tc is calculated by averaging the averaged weight of outgoing edges from every vertex in the graph. A larger tc value denotes a more coherent text.\",\n  \"Evaluation and Results of Document Discrimination Task\": \"We evaluate the proposed methods on two experimental tasks: the document discrimination task and insertion task. In the document discrimination task, sentences in a text are randomly permutated to generate another text; the task is to identify the original text given a pair of the original and the randomised one. The best performing method (PAV) outperformed the unsupervised baseline (Entity Graph) but not the supervised baseline (Entity Grid) in the document discrimination task. The difference was statistically significant at p < 0.05.\",\n  \"Evaluation and Results of Insertion Task\": \"In the insertion task, the coherence measure is evaluated based on to what extent the measure can estimate the original sentence position in a text from which one sentence is taken out randomly. The best performing method (PAV) outperformed both supervised and unsupervised baselines, but the difference is not statistically significant at p < 0.05.\",\n  \"Discussion on Performance and Limitations\": \"Our experimental result showed that our best proposed method (PAV) performed 0.774 in accuracy in the document discrimination task, but only performed 0.356 in the insertion task. There is a big gap in their performance between two tasks. The error analysis revealed a possibility to improve the performance by introducing a more refined representation of sentence vectors and calculation in semantic the similarity between sentences for capturing distant relations between sentences.\"\n}\n```",
    "```json\n{\n  \"Introduction to BioASQ Challenge\": \"BioASQ is a semantic indexing, question answering (QA) and information extraction challenge. We participated in Task B of the challenge which is concerned with biomedical QA. The fifth BioASQ challenge is taking place at the time of writing. Five batches of 100 questions each were released every two weeks. Participating systems have 24 hours to submit their results.\",\n  \"Task B, Phase B: Question Answering in Biomedical Domain\": \"Our system participated in Task B, Phase B: Given a question and gold-standard snippets (i.e., pieces of text that contain the answer(s) to the question), the system is asked to return a list of answer candidates. The questions are categorized into different question types: factoid, list, summary and yes/no. Our work concentrates on answering factoid and list questions.\",\n  \"Evaluation Metrics for Factoid and List Questions\": \"For factoid questions, the system's responses are interpreted as a ranked list of answer candidates. They are evaluated using mean reciprocal rank (MRR). For list questions, the system's responses are interpreted as a set of answers to the list question. Precision and recall are computed by comparing the given answers to the goldstandard answers. F1 score, i.e., the harmonic mean of precision and recall, is used as the official evaluation measure.\",\n  \"Traditional vs. Neural Network Approaches in Biomedical QA\": \"Most existing biomedical QA systems employ a traditional QA pipeline, similar in structure to the baseline system by Weissenborn et al. (2013). They consist of several discrete steps, e.g., named-entity recognition, question classification, and candidate answer scoring. These systems require a large amount of resources and feature engineering that is specific to the biomedical domain. Our system, on the other hand, is based on a neural network QA architecture that is trained end-to-end on the target task.\",\n  \"Neural Architecture of the Proposed System\": \"Our system is a neural network which takes as input a question and a context (i.e., the snippets) and outputs start and end pointers to tokens in the context. At its core, we use FastQA (Weissenborn et al., 2017), a state-of-the-art neural QA system. We generalize the FastQA output layer in order to be able to answer list questions in addition to factoid questions.\",\n  \"Training and Fine-Tuning Process\": \"We train the network in two steps: First, the network is trained on SQuAD, following the procedure by Weissenborn et al. (2017) (pretraining phase). Second, we fine-tune the network parameters on BioASQ (fine-tuning phase). For both phases, we use the Adam optimizer with an exponentially decaying learning rate. We start with learning rates of 10\u22123 and 10\u22124 for the pre-training and fine-tuning phases, respectively.\",\n  \"Handling Factoid and List Questions with Neural Network\": \"By computing the start probability via the sigmoid rather than softmax function (as used in FastQA), we enable the model to output multiple spans as likely answer spans. This generalizes the factoid QA network to list questions. For factoid questions, we output the top five answer strings as our ranked list of answer candidates. For list questions, we use a probability cutoff threshold t, such that {(i, j)|p i,j span \u2265 t} is the set of answers.\",\n  \"Implementation Details and Training Environment\": \"We implemented our system using TensorFlow. It was trained on an NVidia GForce Titan X GPU.\",\n  \"Results and Discussion on BioASQ 5B\": \"We report the results for all five test batches of BioASQ 5 (Task 5b, Phase B) in Table 1. Note that the performance numbers are not final, as the provided synonyms in the gold-standard answers will be updated as a manual step, in order to reflect valid responses by the participating systems. In general, the single and ensemble system are performing very similar relative to the rest of field: Their ranks are almost always right next to each other. Between the two, the ensemble model performed slightly better on average.\",\n  \"Conclusion and Future Work\": \"In this paper, we summarized the system design of our BioASQ 5B submission for factoid and list questions. We use a neural architecture which is trained end-to-end on the QA task. This approach has not been applied to BioASQ questions in previous challenges. Our results show that our approach achieves state-of-the art results on factoid questions and competitive results on list questions.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Graphs and Their Properties\": \"Semantic graphs, such as WordNet, encode the structural qualities of language as a representation of human knowledge. On the local level, they describe connections between specific semantic concepts, or synsets, through individual edges representing relations such as hypernymy ('is-a') or meronymy ('is-part-of'). On the global level, they encode emergent regular properties in the induced relation graphs.\",\n  \"Local vs. Global Properties in Semantic Graphs\": \"Local properties have been subject to extensive study in recent years via the task of relation prediction, where individual edges are found based mostly on distributional methods that embed synsets and relations into a vector space. In contrast, while the structural regularity and significance of global aspects of semantic graphs is well-attested, global properties have rarely been used in prediction settings. In this paper, we show how global semantic graph features can facilitate in local tasks such as relation prediction.\",\n  \"Exponential Random Graph Model (ERGM) and Its Limitations\": \"To estimate weights on local and global graph features, we build on the Exponential Random Graph Model (ERGM), a log-linear model over networks utilizing global graph features. This normalization term grows exponentially in the number of nodes, and in general cannot be decomposed into smaller parts. Approximations are therefore necessary to fit ERGMs on graphs with even a few dozen nodes, and the largest known ERGMs scale only to thousands of nodes. This is insufficient for WordNet, which has an order of 10^5 nodes.\",\n  \"Max-Margin Markov Graph Model (M3GM) for Semantic Graphs\": \"We extend the ERGM framework in several ways. First, we replace the maximum likelihood objective with a margin-based objective, which compares the observed network against alternative networks; we call the resulting model the Max-Margin Markov Graph Model (M3GM), drawing on ideas from structured prediction. The gradient of this loss is approximated by importance sampling over candidate negative edges, using a local relational model as a proposal distribution. The complexity of each epoch of estimation is thus linear in the number of edges, making it possible to scale up to the 10^5 nodes in WordNet.\",\n  \"Feature Extraction and Motifs in Semantic Graphs\": \"Graph Motifs as Features. Based on common practice in ERGM feature extraction, we select the following graph features as a basis: Total edge count; Number of cycles of length k, for k \u2208 {2, 3}; Number of nodes with exactly k outgoing (incoming) edges, for k \u2208 {1, 2, 3}; Number of nodes with at least k outgoing (incoming) edges, for k \u2208 {1, 2, 3}; Number of paths of length 2; Transitivity: the proportion of length-2 paths u \u2192 v \u2192 w where an edge u \u2192 w also exists.\",\n  \"Parameter Estimation and Training of M3GM\": \"The probabilistic formulation of ERGM requires the computation of a normalization term that sums over all possible graphs with a given number of nodes, G N . One solution is to approximate probability using a variant of the Monte Carlo Maximum Likelihood Estimation (MCMLE) produce, log P (G) \u2248 log \u03c8(G) \u2212 log |G |V | | M M G \u223cG |V | \u03c8(G). We propose a margin loss objective: the log score for each negative sampleG should be below the log score for G by a margin of at least 1. This motivates the hinge loss, L(\u0398,G; G) = 1 \u2212 log \u03c8 ERGM+ (G) + log \u03c8 ERGM+ (G) +.\",\n  \"Evaluation of M3GM on WN18RR Dataset\": \"We evaluate M3GM on the relation graph edge prediction task. Data for this task consists of a set of labeled edges, i.e. tuples of the form (s, r, t), where s and t denote source and target entities, respectively. Given an edge from an evaluation set, two prediction instances are created by hiding the source and target side, in turn. The predictor is then evaluated on its ability to predict the hidden entity, given the other entity and the relation type. We evaluate M3GM on the WN18RR dataset, which is a subset curated as WN18, containing 18 relations for about 41,000 synsets extracted from WordNet 3.0.\",\n  \"Synset Embedding Initialization Methods\": \"For the association component of our model, we require embedding representations for WordNet synsets. We explore two methods of transforming pre-trained word embeddings into synset embeddings: Averaging and AutoExtend retrofitting + Mimick. Averaging involves collecting the words representing the synset as surface form within the WordNet dataset and averaging their embeddings. AutoExtend is a method developed specifically for embedding WordNet synsets, in which pre-trained word embeddings are retrofitted to the tripartite relation graph connecting wordforms, lemmas, and synsets.\",\n  \"Analysis of M3GM's Learned Features and Their Impact\": \"Table 3 presents a sample of top-weighted motifs. Lines 1 and 2 demonstrate that the model prefers a broad scattering of targets for the member meronym and has part relations, which are flat and top-downwards hierarchical, respectively, while line 4 shows that a multitude of unique hypernyms is undesired, as expected from a bottom-upwards hierarchical relation. Lines 3, 6, and 7 hint at deeper interactions between the different relation types. Line 3 shows that the model assigns positive weights to hypernyms which have derivationally-related forms, suggesting that the derivational equivalence classes in the graph tend to exist in the higher, more abstract levels of the hypernym hierarchy.\",\n  \"Future Directions and Applications of M3GM\": \"From a broader perspective, M3GM can do more as a probabilistic model than predict individual edges. For example, consider the problem of linking a new entity into a semantic graph, given only the vector embedding. This task involves adding multiple edges simultaneously, while maintaining structural coherence. Our model is capable of scoring bundles of new edges, and in future work, we plan to explore the possibility of combining M3GM with a search algorithm, to automatically extend existing knowledge graphs by linking in one or more new entities. We also plan to explore multilingual applications.\"\n}\n```",
    "```json\n{\n  \"Introduction to Complaints in Computational Linguistics\": \"Complaining is a basic speech act used to express a negative mismatch between reality and expectations towards a state of affairs, product, organization or event. Understanding the expression of complaints in natural language and automatically identifying them is of utmost importance for linguists, psychologists, organizations, and developing downstream NLP applications. However, complaining has yet to be studied using computational approaches.\",\n  \"Definition and Importance of Complaints\": \"The speech act of complaining, as previously defined in linguistics research, has as its core the concept of violated or breached expectations, i.e., the person posting the complaint had their favorable expectations breached by a party, usually the one to which the complaint is addressed. Complaints have been previously analyzed by linguists as distinctly different from expressing negative sentiment towards an entity, with the key being the expression of the breach of expectations.\",\n  \"Related Work in Linguistics and Marketing\": \"Complaints have to date received significant attention in linguistics and marketing research. Olshtain and Weinbach (1987) provide one of the early definitions of a complaint as when a speaker expects a favorable event to occur or an unfavorable event to be prevented and these expectations are breached. Previous linguistics research has qualitatively examined the types of complaints elicited via discourse completion tests and in naturally occurring speech. In the area of linguistic studies on computer mediated communication, V\u00e1squez (2011) performed an analysis of 100 negative reviews on TripAdvisor, which showed that complaints in this medium often co-occur with other speech acts including positive and negative remarks, frequently make explicit references to expectations not being met and directly demand a reparation or compensation.\",\n  \"Data Collection and Annotation\": \"To date, there is no available data set with annotated complaints as previously defined in linguistics. Thus, we create a new data set of written utterances annotated with whether they express a complaint. We use Twitter as the data source because it represents a platform with high levels of self-expression and users directly interact with other users or corporate brand accounts. We manually assembled a list of 93 customer service handles and collected all the tweets that are available to download (the most recent 3,200). We then identified all the original tweets to which the customer support handle responded and randomly sampled an equal number of tweets addressed to each customer support handle for annotation.\",\n  \"Feature Analysis for Complaint Identification\": \"In our analysis and predictive experiments, we use the following groups of features: generic linguistic features proven to perform well in text classification tasks (unigrams, LIWC, word clusters), methods for predicting complaints, and complaint-specific features. We perform analysis of all previously described feature sets using univariate Pearson correlation. Negations, issues, pronouns, punctuation, temporal references, verbs, and topics are analyzed to understand the linguistic markers specific of complaints.\",\n  \"Predictive Models for Complaint Detection\": \"In this section, we experiment with different approaches to build predictive models of complaints from text content alone. We first experiment with feature-based approaches including Logistic Regression classification with Elastic Net regularization. Neural methods such as multi-layer perceptron (MLP) and Long-Short Term Memory (LSTM) networks are also explored. Results show that bag-of-word features reach an F1 of up to 77.5 and AUC of 0.866, while combining all features boosts predictive accuracy to 78 F1 and 0.864 AUC.\",\n  \"Distant Supervision and Domain Adaptation\": \"We explore the idea of identifying extra complaint data using distant supervision to further boost predictive performance. Following the same procedure, seven hashtags were identified with the help of the training data to likely correspond to complaints. We experiment with two techniques for combining distantly supervised data with our annotated data: Pooling and EasyAdapt domain adaptation. Results show that the domain adaptation approach further boosts F1 by 1 point to 79 and ROC AUC by 0.012.\",\n  \"Domain-Specific Performance Analysis\": \"We assess the performance of models trained using the best method and features by using in training: (1) using only in-domain data (In-Domain); (2) adding out-of-domain data into the training set (Pooling); and (3) combining in-and out-of-domain data with EasyAdapt domain adaptation (EasyAdapt). Results show that, in all but one case, adding out-of-domain data helps predictive performance. Domain adaptation is beneficial the majority of domains, lowering performance on a single domain compared to data pooling.\",\n  \"Cross-Domain Experiments and Results\": \"Finally, we present the results of models trained on tweets from one domain and tested on all tweets from other domains, with additional models trained on tweets from all domains except the one that the model is tested on. We observe that predictive performance is relatively consistent across all domains with two exceptions ('Food & Beverage' consistently shows lower performance, while 'Other' achieves higher performance) when using all the data available from the other domains.\",\n  \"Conclusions and Future Research Directions\": \"We presented the first computational approach using methods from computational linguistics and machine learning to modeling complaints as defined in prior studies in linguistics and pragmatics. We introduced the first data set consisting of English Twitter posts annotated with complaints across nine domains. We analyzed the syntactic patterns and linguistic markers specific of complaints and built predictive models of complaints in tweets using a wide range of features reaching up to 79% Macro F1 (0.885 AUC). In the future, we plan to identify the target of the complaint, use additional context and conversational structure to improve performance, and study the role of complaints in personal conversation or in the political domain.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Parsing and CNLVR Dataset\": \"The goal of semantic parsing is to map language utterances to executable programs. Recently, the Cornell Natural Language for Visual Reasoning corpus (CNLVR) was released, and has presented an opportunity to better investigate the problem of spuriousness. In this task, an image with boxes that contains objects of various shapes, colors and sizes is shown. Each image is paired with a complex natural language statement, and the goal is to determine whether the statement is true or false.\",\n  \"Challenges in Weakly-Supervised Semantic Parsing\": \"Training semantic parsers from denotations rather than programs complicates training in two ways: (a) Search: The algorithm must learn to search through the huge space of programs at training time, in order to find the correct program. (b) Spurious-ness: Incorrect programs can lead to correct denotations, and thus the learner can go astray based on these programs. Of the two mentioned problems, spuriousness has attracted relatively less attention.\",\n  \"Abstract Representation for CNLVR\": \"Our core insight is that in closed worlds with clear semantic types, like spatial and visual reasoning, we can manually construct a small lexicon that clusters language tokens and program constants, and create a partially abstract representation for utterances and programs in which the lexical problem is substantially reduced. This can also be done at search time, by augmenting the search state with partial programs that have been shown to be useful in earlier iterations.\",\n  \"Model Architecture and Search Techniques\": \"We base our model on the semantic parser of Guu et al. (2017). In their work, they used an encoder-decoder architecture to define a distribution p \u03b8 (z | x). The utterance x is encoded using a bi-directional LSTM that creates a contextualized representation h i for every utterance token x i, and the decoder is a feed-forward network combined with an attention mechanism over the encoder outputs. To combat the challenge of searching through the large space of programs, we apply several techniques, including beam search and utilizing the semantic typing system to only construct programs that are syntactically valid.\",\n  \"Training from Weak Supervision\": \"To use weak supervision, we treat the program z as a latent variable that is approximately marginalized. We maximize p(y | x) given by: z\u2208Z p \u03b8 (z | x)p(y | z, k) = z\u2208Z p \u03b8 (z | x)R(z, k, y) \u2248 z\u2208B p \u03b8 (z | x)R(z, k, y) where Z is the space of all programs and B \u2282 Z are the programs found by beam search. This basic training method fails for CNLVR due to the difficulties of search and spuriousness. Thus, we turn to learning from abstract examples, which substantially reduce these problems.\",\n  \"Learning from Abstract Examples\": \"We define an abstract representation for utterances and logical forms that is suitable for spatial reasoning. We define seven abstract clusters that correspond to the main semantic types in our domain. Then, we associate each cluster with a small lexicon that contains language-program token pairs associated with this cluster. These mappings represent the canonical ways in which program constants are expressed in natural language. Abstract examples can be used to improve the process of training semantic parsers.\",\n  \"Experimental Evaluation and Results\": \"Our weakly-supervised semantic parser with re-ranking (W.+DISC) obtains 84.0 accuracy and 65.0 consistency on the public test set and 82.5 accuracy and 63.9 on the hidden one, improving accuracy by 14.7 points compared to state-of-the-art. The accuracy of the rule-based parser (RULE) is less than 2 points below MAXENT, showing that a semantic parsing approach is very suitable for this task.\",\n  \"Error Analysis and Insights\": \"Errors correspond to utterances that are more complex syntactically and semantically. In about half of the errors an object was described by two or more modifying clauses or nesting occurred. The model either ignored one of the conditions or applied composition operators wrongly. Other, less frequent reasons for failure were problems with cardinality interpretation, applying conditions to items rather than sets, and utterances with questionable phrasing.\",\n  \"Related Work and Future Directions\": \"Training semantic parsers from denotations has been one of the most popular training schemes for scaling semantic parsers since the beginning of the decade. In future work, we plan to extend this work and automatically learn such a lexicon, which can reduce manual effort and scale to larger domains where there is substantial variability on the language side.\"\n}\n```",
    "```json\n{\n  \"Overview of the BioASQ Challenge 2017\": \"The aim of this paper is to give an overview of the data issued during the BioASQ challenge in 2017 and to present the systems that participated in the challenge and evaluate their performance. The challenge comprised three tasks: large-scale semantic indexing (Task 5a), biomedical semantic QA (Task 5b), and funding information extraction (Task 5c).\",\n  \"Task 5a: Large-scale Semantic Indexing\": \"In Task 5a, the goal is to classify documents from the PubMed digital library into concepts of the MeSH hierarchy. New PubMed articles not yet annotated by MEDLINE indexers are used as test sets. The performance of each system is calculated using standard flat information retrieval measures and hierarchical ones. Participants had 21 hours to provide their answers for each test set.\",\n  \"Task 5b: Biomedical Semantic QA\": \"Task 5b aimed to provide a large-scale question answering challenge where systems had to cope with all stages of a question answering task for four types of biomedical questions: yes/no, factoid, list, and summary questions. The task was split into two phases and five independent batches, with participants receiving 100 questions and having 24 hours to submit their answers.\",\n  \"Task 5c: Funding Information Extraction\": \"Task 5c was introduced for the first time in 2017, focusing on extracting grant information from biomedical articles. The challenge involved extracting grant IDs and funding agencies from the full text of articles, with systems evaluated on the extraction of grant IDs, grant agencies, and full-grant extraction.\",\n  \"Systems and Approaches for Task 5a\": \"The 'Search system' and its variants used a UIMA-based text and data mining workflow with different search strategies. The 'MZ' systems applied Binary Relevance classification and Latent Dirichlet allocation models. The 'Sequencer' systems from the Technical University of Darmstadt used recurrent neural networks for sequence-to-sequence prediction. The 'DeepMeSH' systems used document-to-vector and tf-idf feature embeddings, while the 'AUTH' team improved their previous system with a new ensemble methodology based on label frequencies and multi-label stacking.\",\n  \"Systems and Approaches for Task 5b\": \"The 'Basic QA pipeline' approach used MetaMap for query expansion and the BM25 probabilistic model for matching questions with documents. The 'Olelo' system used various NLP components built on top of the SAP HANA database. The 'USTB' team combined different strategies to enrich query terms, while the 'fdu' systems used a language model for document retrieval and keyword scoring for snippet extraction. The 'UNCC' team focused on the retrieval of relevant concepts and articles using the Stanford Parser.\",\n  \"Systems and Approaches for Task 5c\": \"The Fudan University team used a general approach including parsing articles, classifying sentences as positive or not, and scanning for grant IDs and agencies using regular expressions and hand-made rules. The 'DZG' systems used a linear SVM with tf-idf vectors of bigrams and a variety of sequential learning models. The AUTH team emphasized specific sections of the text and extracted grant IDs and agencies using regular expressions.\",\n  \"Evaluation and Results of Task 5a\": \"Each of the three batches of task 5a was evaluated independently using flat and hierarchical evaluation measures. The DeepMeSH systems and the AUTH systems outperformed the strong baselines, indicating the importance of methodologies such as d2v and tf-idf transformations for semantic indexing.\",\n  \"Evaluation and Results of Task 5b\": \"For phase A, systems were ranked according to the Mean Average Precision (MAP) measure for document, concept, snippet, and RDF triple annotations. In phase B, systems were ranked according to accuracy for yes/no questions, mean reciprocal rank (MRR) for factoids, and mean F-measure for list questions. The results showed high scores in yes/no questions but indicated room for improvement in factoid and list questions.\",\n  \"Evaluation and Results of Task 5c\": \"The evaluation of Task 5c used micro-recall as the evaluation measure for grant IDs, agencies, and full-grant extraction. The overall performance of the participants was very good, outperforming the baseline system, indicating that current techniques can be improved through the use of multiple methodologies.\"\n}\n```",
    "```json\n{\n  \"Introduction to Incremental Domain Adaptation in Machine Translation\": \"Incremental domain adaptation, or online adaptation, has been shown to improve statistical machine translation and especially neural machine translation (NMT) systems significantly. The natural use case is a computer-aided translation (CAT) scenario, where a user and a machine translation system collaborate to translate a document. Each user translation is immediately used as a new training example to adapt the machine translation system to the specific document.\",\n  \"Motivation for Measuring Immediate Adaptation in Interactive Translation\": \"For interactive, adaptive machine translation systems, perceived adaptation performance is a crucial property: An error in the machine translation output which needs to be corrected multiple times can cause frustration, and thus may compromise acceptance of the MT system by human users. A class of errors that are particularly salient are lexical choice errors for domain-specific lexical items. Users of adaptive MT have a reasonable expectation that in-domain vocabulary will be translated correctly after the translation of a term or some related term has been corrected manually.\",\n  \"Proposed Metrics for Measuring Zero-Shot and One-Shot Vocabulary Acquisition\": \"We propose three metrics: one to directly measure one-shot vocabulary acquisition, one to measure zero-shot vocabulary acquisition, and one to measure both. In all three, we measure the recall of target-language content words so that the metrics can be computed automatically by comparing translation hypotheses to reference translations without the use of models or word alignments. We define content words as those words that are not included in a fixed stopword list, as used for example in query simplification for information retrieval.\",\n  \"Evaluation of Different Adaptation Techniques on Online Adaptation Datasets\": \"We adapt an English\u2192German NMT system based on the Transformer architecture trained with an in-house NMT framework on about 100M bilingual sentence pairs. The model has six layers in the encoder, three layers in the decoder, each with eight attention heads with dimensionality 256, distinct input and output embeddings, and vocabulary sizes of around 40,000. The vocabularies are generated with byte-pair encoding. For adaptation we use a learning rate \u03b3 of 10\u22122 (for the bias adaptation a learning rate of 1.0 is used), no dropout, and no label-smoothing.\",\n  \"Analysis of Adaptation Techniques' Impact on Zero-Shot and One-Shot Recall\": \"The lasso technique is the only one that shows an improvement in R0 over the baseline. However, lasso has considerably lower one-shot recall compared to the other adaptation methods, implying that it often must observe a translated term more than once to acquire it. Further results using the raw subword output of the MT systems are depicted in Table 4: R0 for the lasso method is degraded only slightly below the baseline (-1%, compared to +2% for the regular metric), the findings for R1 and R0+1 remain the same as observed before.\",\n  \"Comparison of Traditional MT Metrics with Proposed Metrics\": \"The proposed metrics R0, R1, and R0+1 are useful for measuring immediate adaptation performance, which is crucial in adaptive CAT systems. The same pattern is observed in almost all cases: lasso outperforms the other adaptation techniques in zero-shot recall (R0) and combined recall (R0+1), while full has the highest one-shot recall (R1) on two out of five test sets, being close runner-up to lasso on all others. Overall however, we observe that zero-shot recall R0 is degraded by adaptation, while one-shot recall is improved.\",\n  \"Behavior of Adaptation Methods Over Time\": \"In Figure 2, we plot the difference in cumulative scores of two adapted systems (full and lasso) to the baseline for the proposed metrics as well as the BLEU score. As evident from comparing the curves for BLEU and R0, the BLEU score and the proposed metric give disparate signals for this data. Specifically, there are two distinct dips in the curves for R0 (as well as R0+1) and BLEU: 1. The degradation in R0 around segment 800 is due to significant noise in segment 774, which has a strong impact on the adapted systems, while the baseline system is not affected. 2. The dip in the BLEU score at segment 752, observable for both adapted systems, depicting a relative degradation of about 10%, is due to a pathological repetition of a single character in the output of the adapted MT systems for this segment, which has a large impact on the score.\"\n}\n```"
]