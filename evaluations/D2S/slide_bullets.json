[
    "Slide Topic: Background Semantic Hashing\nSummary:\nFast and accurate similarity search (i.e., finding documents from a large corpus that are most similar to a query of interest) is at the core of many information retrieval applications;\nOne strategy is to represent each document as a continuous vector: such as Paragraph\nCosine similarity is typically employed to measure relatedness;\nSemantic hashing is an effective approach: the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing (binary) codes;\n\nSlide Topic: Motivation and contributions\nSummary:\nExisting semantic hashing approaches typically require two-stage training procedures (e.g. continuous representations are crudely binarized after training);\nVast amount of unlabeled data is not fully leveraged for learning binary document representations.\nwe propose a simple and generic neural architecture for text hashing that learns binary latent codes for documents, which be trained an end-to-end manner;\nWe leverage a Neural Variational Inference (NVI) framework, which introduces data-dependent noises during training and makes effective use of unlabeled information.\n\nSlide Topic: Framework components Hashing under the NVI Framework\nSummary:\nNotations: let x and z denote the input document and its corresponding binary hash code, respectively;\nWe define a generative model that simultaneously accounts for both the encoding distribution, p(z |x), and decoding distribution, p(x |z),\nx <latexit sha1_base64=\"wrYRrS9nqr2/jTKdHNfdRLtLB0k=\">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUG9FLx5bMLbQhrLZTtq1m03Y3Ygl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZekAiujet+O4WV1bX1jeJmaWt7Z3evvH9wr+NUMfRYLGLVDqhGwSV6hhuB7UQhjQKBrWB0M/Vbj6g0j+WdGSfoR3QgecgZNVZqPvXKFbfqzkCWSS0nFcjR6JW/uv2YpRFKwwTVulNzE+NnVBnOBE5K3VRjQtmIDrBjqaQRaj+bHTohJ1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1ISXfsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2ZRsCLXFl5eJd1a9qrrN80r9Ok+jCEdwDKdQgwuowy00wAMGCM/wCm/Og/PivDsf89aCk88cwh84nz9UTYzP</latexit> log x\nWe define approximations q(z |x) and q(x |z) via inference and generative networks, parameterized by and , respectively.\n\nSlide Topic: Framework components Training with Binary Latent Variables\nSummary:\nThe generative term provides a natural training objective for semantic hashing: with the decoder network modeling p(x |z), the key semantic information from x is naturally encapsulated.\nTo tailor the NVI framework for semantic hashing, we cast z as a binary latent variable and assume a multivariate Bernoulli prior on z\nThe encoding (approximate posterior) distribution q(z |x) is restricted to take the form q(z |x) = Bernoulli(h), where h is inferred from x with the encoder network.\nWe can obtain samples from the Bernoulli posterior either deterministically or stochastically:\nSuppose z is a l-bit hash code, the deterministic binarization is defined as (for i l):\nstochastic binarization (where i Uniform(0,\nTo estimate the parameters of the encoder and decoder networks, we maximize a variational lower bound:\nThe KL-divergence DKL(q(z |x)||p(z)) encourages the approximate posterior q(z |x) to be close to the multivariate Bernoulli prior p(z);\nIt is challenging to backpropagate gradients through the discrete\n(binary) latent variable, since the derivative of the sign function is zero for almost all input values;\nInstead, we utilize the straight-through (ST) estimator, which was irst f introduced by [Hinton (2012)]. It simply backpropagates through the hard threshold by approximating the gradient z/(g i(x)) as 1:\n\nSlide Topic: Framework components Injecting Data dependent Noise to z\nSummary:\nWe found that injecting random\nGaussian noise into z makes the decoder a more favorable regularizer for the binary codes;\nx <latexit sha1_base64=\"wrYRrS9nqr2/jTKdHNfdRLtLB0k=\">AAAB53icbVBNS8NAEJ3Ur1q/qh69LBbBU0lFUG9FLx5bMLbQhrLZTtq1m03Y3Ygl9Bd48aDi1b/kzX/jts1BWx8MPN6bYWZekAiujet+O4WV1bX1jeJmaWt7Z3evvH9wr+NUMfRYLGLVDqhGwSV6hhuB7UQhjQKBrWB0M/Vbj6g0j+WdGSfoR3QgecgZNVZqPvXKFbfqzkCWSS0nFcjR6JW/uv2YpRFKwwTVulNzE+NnVBnOBE5K3VRjQtmIDrBjqaQRaj+bHTohJ1bpkzBWtqQhM/X3REYjrcdRYDsjaoZ60ZuK/3md1ISXfsZlkhqUbL4oTAUxMZl+TfpcITNibAllittbCRtSRZmx2ZRsCLXFl5eJd1a9qrrN80r9Ok+jCEdwDKdQgwuowy00wAMGCM/wCm/Og/PivDsf89aCk88cwh84nz9UTYzP</latexit> log x\nThe objective function in (4) can be written in a form similar to the rate-distortion tradeoff:\n\nSlide Topic: Framework components Extension to Supervised Hashing\nSummary:\nWhile labeled data are available, we can explicitly learn a mapping from latent variable z to labels y , here parametrized by a two-layer\nMLP followed by a fully-connected softmax layer.\nAs a result, the loss function is a combination of variational lower bound and discriminative (cross-entropy) loss:\n\nSlide Topic: Experiments Datasets and Experimental Setup\nSummary:\nDatasets: we evaluate the proposed method on three benchmarks:\nReuters21578, 20Newsgroups, TMC (SIAM text mining competition);\nTFIDF features are utilized as the input x for documents;\nwe set the dimension of z i.e., the number of bits within the hashing\nWe employed precision as the evaluation metric: the percentage of documents among the top 100 retrieved ones that belong to the same label (topic) with the query document.\n\nSlide Topic: Experiments Semantic Hashing Evaluation\nSummary:\nTable: Precision of the top 100 retrieved documents on Reuters dataset (Unsupervised hashing).\nConsistently outperform several strong baseline methods;\nEnjoy the attractive property of end-to-end training;\nSame observations on other benchmarks.\n\nSlide Topic: Experiments Ablation study\nSummary:\nFigure: The precisions of the top 100 retrieved Table: Ablation study with different documents for NASH-DN with stochastic or encoder/decoder networks. deterministic binary latent variables.\nLeveraging stochastically sampling during training generalizes better;\nLinear decoder networks gives rise to better empirical results.\n\nSlide Topic: Experiments Qualitative Analysis\nSummary:\nFigure: Examples of learned compact hashing codes on 20Newsgroups dataset.\nNASH typically compresses documents with shared topics into very similar binary codes.\n\nSlide Topic: Conclusions Take away\nSummary:\nThis paper presents a first step towards end-to-end semantic hashing;\nA neural variational framework is introduced to optimize the hash function during training;\nThe connections between the proposed method and rate-distortion theory are established.\n\n",
    "Slide Topic: Abstract\nSummary:\nEmotions, a complex state of feeling results in physical and psychological changes that influence human behavior. Thus, in order to extract the emotional key phrases from psychological texts, here, we have presented a phrase level emotion identification and classification system. The system takes pre- defined emotional statements of seven basic emotion classes\n(anger, disgust, fear, guilt, joy, sadness and shame) as input and extracts seven types of emotional trigrams. The trigrams were represented as Context Vectors. Between a pair of\nContext Vectors, an Affinity Score was calculated based on the law of gravitation with respect to different distance metrics\n(e.g., Chebyshev, Euclidean and Hamming).\n\nSlide Topic: Introduction\nSummary:\nEmotions, a complex state of feeling results in physical and psychological changes that influence human behavior.\nHuman emotions are the most complex and unique features to be described. If we ask someone regarding emotion, he or she will reply simply that it is a 'feeling'.\nPsychological texts contain huge number of emotional words because psychology and emotions are inter-wined, though they are different.\n\nSlide Topic: Corpus Preparation\nSummary:\nThe emotional statements were collected from the ISEAR\n(International Survey on Emotion Antecedents and Reactions) database\nIt is found that only 1096 statements belong to anger, disgust sadness and shame classes whereas the fear, guilt and joy\n\nSlide Topic: Corpus Preparation contd\nSummary:\nEach statement may contain multiple sentences, so after sentence tokenization, it is observed that the anger and fear classes contain the maximum number of sentences.\nIt is observed that the anger class contains the maximum number of tokenized words.\n\nSlide Topic: Context Windows\nSummary:\nThe tokenized words were grouped to form trigrams in order to grasp the roles of the previous and next tokens with respect to the target token.\n(CW) to acquire the emotional phrases.\n\nSlide Topic: Context Windows contd\nSummary:\nIt is considered that, in each of the Context Windows, the first word appears as a non-affect word, second word as an affect word, and third word as a non-affect word (<NAW1>, <AW>,\nA few example patterns of the CWs which follows the pattern\nand, sorry, just (Shame)\n\nSlide Topic: Similar and Dissimilar NAWs\nSummary:\nIt was observed that the stop words are mostly present in\n<NAW1, AW, NAW2> pattern where similar and dissimilar\nNAWs are appeared before and after their corresponding CWs.\n\nSlide Topic: Similar and Dissimilar NAWs contd\nSummary:\nNAW1= Non Affect Word1; AW=Affect Word; NAW2=Non Affect Word2\n\nSlide Topic: Context Vector Formation\nSummary:\nIn order to identify whether the Context Windows (CWs) play any significant role in classifying emotions or not, we have mapped the Context Windows in a Vector space by representing them as vectors.\n\nSlide Topic: Context Vector Formation contd\nSummary:\nT= Total count of CW in an emotion class\n#NAW1 = Total occurrence of a non affect word in NAW1 position\n\nSlide Topic: Affinity Score Calculation\nSummary:\nAn Affinity Score was calculated for each pair of\nrespect to each of the emotion classes.\n\nSlide Topic: Affinity Score Calculation contd\nSummary:\nThe final Score is calculated using the following gravitational formula as described in (Poria et al.,\nScore p q dist p , q\nThe Score of any two context vectors p and q of an emotion class is the dot product of the vectors divided by the square of distance (dist) between p and q. This score was inspired by\nNewtons law of gravitation. This score values reflect the affinity between two context vectors p and q. Higher score implies higher affinity between p and q.\n\nSlide Topic: Affinity Scores using Distance Metrics\nSummary:\nIn the vector space, it is needed to calculate how close the context vectors are in the space in order to conduct better classification into their respective emotion classes. The Score values were calculated for all the emotion classes with respect to different metrics of distance (dist) viz. Chebyshev,\n\nSlide Topic: Distance Metrics\nSummary:\nChebyshev distance (Cd) = max |xi yi | where xi and yi represents two vectors.\nEuclidean distance (Ed) = ||x y||2 for vectors x and y.\nHamming distance (Hd) = (c01 c10) / n where cij is the number of occurrence in the boolean vectors x and y and x[k] = i and y[k] = j for k < n. Hamming distance denotes the proportion of disagreeing components in x and y.\n\nSlide Topic: POS Tagged Context Windows and POS Tagged Windows\nSummary:\nThe sentences were POS tagged using the Stanford POS\nTagger and the POS tagged Context Windows were extracted and termed as PTCW. Similarly, the POS tag sequence from each of the PTCWs were extracted and named each as POS\n\nSlide Topic: Count of CW PTCW PTW\nSummary:\nFigurel:Count of CW,PTCW and PTW\n= No of POS tagged Context Window(CW)\n= No of Unique POS tagged Context Window(CW)\n= No of Unique PTW\nAnger Disgust Fear Guilt Joy Sadness Shame Emotions\n\nSlide Topic: Total Count of CW PTCW PTW\nSummary:\nFigure 2:Total Count of CW,PTCW and PTW\nTotal CW Totla PTCW Total PTW Different windows\n\nSlide Topic: TF and TF IDF Measure\nSummary:\nThe Term Frequencies (TFs) and the Inverse Document\nFrequencies (IDFs) of the CWs for each of the emotion classes were calculated. In order to identify different ranges of the TF and TF-IDF scores, the minimum and maximum values of the\nTF and the variance of TF were calculated for each of the\n\nSlide Topic: Ranking Score of CW\nSummary:\nA ranking score was calculated for each of the context windows. Each of the words in a context window was searched in the SentiWordNet lexicon and if found, we considered either positive or negative or both scores. The summation of the absolute scores of all the words in a Context\nWindow is returned. The returned scores were sorted so that, in windows obtains a emotion class. turn, rank each of the context in its corresponding\nAll the ranks were calculated for each emotion class, successively. Examples from the list of top 12 important context windows according to their rank are much anger when (anger), whom love after (happy), felt sad about (sadness) etc.\n\nSlide Topic: Result Analysis\nSummary:\nWhen Euclidean distance is considered\nClassifiers Test Data 10 fold cross validation\n\nSlide Topic: Result Analysis contd\nSummary:\nWhen Hamming distance is considered\nClassifiers Test Data 10 fold cross validation\nWhen Chebyshev distance is considered\n\nSlide Topic: Conclusion\nSummary:\nIn this paper, vector formation was done for each of the\nContext Windows; TF and TF-IDF measures were calculated.\nThe calculated affinity score, depending on the distance values was inspired from Newton's law of gravitation. To classify these CWs, BayesNet, J48, NaivebayesSimple and\nDecisionTable classifiers is used.\n\nSlide Topic: Future Work\nSummary:\nIn future, we would like to incorporate more number of lexicons to identify and classify emotional expressions.\nMoreover, we are planning to include associative learning process to identify some important rules for classification.\n\n",
    "Slide Topic: Background\nSummary:\nInformation Retrieval (IR) and Recommender Systems (RS) techniques\nhave been used to address:-\nLiterature Review (LR) search tasks\nExplicit and implicit ad-hoc information needs\nExamples of such tasks include\nBuilding a reading list of research papers\nRecommending papers based on query logs\nRecommending papers based on publication history\nSerendipitous discovery of interesting papers and more.\nWhat about recommending papers during manuscript preparation\n\nSlide Topic: Addressed scenarios in mp\nSummary:\nRecommending papers based on Citation Contexts in manuscripts\nRecommending new papers based on To-Be-Cited papers from the\nRecommending papers based on the full text of the draft\nWhat more could be done?\nExplore the total list of papers compiled during literature review\nExplore the article-type preference to vary recommendations correspondingly?\n\nSlide Topic: Enter rec4lrw\nSummary:\nRec4LRW is a task-based assistive system that offers\nrecommendations for the below tasks:-\nTask 1 Building an initial reading list of research papers\nTask 2 Finding similar papers based on a seed set of papers\nTask 3 Shortlisting papers from the final reading list based on\nThe system is based on a threefold intervention framework\nFor better meeting the task requirements\nNovel informational display features\nFor speeding up the relevance judgement decisions\nFor establishing the natural relationships between tasks\n\nSlide Topic: Rec4lrw usage sequence\nSummary:\nSelect papers from Task 2 to the final reading list\nN Execute Task 3 with the final reading list papers\n\nSlide Topic: Corpus\nSummary:\nACM DL extract of papers published between 1951 and 2011 used as\nAnyStyle (https://anystyle.io) parser used to extract article title, venue\nand year from references\nData stored in a MySQL database with the tables related using a\n\nSlide Topic: Task objective and steps\nSummary:\nOBJECTIVE: To identify the important papers from the final reading list\nand vary recommendations count based on article-type preference\nInput: P set of papers in the final reading list\nAT article-type choice of the user\n1: RC the average references count retrieved for AT\n2: R list of retrieved citations & references of papers from P\n3: G directed sparse graph created with papers from R\n4: run edge betweenness algorithm on G to form cluster set C 5: S final list of shortlisted papers 6: if |C| > RC then while |S = RC for each cluster in C do sort papers in the cluster on citation count s top ranked paper from the cluster add s to S end for end while 14: else N while |S = RC N N +1 for each cluster in C do sort papers in the cluster on citation count s N ranked paper from the cluster add s to S end for end while 24: end if 25: display papers from S to user\n\nSlide Topic: User evaluation study\nSummary:\nOBJECTIVE: To ascertain the usefulness and effectiveness\nof the task to researchers\nAscertain the agreement percentages of the evaluation\nRelevance The shortlisted papers are relevant to my article-type preference\nUsefulness The shortlisted papers are useful for inclusion in my manuscript\nImportance The shortlisted papers comprises of important papers from my reading list\nCertainty The shortlisted list comprises of papers which I would definitely cite in my manuscript Good_List This is a good recommendation list, at an overall level Improvement_Needed There is a need to further improve this shortlisted papers list\nShortlisting_Feature I would like to see the feature of shortlisting papers from reading list based on article-type preference, in academic search systems and databases\nIdentify the top preferred and critical aspects of the task\nthrough the subjective feedback of the participants\nFeedback responses were coded by a single coder using an inductive approach\n\nSlide Topic: Study information\nSummary:\nThe study was conducted between November 2015 and January 2016\nPre-screening survey conducted to identify participants who have authored at\nleast one journal or conference paper\n116 participants completed the whole study inclusive of the three tasks in the\n57 participants were Ph.D./Masters students while 59 were research staff,\nacademic staff and librarians\nThe average research experience for students was 2 years while for staff, it\n51% of participants were from the computer science, electrical and electronics disciplines, 35% from information and communication studies discipline while 14% from other disciplines\n\nSlide Topic: Study procedure\nSummary:\nStep Participant selects one of the available 43 topics for executing task 1\nStep Re-run task 1 and select at least five papers for the seed basket\nStep Execute task 2 with the seed basket papers\nStep Re-run task 2 (and task 1) to select at least 30 papers for the final\nStep 5: Execute task 3 with the final reading list papers and article-type\nFour article-type choices: conference full paper, poster, case study and a generic research paper\n\nSlide Topic: Results\nSummary:\nBiggest differences found for the below measures:-\nThe measures with the highest agreement:-\n\nSlide Topic: Qualitative feedback\nSummary:\nRank Preferred Aspects Categories Critical Aspects Categories\nShortlisting Feature & Rec. Quality (24%) Rote Selection of Papers (16%)\nInformation Cue Labels (15%) Limited Dataset Issue (5%)\nView Papers in Clusters (11%) Quality can be Improved (5%)\nRich Metadata (7%) Not Sure of the Usefulness of the Task (4%)\nRanking of Papers (3%) UI can be Improved (3%)\nThe newly introduced informational display features were a big hit\nThe purely experimental nature of the study affected the experience of\nTasks effectiveness needs to be validated with a longitudinal study with a large collection of papers in the final reading list\n\nSlide Topic: Limitations\nSummary:\nLack of an offline evaluation experiment\nStudy procedure involved selection of comparatively fewer number of papers\nin the final reading list\nNot much variations in the final shortlisted papers for the different article-type\nInformation displayed in a purely textual manner\n\nSlide Topic: Future work\nSummary:\nThe scope for this task will be expanded to bring in more variations for the\nInclusion of new papers in the output which could have been missed during\nProvide more user control in the system so that the user can select papers as\nmandatory to be shortlisted\nIntegrate this task with the citation context recommendation task\nRepresent the information in the form of citation graphs\n\n",
    "Slide Topic: Key point Syntactic Information\nSummary:\nTo use or not to use?\nstring-to-string model tree/graph-to-string model\n\nSlide Topic: Forest Linearization\nSummary:\nPacked forest is directed acyclic graph, not tree\nFixed traversal order does not exist.\nOutputs are not always optimal for MT.\nImportant information may be lost\n\nSlide Topic: English Chinese\nSummary:\ns2s is the worst\nMore syntactic information is useful Chinese\nNo score is the worst English-\nScore is useful Chinese\nSoA is better than SoE\nAdjusting attention is better than adjusting word embedding\nForest is better than 1-best English-\nForest (No score) is worse than 1-best (SoE/SoA)\nFS/TN is worse than 1-best (SoE/SoA) English-\nBetter to use score in linearization Chinese\n\nSlide Topic: English Japanese\nSummary:\ns2s is the worst\nNo score is the worst\nSoA is better than SoE\nForest is better than 1-best\nForest (No score) is worse\nFS/TN is worse than 1-best\n\nSlide Topic: Merits and Demerits\nSummary:\nUse syntactic information explicitly\nSimpler model, more information\nRobust to parsing errors\nLots of sentences are filtered out due to lengths\n\n",
    "Slide Topic: Adversarial Attacks Perturbations\nSummary:\nApply a small (indistinguishable) perturbation to the input that elicit large changes in the output\nFigure from Goodfellow et al. (2014)\n\nSlide Topic: Indistinguishable Perturbations\nSummary:\nSmall perturbations are well defined in vision\nSmall l2 ~= indistinguishable to the human eye\n\nSlide Topic: Not all Text Perturbations are Equal\nSummary:\nHes very annoying Hes pretty friendly Hes She friendly Hes very freindly\n[Different meaning] [Similar meaning] [Nonsensical] [Typo]\nCant expect the model to output the same output!\nWhy and How you should evaluate adversarial perturbations\n\nSlide Topic: Problem Definition\nSummary:\nReference They plow it right back into filing\nOriginal Ils le reinvestissent directement en engageant\nBase output They direct it directly by engaging\nA dv. src Ilss le reinvestissent dierctement en engagaent plus de proces. Adv. output .. de plus.\n\nSlide Topic: Source Side Evaluation\nSummary:\nEvaluate meaning preservation on the source side\nWhere is a similarity metric such that\nHes very friendly H es pretty friendly Hes very friendly H es very annoying\nHes very friendly H es pretty friendly Hes very friendly Hes She friendly\n\nSlide Topic: Target Side Evaluation\nSummary:\nEvaluate relative meaning destruction on the target side\n\nSlide Topic: Successful Adversarial Attacks\nSummary:\nSource meaning destruction Target meaning destruction\nDestroy the meaning on the target side more than on the source side\n\nSlide Topic: Which similarity metric to use\nSummary:\nHow would you rate the similarity between the meaning of these two sentences?\n6 point scale, details in paper\nThe meaning is completely different or one of the sentence s is meaningless\nThe topic is the same but the meaning is different\nSome key information is different\nThe key information is the same but the details differ\nMeaning is essentially the same but some expressions are unnatural Meaning is essentially equal and the two sentences are well-formed [Language]\nGeometric mean of n-gram precision + length penalty\nMETEOR [Banerjee and Lavie, 2005]\nWord matching taking into account stemming, synonyms, paraphrases...\nchrF [Popovic, 2015] Character n-gram F-score\n\nSlide Topic: Data and Models\nSummary:\n{Czech, German, French} English\nBoth word and sub-word based models\n\nSlide Topic: Gradient Based Adversarial Attacks on Text\nSummary:\nIdea: Back propagate through the model to score possible substitutions\nLe g ros c hien The big dog .\nThe big dog . <eos>\nIdea: Word substitution Adding word vector difference\nUse the 1st order approximation to maximize the loss\n\nSlide Topic: Constrained Adversarial Attacks kNN\nSummary:\nOnly replace words with 10 nearest neighbors in embedding space\nExample from our fren Transformer source embeddings\ngrand (tall SING+MASC) grands (tall PL+MASC) grande (tall SING+FEM) grandes (tall PL+FEM) gros (fat SING+MASC) grosse (fat SING+FEM) math (math) maths (maths) mathematique (mathematic) mathematiques (mathematics) objective (objective [ADJ] SING+FEM)\n\nSlide Topic: Constrained Adversarial Attacks CharSwap\nSummary:\nOnly swap word internal characters to get OOVs\nadversarial ad vresa rial\nIf thats impossible, repeat the last character\n\nSlide Topic: Choosing an Similarity Metric\nSummary:\nHuman vs automatic (pearson r):\nHumans score original/adversarial outpu t\nCompare scores to automatic metric with\n(Relative Decrease in chrF)\n\nSlide Topic: Effect of Constraints on Evaluation\nSummary:\na feet eae Unconstrained\n\nSlide Topic: Effect of Constraints on Adversarial Training\nSummary:\nAdversarial training training with adversarial examples\n= 0: Standard training\n= 1 : Training only on adversarial examples\nTraining with Unconstrained attacks vs CharSwap attacks\nRobustness to CharSwap attacks on the validation set\nAccuracy on non-adversarial data\nAdversarial training better robustness\nAccuracy on Non-Adversarial Input\nTarget chrF on the original test set\nUnconstrained attacks hurts accuracy\n\nSlide Topic: Takeway\nSummary:\nHow would you rate the similarity between the meaning of these two sentences?\nThe meaning is complete ly different or one of the sentence s is meaningless\nThe topic is the same but the meaning is different Some key information is different\nWhen doing adversarial attacks\nThe key information is th e same but the details differ Meaning is essentially the same but some expressions are unnatural Meaning is essentially eq ual and the two sentences are we ll-formed [Language]\nEvaluate meaning preservation on the source side\nWhen doing adversarial training\nConsider adding constraints to your attacks\nNot only true for seq2seq!\nEasily transposed to classification, etc..\nJust adapt and accordingly\n\nSlide Topic: Human Evaluation the Gold Standard\nSummary:\nCheck for semantic similarity and fluency\nHow would you rate the similarity between the meaning of these two sentences?\nThe meaning is completely different o r one of the sentences is meaningless\nThe topic is the same but the meaning is different\nSome key information is different\nThe key information is the same but the details differ\nMeaning is essentially the same but some expressions are unnatural\nMeaning is essentially equal and the two sentences are well-formed [Language]\n\nSlide Topic: Example of a Successful Attack\nSummary:\nOriginal Ils le reinvestissent directement en engageant plus de proces.\nAdv. src. Ilss le reinvestissent dierctement en engagaent plus de proces.\nRef. They plow it right back into filing more troll lawsuits.\nBase output They direct it directly by engaging more cases.\nAdv. output .. de plus.\n\nSlide Topic: Example of an Unsuccessful Attack\nSummary:\nOriginal Cetait en Juillet 1969.\nAdv. src. Cetiat en Jiullet\nBase output This was in July 1969.\n\n",
    "Slide Topic: Do we really need context\nSummary:\nIt has 48 columns.\nWhat does it refer to?\nPossible translations into Russian:\n48 . (masculine or neuter)\nWhat do columns mean?\nUnder the cathedral lies the antique chapel.\n\nSlide Topic: Recap antecedent and anaphora resolution\nSummary:\nUnder the cathedral lies the antique chapel. It has 48 columns.\nAn antecedent is an expression that gives its meaning to\na proform (pronoun, pro-verb, pro-adverb, etc.)\nAnaphora resolution is the problem of resolving references to earlier\nor later items in the discourse.\n\nSlide Topic: Context in Machine Translation\nSummary:\nfocused on handling specific phenomena\ndirectly provide context to an NMT system at training time\nwhat kinds of discourse phenomena are successfully handled\nhow they are modeled\n\nSlide Topic: Plan\nSummary:\nwe introduce a context-aware neural model, which is effective\nan d has a sufficiently simple and interpretable interface between Model Archit cture\nthe context and the rest of the translation model\nwe analyze the flow of information from the context and identify\nOverall performance pr onoun translation as the key phenomenon captured by the\nby comparing to automatically predicted or human-annotated Analys s\ncoreference relations, we observe that the model implicitly\n\nSlide Topic: Context aware model architecture\nSummary:\nstart with the Transformer [Vaswani et al, 2018]\nincorporate context information on the encoder side\nuse a separate encoder for context\nshare first N-1 layers of source and context encoders\nthe last layer incorporates contextual information\n\nSlide Topic: Overall performance\nSummary:\nDataset: OpenSubtitles2018 (Lison et al., 2018) for English and Russian\n\nSlide Topic: Overall performance models comparison\nSummary:\n(context is the previous sentence)\nconcatenation: modification of the\ncontext encoder (our approach by [Tiedemann and work) Scherrer,\n\nSlide Topic: Our model different types of context\nSummary:\nNext sentence does not appear\nprevious sentence Performance drops for a random\nModel is robust towards being\nshown a random context\n(the only significant at p<0.01 difference is with the best model;\ndifferences between other results are not significant)\n\nSlide Topic: Analysis\nSummary:\nwe introduce a context-aware neural model, which is effective\nand ha s a sufficiently simple and interpretable interface between Top words influenced by context\nthe context and the rest of the translation model\nwe an alyze the flow of information from the context and identify Non-lexical patterns affecting attention\nprono un translation as the key phenomenon captured by the to context\nby com paring to automatically predicted or human-annotated Latent anaphor resolution\ncoreference relations, we observe that the model implicitly\n\nSlide Topic: What do we mean by attention to context\nSummary:\nattention from source to context\nmean over heads of per-head attention\ntake sum over context words\n(excluding <bos>, <eos> and punctuation)\n\nSlide Topic: Top words influenced by context\nSummary:\nit Need to know gender, because\nyours verbs must agree in gender with I\n(in past tense) yes\nyes Many of these words appear at\ni sentence initial position.\nyou Maybe this is all that matters?\nword pos word pos\nOnly positions i after the first m\n\nSlide Topic: Dependence on sentence length\nSummary:\nhigh attention to context\n\nSlide Topic: Ambiguous pronouns and translation quality how to evaluate\nSummary:\nMetric: BLEU (standard metric for MT)\nfeed CoreNLP (Manning et al., 2014) with pairs of sentences\npick examples with a link between the pronoun and a noun group in a context\ngather a test set for each pronoun\nuse the test sets to evaluate the context-aware NMT system\n\nSlide Topic: Ambiguous it noun antecedent\nSummary:\nmasculine feminine neuter plural\n\nSlide Topic: It with noun antecedent example\nSummary:\nIt was locked up in the hold with 20 other boxes of supplies.\nPossible translations into Russian:\nYou left money unattended?\n\nSlide Topic: Hypothesis\nSummary:\nLarge improvements in BLEU on test sets with pronouns\nco-referent with an expression in context\nAttention mechanism Latent anaphora resolution\n\nSlide Topic: How to test the hypothesis agreement with CoreNLP\nSummary:\nFind an antecedent noun phrase (using CoreNLP)\nPick examples where the noun phrase contains a single noun\nPick examples with several nouns in context\nIdentify the token with the largest attention weight (excluding punctuation,\nIf the token falls within the antecedent span, then its an agreement\n\nSlide Topic: Does the model learn anaphora\nSummary:\nor just some simple heuristic?\n\nSlide Topic: Agreement with CoreNLP predictions\nSummary:\nrandom first last attention agreement of attention is the\nfirst noun is the best heuristic\n\nSlide Topic: Compared to human annotations for it\nSummary:\npick 500 examples from the\nask human annotators to mark\npick examples where an\nantecedent is a noun phrase\ncalculate the agreement with\n\nSlide Topic: Attention map examples\nSummary:\nThere was a time I would\nhave lost my heart to a\nAnd you, no doubt, would\n\nSlide Topic: Conclusions\nSummary:\nintroduce a context-aware NMT system based on the Transformer\nthe model outperforms both the context-agnostic baseline and a simple\ncontext-aware baseline (on an En-Ru corpus)\npronoun translation is the key phenomenon captured by the model\nthe model induces anaphora relations\n\n",
    "Slide Topic: Introduction\nSummary:\nState-of-the-art MT models still use a simplistic view of the data\nI words typically treated as independent, unrelated units\nI relations between words only captured through linear context\nUnified semantic representations, such as Abstract Meaning\nRepresentation (AMR, Banarescu et al. 2013), (re)gaining popularity\nAbstraction from surface words, semantic relations made explicit, related words brought together (possibly distant in the surface realization)\nI Richer models of source context our work\nI Target-side (or joint) models to capture semantic coherence\nI Semantic transfer followed by target-side generation\n\nSlide Topic: Semantic Representation\nSummary:\nLogical Form transformed into an AMR-style representation\nLabeled directed graph, not necessarily acyclic (e.g. coreference)\nNodes content words, edges semantic relations\nFunction words (mostly) not represented as nodes\nBits capture various linguistic properties\nFigure 1 : Logical Form (computed tree) for the sentence: I would like to give you a sandwich taken from the fridge.\n\nSlide Topic: Graph to String Translation\nSummary:\nTranslation = generation of target-side surface words in order, conditioned on source semantic nodes and previously generated words.\nStart in the (virtual) root\nAt each step, transition to a semantic node and emit a target word\nA single node can be visited multiple times\nOne transition can move anywhere in the LF\n\nSlide Topic: Translation Example\nSummary:\nFigure 2 : An example of the translation process illustrating several first steps of translating the sentence into German (Ich mochte dir einen Sandwich...).\nLabels in italics correspond to the shortest undirected paths between the nodes.\n\nSlide Topic: Alignment of Graph Nodes\nSummary:\nHow do we align source-side semantic nodes to target-side words?\n\nSlide Topic: Alignment of Graph Nodes Gibbs Sampling\nSummary:\nAlignment ( transition) distribution P(ai ) modeled as a categorical distribution:\nTranslation ( emission) distribution modeled as a set of categorical distributions, one for each source semantic node:\nP(ei |nai c(lemma(nai ei\nSample from the following distribution:\n\nSlide Topic: Alignment of Graph Nodes Evaluation\nSummary:\nI Linearize the LF, run GIZA++ (standard word alignment)\nI Heuristic linearization, try to preserve source surface word order\nI Source-side nodes to source-side tokens\nI Source-target word alignment GIZA++\nManual inspection of alignments\nAlignment composition clearly superior\nNot much difference between GIZA++ and parser alignments\n\nSlide Topic: Discriminative Translation Model\nSummary:\nPossible classes: top 50 translations observed with given lemma\nOnline learning with stochastic gradient descent\nLearning rate 0.05, cumulative L1 regularization with weight 1, batch size 1, 22 hash bits\nEarly stopping when held-out perplexity increases\nParallelized (multi-threading) and distributed learning for tractability\n\nSlide Topic: Feature Set\nSummary:\nIch mochte dir einen Sandwich\nCurrent node, previous node, parent node lemma, POS, bits\nPath from previous node path length, path description\nBag of lemmas capture overall topic of the sentence\nGraph context features from nodes close in the graph (limited by the length of shortest undirected path)\nGenerated tokens fertility; some nodes should generate a function word first (e.g. an article) and then the content word\nPrevious tokens target-side context July 30, 2015\n\nSlide Topic: Experiments\nSummary:\nEvaluated in a n-best re-ranking experiment\nI Generate 1000-best translations of devset sentences\nI Add scores from our model\nI Re-run MERT on the enriched n-best lists\nBasic phrase-based system, FrenchEnglish\n1 million parallel training sentences\nObtained small but consistent improvements\nDifferences would most likely be larger after integration in decoding\nTable 1 : BLEU scores of n-best reranking in FrenchEnglish translation.\n\nSlide Topic: Conclusion\nSummary:\nInitial attempt at including semantic features in statistical MT\nFeature set comprising morphological, syntactic and semantic properties\nSmall but consistent improvement of BLEU\nIntegrate directly in the decoder\nParser accuracy limited use multiple analyses\nExplore other ways of integration\nI Target-side models of semantic plausibility\nI Semantic transfer and generation\n\n",
    "Slide Topic: Learning under Domain Shift\nSummary:\nState-of-the-art domain adaptation approaches\nevaluate on proprietary datasets or on a single benchmark\nOnly compare against weak baselines\nAlmost none evaluate against approaches from the extensive semi-supervised learning (SSL) literature\n\nSlide Topic: Revisiting Semi Supervised Learning\nSummary:\nClassics in a Neural World\nHow do classics in SSL compare to recent advances?\nCan we combine the best of both worlds?\nHow well do these approaches work on out-of-distribution data?\n\nSlide Topic: Self training\nSummary:\n1. Train model on labeled data.\n2. Use confident predictions on unlabeled data as training examples. Repeat.\n- Er ror a mpli\n\nSlide Topic: Self training variants\nSummary:\nOutput probabilities in neural networks are poorly calibrated.\nThrottling (Abney, 2007), i.e. selecting the top n highest confidence unlabeled examples works best.\nTraining until convergence on labeled data and then on unlabeled data works best.\n\nSlide Topic: Tri training\nSummary:\n1. Train three models on bootstrapped samples.\n2. Use predictions on unlabeled data for third if two agree.\nFinal prediction: majority voting\n\nSlide Topic: Tri training with disagreement\nSummary:\n1. Train three models on bootstrapped samples.\n2. Use predictions on unlabeled data for third if two agree and prediction differs.\ndependen t mo dels\n\nSlide Topic: Tri training hyper parameters\nSummary:\nProducing predictions for all unlabeled examples is expensive\nSample number of unlabeled examples\nNot effective for classic approaches, but essential for our method\n\nSlide Topic: Multi task Tri training\nSummary:\n1. Train one model with 3 objective functions.\n2. Use predictions on unlabeled data for third if two agree.\nRestrict final layers to use different representations.\nTrain third objective function only on pseudo labeled to bridge domain shift.\nm2 F orthogonality constraint (Bousmalis et al., 2016)\nLoss: L() = log Pmi(y h Lorth\n\nSlide Topic: Data and Tasks\nSummary:\nSentiment analysis on Amazon reviews dataset (Blitzer et al, 2006)\nPOS tagging on SANCL 2012 dataset (Petrov and McDonald, 2012)\n\nSlide Topic: Sentiment Analysis Results\nSummary:\nAvg over 4 target domains\nVFAE* Self-training DANN* Tri-training Asym* Tri-training-Disagr. Source only MT-Tri * result from Saito et al., (2017) Multi-task tri-training slightly outperforms tri-training, but has higher variance.\n\nSlide Topic: POS Tagging Results\nSummary:\nTrained on 10% labeled data (WSJ)\nAvg over 5 target domains\nSource (+embeds) Tri-training-Disagr. Self-training MT-Tri Tri-training\nTri-training with disagreement works best with little data.\nTrained on full labeled data (WSJ)\nTnT Tri-training Stanford* Tri-training-Disagr. Source (+embeds) MT-Tri * result from Schnabel & Schutze (2014)\nTri-training works best in the full data setting.\n\nSlide Topic: POS Tagging Analysis\nSummary:\nAccuracy on out-of-vocabulary (OOV) tokens\nAccuracy on OOV tokens\nAnswers Emails Newsgroups Reviews Weblogs UWT rate Src Tri MT-Tri FLORS*\nOOV tokens Src Tri MT-Tri\nClassic tri-training works best on OOV tokens.\nMT-Tri does worse than source-only baseline on OOV.\nPOS accuracy per binned log frequency\nAccuracy delta vs. src-only baseline\nTri-training works best on low-frequency tokens (leftmost\nAccuracy on unknown word-tag (UWT) tokens\n% UWT tokens 2\nAccuracy on UWT tokens\nNo bootstrapping method works well on unknown word- tag combinations. * result from Schnabel & Schutze (2014)\nicult ca very dif f ses\nLess lexicalized FLORS approach is superior.\n\nSlide Topic: Takeaways\nSummary:\nClassic tri-training works best: outperforms recent state-of-the-art methods for sentiment analysis.\nWe address the drawback of tri-training (space & time complexity) via the proposed MT-Tri model\nMT-Tri works best on sentiment, but not for POS.\nComparing neural methods to classics (strong baselines)\nEvaluation on multiple tasks domains\n\n",
    "Slide Topic: Contributions\nSummary:\nQuestion Answering (Q&A) and Spoken Language Understanding (SLU) under the same parsing framework:\nPublic Q&A corpora (English)\nProprietary Alexa SLU corpus (English)\nTransfer learning to learn parsers on low-resource domains, for both Q&A and SLU:\n\nSlide Topic: SLU Alexa Data\nSummary:\nAlexa data is annotated for intent/slot tagging:\nWhich cinemas screen Star|Title Wars|Title tonight|Time\nWhich we converted into trees:\nTree parsing allows to make more complex requests:\nAdd apples and oranges to shopping list and play music\nDOMAIN SIZE TER NT WORDS closet bookings cinema recipes search\n\nSlide Topic: Q and A Data\nSummary:\nQuestions annotated with Lambda DCS (Liang, 2013);\nDivided in 8 domains;\nKobe Bryant num blocks\nHow many blocks were made by Kobe Bryant?\nQuestions about geographical facts;\nname Edinburgh amenity prison\nHow many prisons does Edinburgh count?\nDATASET DOMAIN SIZE TER NT Words publications\nOvernight calendar housing recipes restaurants basketball blocks social\nOvernight publications calendar housing recipes restaurants basketball blocks social\n\nSlide Topic: Parser\nSummary:\nWhich cinemas screen Star Wars tonight?\nTime Title Title Time\ntonight Title Title Time\nTransition-based parser of Cheng et al. (2017) + character-level embeddings and copy mechanism:\nt0 tn x0 xn nt0 ntn\n\nSlide Topic: Results\nSummary:\nDATA TASK DOMAIN ACCURACY\nOvernight Q&A publications calendar housing recipes restaurants basketball blocks social\nAlexa SLU search recipes cinema bookings closet\nDATA TASK DOMAIN BASELINE Copy\nDATA TASK DOMAIN BASELINE Attention\n\nSlide Topic: Reminder Q and A Data\nSummary:\nDATASET DOMAIN SIZE TER NT Words\nOvernight publications calendar housing recipes restaurants basketball blocks social\n\nSlide Topic: Transfer Learning Pretraining\nSummary:\nTER COPY NT t0 tn x0 xn nt0 ntn\nLOW-RESOURCE DOMAIN FEED-FORWARD LAYERS\n\nSlide Topic: Transfer Learning Multi task Learning\nSummary:\nHR DOMAIN LR DOMAIN\nTER COPY TER COPY\nt0 tn x0 xn t0 tn x0 xn\n\nSlide Topic: Trasfer Learning Multi task Learning\nSummary:\nHR DOMAIN LR DOMAIN\nTER COPY TER COPY\nt0 tn x0 xn t0 tn x0 xn\n\nSlide Topic: Transfer Learning Results on Overnight Q and A\nSummary:\nQ&A transfer learning helps for low-resource domains\nBASELINE MULTI-TASK LEARNING PRETRAING\n\nSlide Topic: Multi task Learning for Alexa SLU\nSummary:\nt0 tn x0 xn nt0 ntn\n\nSlide Topic: Transfer Learning Results on Alexa SLU\nSummary:\nSLU transfer learning helps for low-resource domains:\nBASELINE MULTI-TASK LEARNING PRETRAING\n\nSlide Topic: Transfer learning from SLU to Q and A\nSummary:\nRecipe domain exist in both Q&A and SLU;\nPretrain with SLUs recipe for Q&As recipes;\n\nSlide Topic: Takeaways\nSummary:\nExecutable semantic parsing unifies Q&A and SLU;\nOne model for all is fine but some choices must be revisited (e.g. attention, copy);\nTransfer learning for low-resource domains on Q&A and SLU.\n\n",
    "Slide Topic: Abstract Meaning Representation AMR\nSummary:\nHe ate the pizza with his fingers.\n\nSlide Topic: AMR to text generation English\nSummary:\nHe ate the pizza with his fingers.\n\nSlide Topic: Previous work\nSummary:\nKonstas et al. (2017): sequential encoder;\n\nSlide Topic: This work\nSummary:\nHe ate the pizza with his fingers.\nAre improvements in graph encoders due to reentrancies?\nGraph: Graph Convolutional Network (GCN; Kipf and Welling, 2017).\n\nSlide Topic: Sequential input Konstas et al 2017\nSummary:\n:arg0 he :arg1 pizza :instrument finger :part-of he eat-01\n:arg0 eat-01 he :arg1 pizza :instr. finger part-of he\neat-01 :arg0 he :arg1 pizza :instrument finger :part-of he\n\nSlide Topic: Tree structured input\nSummary:\neat-01 :arg0 he :arg1 pizza :instrument finger :part-of he\n:arg0 he :arg1 pizza :instr. finger part-of he eat-01\n\nSlide Topic: Graph structured input\nSummary:\n:arg0 he :arg1 pizza :instrument finger :part-of he eat-01\neat-01 :arg0 he :arg1 pizza :instrument finger :part-of he\n:arg0 he :arg1 pizza :instr. finger part-of he eat-01\n\nSlide Topic: Comparison between models dev set R1\nSummary:\nSeq TreeLSTM GCN-Tree GCN-Graph\n\nSlide Topic: Comparison with previous work test set R1\nSummary:\nKonstas(seq) Song(graph) GCN-Tree GCN-Graph\nKonstas: sequential baseline, Konstas et al. (2017)\n\nSlide Topic: Reentrancies\nSummary:\nHe ate the pizza with his fingers.\n:arg0 he :arg1 pizza :instrument finger :part-of he eat-01\n\nSlide Topic: Long range dependencies\nSummary:\nHe ate the pizza with a fork.\neat-01 :arg0 he :arg1 pizza :instrument fork\nModel Max dependency length\n\nSlide Topic: Generation example\nSummary:\ncommunicate-01 lawyer significant-other ex\nREF tell your ex that all communication needs to go through the lawyer\nSeq tell that all the communication go through lawyer\nTree tell your ex, tell your ex, the need for all the communication\nGraph tell your ex the need to go through a lawyer\n\nSlide Topic: Conclusions\nSummary:\nGraph encoders based on GCN and BiLSTM gives best results for AMR-to-text generation;\nReentrancies and long-range dependencies contribute to the improvements of graph encoders;\nDemo and source code: http://cohort.inf.ed.ac.uk/amrgen.html\n\nSlide Topic: Do reentrancies help with generating pronouns\nSummary:\nHe ate the pizza with his fingers He ate the pizza with he fingers\n:arg0 he :arg1 pizza :instrument finger :part-of he eat-01\nContrastive pair analysis (Sennrich, 2017):\nCompute probability of a reference output sentence and the probability of a sentence containing a mistake;\nCompute accuracy of model in assigning a higher probability to the reference sentence.\nModel Antecedent Type Num. Gender\n\nSlide Topic: More examples\nSummary:\nGraph i dont tell him but he finds out. i didnt tell him but he was out. i dont tell him but found out. i dont tell him but he found out.\nGraph if you tell people they can help you , if you tell him, you can help you ! if you tell person_name you, you can help you . if you tell them, you can help you .\nGraph i d recommend you go and see your doctor too. i recommend you go to see your doctor who is going to see your doctor. you recommend going to see your doctor too. i recommend you going to see your doctor too.\n\n",
    "Slide Topic: Sentence Summarization\nSummary:\nGenerate a shorter version of a given sentence\nPreserve its original meaning\nDesign or refine appealing headlines\n\nSlide Topic: Seq2seq Summarization\nSummary:\nRequire less human efforts\nAchieve the state-of-the-art performance\n\nSlide Topic: Problems of Seq2seq Summarization\nSummary:\nSolely depend on the source text to generate summaries\n3% of summaries 3 words\n4 summaries repeat a word for 99 times\nFocus on extraction rather than abstraction\n\nSlide Topic: Template based Summarization\nSummary:\nA traditional approach to abstractive summarization\nFill an incomplete with the input text using the manually defined rules\nBe able to produce fluent and informative summaries\nTemplate [REGION] shares [open/close] [NUMBER] percent [lower/higher]\nSource hong kong shares closed down #.# percent on friday due to an absence of buyers and fresh incentives .\nSummary hong kong shares close #.# percent lower\n\nSlide Topic: Problems of Template based Summarization\nSummary:\nTemplate construction is extremely time-consuming and requires a plenty of domain knowledge\nIt is impossible to develop all templates for summaries in various domains\n\nSlide Topic: Motivation\nSummary:\nUse actual summaries in the training datasets as soft templates to combine seq2seq and template-based summarization\nSeq2seq Guide the generation of seq2seq\nTemplate-based Automatically learn to rewrite from soft templates\n\nSlide Topic: Proposed Method\nSummary:\nRe3Sum: consists of three modules: Retrieve, Rerank and\nUse Information Retrieval to find out candidate soft templates from the training dataset (Retrieve).\nExtend the seq2seq model to jointly learn template saliency measurement (Rerank) and final summary generation\n\nSlide Topic: Contributions\nSummary:\nIntroduce soft templates to improve the readability and stability in seq2seq\nExtend seq2seq to conduct template reranking and template-aware summary generation simultaneously\nFuse the IR-based ranking technique and seq2seq-based generation technique, utilizing both supervisions\nDemonstrate potential to generate diversely\n\nSlide Topic: Flow Chat\nSummary:\nRetrieve Search actual summaries as candidate soft templates\nRerank Find out the most proper soft template from the candidates\nRewrite Generate the summary based on source sentence and soft template\nRetrieve Rerank Rewrite Sentence Candidates Template Summary\n\nSlide Topic: Retrieve\nSummary:\nAssumption: Similar sentences, similar summary patterns\nOutput 30 actual summaries in the training dataset whose sources are the most similar to the input sentence\n\nSlide Topic: Jointly Rerank and Rewrite\nSummary:\nSaliency Bilinear Decoder Summary\n\nSlide Topic: Rerank\nSummary:\nRetrieve ranks templates according to the text similarity between sentences\ns(r, x) = sigmoid(hrWshT x bs)\n\nSlide Topic: Rewrite\nSummary:\nA soft template accords with the facts in the input sentences\nUse Seq2seq to generate more faithful and informative summaries\nConcatenate the encoders of sentence and template\nUse attentive RNN decoder to generate summaries\n\nSlide Topic: Learning\nSummary:\nCross Entropy (CE) for Rerank\nNegative Log-Likelihood (NLL) for Rewrite\nAdd the above two costs as the final loss\n\nSlide Topic: Setting\nSummary:\nDataset Gigaword (sentence, headline) pairs\nDataset Train Dev. Test\n\nSlide Topic: ROUGE Performance\nSummary:\nRe3Sum significantly outperforms other approaches\nModel ROUGE-1 ROUGE-2 ROUGE-L\n\nSlide Topic: Linguistic Quality Performance\nSummary:\nLow LEN DIF and LESS 3 Stable\nLow NEW NE and NEW UP Faithful\nItem Template OpenNMT Re3Sum\n\nSlide Topic: Effects of Template\nSummary:\nPerformance highly relies on templates\nThe rewriting ability is strong\nType ROUGE-1 ROUGE-2 ROUGE-L\n\nSlide Topic: Generation Diversity\nSummary:\nOpenNMT Beam search n-best outputs\nRe3Sum Provide different templates\nSource anny ainge said thursday he had two one-hour meetings with the new owners of the boston celtics but no deal has been completed for him to return to the franchise .\nTarget ainge says no deal completed with celtics major says no deal with spain on gibraltar Templates roush racing completes deal with red sox owner\nRe3Sum ainge says no deal done with celtics ainge talks with new owners ainge talks with celtics owners OpenNMT ainge talks with new owners\n\nSlide Topic: Conclusion\nSummary:\nIntroduce soft templates as additional input to guide seq2seq summarization\nCombine IR-based ranking techniques and seq2seq-based generation techniques to utilize both supervisions\nImprove informativeness, stability, readability and diversity\n\n",
    "Slide Topic: Conversational Agents\nSummary:\nSorry, I dont understand what youre saying\nData augmentation might help\n\nSlide Topic: Paraphrase Generation\nSummary:\nRephrasing a given text in multiple ways\nParaphrases how could i increase my height ? what should i do to increase body height ? what are the ways to increase height ? are there some ways to increase body height ?\n\nSlide Topic: Current State\nSummary:\nSource how do i increase body height ?\nSynonym how do i grow body height ?\nPhrase how do i increase the body measurement vertically?\nBeam how do i increase my height ? how do i increase my body height how do i increase the height ? how would i increase my body height\n\nSlide Topic: What can we do\nSummary:\nSource how do i increase body height ?\nBeam how do i increase my height ? how can i decrease my body weight ? what do i do to increase the height ? i am 17, what steps to take to decrease weight ?\n\nSlide Topic: What we need\nSummary:\nFind k diverse paraphrases with high fidelity\nMethod based on subset selection of candidate (sub)sequences\n\nSlide Topic: Subset Selection\nSummary:\nhow do i increase my how can i decrease the how can i grow the what ways exist to increase how would I increase the how do I decrease the\nhow do i increase my how can i decrease the how can i grow the how do i increase my what ways exist to increase how can i grow the how would I increase the what ways exist to increase how do I decrease the are there ways to increase\nIf F is sub modular + monotone = Greedy algo. with good bounds exists\n\nSlide Topic: Sub modularity\nSummary:\nF = # Unique Coloured items\n\nSlide Topic: DiPS\nSummary:\nInduce Diversity while not compromising on Fidelity\nDiversity C omponents Fidelity Co mponents\nwhere can film I How find that that picture\nI get can I Where can I : 3k Candidate Subsequ ences Source Sentence\nWhere can I find t h a t film? Where can I get t h a t movie?\nRewards unique n-grams How can I get that picture?\nWhere can I get that film?\nI find that picture\nWhere can I get that movie?\nWhere can I <eos> <sos> ENCODER DECODER\nEnc o d e r k- sequences\nEnc o d e r D e c o d er k- sequences\nHow can I get that picture Fidelity\n\nSlide Topic: Diversity Components\nSummary:\nRewards Structural Coverage where can film I How N find that that picture\nI get can I Where can I : 3k Candidate Subsequences Source Sentence\nn xngram : 3k Candidate Subse q uences Source Sentence\nWhere can I n=1 xX find t h a t film? Where can I get t h a t movi Where can I get that movie? Rewards unique n-grams How can I get th at picture?\nRewards unique n-grams How can I get that picture?\nSynonym (similar embeddings) W here can I get that film? S t r uct u r al C o v era g e Where can I find that pictu re Ho w can I g et that pictu re (x i, xj) k- sequences\nWhere can I find that pictu re Ho w can I g et that pictu re k- sequences\nRew ar ds Stru ct ural C over age xi V (t) xjX\nWhere can I get that Where movie? can I <eos> <sos> ENCODER DECODER\n: 3k Candidate Subse q uences n xngram\nI get can I Where can I Source Sentence\nWhere can I find t h a t film Where can I get t h a t movie\n(xi, xj) EditDistance(xi, xj)\nWhere can I get that Where movie? can I <eos> <sos> |xi |xj\n\nSlide Topic: Fidelity Components\nSummary:\nwhere can film I How find that that picture\nI get can I Where can I : 3k Candidate Subsequences Source Sentence N\nWhere can I find t h a t film? Where can I get t h a t movie n |xn-gram sn-gram\nxX n=1 Rewards unique n-grams How can I get that picture?\nWhere can I get that film? Embedding based Similarity\nWhere can I find that picture\nwix Where can I get that movie?\nwhere can film I How Lexical Similarity find that that picture\nHow can I get that picture (x, s)\nRewards Structural Coverage xX\n\nSlide Topic: DiPS Objective\nSummary:\nDDiivveerrssiittyy C Coommppoonneenntts s FFiiddeelliittyy CCoo mmppoonneenntts s\nwhere where can can film film I I How How find find that that that that picture picture\nI I get get can can I I Where Where can can I I : : 3k 3k Candidate Candidate Subsequences Subsequences Source Source Sentence Sentence\nWhere Where can can I I find find t t h h a a t t film? film Where can I get t h a t movie Where can I get t h a t movie\nRewards Rewards unique unique n-grams n-grams How can I get that picture? How can I get that picture?\nSynonym (similar embeddings) Synonym (similar embeddings) Where Where can can I I get get that that film? film?\nWhere Where can can can can I I find find that that picture picture How How I I get get that that picture picture Rewards Rewards Structural Structural Coverage Coverage\nWhere Where can can I I get get that that movie? movie?\n\nSlide Topic: Fidelity and Diversity\nSummary:\nSBS DBS VAE-SVG DPP SSR DiPS (Ours) DiPS induces diversity without\ncompro4m-Diisstininctg (D iovenrs itfiy) delity\n\nSlide Topic: Data Augmentation Paraphrase Detection\nSummary:\nNo Aug SBS DPP SSR DBS DiPS (Ours)\nDi PS data augmentation helps in paraphrase detection\n\nSlide Topic: Data Augmentation for Intent Classification\nSummary:\nNo. Aug SBS DBS Syn. Rep Cont. Aug DiPS (Ours)\nDa ta augmentation using DiPS improves inten t classification\n\nSlide Topic: Conclusion\nSummary:\nWithout compromising on fidelity\nDiPS Seq2Seq + Diversity\n\n",
    "Slide Topic: Neural Question Answering\nSummary:\nQuestion: What color is the sky?\nPassage: Air is made mainly from molecules of nitrogen and oxygen.\nThese molecules scatter the blue colors of sunlight more effectively than the green and red colors. Therefore, a clean sky appears blue.\n\nSlide Topic: Open Question Answering\nSummary:\nQuestion: What color is the sky?\nRelevant Text Model Answer Span Document Retrieval\n\nSlide Topic: Challenge Scaling Models to Documents\nSummary:\nModern reading comprehension models have many layers and parameters\nThe trend is continuing in this direction, for example with the use of large language models\nReduced efficiency as the paragraph length increases due to long RNN chains or\nLimits the model to processing short paragraphs\n\nSlide Topic: Two Possible Approaches\nSummary:\nSelect a single paragraph from the input, and run the model on that paragraph\nRun the model on many paragraphs from the input, and have itassign a confidence score to its results on each paragraph\n\nSlide Topic: This Work\nSummary:\nImprove several of the key design decision that arise when training on document-level data\nStudy ways to train models to produce correct confidence scores\n\nSlide Topic: Pipeline Method Paragraph Selection\nSummary:\nTrain a shallow linear model to select the best paragraphs\nFeatures include TF-IDF, word occurrences, and its position within the document\nIf there is just one document TF-IDF alone is effective\nImproves change of selecting an answering-containing paragraph from 83.0 to\n\nSlide Topic: Pipeline Method Noisy Supervision\nSummary:\nDocument level data can be expected to be distantly supervised:\nQuestion: Which British general was killed at Khartoum in 1885?\nIn February 1884 Gordon returned to the Sudan to evacuate Egyptian forces.\nRebels broke into the city , killing Gordon and the other defenders. The British public reacted to his death by acclaiming ' Gordon of Khartoum , a saint.\nHowever, historians have since suggested that Gordon defied orders and.\nNeed a training objective that can handle multiple (noisy) answer spans\nUse the summed objective from Kadlec et al (2016), that optimizes the log sum of\nthe probability of all answer spans\nRemains agnostic to how probability mass is distributed among the answer spans\n\nSlide Topic: Pipeline Method Model\nSummary:\nConstruct a fast, competitive model\nUse some keys ideas from prior work,\nbidirectional-attention, self-attention, character- embeddings, variational dropout\nAlso added learned tokens for document and\n< 5 hours to train for 26 epochs on SQuAD\n\nSlide Topic: Confidence Methods\nSummary:\nWe can derive confidence scores from the logit scores given to each span by the\nmodel, i.e., the scores given before the softmax operator is applied\nWithout re-training this can work poorly\n\nSlide Topic: Example from SQuAD\nSummary:\nQuestion: When is the Members Debate held?\nModel Extraction: ..majority of the Scottish electorate voted for it in a referendum to be held on 1 March 1979 that represented at least...\nCorrect Answer: Immediately after Decision Time a Members Debate is held, which lasts for\n\nSlide Topic: Learning Well Calibrated Confidence Scores\nSummary:\nTrain the model on both answering-containing and non-answering containing\nparagraph and use a modified objective function\nMerge: Concatenate sampled paragraphs together\nNo-Answer: Process paragraphs independently, and allow the model to place\nprobability mass on a no-answer output\nSigmoid: Assign an independent probability on each span using the sigmoid\nShared-Norm: Process paragraphs independently, but compute the span\nprobability across spans in all paragraphs\n\nSlide Topic: Datasets\nSummary:\nIncludes three setting, Web (a single document for each questions) Wiki (multiple wikipedia documents for each questions) and Unfiltered (Multiple documents for each questions)\nSQuAD: Turker-generated questions about Wikipedia articles\nWe use the questions paired with the entire article\nManual annotation shows most (90%) of questions are answerable as given the document it was generated from\n\nSlide Topic: Pipeline Method Results on TriviaQA Web\nSummary:\nUses BiDAF as the model\nSelect paragraphs by truncating documents\nSelect answer-spans randomly EM\nword embeddings (Peters et al., 2017)\nTriviaQA Baseline Our Baseline +TF-IDF +Sum +TF-IDF +Sum +Model +TF-IDF +Sum\n\nSlide Topic: TriviaQA Leaderboard Exact Match Scores\nSummary:\nModel Web-All Web-Verified Wiki-All Wiki-Verified\nBest leaderboard entry (mingyan)\nDynamic Integration of Background\n\nSlide Topic: Error Analysis\nSummary:\nManually annotated 200 errors made by the TriviaQAWeb model\n40.5% are due to noise or lack of context in the relevant documents\n\nSlide Topic: Building an Open Question Answering System\nSummary:\nUse Bing web search and a Wikipedia entity linker to locate relevant documents\nExtract the top 12 paragraphs, as found using the linear paragraph ranker\nUse the model trained for TriviaQA Unfiltered to find the final answer\n\n",
    "Slide Topic: Time Critical Events\nSummary:\nDisaster events (earthquake, flood) Urgent needs for affected people\nInformation gathering in real-time is the most challenging part\nRelief operations Humanitarian organizations and local administration need information to help and launch response\n\nSlide Topic: Artificial Intelligence for Digital Response AIDR\nSummary:\nResponse time-line today Response time-line our target\nDelayed decision-making Delayed crisis response Target Early decision-making Rapid crisis response\n\nSlide Topic: Artificial Intelligence for Digital Response\nSummary:\nInformative Not informative Dont know or cant judge Facilitates decision makers Hurricane Irma Hurricane Hurricane California Mexico Iraq & Iran Sri Lanka Harvey Maria wildfires earthquake earthquake f loods\nSmall amount of labeled data and large amount of unlabeled data at the beginning of the event\nLabeled data from the past event. Can we use them?\nWhat about domain shift?\n\nSlide Topic: Our Solutions Contributions\nSummary:\nHow to use large amount of unlabeled data and small amount of labeled data from the same event?\nHow to transfer knowledge from the past events\n=> Adversarial domain adaptions\n\nSlide Topic: Domain Adaptation with Adversarial Training and Graph Embeddings\nSummary:\nWe seek parameters that minimizes the classification loss of the class labels and maximizes domain discriminator loss\n{U,V} Convolution filters and dense layer parameters\n{Vc,W} Parameters specific to the supervised part\n{Vg,C} Parameters specific to the semi-supervised part\n{Vd,wd} Parameters specific to the domain discriminator part\n\nSlide Topic: Semi Supervised Learning\nSummary:\nL: number of labeled instances (x1:L, y1:L)\nU: number of unlabeled instances (xL+1:L+U)\nDesign a classifier f: x y\n\nSlide Topic: Graph based Semi Supervised Learning\nSummary:\nNodes: Instances (labeled and unlabeled)\nEdges: n x n similarity matrix\nEach entry ai,j indicates a similarity between instance i and j\nWe construct the graph using k-nearest neighbor (k=10)\nRequires n(n-1)/2 distance computation\nK-d tree data structure to reduce the computational complexity\nFeature Vector: taking the averaging of the word2vec vectors\nSemi-Supervised component: Loss function\nLearns the internal representations (embedding) by predicting a node in the graph context\nTwo types of context\n1. Context is based on the graph to encode structural\n2. Context is based on the labels to inject label information into the embeddings\n{U,V} Convolution filters and dense layer parameters\n{Vc,W} Parameters specific to the supervised part\n{Vg,C} Parameters specific to the semi-supervised part\n\nSlide Topic: Domain Adaptation with Adversarial Training\nSummary:\nDomain discriminator is defined by:\nNegative log probability of the discriminator loss:\nDomain adversary loss is defined by:\nd {0,1} represents the domain of the input tweet t\n{U,V} Convolution filters and dense layer parameters\n{Vd,wd} Parameters specific to the domain discriminator part\n\nSlide Topic: Corpus\nSummary:\nA small part of the tweets has been annotated using crowdflower\nRelevant: injured or dead people, infrastructure damage, urgent needs of affected people, donation requests\nDataset Relevant Irrelevant Train Dev Test\nNepal earthquake: 50K Queensland flood: 21K\n\nSlide Topic: Experiments and Results\nSummary:\nModel trained using Convolution Neural Network (CNN)\nModel trained using CNN were used to automatically label unlabeled data\nInstances with classifier confidence >=0.75 were used to retrain a new model\nExperiments AUC P R F1\nDomain Adaptation Baseline (Transfer Baseline):\nTrained CNN model on source (an event) and tested on target (another event)\nSource Target AUC P R F1\nCombining all the components of the network\nDomain Adversarial with Graph Embedding\n\nSlide Topic: Summary\nSummary:\nWe have seen how graph-embedding based semi-supervised approach can be useful for small labeled data scenario\nHow can we use existing data and apply domain adaptation technique\nWe propose how both techniques can be combined\n\nSlide Topic: Limitation and Future Study\nSummary:\nGraph embedding is computationally expensive\nGraph constructed using averaged vector from word2vec\nExplored binary class problem\nConvoluted feature for graph construction\nDomain adaptation: labeled and unlabeled data from target\n\n",
    "Slide Topic: Background\nSummary:\nThere are tons of articles exploiting Wikipedia as a comparable corpus\nLittle attention is paid to identifying a domain-specific high-quality comparable corpus\nDomain-specific corpora is a key factor in different tasks, including MT\nWikipedia includes (somehow) all the information necessary to extract such a resource\nOur aim is to identify those domain-specific comparable corpora from\n\nSlide Topic: Background Strategy Overview\nSummary:\nIdentify comparable articles (easy)\nBuild a characteristic vocabulary for the domain of interest (not so easy)\nExplore the Wikipedia categories graph to select the subset of categories in the domain (difficult)\nBrute-force sentence-wise comparison for parallel pairs identification\n\nSlide Topic: Comparable Corpora\nSummary:\nProblem No large collections of comparable texts for all domains and language pairs exist\nObjective To extract high-quality comparable corpora on specific domains\nPilot language pair EnglishSpanish\nPilot domains Science, Computer Science, Sports\nCurrently experimenting on more than 700 domains and 10 languages\n\nSlide Topic: Comparable Corpora Characteristic Vocabulary\nSummary:\nRetrieve every article associated to the top category of the domain\nMerge the articles contents and apply standard and ad-hoc pre-processing\nSelect the top-k tf-sorted tokens as the characteristic vocabulary\n(we consider 10% of the tokens)\nArticles Vocabulary en es en es\n\nSlide Topic: Comparable Corpora Graph exploration\nSummary:\nSlice of the Spanish Wikipedia category graph departing from categories\nSport and Science (as in Spring 2015)\nScientific Sport Science disciplines\nMountain Earth Sports sports sciencies\nGeology Mountains Mountaineering Geology by country\nMountains by country Mountains of Andorra Mountain ran- ges of Spain Geology of Spain\nMountains of the Pyrenees\nPerform a breadth-first search departing from the root category\nVisit nodes only once to avoid loops and repeating traversed paths\nStop at the level when most categories do not belong to the domain\nHeuristic A category belongs to the domain if its title contains at least one term from the characteristic vocabulary\nExplore until a minimum percentage of the categories in a tree level belong to the domain\nCategory pato in Spanish -literally \"duck\"- refers to a sport rather than an animal!!!\nArticle pairs selected according to two criteria: 50% and 60%\nArticles Distance from the root\nen-es en-es en es en es\n\nSlide Topic: Parallelisation Similarity Models\nSummary:\nCharacter 3-grams (cosine) [McNamee and Mayfield, 2004]\nTranslated word 1-grams in both directions (cosine)\nLength factor [Pouliquen et al., 2003]\nProbable lengths of translations of d\n\nSlide Topic: Parallelisation Corpus for Preliminary Evaluation\nSummary:\n30 article pairs (10 per domain)\nAnnotated at sentence level\nThree classes: parallel, comparable, and other\nEach pair was annotated by 2 volunteers mean Cohens\n\nSlide Topic: Parallelisation Threshold Definition\nSummary:\nc3g cog monoen monoes len\nS Slen S F1 S F1len\n\nSlide Topic: Impact Corpora\nSummary:\nin domain out of domain\nGeneration of the Wikipedia dev and test sets\nSelect only sentences starting with a letter and longer than three tokens\nCompute the perplexity of each sentence pair (with respect to a\nSort the pairs according to similarity and perplexity\nManually select the first k parallel sentences\n\nSlide Topic: Impact Corpora Statistics\nSummary:\nCS Sc Sp All\n\nSlide Topic: Impact Phrase based SMT System\nSummary:\nLanguage model 5-gram interpolated Kneser-Ney discounting, SRILM\nTranslation model Moses package\nWeights optimization MERT against BLEU\n\nSlide Topic: Impact Experiments definition\nSummary:\nOut of domain Training Wikipedia and Europarl\nTest Wikipedia (+Gnome for CS)\n\nSlide Topic: Impact Results on Wikipedia in domain\nSummary:\nCS Sc Sp Un\n\nSlide Topic: Impact Translation Instances\nSummary:\nSource All internet packets have a source IP address and a destination\nEP Todos los paquetes de internet tienen un origen direccion IP y destino direccion IP.\nEP+union-CS Todos los paquetes de internet tienen una direccion IP de origen y una direccion IP de destino.\nAwareness of terms (possible overfitting?)\nSource Attack of the Killer Tomatoes is a 2D platform video game developed by Imagineering and released in 1991 for the NES.\nEP el ataque de los tomates es un asesino 2D plataforma video-juego desarrollados por Imagineering y liberados en\nReference Attack of the Killer Tomatoes es un videojuego de plataformas en 2D desarrollado por Imagineering y lanzado en 1991 para el NES.\nSource Fractal compression is a lossy compression method for digital images, based on fractals.\nEP Fractal compresion es un metodo para lossy compresion digital imagenes , basada en fractals.\nEP+union-CS La compresion fractal es un metodo de compresion con perdida para imagenes digitales, basado en fractales.\n\nSlide Topic: Impact Results on News out of domain\nSummary:\nCS Sc Sp Un\n\nSlide Topic: Final Remarks\nSummary:\nA simple model to extract domain-specific comparable corpora from\nThe domain-specific corpora showed to be useful to feed SMT systems, but other tasks are possible\nWe are currently comparing our model against an IR-based system\nThe platform currently operates in more language pairs, including\nFrench, Catalan, German, and Arabic; but it can operate in any language and domain\nThe prototype is coded in Java (and depends on JWPL). We plan to release it in short!\n\n",
    "Slide Topic: Time is important\nSummary:\nUnderstanding time is key to understanding events\nTimelines (in stories, clinical records), time-slot filling, Q&A, common sense\n[June, 1989] Chris Robin lives in England and he is the person that you read about in Winnie the Pooh. As a boy, Chris lived in\nCotchfield Farm. When he was three, his father wrote a poem about him. His father later wrote Winnie the Pooh in 1925.\nWhere did Chris Robin live? Clearly, time sensitive.\nWhen was Chris Robin born? poem [Chris at age 3]\nRequires identifying relations between events, and temporal reasoning.\nEvents are associated with time intervals:\nA happens BEFORE/AFTER B; Time is often expressed implicitly\n2 explicit time expressions per 100 tokens, but 12 temporal relations\n\nSlide Topic: Example\nSummary:\nFriday in the middle of a group of men playing volleyball.\nTemporal question: Which one happens first?\ne1 appears first in text. Is it also earlier in time? e2 was on Friday, but we dont know when e1 happened.\nNo explicit lexical markers, e.g., before, since, or during.\n\nSlide Topic: Example temporal determined by causal\nSummary:\nMore than 10 people (e1: died), he said. A car (e2: exploded)\nFriday in the middle of a group of men playing volleyball.\nTemporal question: Which one happens first?\nObviously, e2:exploded is the cause and e1:died is the effect.\nSo, e2 happens first.\nIn this example, the temporal relation is determined by the causal relation.\nNote also that the lexical information is important here; its likely that explode BERORE die, irrespective of the context.\n\nSlide Topic: Example causal determined by temporal\nSummary:\nPeople raged and took to the street the government\nDid the government stifle people because people raged?\nOr, people raged because the government stifled people?\nBoth sound correct and we are not sure about the causality here.\nPeople raged and took to the street (after) the government\nSince stifled happened earlier, its obvious that the cause is stifled and the result is raged.\nIn this example, the causal relation is determined by the temporal relation.\n\nSlide Topic: This paper\nSummary:\nEvent relations: an essential step of event understanding, which\nsupports applications such as story understanding/completion, summarization, and timeline construction.\n[There has been a lot of work on this; see Ning et al. ACL18, presented yesterday. for a discussion of the literature and the challenges.]\nThis paper focuses on the joint extraction of temporal and\nA temporal relation (T-Link) specifies the relation between two events along the temporal dimension.\nA causal relation (C-Link) specifies the [cause effect] between two events.\n\nSlide Topic: Temporal and casual relations\nSummary:\nT-Link Example: John worked out after finishing his work.\nC-Link Example: He was released due to lack of evidence.\nTemporal and causal relations interact with each other.\nFor example, there is also a T-Link between released and lack\nThe decisions on the T-Link type and the C-link type depend on each other, suggesting that joint reasoning could help.\n\nSlide Topic: Related work\nSummary:\nObviously, temporal and causal relations are closely related\n(were not the first who discovered this).\nNLP researchers have also started paying attention to this direction recently.\nCaTeRs: Mostafazadeh et al. (2016) proposed an annotation framework,\nCaTeRs, which captured both temporal and causal aspects of event relations in common sense stories.\nCATENA: Mirza and Tonelli (2016) proposed to extract both temporal and\ncausal relations, but only by post-editing temporal relations based on causal predictions.\n\nSlide Topic: Contributions\nSummary:\n1. Proposed a novel joint inference framework for temporal and causal reasoning\nAssume the availability of a temporal extraction system and a causal extraction system\nEnforce declarative constraints originating from the physical nature of causality\n2. Constructed a new dataset with both temporal and causal relations.\nWe augmented the EventCausality dataset (Do et al., 2011), which comes with causal relations, with new temporal annotations.\n\nSlide Topic: Temporal relation extraction an ilp approach\nSummary:\n--Event node set. are events.\n' --temporal relation label\n-Boolean variable is there a of relation r between \" -./ $? (Y/N)\n0*(+,)--score of event pair having relation\nGlobal assignment of relations: scores in this document\n'K--the relation dictated by 'F and 'G\n\nSlide Topic: Proposed joint approach\nSummary:\n--Event node set. are events.\n' --temporal relation label\n-Boolean variable is there a of relation r between \" -./ $? (Y/N)\n0*(+,)--score of event pair having relation\n3 4--causal relation; with corresponding variables and\nT & C relations\nCause must be before effect\n\nSlide Topic: Scoring functions\nSummary:\nTwo scoring functions are needed in the objective above\n:;(=>)--score of event pair having temporal relation\nAB(=>)--score of event pair having causal relation C\nWe use the soft-max scores from temporal/causal classifiers (or the log of the soft- max scores)\nChoose your favorite model for the classifiers; here: sparse averaged perceptron\nFeatures for a pair of events:\nPOS, token distance Can we use more than just this\nlocal information? modal verbs in-between (i.e., will, would, can, could, may and might)\ntemporal connectives in-between (e.g., before, after and since)\nWhether the two verbs have a common synonym from their synsets in WordNet The head word of the preposition phrase that covers each verb\n\nSlide Topic: Back to the example temporal determined by causal\nSummary:\nMore than 10 people (e1: died), he said. A car (e2: exploded)\nFriday in the middle of a group of men playing volleyball.\nTemporal question: Which one happens first?\nObviously, e2:exploded is the cause and e1:died is the effect.\nSo, e2 happens first.\nIn this example, the temporal relation is determined by the\nNote also that the lexical information is important here; its\nlikely that explode BERORE die, irrespective of the context.\n\nSlide Topic: Temprob probabilistic knowledge base\nSummary:\nPreprocessing: Semantic Role Labeling & Temporal relations model\nResult: 51K semantic frames, 80M relations\nThen we simply count how many times one frame is before/after another frame, as follows. http://cogcomp.org/page/publication_view/830\nFrame 1 Frame 2 Before After\n\nSlide Topic: Scoring functions additional feature for causality\nSummary:\nTwo scoring functions are needed in the objective above\n:;(=>)--score of event pair having temporal relation\nAB(=>)--score of event pair having causal relation C\nHow to obtain the scoring functions\nWe argue that this prior distribution based on TemProb is correlated with causal directionality, so it will be a useful feature when training AB(=>).\n\nSlide Topic: Result on timebank dense\nSummary:\nTimeBank-Dense: A Benchmark Temporal Relation Dataset\nThe performance of temporal relation extraction:\nCAEVO: the temporal system proposed along with TimeBank-Dense\nCATENA: the aforementioned work post-editing temporal relations based on causal predictions, retrained on TimeBank-Dense.\nSystem P R F1\n\nSlide Topic: A new joint dataset\nSummary:\nTimeBank-Dense has only temporal relation annotations, so in the evaluations above, we only evaluated our temporal performance.\nEventCausality dataset has only causal relation annotations.\nTo get a dataset with both temporal and causal relation annotations, we choose to augment the EventCausality dataset with temporal relations, using the annotation scheme we proposed in our paper [Ning et al., ACL18. A multi-axis annotation scheme for\nevent temporal relation annotation.]\nDoc Event T-Link C-Link\n*due to re-definition of events\n\nSlide Topic: Result on our new joint dataset\nSummary:\nP R F Acc.\nThe temporal performance got strictly better in P, R, and F1.\nThe causal performance also got improved by a large margin.\nComparing to when gold temporal relations were used, we can see that theres still much room for causal improvement.\nComparing to when gold causal relations were used, we can see that the current joint algorithm is very close to its best.\n\nSlide Topic: Conclusion\nSummary:\nWe presented a novel joint inference framework, Temporal and\nUsing an Integer Linear Programming (ILP) framework applied to the extraction problem of temporal and causal relations between events.\nTo show the benefit of TCR, we have developed a new dataset that jointly annotates temporal and causal annotations\nShowed that TCR can improve both temporal and causal components\n\n",
    "Slide Topic: Updates in WMT19\nSummary:\nI reference-based human evaluation monolingual\nI reference-free human evaluation bilingual\nI standard reference-based metrics\nI reference-less metrics QE as a Metric\nI Hybrid supersampling was not needed for sys-level:\nI Sufficiently large numbers of MT systems serve as datapoints.\n\nSlide Topic: System and Segment Level Evaluation\nSummary:\nI Participants compute one\nscore for the whole test set, as translated by each of the systems\nThe new in The company m From Friday's joi \"The unification Cermak, which New common D\nI Segment Level Econo For exam The new in\nscore for each sentence of each systems translation\n\nSlide Topic: Past Metrics Tasks\nSummary:\nRat. of Concord. Pairs\nPearson Corr Coeff based on\nRR RR RR RR RR RR RR RR RR\nmain and secondary score reported for the system-level evaluation. and are slightly different variants regarding ties.\nRR, DA, daRR are different golden truths.\nIncrease in number of participating teams?\nI Baseline metrics: 9 + 2 reimplementations\nI sacreBLEU-BLEU and sacreBLEU-chrF.\nI Submitted metrics: 10 out of 24 are QE as a Metric.\n\nSlide Topic: Data Overview This Year\nSummary:\nI Direct Assessment (DA) for sys-level.\nI Derived relative ranking (daRR) for seg-level.\nI Multiple languages (18 pairs):\nI English (en) to/from Czech (cs), German (de), Finnish (fi),\nGujarati (gu), Kazakh (kk), Lithuanian (lt), Russian (ru), and\nChinese (zh), but excluding cs-en.\nI German (de)Czech (cs) and German (de)French (fr).\n\nSlide Topic: Baselines\nSummary:\nMetric Features Seg-L Sys-L sentBLEU\nCDER chrF chrF+ sacreBLEU-BLEU sacreBLEU-chrF n-grams n-grams n-grams\nLevenshtein distance edit distance, edit types edit distance, edit types edit distance, edit types character n-grams character n-grams n-grams n-grams\nWe average ( ) seg-level scores.\n\nSlide Topic: Participating Metrics\nSummary:\nFeatures char. n-grams, permutation trees contextual word embeddings char. edit distance, edit types char. edit distance, edit types learned neural representations surface linguistic features surface linguistic features word alignments\nMeteor++ 2.0 (syntax+copy) word alignments\nYiSi-1 srl psuedo-references, paraphrases word mover distance semantic similarity semantic similarity semantic similarity\nUniv. of Amsterdam, ILCC\nDublin City University, ADA\nWe average ( ) their seg-level scores.\n\nSlide Topic: Participating QE Systems\nSummary:\nLM log probs., ibm1 lexicon contextual word emb., MT log prob. contextual word embeddings\nsemantic similarity semantic similarity\nWe average ( ) their seg-level scores.\n\nSlide Topic: Golden Truth for Sys Level DA Pearson\nSummary:\nYou have scored individual sentences: (Thank you!)\nNews Task has filtered and standardized this (Ave z).\nWe correlate it with the metric sys-level score.\nAve z BLEU CUNI-Transformer uedin online-B online-A online-G\n\nSlide Topic: Segment Level News Task Evaluation\nSummary:\nYou scored individual sentences: (Same data as above.)\nStandardized, averaged seg-level golden truth score.\nCould be correlated to metric seg-level scores.\nbut there are not enough judgements for indiv. sentences.\n\nSlide Topic: daRR Interpreting DA as RR\nSummary:\nI If score for candidate A better than B by more than 25 points\ninfer the pairwise comparison: A B.\nI No ties in golden daRR.\nI Evaluate with the known Kendalls\nI On average, there are 319 of scored outputs per src segm.\nI From these, we generate 4k327k daRR pairs.\n\nSlide Topic: Sys Level into English Official\nSummary:\nde-en fi-en gu-en kk-en lt-en ru-en zh-en\nchrF chrF+ EED ESIM hLEPORa baseline hLEPORb baseline Meteor++ 2.0(syntax) Meteor++ 2.0(syntax+copy) NIST PER PReP sacreBLEU.BLEU sacreBLEU.chrF TER WER WMDO YiSi-0 YiSi-1 YiSi-1 srl QE as a Metric: ibm1-morpheme ibm1-pos4gram LASIM LP UNI UNI+ YiSi-2 YiSi-2 srl newstest2019\nI Top: Baselines and regular metrics. Bottom: QE as a metric.\nI Bold: not significantly outperformed by any others.\n\nSlide Topic: Sys Level Results Into Out of Excl EN\nSummary:\nCorrelation de-en fi-en gu-en kk-en lt-en ru-en zh-en\nCDER CharacTER chrF chrF+ EED ESIM hLEPORa baseline hLEPORb baseline Meteor++ 2.0(syntax) n\nCorrelation en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh\nBEER n de-cs de-fr fr-de\nBEER BLEU CDER CharacTER chrF chrF+ EED ESIM NIST PER PReP sacreBLEU.BLEU sacreBLEU.chrF TER WER WMDO YiSi-0 YiSi-1 YiSi-1 srl\nCorrelation |r |r |r\nsacreBLEU.BLEU sacreBLEU.chrF hLEPORa baseline hLEPORb baseline TER WER YiSi-0 YiSi-1\nNIST PER sacreBLEU-BLEU sacreBLEU-chrF YiSi-1 srl QE as a Metric: ibm1-morpheme ibm1-pos4gram LASIM LP UNI UNI+ USFD USFD-TL YiSi-2 YiSi-2 srl\nQE as a Metric: ibm1-morpheme ibm1-pos4gram LASIM LP UNI UNI+ YiSi-2 YiSi-2 srl\nTER WER YiSi-0 YiSi-1 YiSi-1 srl QE as a Metric: ibm1-morpheme ibm1-pos4gram YiSi-2 newstest2019\nI *-EN (except FI-EN) sufficiently discerning.\nI EN-* and pair excluding EN somewhat more mixed.\n\nSlide Topic: Summary of Sys Level Wins Metrics\nSummary:\nLPs LPs LPs Corr Wins Overall wins\nBLEU PER sacreBLEU-BLEU BERTr Met++ 2.0(s.) Met++ 2.0(s.+copy) WMDO hLEPORb baseline PReP\n\nSlide Topic: Summary of Sys Level Wins QE\nSummary:\nLPs LPs LPs Corr Wins ibm1-morpheme ibm1-pos4gram\n\nSlide Topic: Seg Level Results Into Out of Excl EN\nSummary:\nHuman Evaluation n de-en fi-en gu-en kk-en lt-en ru-en zh-en\ndaRR daRR daRR daRR daRR daRR daRR daRR\nBERTr CharacTER chrF chrF+ EED ESIM hLEPORa baseline Meteor++ 2.0(syntax)\nHuman Evaluation n en-cs en-de en-fi en-gu en-kk en-lt en-ru en-zh\nBEER CharacTER chrF chrF+ EED ESIM hLEPORa baseline sentBLEU YiSi-0 YiSi-1 YiSi-1 srl\nHuman Evaluation n de-cs de-fr fr-de\nMeteor++ 2.0(syntax+copy) PReP sentBLEU WMDO YiSi-0 YiSi-1 YiSi-1 srl\nQE as a Metric: ibm1-morpheme ibm1-pos4gram LASIM LP UNI UNI+ USFD USFD-TL YiSi-2 YiSi-2 srl sentBLEU YiSi-0 YiSi-1 YiSi-1 srl QE as a Metric: ibm1-morpheme ibm1-pos4gram LASIM LP UNI UNI+ YiSi-2 YiSi-2 srl\nQE as a Metric: ibm1-morpheme ibm1-pos4gram YiSi-2 newstest2019\nI YiSi-1* win across the board and ESIM not far.\nI FR-DE is not discerning.\n\nSlide Topic: Summary of Seg Level Wins Metrics\nSummary:\nLPs LPs LPs Corr Wins Tot\n\nSlide Topic: Summary of Seg Level Wins QE\nSummary:\nLPs LPs LPs Corr Wins ibm1-morpheme ibm1-pos4gram\n\nSlide Topic: Stability across MT Systems\nSummary:\nTop 4 Top 6 Top 8 Top 10 Top 12 Top 15 All systems\nI ENDE sys-level sacreBLEU-BLEU vs. golden truth.\nI One outlier makes the task for metrics too easy.\nI Get correlation when MT systems reduced to top-N ones. sacreBLEU-BLEU\nI Baseline metrics are plotted in grey.\nI In general, most metrics show a strong degrading pattern\nwith the top-N systems across most language pairs.\nI Some QE as a metric have upward correlation trends.\n\nSlide Topic: Overall Status of MT Metrics\nSummary:\nI Sys-level very good overall:\nI Pearson Correlation >.90 mostly, best reach >95 or\nI Low pearsons exist but not many.\nI Correlations are heavily affected by the underlying set of MT\nI System-level correlations are much worse when based on only the better\nI No clear winners, but have a look at this years posters.\nI Seg-level much worse:\nI The top Kendalls only .59.\nI standard metrics correlations varies between 0.03 and 0.59.\nI QE a metric obtains even negative correlations.\nI Methods using embeddings are better:\nI YiSi-*: Word embeddings + other types of available resources.\nI ESIM: Sentence embeddings.\n\nSlide Topic: Next Metrics Task\nSummary:\nI Yes, we will run the task!\nI Big Challenge remains: References possibly worse than MT.\nI Yes, we like the QE as a metric track.\nI We will report the top-N plots.\nI We have to summarize them somehow, though.\nI Doc-level golden truth did not seem different from sys-level.\nI This may change We might run doc-level metrics.\n\n",
    "Slide Topic: Motivations\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nInsufficient or even unavailable training data of emerging classes is a big\nchallenge in real-world text classification.\nZero-shot text classification recognising text documents of classes that\nhave never been seen in the learning stage\nIn this paper, we propose a two-phase framework together with data\naugmentation and feature augmentation to solve this problem.\n\nSlide Topic: Zero shot Text Classification\nSummary:\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nLet and be disjoint sets of seen and unseen classes of the classification\nIn the learning stage, a training set is given where\nis the document containing a sequence of words\nis the class of\nIn the inference stage, the goal is to predict the class of each document, , in\nSupportive semantic knowledge is needed to generally infer the features of unseen classes using patterns learned from seen classes.\n\nSlide Topic: Our Proposed Framework Overview\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nWe integrate four kinds of semantic\nknowledge into our framework:\nData augmentation technique helps the classifiers be aware of the existence of unseen classes without accessing their real data. Feature augmentation provides additional information which relates the document and the unseen classes to generalise the zero-shot reasoning.\n\nSlide Topic: Phase 1 Coarse grained Classification\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nEach seen class has its own CNN text classifier to predict\nThe classifier is trained with all documents of its class in the training set\nas positive examples and the rest as negative examples.\nFor a test document , this phase computes for every seen\nIf there exists a class such that > , it predicts\nis a classification threshold for the class , calculated based on the\nthreshold adaptation method from (Shu et al., 2017)\n\nSlide Topic: Phase 1 Data Augmentation\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nWe use the idea of Topic translation translating an original document\nfrom a seen class into an augmented document of an unseen class.\nMitra perdulca is a species of sea\nsnail a marine gastropod mollusk\nin the family Mitridae the miters or\nMira perdulca is a swimmer of\nsailing sprinter an Olympian\nlimpets gastropod in the basketball\nMiddy the miters or miter skater.\nUsing analogy questions, e.g., animal:species :: athlete:? ? = swimmer\nSolved by the 3CosMul method by Levy and Goldberg (2014)\n\nSlide Topic: Phase 2 Fine grained Classification\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nThe traditional classifier is a multi-class classifier (|| classes) with a softmax\noutput, so it requires only the word embeddings as an input.\nThe zero-shot classifier is a binary classifier with a sigmoid output. It takes a text document and a class as inputs and predicts the confidence\n\nSlide Topic: Phase 2 Zero shot Classifier\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nThe zero-shot classifier predicts\nshows how the word and\nthe class are related considering\nthe relations in a general\nThis classifier is trained with a training data from seen classes only.\n\nSlide Topic: Phase 2 Feature Augmentation\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nStep 1: represent a class as three sets of nodes in ConceptNet\nIf is the class Educational Institution\n(1) educational_institution, educational, institution\n(3) place, people, ages, education.\nStep 2: To construct , we consider whether the word is connected to\nthe members of the three sets within hops.\n\nSlide Topic: Experiments\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nDBpedia ontology : 14 classes\n20newsgroups : 20 classes\n\nSlide Topic: An Experiments for Phase 1\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nCompare with DOC a\nFor seen classes, our\nDOC on both datasets.\nimproved the accuracy of\nunseen classes clearly and led to higher overall accuracy in every setting.\n\nSlide Topic: An Experiments for Phase 2\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nUsing only could not find\nout the correct unseen class\naccuracy of predicting unseen\nhighest accuracy in all settings.\n\nSlide Topic: An Experiments for the Whole Framework\nSummary:\nImperial College Integrating Semantic Knowledge to Tackle Zero-shot Text Classification\nLondon Jingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nTable 2: The accuracy of the whole framework compared with the baselines.\nLabel RNN + FC\nUnseen / - Similarity RNN (Pushp and 5\nDataset rate Yi Count-based (Sappadla Autoencoder Srivastava, CNN + FC Ours\n\nSlide Topic: Conclusions\nSummary:\nIntegrating Semantic Knowledge to Tackle Zero-shot Text Classification\nJingqing Zhang, Piyawat Lertvittayakumjorn, and Yike Guo\nTo tackle zero-shot text classification, we proposed a novel CNN-based two-\nphase framework together with data augmentation and feature augmentation.\nThe experiments show that\ndata augmentation improved the accuracy in detecting instances from unseen\nfeature augmentation enabled knowledge transfer from seen to unseen classes\nour work achieved the highest overall accuracy compared with all the baselines\nand recent approaches in all settings.\nmulti-label classification with a larger amount of data\nutilise semantic units defined by linguists in the zero-shot scenario\n\n",
    "Slide Topic: Motivation\nSummary:\nMost high-performance data-driven models rely on a large amount of labeled training data. However, a model trained on one language usually performs poorly on another language.\nExtend existing services to more languages:\nCollect, select, and pre-process data\nCompile guidelines for new languages\nTrain annotators to qualify for annotation tasks\nAdjudicate annotations and assess the annotation quality and inter-annotator agreement\nAdjudicate annotations and assess inter-annotator agreement\nlanguages are spoken today\nRapid and low-cost development of capabilities for low-resource languages.\nDisaster response and recovery\n\nSlide Topic: TRANSFER LEARNING and MULTI TASK LEARNING\nSummary:\nLeverage existing data of related languages and tasks and transfer knowledge to our target task.\nThe Tasman Sea lies between lAustralie est separee de lAsie par les mers dArafuraet\nAustralia and New Zealand. de Timor et de la Nouvelle-Zelande par la mer de Tasman\nMulti-task Learning (MTL) is an effective solution for knowledge transfer across tasks.\nIn the context of neural network architectures, we usually perform MTL by sharing parameters across models.\nTask A Data Parameter Sharing: When optimizing model A , we update\nand hence . In this way, we can partially train model B as .\n\nSlide Topic: Sequence labeling\nSummary:\nTo illustrate our idea, we take sequence labeling as a case study.\nIn the NLP context, the goal of sequence labeling is to assign a categorical label (e.g., Part-of-speech tag) to each token in a sentence.\nIt underlies a range of fundamental NLP tasks, including POS Tagging, Name Tagging, and Chunking.\nKoalas are largely sedentary and sleep up to 20 hours a day.\nNNS VBP RB JJ CC VB IN TO CD NNS DT NN\nPER NAME TAGGING B-PER E-PER GPE GPE\nItamar Rabinovich, who as Israel's ambassador to Washington conducted unfruitful negotiations with\nSyria, told Israel Radio it looked like Damascus wated to talk rather than fight.\nB-, I-, E-, S-: beginning of a mention, inside of a mention, the end of a mention and a single-token mention\nO: not part of any mention Although we only focus on sequence labeling in this work, our architecture can be adapted for many NLP tasks with slight modification.\n\nSlide Topic: Base model lstm crf chiu and nichols 2016\nSummary:\nThe CRF layer models the dependencies between labels.\nThe linear layer projects hidden states to label space.\nThe Bidirectional LSTM (long-short term memory) processes the input sentence from both directional, encodeing each token and its context into a vector\nInput Sentence Each token in the given sentence is\nrepresented as the combination of its word embedding and character feature vector.\nFeatures Character- level CNN\nWord Embedding Character Embedding\n\nSlide Topic: Previous transfer models for sequence labeling\nSummary:\nT-A: Cross-domain transfer T-B: Cross-domain transfer With disparate label T-C: Cross-lingual Transfer sets\nYang et al. (2017) proposed three transfer learning architectures for different use cases.\n* Above figures are adapted from (Yang et al., 2017)\n\nSlide Topic: Our model multi lingual multi task architecture\nSummary:\ncombines multi-lingual transfer and multi-task transfer is able to transfer knowledge from multiple sources\n\nSlide Topic: Our model multi lingual multi task model\nSummary:\nCross-task Transfer POS Tagging Name Tagging\nCross-lingual Transfer English Spanish\nThe bidirectional LSTM, character embeddings and character-level networks serve as the basis of the architecture. This level of parameter sharing aims to provide universal word representation and feature extraction capability for all tasks and languages\n\nSlide Topic: Our model multi lingual multi task model cross lingual transfer\nSummary:\nFor the same task, most components are shared between languages.\nAlthough our architecture does not require aligned cross-lingual word embeddings, we also evaluate it with aligned embeddings generated using MUSEs unsupervised model (Conneau et al. 2017).\n\nSlide Topic: Our model multi lingual multi task model linear layer\nSummary:\nEnglish: improvement, development, payment,\nFrench: vraiment, completement, immediatement\nWe combine the output of the shared linear layer and the output of the language-specific linear layer using\nwhere . and are optimized during training. is the LSTM hidden states. As is a square matrix, , , and have the same dimension\nWe add a language-specific linear layer to allow the model to behave differently towards some features for different languages.\n\nSlide Topic: Our model multi lingual multi task model cross task transfer\nSummary:\nLinear layers and CRF layers are not shared between different tasks.\nTasks of the same language use the same embedding matrix: mutually enhance word representations\n\nSlide Topic: Alternating training\nSummary:\nTo optimize multiple tasks within one model, we adopt the alternating training approach in (Luong et\nAt each training step, we sample a task with probability:\nIn our experiments, instead of tuning mixing rate , we estimate it by:\nwhere is the task coefficient, is the language coefficient, and is the number of training examples. (or ) takes the value 1 if the task (or language) of is the same as that of the target task; Otherwise it takes the value 0.1.\n\nSlide Topic: Experiments data sets\nSummary:\nSpanish and Dutch: CoNLL 2002\nRussian: LDC2016E95 (Russian Representative Language Pack)\nChechen: TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus\nPart-of-speech Tagging: CoNLL 2017 (Universal Dependencies)\n\nSlide Topic: Experiments setup\nSummary:\n50-dimensional pre-trained word embeddings\nEnglish, Spanish and Dutch: Wikipedia\nChechen: TAC KBP 2017 10-Language EDL Pilot Evaluation Source Corpus\nCross-lingual word embedding: we aligned mono-lingual pre-trained word embeddings with MUSE\n50-dimensional randomly initialized character embeddings\nOptimization: SGD with momentum (), gradient clipping (threshold: 5.0) and exponential learning rate decay.\nHighway Activation Function SeLU\nLSTM Hidden State Size\n\nSlide Topic: Experiments comparison of different models\nSummary:\nTarget task: Dutch Name Tagging\nAuxiliary task: Spanish POS Tagging, English Name Tagging, English POS Tagging\nTarget task: Spanish Name Tagging\nTarget task: Chechen Name Tagging\nAuxiliary task: Russian POS Tagging + Name Tagging or English POS Tagging + Name Tagging\n\nSlide Topic: Experiments comparison with state of the art models\nSummary:\nOur Model We also compared our model with state-of-the-art models with all training data.\n\nSlide Topic: Experiments cross task transfer vs cross lingual transfer\nSummary:\nWith 100 Dutch training sentences:\nThe baseline model misses the name\nThe cross-task transfer model finds the name but assigns a wrong tag to Marx.\nThe cross-lingual transfer model correctly identifies the whole name.\nThe task-specific knowledge that B-PER\nS-PER is an invalid transition will not be learned in the POS Tagging model.\nThe cross-lingual transfer model transfers such knowledge through the shared CRF layer.\n\nSlide Topic: Experiments ablation studies\nSummary:\nC: Character embedding; L: Shared LSTM; S: Language-specific\nH: Highway Networks; D: Dropout\nGenerally, all components improve the performance. Sharing the LSTM layer slightly hurts the performance in the high-resource setting. Language-specific Layer can impair the performance in extreme low-resource settings because this layer is trained only on the target task data.\n\nSlide Topic: Experiments effect of the amount of auxiliary task data\nSummary:\nDoes our model heavily rely on the amount of auxiliary task data?\nThe performance goes up when we increase the sample rate from 0 to 0.2 for auxiliary task data.\nHowever, we do not observe substantial improvement when we further increase the sample rate.\nUsing only 1% auxiliary data, our model already obtains 3.7%-9.7% absolute F-score gains.\n\n",
    "Slide Topic: Discourse Marker\nSummary:\nA discourse marker is a word or a phrase that plays a role in managing the flow and structure of discourse.\nExamples: so, because, and, but, or\n\nSlide Topic: Discourse Marker and NLI\nSummary:\nBut Because If Although And So\n\nSlide Topic: Related Works\nSummary:\nSOTA Neural Network Models\nTransfer Learning for NLI\n\nSlide Topic: Discourse Marker Prediction DMP\nSummary:\nIts rainy outside But + We will not take the umbrella\n(S1, S2) Neural Networks M\nMax pooling over all the hidden states Prediction\n\nSlide Topic: Discourse Marker Augmented Network NLI Model\nSummary:\nGlove Char POS NER EM BiLSTM\nInteraction ------ Similarity Matrix\nThe sentence representation of the premise\nThe sentence representation of the hypothesis\nThe i-th word of the premise\nThe j-th word of the hypothesis\nModeling vector of the premise Attention\nMechanism Modeling vector of the hypothesis\n\nSlide Topic: Training\nSummary:\nOriginal Labels: neutral, neutral, entailment, entailment, neutral\nPrevious action policy that predicts the label given P and H.\n\nSlide Topic: Experiments Datasets\nSummary:\nStanford Natural Language Inference (SNLI) (Bowman et al., 2015)\n433k human annotated sentences pairs\nMulti-Genre Natural Language Inference\n6.5M pairs of sentences for 8 discourse markers\n\nSlide Topic: Experiments Analysis\nSummary:\nPremise: 3 young man in hoods standing in the middle of a quiet street facing the camera. Hypothesis: Three people sit by a busy street bare-headed.\n\nSlide Topic: Conclusion\nSummary:\nWe solve the task of the natural language inference via transferring knowledge from another supervised task.\nWe propose a new objective function to make full use of the labels information.\nIn the future work, we would like to explore some other transfer learning sources.\n\n",
    "Slide Topic: Executing Context Dependent Instructions\nSummary:\nTask: map a sequence of instructions to actions\nModeling Context Learning from\n\nSlide Topic: Executing a Sequence of Instructions\nSummary:\nEmpty out the leftmost beaker of purple chemical\nThen, add the contents of the first beaker to the second\nThen, drain 1 unit from it\nSame for 1 more unit\n\nSlide Topic: Problem Setup\nSummary:\nTask: follow sequence of instructions\nLearning from instructions and corresponding world states\nEmpty out the leftmost beaker of purple chemical\nThen, add the contents of the first beaker to the second\nThen, drain 1 unit from it\nSame for 1 more unit\n\nSlide Topic: Related Work\nSummary:\nMiller et al. 1996, Zettlemoyer and\nCollins Suhr et al. 2018\nEnvironments that change over time while instructions are given\nFollowing instructions in isolation; varying levels of supervision\n\nSlide Topic: Today\nSummary:\n1. Attention-based model for generating sequences of system actions that modify the environment\n2. Exploration-based learning procedure that avoids biases learned early in training\n\nSlide Topic: System Actions\nSummary:\nEach beaker is a stack\nActions are pop and push\npop pop pop push brown; push brown; push brown;\n\nSlide Topic: Meaning Representation\nSummary:\npush brown; push brown; push brown;\n\nSlide Topic: Model\nSummary:\nP revious instructions Current instruction\nT hrow out first beaker Pour sixth beaker into last one It turns brown\nOutput: a sequence of actions\nAttend over each input when generating actions\nI nitial state Attention Attention\nAttend over current instruction\nAttend over previous instructions\nPrevious instructions MLP Current state Initial state\nAttend over initial state\nAttend over current state\nExecute action, update state\nAttend over new state\nCurrent state push 7 brown\npush brown push brown\n\nSlide Topic: Learning from World State Annotation\nSummary:\nGoal: learn a policy that maps from instructions and environment states to actions\nEmpty out the leftmost beaker of purple chemical\nThen, add the contents of the first beaker to the second\nThen, drain 1 unit from it\nSame for 1 more unit\nLearn through exploring the environment and observing rewards\nPolicy gradient with contextual bandit\nChallenge: overcome biases acquired early during learning\n\nSlide Topic: Reward Function\nSummary:\nSource state s s0 Target state\nif if a stops the sequence and a stops the sequence and s0 s0 is the goal state is not the goal state\nis closer to the goal state than is closer to the goal state than s0\n\nSlide Topic: Learning Example\nSummary:\nAdd the third beaker to the first\npush 1 green; pop push 1 yellow;\nNo positive reward for push actions\npush 1 green; pop push 1 green;\nQuickly learned a strong bias against push actions\n\nSlide Topic: Learned Biases\nSummary:\nEarly during learning, model learns it can get positive reward by predicting the pop actions\nLess likely to get positive reward with push action\nBecomes biased against push - during later exploration, push is never sampled!\nCompounding effect: never learns to generate push actions\n\nSlide Topic: Single step Reward Observation\nSummary:\nOur approach: observe reward of all actions by looking one step ahead during exploration\nObserve reward for actions like push\n\nSlide Topic: Learning Algorithm\nSummary:\nFor each training example:\nRollout: sample sequence of state- action pairs from the current policy\nFor each state visited in the rollout,\nA. For each possible action, execute action and observe reward\nUpdate parameters based on observed rewards for all states and actions\n\nSlide Topic: Simple Exploration\nSummary:\nOnly observe states along sampled trajectory\nObserve sampled states and single-step ahead\n\nSlide Topic: Single step Observation\nSummary:\nAdd the third beaker to the first\npush 1 orange push 1 yellow\n\nSlide Topic: Experimental Setup\nSummary:\nTraining data: start state and a sequence of instructions and goal states\nStandard evaluation metric: after following a sequence of instructions, is the world state correct?\n\nSlide Topic: Alchemy\nSummary:\npop pop pop push brown; push brown; push brown;\n\nSlide Topic: Scene\nSummary:\nT he person with a red shirt and a blue hat moves t o the right end\nremove_person remove_hat add_person red add_hat blue\n\nSlide Topic: Tangrams\nSummary:\nSwap the third and fourth figures\nremove 4 insert 3 boat\n\nSlide Topic: Results\nSummary:\nwhile mapping directly to system actions\nPerformance is comparable to direct supervision\n\nSlide Topic: Learning Methods\nSummary:\nSingle-step observations overcome biases that get model stuck\n\nSlide Topic: Ablations\nSummary:\nWithout World State Context\nNeed access to previous instructions\nNeed access to world state\n\n",
    "Slide Topic: Semantic Role Labelling\nSummary:\nSubject Manner Verb Object Time\nJohn surreptitiously ate the burrito at 2am.\nApplied to improve state-of-the-art in NLP tasks such as Question Answering\nCommonly used interface to facilitate Data Exploration and Information Extraction [Stanovsky et al 2018] [Chiticariu et al. 2018]\nConsiderable interest in general-purpose SRL parsers\n\nSlide Topic: Qa srl\nSummary:\nSubject Manner Verb Object Time\nJohn surreptitiously ate the burrito at 2am.\nng ing ea ten\nteso methi aten? thi ng ea ten\nWho a was s ometh\nHow When was some\nWh Aux Subj Verb Obj Prep Obj2\nHow did didnt might will someone something stem past past participle present someone something on to by from someone something\ndidnt might will stem someone What Where When Why How someone something past something on to by from someone something past participle present\nWhat did someone eat? the burrito\nWho someone stem What did\nWhere When Why How didnt might will something past someone something on to by from someone something past participle present\nWho ate something? John\n\nSlide Topic: Goal\nSummary:\nA high-quality, large-scale parser for QA-SRL\n\nSlide Topic: Challenges\nSummary:\n1. Scale up QA-SRL data annotation\n75k sentence dataset in 9 days\n2. Train a QA-SRL Parser\nA much larger super eruption in Colorado produced over 5,000 cubic kilometers of material.\nWhat produced something? A much larger super eruption Produced Where did something produce something? in Colorado What did something produce? over 5,000 cubic kilometers of material Where didnt someone appear to do something? In the video Who didnt appear to do something? the perpetrators appeared When did someone appear? never In the video, the perpetrators never appeared to look at the camera. look at the camera What didnt someone appear to do? to look at the camera Where didn't someone look at something? In the video look Who didnt look? the perpetrators What didnt someone look at? the camera Some of the vegetarians Who met someone? vegetarians met Some of the vegetarians he met were members of the Theosophical Society, which had been founded in 1875 to further universal brotherhood, and which was devoted to the study of Buddhist and Hindu literature.\nWho met? he What did someone meet? members of the Theosophical Society members of the Theosophical Society What had been founded? the Theosophical Society founded in 1875 When was something founded? Why has something been founded? to further universal brotherhood What was devoted to something? members of the Theosophical Society\n\nSlide Topic: Large scale QA SRL\nSummary:\n1. Scale up QA-SRL data annotation\n2. Train a QA-SRL Parser\n\nSlide Topic: Easier Annotation\nSummary:\nUCCA ~6k sentences 4 Trained Annotators\nSemantic Proto-roles ~7k sentences MTurk [Reisinger et al. 2015]\nGroningen Meaning Bank ~40k sentences [Basile et al. 2012]\nQASRL 1.0 ~3k sentences Trained annotators [He et al. 2015]\nQA-SRL 2.0 75k sentences MTurk\n\nSlide Topic: Annotation Pipeline\nSummary:\nx John surreptitiously ate the burrito at 2am\nPredicate detection Identify verbs with POS + heuristics\nOne worker writes as many QA-SRL questions as possible, and provides the answer\nValidation 2 workers are shows questions, provide answers or mark as invalid\n\nSlide Topic: Dataset\nSummary:\n1 annotator provides questions\n2 annotators validate -> 3 spans / question\nQuestion invalid if any annotator marks invalid\nAdditional 3 validators for small dense dev and test set\n3000 sentences 75k sentences\nSeveral weeks 9 days\n2.43 questions / verb questions / verb\n\nSlide Topic: QA SRL Parsing\nSummary:\nArgument detection John surreptitiously the burrito at 2pm\nLocal Question generation Sequential Who ate something? How did someone eat something? What did someone eat? When did someone eat something?\nSpan-based Model John surreptitiously the burrito at 2pm\n\nSlide Topic: Argument Detection BIO Model\nSummary:\nAlternating Bi-LSTM with Highway Connections and Recurrent Dropout [He et al 2017]\nInput includes predicate indicator\nJohn surreptitiously ate the burrito at 2pm\nB B O B I B I\nJohn surreptitiously the burrito at 2pm\n\nSlide Topic: Argument Detection Span Model\nSummary:\nForm a representation of every possible span\nJohn surreptitiously ate the burrito at 2pm\nJohn John surreptitiously surreptitiously ate the the burrito the burrito at at 2pm\n\nSlide Topic: Argument Detection\nSummary:\n4 layer Alternating Bi-LSTM with Highway Connections and\nRecurrent Dropout [He et al 2017]\nTrained to maximize log-likelihood\n\nSlide Topic: Question Generation\nSummary:\nWh Aux Subj Verb Obj Prep Obj2\nHow did didnt might will someone something stem someone past something past participle present on to by from someone something\n4 layer Alternating Bi-LSTM with Highway Connections and\nRecurrent Dropout [He et al 2017]\nTrained to maximize log-likelihood\nSlot Accuracy Exact Match\n\nSlide Topic: Question Generation Local\nSummary:\nJohn surreptitiously the burrito at 2pm\nJohn surreptitiously ate the burrito at 2pm\nWh Aux Subj Verb Obj1 Prep Obj2\nWhat did someone past-tense\n\nSlide Topic: Question Generation Sequential\nSummary:\nWh Aux Subj Verb Obj1 Prep Obj2\nWhat did someone past-tense\n\nSlide Topic: Evaluation Questions\nSummary:\nWho ate something? Who eaten was something by?\nExact Match (full question)\n\nSlide Topic: Full Parsing Accuracy\nSummary:\nExact match f-score (Span & Question)\n\nSlide Topic: Large scale QA SRL Parsing\nSummary:\n1. Scale up QA-SRL data annotation\n2. Train a QA-SRL Parser\n\nSlide Topic: Data Expansion\nSummary:\nOvergenerate questions with low span threshold\nSpan Detection (F-score) Question Generation (Exact Match) Full Parsing (Exact Match)\n\nSlide Topic: Evaluation\nSummary:\nExact Match for Question Generation is overly harsh\nWho ate something? Who eaten was something by?\nPenalizes correct predictions missing from data\n\nSlide Topic: Human Evaluation\nSummary:\nValidate model predictions with 6 annotators\nGenerated Question valid if 5 out of 6 annotators provided answers\nPredicted span correct if exactly matches any annotators answer\n\nSlide Topic: Example Output\nSummary:\nSome of the vegetarians he met were members of the Theosophical Society, which had been founded in 1875 to further universal brotherhood, and which was devoted to the study of Buddhist and Hindu literature.\nWho met someone? Some of the vegetarians met Who met? he\nWhat did someone meet? members of the Theosophical Society members of the Theosophical Society What had been founded? the Theosophical Society founded When was something founded? in 1875\nWhy has something been founded? to further universal brotherhood\nWhat was devoted to something? members of the Theosophical Society devoted What was something devoted to? the study of Buddhist and Hindu literature\n\nSlide Topic: Conclusion\nSummary:\nLarge crowdsourced dataset of QA-SRL annotations\nHigh quality QA-SRL Parser\nTechniques for data expansion\n\n",
    "Slide Topic: Introduction\nSummary:\nChinese spelling checkers are difficult\nNo word delimiters exist among Chinese words\nA Chinese word can contain only\ncharacter or mulGple characters\nMore than 13 thousand characters\nThe spelling checker is expected to idenGfy all possible spelling errors, highlight their locaGons and suggest possible correcGons\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Chinese Spelling Check Evaluations\nSummary:\nThe 1st Chinese Spelling Check Bake-off\nSIGHAN-2013 workshop @ Nagoya, Japan\nChinese as a foreign language learners\nCIPS-SIGHAN joint CLP-2014 conference @ Wuhan\nSIGHAN-2015 workshop @ Beijing, China\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Task Description\nSummary:\nThe input instance is given a unique passage number PID\nEach character or punctuaGon mark occupies 1 spot for counGng locaGon\nIf the passage contains no spelling errors, the checker should return PID, 0\nIf an input passage contains at least one spelling error, the output format is PID, [, locaGon, correcGon]+\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Testing Examples\nSummary:\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Data Preparation\nSummary:\nThe essay secGon of the computer-based Test of Chinese as a Foreign Language (TOCFL)\nThe spelling errors were manually annotated by trained naGve Chinese speakers, who also provided correcGons corresponding to each error.\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Training Set\nSummary:\nThis set included selected essays with a total of spelling errors.\nEach essay is shown in terms of format\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Dryrun Set\nSummary:\nA total of 39 passages were given to parGcipants to familiarize themselves with the f inal tesGng process.\nThe purpose is to validate the submiked output format only, and no dryrun outcomes were considered in the official evaluaGon\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Test Set\nSummary:\nThis set consists of 1,100 tesGng passages. Half of these passages contained no spelling errors, while the other half included at least one spelling error\nOpen test policy: employing any linguisGc and computaGonal resources to detect and correct spelling errors are allowed.\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Performance Metrics\nSummary:\nCorrectness is determined at two levels\nFalse posiGve rate (FPR) = FP / (FP+TP)\nPrecision = TP / (TP+FP)\nF1 = 2 * Precision * Recall / (Precision+Recall)\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Evaluation Examples\nSummary:\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: 9 Participants and 15 Runs\nSummary:\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Testing Results\nSummary:\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: A Summary of Developed Systems\nSummary:\nSIGHAN 2015 @ Beijing, China\n\nSlide Topic: Conclusions and Future Work\nSummary:\nAll submissions contribute to the knowledge in search for an effecGve Chinese spell checkers\nThe individual reports in the Bake-off proceedings provide useful insight into Chinese language processing\nThe future direcGon focuses on the development of Chinese grammaGcal error correcGon\nSIGHAN 2015 @ Beijing, China\n\n",
    "Slide Topic: Task definition\nSummary:\nGiven a name, what is its language?\nSame script (no diacritics)\n\nSlide Topic: Motivation\nSummary:\nImproving letter-to-phoneme performance (Font\nImproving machine transliteration performance\nAdjusting for different semantic transliteration rules\n\nSlide Topic: Previous approaches\nSummary:\nCharacter language models (Cavnar and Trenkle, 1994)\nConstruct models for each language, then choose the language with the most similar model to the test data\naccuracy given >300 characters & 14 languages\nGiven 50 bytes (and 17 languages), language models give\nBetween 13 languages, average F1 on last names is full names gives (Konstantopoulos, 2007)\nEasier with more dissimilar languages: English vs.\nChinese vs. Japanese (same script) gives (Li et al.,\n\nSlide Topic: Using SVMs\nSummary:\nSubstrings (n-grams) of length n for n=1 to 5\nInclude special characters at the beginning and the end to account for prefixes and suffixes\nOther kernels (polynomial, string kernels) did not work well\n\nSlide Topic: Evaluation Transfermarkt corpus\nSummary:\nEuropean national soccer player names\n(Konstantopoulos, 2007) from 13 national languages\n~15k full names (average length 14.8 characters)\n~12k last names (average length 7.8 characters)\ne.g. Dario Dakovic born in Bosnia but plays for Austria, so annotated as German\nLast names Full names\ncs da de en es fr it nl no pl pt se yu Recall cs da de en es fr it nl no pl pt se yu\n\nSlide Topic: Evaluation CEJ corpus\nSummary:\nJapanese names (Li et\n~97k total names, average length 7.6 characters\nDemonstrates a higher baseline with dissimilar languages\nLinear SVM only (RBF and sigmoid were slow)\n\nSlide Topic: Application to machine transliteration\nSummary:\nLanguage origin knowledge may help machine transliteration systems pick appropriate rules\nTo test, we manually annotated data\nEnglish-Hindi transliteration data set from the NEWS 2009\n454 Indian names, 546 non-Indian names\nAverage length 7 characters\nSVM gives 84% language identification accuracy\nBasic idea: use language identification to split data into two language-specific sets\nTrain two separate transliteration models (with less data per model), then combine\nWe use DirecTL (Jiampojamarn et al., 2009)\nBaseline comparison: random split\nDirecTL with random split (Random)\nDirecTL with language identificationinformed split (LangID)\n\nSlide Topic: Conclusion\nSummary:\nLanguage identification of names is difficult\nSVMs with n-grams as features work better than language models\nNo significant effect on machine transliteration\nBut there does seem to be some useful information\n\nSlide Topic: Future work\nSummary:\nOther ways of incorporating language information for machine transliteration\nDirect use as a feature\n\n",
    "Slide Topic: Background\nSummary:\nThere are two types of citations to retracted articles: Citations that a retracted article received prior to its retraction and citations that are received post retraction and despite retraction notices.\nIn this study we sought out to find the context around post-retraction citations with the main purpose of finding out whether they are negatively, positively or neutrally mentioned.\n\nSlide Topic: Data Collection\nSummary:\nScienceDirect, Elseviers full text database was accessed in October 2014. The database was queried for the term RETRACTED in the article title and its retraction notice.\nFor this study we selected the five top articles that were found to be highly cited since 2015. This ensured that the papers all cite retracted articles\nA total of 1,203 results retrieved from which 988 were retracted articles. The results excluded were retraction notices, duplicates and papers whose original titles included the word \"retracted\".\n\nSlide Topic: Case study\nSummary:\nFor each article we extracted the citing documents and analyzed the ones appearing in 2015 and 2016. Overall, we analyzed 120 citing documents.\nEach mention was categorized as follows:\nPositive: A positive citation indicates that the retracted article was cited as legitimate prior work and its findings used to corroborate the author/s current study.\nNegative: A negative citations indicates that the authors mentioned the retracted article as such and its findings as inappropriate.\nNeutral: A neutral citation indicates that the retracted article was mentioned as a publication that appears in the literature and does not include judgement on its validity.\n\nSlide Topic: Citations in Context\nSummary:\nThis article was cited 109 times since its publication in 2012 with\n28 citations tracked after 2014\nMore citations are seen to be negative, the positive and neutral ones are also present\nThe negative citations mostly point to the media frenzy around the results.\nThe study was republished in 2014 by Environmental Sciences Europe. The republished article received 17 citations in 2015 and 2016. The vast majority of them being positive mentions\nNA Negative Neutral Positive\n\nSlide Topic: Conclusions\nSummary:\nRetracted articles continue to be cited years after retraction and despite retraction notices being posted on publishers platforms.\nThis could be the result of general interest by the public or media.\nIn other cases, the reason for retraction does not deter others from citing the article.\nWe recommend that publishers use reference checks to all submitted articles to detect citations of retracted articles and remove them or at least request an explanation from the authors for citing a retracted paper in a positive or neutral manner\n\n",
    "Slide Topic: A NLG system Architecture\nSummary:\nCommunicative Goal Document Plans Sentence Plans Surface Text\nEhud Reiter and Robert Dale, Building Natural Language\nGeneration Systems, Cambridge University Press, 2000.\nIn this paper, we study surface realization, i.e. mapping meaning representations to natural language sentences.\n\nSlide Topic: Meaning Representation\nSummary:\nLogic form, e.g. lambda calculus\n\nSlide Topic: Graph Structured Meaning Representation\nSummary:\nDifferent kinds of graph-structured semantic representations:\nSemantic Dependency Graphs (SDP)\nAbstract Meaning Representations (AMR)\nDependency-based Minimal Recursion Semantics (DMRS)\nElementary Dependency Structures (EDS)\nBV ARG1 ARG2 BV\n\nSlide Topic: Type Logical Semantic Graph\nSummary:\nEDS graphs are grounded under type-logical semantics. They are usually very flat and multi-rooted graphs.\nBV ARG1 ARG2 BV\nThe boy wants the girl to believe him.\n\nSlide Topic: Previous Work\nSummary:\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin Choi, and\nLuke Zettlemoyer. 2017. Neural AMR: Sequence-to-sequence models for parsing and generation.\nSynchronous Node Replacement Grammar. (AMR-to-text)\nLinfeng Song, Xiaochang Peng, Yue Zhang, Zhiguo Wang, and\nDaniel Gildea. 2017. AMR-to-text generation with synchronous node replacement grammar.\nOther Unification grammar-based methods\nCarroll, John and Oepen, Stephan 2005. High efficiency realization for a wide-coverage unification grammar\n\nSlide Topic: Formalisms for Strings Trees and Graphs\nSummary:\nChomsky hierarchy Grammar Abstract machines\nManipulating Graphs: Graph Grammar and DAG Automata.\n\nSlide Topic: Existing System\nSummary:\nDavid Chiang, Frank Drewes, Daniel Gildea, Adam Lopez and\nGiorgio Satta. Weighted DAG Automata for Semantic Graphs.\nthe longest NLP paper that Ive ever read\nDaniel Quernheim and Kevin Knight. 2012. Towards probabilistic acceptors and transducers for feature structures\n\nSlide Topic: DAG Automata\nSummary:\nA weighted DAG automaton is a tuple\nA run of M on DAG D V,E, is an edge labeling function\nThe weight of is the product of all weight of local transitions:\n\nSlide Topic: DAG Automata Toy Example\nSummary:\nJohn wants to go. {} _want_v_\nnamed(John) } named(John Failed !\nAccept ! } named(John\n\nSlide Topic: DAG to Tree Transducer\nSummary:\nWANT qnomb want s qinfb qnomb want s INF INF\nBOY GIRL BOY GIRL\nqnomb want s INF INF\nBE LIE VE BE LIE VE qaccg to believe qaccb NP NP NP\nqaccg qaccb NP NP NP to believe\nt he b oy want s t he gir l t o b elieve him BOY G IR L BOY G IR L BOY G IR L\nq S S S\nChallenges for DAG-to-tree transduction on EDS graphs:\nCannot easily reverse the directions of edges\nCannot easily handle multiple roots\n\nSlide Topic: Our DAG to program transducer\nSummary:\nRewritting: directly generating a new data structure piece by piece, during recognizing an input DAG.\nObtaining target structures based on side effects of the\nStates: The output of our transducer is a program:\nJohn wants to go.\nS John want to go\n\nSlide Topic: Transducation Rules\nSummary:\nA valid DAG Automata transition\nWe use parameterized states:\nThe range of direction: unchanged, empty, reversed.\n\nSlide Topic: Toy Example\nSummary:\nRule For Recognition For Generation\nRecognition: To find an edge labeling function . The red dashed edges make up an intermediate graph T().\nof edge ei with variable xij and L with the output string in the statement templates.\n\nSlide Topic: DAG Transduction based NLG\nSummary:\nDAG Transducer Seq2seq Model\nSemantic Graph Sequential Lemmas Surface string\n\nSlide Topic: Inducing Transduction Rules\nSummary:\nthe decline is even steeper than in September, he said.\nFinding intermediate tree Assigning spans Assigning labels Generating statement templates\n\nSlide Topic: NLG via DAG transduction\nSummary:\nData: DeepBank + Wikiwoods\nDecoder: Beam search (beam size = 128)\nAbout 37,000 induced rules are directly obtained from\nDeepBank training dataset by a group of heuristic rules.\nDisambiguation: global linear model\nTransducer Lemmas Sentences Coverage induced rules induced and exteneded rules induced, exteneded and dynamic rules\n\nSlide Topic: Fine to coarse Transduction\nSummary:\nTo deal with data sparseness problem, we use some heuristic rules to generate extened rules by slightly changing an induced rule.\nGiven a induced rule:\nNew rule generated by deleting:\nNew rule generated by copying:\n\nSlide Topic: Fine to coarse transduction\nSummary:\nDuring decoding, when neither induced nor extended rule is applicable, we use markov model to create a dynamic rule\nC {q1, qm},D represents the context. r1, rn denotes the outgoing states.\n\nSlide Topic: Conclusion and Future Work\nSummary:\nEnglish Resouce Semantics is fantastic!\nFormalism works for graph-to-string mapping, not surprisingly or surprisingly\nIs the decoder perfect? No, not even close\nIs the disambiguation model a neural one? No, graph embedding is non-trivial.\n\n",
    "Slide Topic: Question Answering with Knowledge Base\nSummary:\nLarge-scale Knowledge Base 5\nProperties of billions of entities papa BO. mreebace\n- Plus relations among them barack Obama\nEa sea Matsbaopane YAGO\n: : child-o child-of\n* Question Answering _ | OpenlE/ReVerb\nWhat are the names of Obamas daughters? Microsoft Satori\nAx. parent(Obama, x) A gender(x, Female)\n\nSlide Topic: Search Engine QA Engine\nSummary:\nwho was Katy Perry's husband second tallest mountain in england|\nwho was tom cruise's first wife\nWet Web Shopping Maps News Images More + Search tools\nWeb News Images Shopping\nC7 when did minnesota becor\nMinnesota - Scafell Pike\n\nSlide Topic: Generic Semantic Parsing eg Kwiatkowski 13\nSummary:\nWho is Justin Biebers sister?\n. sibling_of(justin_bieber, x) gender(x, female)\n\nSlide Topic: KB Specitic Semantic Parsing eg Berant 13\nSummary:\nWho is Justin Biebers sister?\n. sibling_of(justin_bieber, x) gender(x, female)\n\nSlide Topic: Key Challenges\nSummary:\nWhat was the date that Minnesota became a state?\nWhen was the state Minnesota created?\nMinnesota's date it entered the union?\n\nSlide Topic: Staged Query Graph Generation\nSummary:\n(1) Link Topic Entity\n(2) Identify Core Inferential Chain\nFamily Guy cast y actor x\nFamily Guy Family Guy writer y start x\nFamily Guy genre x\ns7 argmin Meg Griffin\nFamily Guy y x\n\nSlide Topic: Knowledge Base\nSummary:\nFamily Guy cvt2 Lacey Chabert\n\nSlide Topic: Query Graph\nSummary:\ntopic entity core infe rential chain\nFamily Guy cast y x\n\nSlide Topic: Identity Core Inferential Chain\nSummary:\nFamily Guy cast y actor x\nFamily Guy Family Guy writer y start x\nFamily Guy genre x\nWho first voiced Meg on Family Guy?\n\nSlide Topic: Relation Matching using Deep Convolutional Neural Networks DSSM Shen 14\nSummary:\nInput is mapped to two -dimensional vectors\nR R max max max\nwho voiced meg on castactor <s> w1 w2 wT </s>\n\nSlide Topic: Augment Constraints\nSummary:\nFamily Guy cast y actor x Family Guy y x\nWho voiced Family Guy\nOne or more constraint nodes can be added to or\n: Additional property of this event (e.g., character MegGriffin\n: Additional property of the answer entity (e.g., gender)\n\nSlide Topic: Learning Reward Function y\nSummary:\nWho first voiced Meg on Family Guy?\nFamily Guy cast y actor x\ns4 Family Guy writer y start x\nFamily Guy y x\n\nSlide Topic: Learning Reward Function Features\nSummary:\n=Who first voiced Meg on Family Guy?\nFamily Guy cast y x\n\nSlide Topic: WebQuestions Dataset Berant 13\nSummary:\nWhat character did Natalie Portman play in Star Wars? Padme Amidala\nWhat currency do you use in Costa Rica? Costa Rican colon\nWhat did Obama study in school? political science\nWhat do Michelle Obama do for a living? writer, lawyer\nWhat killed Sammy Davis Jr? throat cancer [Examples from Berant]\n\nSlide Topic: Creating Training Data trom Q A Pairs\nSummary:\nRelation Matching (Identifying Core Inferential Chain)\nwhat was <e> known for people.person.profession\nwhat kind of government does <e> have location.country.form_of_government\nwhat year were the <e> established sports.sports_team.founded\nwhat city was <e> born in people.person.place_of_birth\nwhat did <e> die from people.deceased_person.cause_of_death\nwho married <e> people.person.spouse_s\n\nSlide Topic: Contribution from Entity Linking\nSummary:\nMethod #Entities Covered Ques. Labeled Ent.\n\nSlide Topic: Contribution from Relation Matching\nSummary:\nFy score of query graphs that have only a core inferential\nQuestions trom search engine users are short & simple\nEven if the correct parse requires more constraints, the less constrained graph still gets a partial score\n\nSlide Topic: Error Analysis\nSummary:\nA random sample of 100 incorrectly answered questions\n\n",
    "Slide Topic: Motivation\nSummary:\nUser attribute prediction from text is successful:\nI Gender (Burger et al. 2011 EMNLP)\nI Location (Eisenstein et al. 2011 EMNLP)\nI Personality (Schwartz et al. 2013 PLoS One)\nI Impact (Lampos et al. 2014 EACL)\nI Political orientation (Volkova et al. 2014 ACL)\nI Mental illness (Coppersmith et al. 2014 ACL)\nDownstream applications are benefiting from this:\nI Sentiment analysis (Volkova et al. 2013 EMNLP)\nI Text classification (Hovy 2015 ACL)\n\nSlide Topic: However\nSummary:\nSocio-economic factors (occupation, social class, education, income) play a vital role in language use\nNo large scale user level dataset to date\nI sociological analysis of language use\nI embedding to downstream tasks (e.g. controlling for\n\nSlide Topic: At a Glance\nSummary:\nI Predicting new user attribute: occupation\nI New dataset: user occupation\nI Gaussian Process classification for NLP tasks\nI Feature ranking and analysis using non-linear methods\n\nSlide Topic: Standard Occupational Classification\nSummary:\nStandardised job classification taxonomy\nDeveloped and used by the UK Office for National Statistics\nJobs grouped by skill requirements\nC1 Managers, Directors and Senior Officials\nI 11 Corporate Managers and Directors\nI 1115 Chief Executives and Senior Officials\nJob: chief executive, bank manager\nI 1116 Elected Officers and Representatives\nI 112 Production Managers and Directors\nI 113 Functional Managers and Directors\nI 115 Financial Institution Managers and Directors\nI 116 Managers and Directors in Transport and Logistics\nI 117 Senior Officers in Protective Services\nI 118 Health and Social Services Managers and Directors\nI 119 Managers and Directors in Retail and Wholesale\nI 12 Other Managers and Proprietors\nJob: mechanical engineer, pediatrist, postdoctoral researcher\nC3 Associate Professional and Technical Occupations\nJob: system administrator, dispensing optician\nC4 Administrative and Secretarial Occupations\nJob: legal clerk, company secretary\nC5 Skilled Trades Occupations\nJob: electrical fitter, tailor\nC6 Caring, Leisure, Other Service Occupations\nJob: school assistant, hairdresser\nC7 Sales and Customer Service Occupations\nJob: sales assistant, telephonist\nC8 Process, Plant and Machine Operatives\nJob: factory worker, van driver\nJob: shelf stacker, bartender\n\nSlide Topic: Data\nSummary:\nUsers collected by self-disclosure of job title in profile\nManually filtered by the authors\n10M tweets, average 94.4 users per 3-digit group\nHere we classify only at the 1-digit top level group (9 classes)\nFeature representation and labels available online\nRaw data available for research purposes on request (per\n\nSlide Topic: Features\nSummary:\nUser Level features (18), such as:\nFocus on interpretable features for analysis\nCompute over reference corpus of 400M tweets:\nI SVD embeddings and clusters\nI Word2Vec (W2V) embeddings and clusters\n\nSlide Topic: SVD Features\nSummary:\nCompute word word similarity matrix\nSimilarity metric is Normalized PMI (Bouma 2009) using the entire tweet as context\nUser is represented by summing its word representations\nThe low-dimensional features offer no interpretability\nSpectral clustering to get hard clusters of words (30, 50, 100, 200 clusters)\nEach cluster consists of distributionally similar words topic\nUser is represented by the number of times he uses a word from each cluster.\n\nSlide Topic: Word2Vec Features\nSummary:\nTrained Word2Vec (layer size 50) on our Twitter reference corpus\nSpectral clustering on the word word similiarity matrix (30,\nSimilarity is cosine similarity of words in the embedding space\n\nSlide Topic: Gaussian Processes\nSummary:\nBrings together several key ideas in one framework:\nElegant and powerful framework, with growing popularity in machine learning and application domains\n\nSlide Topic: Gaussian Process Graphical Model View\nSummary:\nI f RD R is a latent\nI y is a noisy realisation\nI k is the covariance\nI m and are learnt\n\nSlide Topic: Gaussian Process Classification\nSummary:\nPass latent function through logistic function to squash the input from (,) to obtain probability, (x) p(yi fi)\n(similar to logistic regression)\nThe likelihood is non-Gaussian and solution is not analytical\nInference using Expectation propagation (EP)\nFITC approximation for large data\nARD kernel learns feature importance features most discriminative between classes\nWe learn 9 one-vs-all binary classifiers\nThis way, we find the most predictive features consistent for all classes\n\nSlide Topic: Gaussian Process Resources\nSummary:\nI GPs for Natural Language Processing tutorial (ACL 2014)\nI GP Schools in Sheffield and roadshows in Kampala,\nI Annotated bibliography and other materials\nI GPy Toolkit (Python)\n\nSlide Topic: Prediction\nSummary:\nLR SVM-RBF GP Baseline\nStratified 10 fold cross-validation\n\nSlide Topic: Prediction Analysis\nSummary:\nUser level features have no predictive value\nWord2Vec features are better than SVD/NPMI for prediction\nNon-linear methods (SVM-RBF and GP) significantly outperform linear methods\n52.7% accuracy for 9-class classification is decent\n\nSlide Topic: Class Comparison\nSummary:\nJensen-Shannon Divergence between topic distributions across occupational classes\nSome clusters of occupations are observable\n\nSlide Topic: Feature Analysis\nSummary:\nRank Manual Label Topic (most frequent words)\nArts art, design, print, collection, poster, painting, custom, logo, printing, drawing\nHealth risk, cancer, mental, stress, pa- tients, treatment, surgery, dis- ease, drugs, doctor\nBeauty Care beauty, natural, dry, skin, mas- sage, plastic, spray, facial, treat- ments, soap\nHigher Education students, research, board, stu- dent, college, education, library, schools, teaching, teachers\nSoftware Engineering service, data, system, services, access, security, development, software, testing, standard\nMost predictive Word2Vec 200 clusters as given by Gaussian\nFootball van, foster, cole, winger, terry, reckons, youngster, f ielding, kenny rooney,\nCorporate patent, industry, reports, global,\nCooking recipe, meat, salad, egg, soup, sauce, beef, served, pork, rice\nElongated Words wait, till, til, yay, ahhh, hoo, woo, woot, whoop, woohoo\nPolitics human, culture, justice, religion, democracy, religious, humanity, tradition, ancient, racism\nComparison of mean topic usage between supersets of\n\nSlide Topic: Feature Analysis Cumulative density functions\nSummary:\nTopic more prevalent CDF line closer to bottom-right corner\n\nSlide Topic: Take Aways\nSummary:\nUser occupation influences language use in social media\nNon-linear methods (Gaussian Processes) obtain significant gains over linear methods\nTopic (clusters) features are both predictive and interpretable\nNew dataset available for research\n\n",
    "Slide Topic: What is this presentation about\nSummary:\nSummarize the history and current state of efforts related to the\nIllustrate the challenges of maintaining a community Project\nInvite the community to extend the capabilities of the Anthology\nCall you to join the Anthology team\nSummary History Future-proofing Upcoming Future\n\nSlide Topic: The Anthology in summary\nSummary:\nOpen access service for all\nAlso hosts posters and additional data\nPaper search and author pages\n45K papers and 4.5K daily hits\nNew papers added in collaboration with proceedings editors\nHistory Future-proofing Upcoming Future\n\nSlide Topic: A brief History of the Anthology\nSummary:\nProposed in 2001 by Steven Bird\nFirst version online in 2002, with Steven Bird as editor\nMin-Yen Kan becomes the\nA new version of the Anthology with extra functionality is released in 2012\nSteven Bird Min-Yen Kan\nHosting of the Anthology moves from the National University of Singapore to Saarland University\nSummary Future-proofing Upcoming Future\n\nSlide Topic: How to Future proof the Anthology\nSummary:\nLimited resources for day-to-day code maintenance\nDocker container for easier set-up and sandboxing\nCollaborative documentation efforts to ease onboarding\nMigration plan on the pipeline, including upgrades and test cases\nSummary History Upcoming Future\n\nSlide Topic: Upcoming major steps\nSummary:\nHosting the Anthology within the main ACL website\nRecruit a new Anthology editor\n(possibly) pay for extra support for the Anthology\nSummary History Future-proofing Future\n\nSlide Topic: Exercise Importing of your slides\nSummary:\nWe import slides, datasets, videos from your own\nCurrently done by email\n(try it yourself! yes, now)\nBetter workflow: pull request against the\nAnthology XML (a la csrankings.org)\nSummary History Future-proofing Future\n\nSlide Topic: Possible future directions\nSummary:\nContains useful information both for CL researchers and about CL researchers. Useful for identifying suitable reviewers.\nMove focus from day-to-day operations towards development\nEstablish a network of mirrors\nSummary History Future-proofing Upcoming\n\n",
    "Slide Topic: Motivations\nSummary:\nRule-Based Machine Translation (RBMT)\nWe have been developed RBMT for more than 30 years.\nLarge technical dictionaries and translation rules\nPre-ordering SMT and Tree/Forest to String\nEffective solutions for Asian language translation (WAT2014)\nBut, pre-ordering rules and parsers are needed.\nStatistical Post Editing (SPE) (same as WAT2014)\nVerify effectiveness in all tasks\nSystem combination between SPE and SMT (new in WAT2015)\n\nSlide Topic: Statistical Post Editing SPE\nSummary:\nTranslating RBMT results to post-edited results.\n1) We first translate source Parallel Corpus\nsentences by RBMT. RB M T (ASPEC / JPC)\nTranslated Sentence 2) We train SPE model by\nRBMT TM LM (ja -> ja)\nInput Sentence Translated Sentence SPE Model SPE Result\n\nSlide Topic: Features of SPE\nSummary:\nCorrect mistranslations / Translate unknown words\nPhrase-level correction (domain adaptation)\nUse of more fluent expressions\nFrom SMTs standpoint SPE:\nReduction of NULL alignment (subject/particle)\nUse of syntax information (polarity/aspect)\n\nSlide Topic: SPR for Patent Translation\nSummary:\nCorpus: JPO-NICT patent corpus\n# of automatic evaluation: 2,000\n# of human evaluation: 200\nRBMT SMT SPE RBMT SMT SPE RBMT SMT SPE\nen-ja en-ja zh-ja zh-ja ko-ja ko-ja\nSPE shows: Automatic evaluation for en-ja/zh-ja/ko-ja\n- Better scores than PB-SMT in automatic evaluation\n- Improvements of understandable level (>=C in acceptability)\nA AA RBMT SMT SPE RBMT SMT SPE Human evaluation for zh-ja\n\nSlide Topic: System Combination\nSummary:\nSelection based on SMT scores and/or other features.\nSelection based on estimated score (Adequacy? Fluency? )\nNeed data to learn the relationship\nOur approach in WAT2015:\nMerge n-best candidates and rescore them.\nWe used RNNLM for reranking.\nSPE Merge and Rescore Final translation\n\nSlide Topic: RNNLM reranking and Tuning\nSummary:\nReranking on the log-linear model\nAdding RNNLM score to default features of Moses.\nRNNLM trained by rnnlm toolkit (Mikolov 12).\n500,000 sentences for each language\nUsing tuned weights without RNNLM, we ran only 1 iteration.\n(to reduce tuning time)\nWlm=0.2 Ad ding RNNLM Linear interpolation SMT Wtrans=0.3\nWlm=0.3 Default Tuned Wtrans=0.2 features weights MERT Dev Wrnnlm=0.0\nSPE Wlm=0.4 Wtrans=0.1 New features Initial weights Tuned weights\nDefault Tuned features weights 2015 Toshiba Corporation\n\nSlide Topic: Experimental Results\nSummary:\n*SMT and SPE are 1-best results.\nSMT SPE COMB ja-en SMT SPE COMB en-ja SMT ja-zh SPE COMB SMT zh-ja SPE COMB\nja-en en-ja ja-zh zh-ja\nSMT JPOzh-ja SPE COMB SMT JPOko-ja SPE COMB JPCzh-ja JPCko-ja 2015 Toshiba Corporation\nBLEU RIBES BLEU RIBES BLEU RIBES BLEU RIBES\nSystem Combination (COMB) achieved\nimprovements of BLEU and RIBES score than SPE.\nCOMB is the best system except JPCko-ja task.\nJPCzh-ja JPCko-ja Systems Rerank BLEU RIBES BLEU RIBES\nSMT No Yes SPE No Yes COMB Yes 2015 Toshiba Corporation\n\nSlide Topic: Which systems did the combination selected\nSummary:\nSAME SAME SMT SAME SAME\nja-en en-ja ja-zh zh-ja\nja-en/en-ja/zh-ja: about 80% translations come from SPE.\nja-zh and JPCzh-ja: COMB selected SPE and SMT, equivalently.\n(Because RBMT couldnt translate well, % of SMT increased. )\nSPE SPE SMT SMT\nJPCzh-ja JPCko-ja same means that COMB results were included both SMT and SPE. 2015 Toshiba Corporation\n\nSlide Topic: Toshiba MT system of WAT2015\nSummary:\nWe additionally applied some pre/post processing.\nTechnical Term English Word KATAKANA\nfrequent notations for .\n+ JPO patent dictionary\nfor JPCzh-ja) continous -> continuous\n\nSlide Topic: Official Results\nSummary:\nSPE and SMT ranked in the top 3 HUMAN in ja-en/ja-zh/JPCzh-ja.\nja-en en-ja ja-zh zh-ja\nBLEU RIBES HUMAN BLEU RIBES HUMAN BLEU RIBES HUMAN BLEU RIBES HUMAN\nJPCzh-ja JPCko-ja System BLEU RIBES HUMAN BLEU RIBES HUMAN\nThe correlation between BLEU/RIEBES and HUMAN is not clear in our\n\nSlide Topic: Crowdsourcing Evaluation\nSummary:\nAnalysis of JPCko-ja result (COMB vs Online A)\nIn in-house evaluation, COMB is better than Online A.\nBaseline COMB Online A\nEffected by differences in number expressions !?\nSRC : Online A:\nEqually evaluated in-house evaluation.\nCrowd-workers should be provided an evaluation guideline by\nwhich such a difference is considered.\n\nSlide Topic: Summary\nSummary:\nToshiba MT system achieved a combination method\nbetween SMT and SPE by RNNLM reranking.\nOur system ranked the top 3 HUMAN score in ja-en/ja-\nWe will aim for practical MT system by more effective\ncombination systems (SMT, SPE , RBMT and more...)\n\n",
    "Slide Topic: SMT Experiments\nSummary:\nExperimental results of SMT\nst Moses Aligner BLEU RIBES Training time\nTable: Evaluation results by using different aligner (GIZA++ and MGIZA) based on the\nAnymalign + Cutnalign BLEU Training time Timeout (s) i\nzh-ja zh-ja zh-ja zh-ja\nTable: Evaluation results by using the alignment method of combining sampling-based\nalignment and bilingual hierarchical sub-sentential alignment methods.\n\n",
    "Slide Topic: Adequacy in Neural Machine Translation\nSummary:\nSource: und wir benutzen dieses wort mit solcher verachtung\nRepetitions Reference: and we say that word with such contempt\nTranslation: and we use this word with such contempt contempt\nEin 28-jahriger Koch, der kurzlich nach Pittsburgh gezogen war, wurde diese Woche im Treppenhaus eines ortlichen Einkaufszentrums tot aufgefunden .\nTranslation: A 28-year-old chef who recently moved to Pittsburgh was found dead in the staircase this week .\nPittsburgh was found dead in the staircase of a local shopping mall this week .\n\nSlide Topic: Previous Work\nSummary:\nConditioning on coverage vectors to track\nGating architectures and adaptive attention to control\nCoverage penalty during decoding (Wu, 2016).\n\nSlide Topic: Main Contributions\nSummary:\nJ'ai mange le sandwich\n1. Fertility-based Neural Machine Translation Model\n(Bounds on source attention weights)\n2. Novel attention transform function: Constrained Sparsemax\n3. Evaluation Metrics: REP-Score and DROP-Score\n\nSlide Topic: Attention Transform Functions\nSummary:\nSparsemax: Euclidean projection of z provides sparse probability distributions.\nConstrained Softmax: Returns the distribution closest to softmax whose attention probabilities are bounded by upper bounds u.\n\nSlide Topic: Constrained Sparsemax\nSummary:\nProvides sparse and bounded probability distributions.\nThis transformation has two levels of sparsity: over time steps & over attended words at each step.\nEfficient linear and sublinear time algorithms for forward and backward propagation.\n\nSlide Topic: Visualization Attention transform functions\nSummary:\ncsparsemax provides sparse and constrained probabilities.\n\nSlide Topic: Fertility based NMT\nSummary:\nAllocate fertilities for each source word as attention budgets that exhaust over decoding.\nFertility Predictor : Train biLSTM model supervised by fertilities from fast_align (IBM Model 2).\nExhaustion strategy to encourage more attention for words with larger credit remaining:\n\nSlide Topic: Experiments\nSummary:\nJoint BPE with 32K merge operations.\nDefault hyperparameter settings in OpenNMT-Py.\nBaselines: Softmax, + CovPenalty (Wu, 2016) and\n\nSlide Topic: Evaluation Metrics REP Score and DROP Score\nSummary:\nPenalizes n-gram repetitions in predicted translations.\nNormalize by number of words in reference corpus.\nFind word alignments from source to reference & source to predicted.\n% of source words aligned with some word in reference, but not with any word in predicted translation.\n\nSlide Topic: Results\nSummary:\nsoftmax softmax+CovPenalty softmax+CovVector csparsemax\n\n",
    "Slide Topic: Direct Transfer for NER\nSummary:\nInput: Unlabelled sentences in the target language encoded with cross-lingual embeddings\nO B-PER O O B-LOC O O O B-PER O O O O O O O B-LOC O\nkailangan namin ng mas maraming dugo sa Pagasanjan. k ailangan namin ng mas maraming d ugo sa Pagasanjan. kailangan namin ng mas maraming dugo sa Pagasanjan.\n\nSlide Topic: Direct Transfer Results NER F1 score WIkiANN\nSummary:\nae a fae} pL]\n= TopeEn4aMV Target Language\n\nSlide Topic: Voting and English are often poor\nSummary:\nra a oe sd om e +79 = ae ye IP S me | op om ne 4 so\na ae | ye\n= Tope En4MV Target Language\n\nSlide Topic: General findings\nSummary:\nTransfer strongest within language family\nAsymmetry between use as source vs target language (Slavic-Cyr,\nBut lots of odd results & overall highly noisy\n\nSlide Topic: Problem Statement\nSummary:\nN black-box source models\nUnlabelled data in target language\nLittle or no labelled data (few shot and zero shot)\nGood predictions in the target language\n\nSlide Topic: Model 1 Few Shot Ranking and Retraining RaRe\nSummary:\nSource Model AR F1AR\nSource Model EN F1EN\nSource Model VI F1VI\nSource Model AR Dataset AR\n20k unlabelled sents in Tagalog\nSource Model EN Dataset EN\nSource Model VI Dataset VI\nN training sets in Tagalo g\nFinal training set, a mixture of distilled knowledge\n1. Train an NER model on the mixture datasets.\n2. Fine-tune on 100 gold samples.\nZero-shot variant: uniform sampling without fine-tuning\n\nSlide Topic: Hierarchical BiLSTM CRF as model\nSummary:\nOur method is independent of model choice.\n\nSlide Topic: Model 2 Zero Shot Transfer BEA\nSummary:\nWhat if no gold labels are available?\nTreat gold labels Z as hidden variables\nEstimate Z that best explains all the observed predictions\nRe-estimate the quality of source models\nInspired by Kim and Ghahramani (2012)\nTrue label of instance i\nvariational mean- field approx.\n\nSlide Topic: Extensions to BEA\nSummary:\nAfter running BEA, estimate source model qualities and remove bottom k, run BEA again (BEAunsx2)\n2. Few shot scenario:\nGiven 100 gold sentences, estimate source model confusion matrices, then run BEA (BEAsup)\n3. Token vs Entity application\n\nSlide Topic: Benchmark BWET Xie et al 2018\nSummary:\nSingle source annotation projection with bilingual dictionaries from cross-lingual word embeddings\nTransfer english training data to German, Dutch, and\nTrain a transformer NER on the projected training data.\nState-of-the-art onzero-shotNER transfer (orthogonal to this)\n\nSlide Topic: CoNLL Results avg F1 over de nl es\nSummary:\nNothman et al. (2013) Use parallel data,\nAVG F1 over de, nl and es 27\nRaRe uns Zero shot\n=> mam OOO nr el YS oS oc LS &\nBait une Zero shot\nhsup B - High-resource\n\nSlide Topic: WIKIANN NER Datasets Pan et al 2017\nSummary:\nSilver annotations from Wikipedia for languages.\nWe picked languages based on availability of bilingual dictionaries.\nCreated balanced training/dev/test partitions\n(varying size of training according to data availability)\n\nSlide Topic: Word representation FastText MUSE\nSummary:\nUse fasttext monolingual wiki embeddings mapped to\nEnglish space using Identical Character Strings.\n\nSlide Topic: Results WikiANN\nSummary:\nMV between top 3 sources\n\nSlide Topic: Effect of increasing source languages\nSummary:\nMethods robust to many varying quality source languages.\nEven better with few-shot supervision.\n\nSlide Topic: Takeawauys I\nSummary:\nTransfer from multiple source languages helps because for many languages we dont know the best source language.\ntakeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary\n\nSlide Topic: Takeawauys II\nSummary:\nWith multiple source languages, you need to estimate their qualities because uniform voting doesnt perform well.\ntakeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary\n\nSlide Topic: Takeaways III\nSummary:\nA small training set in target language helps, and can be done cheaply and quickly\ntakeaway / noun [uk/aus/nz]: a meal cooked and bought at a shop or restaurant but taken somewhere else... Cambridge English Dictionary\n\nSlide Topic: Future Work\nSummary:\nMap all scripts to IPA or Roman alphabet\n(good for shared embeddings and character-level transfer)\nCan we estimate the quality of source models/languages for a specific target language based on language\nTechnique should apply beyond NER to other tasks.\n\n",
    "Slide Topic: Empirically evaluate various models in EJ task\nSummary:\nmulti-layer encoder-decoder model soft-attention model\nThree recurrent units Two kinds of training data\nLSTM, GRU, IRNN naturally-ordered, pre-reordered\n\nSlide Topic: Results evaluation scores\nSummary:\nBaseline hierarchical phrase-based SMT\nSubmitted system 2 (NMT + System combination)\nBest competitor 1: NAIST (Travatar System with NeuralMT Reranking)\nBest competitor 2: naver (SMT t2s + Spell correction + NMT reranking)\n\nSlide Topic: Finding and Insights\nSummary:\nSoft-attention models outperforms multi-layer encoder-decoder models\nTraining models on pre-reordered data hurts the performance\nNMT models tend to make grammatically valid but incomplete translations\n\n",
    "Slide Topic: Introduction\nSummary:\nThey have been married for three years.\nEvent Trigger ismarried, which represents a marry event\n\nSlide Topic: Motivation\nSummary:\n... I knew it was time to leave.\nA single sentence may cause ambiguous\nIs not that a great argument for term limits?\nThe contextual information of a individual sentence offers\nmore confident for classifying\nLiao and Grishman, ACL, 2010\nHuang and Riloff, AAAI, 2012\n\nSlide Topic: DEEB RNN The Proposed Model\nSummary:\nI qi qi qL\nI I I I I I I I I I I I I I I I I I I I I I t a N SN\nfi a fit we fir\nrj document e embedding > eld \\e) word embedding entity type embedding\nDocument-level Enhanced Event Detector\n\nSlide Topic: Model ED Oriented Document Embedding Learning\nSummary:\nIndicatedis a event trigger and is setted as 1, other words are setted as 0.\nThe square error as the general loss of the attention at sentence level to supervise the learning process.\nS1, S3 and SL are sentences with event triggers and is setted as 1, other sentences are setted as 0.\n\nSlide Topic: Model Document level Enhanced Event Detector\nSummary:\nsoftmax output layer to get the predicted probability for each word\n\nSlide Topic: Experiments Configuration\nSummary:\nGRU w ,GRU s ,GRUe\nentity type embeddings 50 (randomly initialized)\n\nSlide Topic: Experiments Model analysis\nSummary:\nboth gold attention signals\n\nSlide Topic: Experiments Baselines\nSummary:\n* Feature-based methods without document-level information :\n* Representation-based methods without document-level information :\n* Feature-based methods using document level information :\n\nSlide Topic: Summary\nSummary:\nhierarchical and supervised attention\ngold word- and sentence-level attentions\n\n",
    "Slide Topic: Semantic Role Labeling SRL\nSummary:\nFind out who did what to whom in text\nI ate pizza with friends\n\nSlide Topic: SRL as BIO Tagging\nSummary:\nARG0 V ARG1 AM-PRP\nInput1 Many tourists visit Disney to meet their favorite cartoon characters\nNeeds target predicate as input!\n(Prior works typically used gold predicates)\nNe eds to re-run the tag ger f or ea ch pre dicate\n\nSlide Topic: SRL as Predicting Word Span Relations\nSummary:\nMany tourists visit Disney to meet their favorite cartoon characters\n(similar to Punyakanok08, FitzGerald15, inter alia)\n* Too many possible edges (n2 argument spans x n predicates)\n\nSlide Topic: Our Model Overview\nSummary:\nMany tourists visit Disney to meet their favorite cartoon characters\ntourists visit Disney their favorite cartoon\nMany tourists Disney to meet their cartoon characters\n(1) Construct span representations for all n2 spans!\n(2) Local classifier over labels (including NULL) for all possible (predicate, argument) pairs\n(3) Greedy beam pruning for spans\n\nSlide Topic: End to End SRL Results\nSummary:\nHe17 Ours He17 (Ensemble) Ours+ELMo\nBIO-based, pipelined predicate ID\nCoNL05 WSJ Test CoNL05 Brown Test CoNLL2012 (OntoNotes)\nWith ELMo, over 3 points improvement over SotA ensemble!\n*ELMo: Deep Contextualized Word Representations, Peters et al., 2018\n\nSlide Topic: Span based vs BIO\nSummary:\nPredicate Identification Pipelined Joint\nPredicate Identification Due to the strong independence Pipelined Joint\nGlobal Consistency By allowing direct interaction\nbetween predicates and arguments\n\nSlide Topic: Conclusion\nSummary:\nJoint prediction of predicates and arguments\n1. Contextualized span representations\n2. Local label classifiers\n3. Greedy span pruning\nFuture work: Improve global consistency, use span representations for downstream tasks, etc.\n\n",
    "Slide Topic: Again\nSummary:\nHeard on the campaign trail:\nMake the middle class mean something again, with rising incomes and broader horizons.\nMake America great again.\n\nSlide Topic: What is presupposition\nSummary:\nPresuppositions: assumptions shared by discourse participants in an\nPresupposition triggers: expressions that indicate the presence of presuppositions.\nOops! I did it again Trigger\nPresupposes Britney did it before\n\nSlide Topic: Linguistic Analysis\nSummary:\nPresuppositions are preconditions for statements to be true or false\nClasses of construction that can trigger presupposition (Zare et al., 2012):\nDefinite descriptions (Kabbara et al., 2016), e.g.: The queen of the United\nStressed constituents (Krifka, 1998), e.g.: Yes, Peter did eat pasta.\nFactive verbs, e.g.: Michael regrets eating his mothers cookies.\nImplicative verbs, e.g.: She managed to make it to the airport on time.\n\nSlide Topic: Motivation and Applications\nSummary:\nInteresting testbed for pragmatic reasoning: investigating presupposition triggers requires understanding preceding context.\nPresupposition triggers influencing political discourse:\nThe abundant use of presupposition triggers helps to better communicate political messages and consequently persuade the audience (Liang and Liu,\nTo improve the readability and coherence in language generation applications (e.g., summarization, dialogue systems).\n\nSlide Topic: Adverbial Presupposition Triggers\nSummary:\nIndicate the recurrence, continuation, or termination of an event in the discourse context, or the presence of a similar event.\nThe most commonly occurring presupposition triggers (after existential triggers) (Khaleel, 2010).\nLittle work has been done on these triggers in the computational literature from a statistical, corpus-driven perspective.\nAll others (lexical and structural)\n\nSlide Topic: This Work\nSummary:\nComputational approach to detecting presupposition triggers.\nCreate new datasets for the task of detecting adverbial presupposition triggers.\nControl for potential confounding factors such as class balance and syntactic governor of the triggering adverb.\nPresent a new weighted pooling attention mechanism for the task.\n\nSlide Topic: Task\nSummary:\nDetect contexts in which adverbial presupposition triggers can be used.\nRequires detecting recurring or similar events in the discourse context.\nFive triggers of interest: too, again, also, still, yet.\nFrame the learning problem as a binary classification for predicting the presence of an adverbial presupposition (as opposed to the identity of the adverb).\n\nSlide Topic: Sample Configuration\nSummary:\n3-tuple: label, list of tokens, list of POS tags.\nBack to our example:\nMake America great again. Trigger\n(aka governor of again)\nSpecial token: to identify the candidate context in the passage to the model.\nMake, America, great], Tokens\nVB, NNP, JJ ] POS tags\n\nSlide Topic: Positive vs Negative Samples\nSummary:\nSame governors as in the positive cases but without triggering presupposition.\nExample of positive sample:\nJuan is coming to the event too.\nExample of negative sample:\nWhitney is coming tomorrow.\n\nSlide Topic: Extracting Positive Samples\nSummary:\nScan through all the documents to search for target adverbs.\nFor each occurrence of a target adverb:\nStore the location and the governor of the adverb.\nExtract 50 unlemmatized tokens preceding the governor, together with the tokens right after it up to the end of the sentence (where the adverb is).\n\nSlide Topic: Extracting Negative Samples\nSummary:\nExtract sentences containing the same governors (as in the positive cases) but not any of the target adverbs.\nNumber of samples in the positive and negative classes roughly balanced.\nNegative samples are extracted/constructed in the same manner as the positive examples.\n\nSlide Topic: Position Related Confounding Factors\nSummary:\nWe try to control position-related confounding factors by two randomization approaches:\nRandomize the order of documents to be scanned.\nWithin each document, start scanning from a random location in the document.\n\nSlide Topic: Learning Model\nSummary:\nPresupposition involves reasoning over multiple spans of text.\nAt a high level, our model extends a bidirectional LSTM model by:\nComputing correlations between the hidden states at each timestep.\nApplying an attention mechanism over these correlations.\nNo new parameters compared to standard bidirectional LSTM.\n\nSlide Topic: Learning Model Input\nSummary:\nOptionally concatenate with POS tags.\n\nSlide Topic: Learning Model RNN\nSummary:\nconcatenates all hidden states.\nWe continue to feel that the stock market biLSTM is the @@@@ place to be for long-term appreciation.\n\nSlide Topic: Learning Model Matching Matrix\nSummary:\nPair-wise matching matrix M\n\nSlide Topic: Learning Model Softmax\nSummary:\nLearn how to aggregate.\nRow-wise softmax: Attention distribution over words.\n\nSlide Topic: Learning Model Attention Score\nSummary:\nSchool of Computer Science\nColumn-wise softmax Row-wise softmax\nThe columns of M\" are then '\naveraged, forming vector [.\nbiLSTM: hidden states concatenated to\nembeddings concatenated [ word embedding lookup ] :\nt The Old Granary... @@@@_ included Bertrand Russell\nbased on (Cui et al.,\n\nSlide Topic: Datasets\nSummary:\nNew datasets extracted from:\nThe English Gigaword corpus:\nIndividual sub-datasets (i.e., presence of each adverb vs. absence).\nALL (i.e., presence of one of the 5 adverbs vs. absence).\nThe Penn Tree Bank (PTB) corpus:\n\nSlide Topic: Results Overview\nSummary:\nOur model outperforms all other models in 10 out of 14 scenarios\n(combinations of datasets and whether or not POS tags are used).\nWP outperforms regular LSTM without introducing additional parameters.\nFor all models, we find that including POS tags benefits the detection of adverbial presupposition triggers in Gigaword and PTB datasets.\n\nSlide Topic: Results WSJ\nSummary:\nWP best on WSJ.\nWSJ - Accuracy MFC: Most Frequent Class\nbaselines by large margin.\nModels Variants All adverbs\nNetwork based on (Kim\n+ POS LSTM - POS\n+ POS WP - POS\n\nSlide Topic: Results Gigaword\nSummary:\nModels Variants All adverbs Again Still Too Yet Also\n+ POS CNN - POS\n+ POS LSTM - POS\n+ POS WP - POS\nin 10 out of cases.\nBetter performance with POS.\n\nSlide Topic: Qualitative Analysis\nSummary:\n... We continue to feel that the stock market is the place to be for long-term appreciation.\n... Careers count most for the well-to-do. Many affluent people place personal success and money above family.\n\nSlide Topic: Conclusion\nSummary:\nNew task, detection of adverbial presupposition triggers\nNew datasets for the task.\nNew attention model tailored for the task.\nOur model outperforms other strong baselines without additional parameters over the standard LSTM model.\n\nSlide Topic: Future Directions\nSummary:\nIncorporate such a system in an NLG pipeline (e.g., dialogue or summarization with text rewriting).\nDiscourse analysis with presupposition (e.g., political speech).\nInvestigate other types of presupposition.\n\n",
    "Slide Topic: Extractive Summarization\nSummary:\nSelect salient sentences from input document to create a summary\nINPUT Document with sentences S1, S2,.., Sn\n\nSlide Topic: Our Contribution\nSummary:\nA Deep Learning Architecture for training an extractive summarizer: SWAP-NET\nUnlike previous methods, SWAP-NET uses keywords for sentence selection\nPredicts both important words and sentences in document\nTwo-level Encoder-Decoder Attention model\nOutperform state of the art extractive summarisers.\nINPUT Document with sentences S1, S2,.., Sn\n\nSlide Topic: Extractive Summarization Methods\nSummary:\nPre-trained word embeddings Word Encodings wrt other words Sentence Encoding wrt words in it Sentence Encodings wrt other sentences Document Encoding wrt its sentences\nSentence encodings wrt other sentences\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of docments. In Association for the Advancement of Artificial Intelligence, pages 30753081. Jianpeng Cheng and Mirella Lapata. 2016. Neural summarization by extracting sentences and words. 54th Annual Meeting of the Association for Computational Linguistics.\nSummaRuNNer (Nallapati et al., 2017)\nBoth assume saliency of sentence s depends on salient sentences appearing before s\nWord Label Prediction (with decoder)\nSentence Label Prediction (with decoder)\n\nSlide Topic: Intuition Behind Approach\nSummary:\nQuestion: Which sentence should be considered salient (part of summary)?\nOur hypothesis: saliency of a sentence depends on both salient sentences and words appearing before that sentence in the document\nSimilar to graph based models by Wan et al. (2007)\nAlong with labelling sentences we also label words to determine their saliency\nMoreover, saliency of a word depends on previous salient words and sentences\nXiaojun Wan, Jianwu Yang, and Jianguo Xiao. 2007. Towards an iterative reinforcement approach for simultaneous document summarization and keyword extraction. In Proceedings of the 45th annual meeting of the association of computational linguistics, pages 552559.\nThree types of Interactions:\n\nSlide Topic: Intuition Interaction Between Sentences\nSummary:\nA sentence should be salient if it is heavily linked with other salient sentences\n\nSlide Topic: Intuition Interaction Between Words\nSummary:\nA word should be salient if it is heavily linked with other salient words\n\nSlide Topic: Intuition Words and Sentences Interaction\nSummary:\nA sentence should be salient if it contains many salient words\nA word should be salient if it appears in many salient sentences\nGenerate extractive summary using both important words and sentences\nWord-Word Important Sentences: S3 Important Words: V2, V3\n\nSlide Topic: Keyword Extraction and Sentence Extraction\nSummary:\nSentence to Sentence Interaction as Sentence Extraction\nWord to Word Interaction as Word Extraction\nFor discrete sequences, pointer networks have been successfully used to learn how to select positions from an input sequence\nWe use two pointer networks one at word-level and another at sentence-level\n\nSlide Topic: Pointer Network\nSummary:\nEncoder-Decoder architecture with Attention\nAttention mechanism is used to select one of the inputs at each decoding step\nThus, effectively pointing to an input\n\nSlide Topic: Three Interactions SWAP NET\nSummary:\nA Mechanism to Combine\nWord Level Attentions and\n\nSlide Topic: Questions\nSummary:\nQ1 : How can the two attentions be combined?\nQ2 : How can the summaries be generated considering both the attentions?\nA Mechanism to Combine\nWord Level Attentions and\n\nSlide Topic: SWAP NET Architecture Word Level Pointer Network\nSummary:\nSimilar to Pointer Network,\nThe word encoder is bi-directional LSTM\nWord-level decoder learns to point to important words\nE W E W E W E W E W D W D W D W\nPurple line: attention vector given as input to each decoding step\nSum of word encodings weighted by attention probabilities generated in previous step\nProbability of word i, at decoding step j\n\nSlide Topic: SWAP NET Architecture\nSummary:\nSentence-Level Hierarchical Pointer Network\nSentence is represented by encoding of last word of that sentence\nWord Encoder E W E W E W E W E W D W D W D W\nAttention vectors are sum of sentence encodings weighted by attention probabilities by previous decoding step\nProbability of sentence k, at decoding step j\n\nSlide Topic: Combining Sentence Attention and Word Attention\nSummary:\nQ1 : How can the two attentions be combined?\nA document with three sentences and corresponding words is shown\n\nSlide Topic: Sentence and Word Interactions\nSummary:\nStep 1: Hold sentence processing. Then group all words and determine their saliency sequentially\nStep 2: Using output of step 1, i.e., using keywords, process sentences to determine salient sentences\nINCOMPLETE SOLUTION This methods processes sentence depending on words but does not use sentences for processing words.\nGroup each sentence and its words separately and process them sequentially\nHold sentence processing. Determine saliency of words in S1\nUsing information about saliency of words in S2 and saliency of previous sentence S1\nHold word processing and resume sentence processing.\nUsing information about saliency of both S1 and its words\nHold sentence processing and resume word processing.\nDetermine saliency of words in next sentence S2\nDetermine saliency of sentence S2\nThis methods ensures that saliency of word and sentence is determined from previously predicted both salient sentences and words\nUsing previously predicted salient word and sentences\nSynchronising Decoding Steps: Decide when to turn off and on word processing and sentence processing to synchronise word and sentence prediction\nSharing Attention Vectors: Determine salient words and sentences\n\nSlide Topic: SWAP NET Switch Mechanism\nSummary:\nSharing both attention vectors (purple and orange lines) between the two decoder\nSynchronising decoding steps of the two decoders by allowing only one decoder output at a step\nFeedforward Netw ork Switch Probability\nE W E W E W E W E W D W D W D W Word Decoder Hidden State\nOutput is selected with maximum of final word and sentence probabilities\nFinal Word Probabilities E W E W E W E W E W D W D W D W\n\nSlide Topic: Prediction with SWAP NET Encoding\nSummary:\nE W E W E W E W E W\nInput Document w1 w2 w3 w4 w5\n\nSlide Topic: Prediction with SWAP NET Decoding Step\nSummary:\nSwitch has two states,\nQ = 0 : word selection and\nQ = 1 : sentence selection\nQ=0 E W E W E W E W E W D W D W D W\n\nSlide Topic: Summary Generation\nSummary:\nHouse prices across the UK will rise at a fraction of last years frenetic pace, forecasts show.\nprices rise fraction frenetic pace forecasts show\nProbability KeyWord P1 P2 P3 P4 P5 P6 P7\nScore of Given Sentence = (Sentence Probability) + (Sum of its keyword Probabilities)\nk = Ps + Pi where k is number of keywords in sentence S i=1\nTop 3 sentences with maximum scores are chosen as summary\n\nSlide Topic: Dataset and Evaluation\nSummary:\nLarge Benchmark Dataset CNN/DailyMail News Corpus\nNews articles from CNN/DailyMail along with human generated summary (gold summary) for each article\nGroundTruth Binary Labels For Training\nSentences: Anonymised version of dataset given by (Cheng and Lapata, 2016)\nWords: Extract keywords from each gold summary using RAKE\nDataset Training Validation Test\nStandard Evaluation Metric: Three Variates of Rouge Score\nComparing generated summaries and gold summaries for matching:\nROUGE-L (RL): Longest Common Subsequences Stuart Rose, Dave Engel, Nick Cramer, and Wendy Cowley. 2010. Automatic key word extraction from individual documents. Text Mining: Applications and Theory.\n\nSlide Topic: Results\nSummary:\nPerformance on DailyMail Dataset using limited length recall of Rouge\nPerformance on CNN and Daily-Mail test set using the full length Rouge F score\n\nSlide Topic: Example\nSummary:\nMeet the four immigrant students each accepted to ALL EIGHT Ivy League schools who want to pay back their parents who moved to the U.S. to give them a better\nTheir parents came to the U.S. for opportunities and now these four teens have them in abundance .\nThe high-achieving high schoolers have each been accepted to all eight Ivy League schools : Brown University , Columbia University ,\nCornell University , Dartmouth College , Harvard University , University of Pennsylvania , Princeton University and Yale University .\nAnd as well as the Ivy League colleges , each of them has also been accepted to other top schools . While they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria . Summary Generated\nMunira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia , Bulgaria or Nigeria - and say they have their parents ' hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons\n\nSlide Topic: SWAP NET Predicted Keywords\nSummary:\nSummary Generated by SWAP-NET\nWhile they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria\nAnd all four - Munira_Khalif from Minnesota , Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank .\nNow they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon\nSWAP-NET predictions highlighted in green\n\nSlide Topic: Keywords Ground truth vs SWAP NET predictions\nSummary:\nSWAP-NET key words (green) and Ground truth (blue)\nWhile they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria , Somalia or Nigeria\nAnd all four - Munira_Khalif from Minnesota Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon\nNow they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon\nMunira_Khalif from Minnesota Stefan_Stoykov from Indiana Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia Bulgaria or Nigeria - and say they have their parents hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons\n\nSlide Topic: Observations\nSummary:\nAlmost no keyword is repeated across different sentence in the summary\nSummary Generated by SWAP-NET:\nPresence of key words in all the overlapping segments of text with the gold summary\nWhile they all grew up in different cities , the students are the offspring of immigrant parents who moved to America - from Bulgaria Somalia or Nigeria And all four - Munira_Khalif from Minnesota Stefan_Stoykov from Indiana , Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York - say they have their parents hard work to thank . Now they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon\nBulgaria , Somalia or Nigeria\nAnd all four - Munira_Khalif from Minnesota ,\nMunira_Khalif from Minnesota Stefan_Stoykov from Indiana Victor_Agbafe from North_Carolina , and Harold_Ekeh from New_York got multiple offers All have immigrant parents - from Somalia Bulgaria or Nigeria - and say they have their parents hard work to thank for their successes They hope to use the opportunities for good , from improving education across the world to becoming neurosurgeons\nNorth_Carolina , and Harold_Ekeh from New_York - say they have their parents ' hard work to thank .\nNow they hope to use the opportunities for good - whether its effecting positive social change , improving education across the world or becoming a neurosurgeon\nMost of the predicted keywords are actual keywords\nMost of the extracted summary sentences contain keywords\nGold Summary: Large proportion of key words from the\ngold summary present in the generated summary\n\nSlide Topic: Experiments\nSummary:\nKey word coverage measures the proportion of key words from those in the gold summary present in the generated summary\nSentences with key words measures the proportion of sentences containing at least one key word\nAverage pairwise cosine distance between paragraph vector representations of sentences in summaries to measure semantic redundancy in summaries\nSWAP-NET summaries are similar in redundancy to the Gold summary\nHighlights the importance of key words in finding salient sentences for extractive summaries\n\nSlide Topic: Conclusion\nSummary:\nWe develop SWAP-NET, a neural sequence-to- sequence model for extractive summarization\nBy effective modelling of interactions between sentences and key words,\nSWAP- NET outperforms state-of-the-art extractive single-document summarizers\nSWAP-NET models these interactions using a new two-level pointer network based architecture with a switching mechanism\nExperiments suggest that modelling sentence-keyword interaction has the desirable property of less semantic redundancy in summaries generated by SWAP-NET\nAn implementation of SWAP-NET and generated summaries from the test sets are available online: https://github.com/aishj10/swap-net\n\n",
    "Slide Topic: Story\nSummary:\n3. Conclusions and Future Work\nRepresentations as Multimodal Embeddings. AAAI\nLearn mapping f text vision.\nFinding 1: Imagined vectors, f (text), outperform original visual vectors in 7/7 word similarity tasks.\nSo, why are mapped vectors multimodal? We conjecture:\nContinuity. Output vector is nothing but the input vector transformed by a continuous map: f (x x\nFinding 2 (not in AAAI paper): Vectors imagined with an untrained network do even better.\n\nSlide Topic: Motivation\nSummary:\n3. Conclusions and Future Work\nApplications (e.g., zero-shot image tagging, zero-shot translation or cross-modal retrieval):\nUse linear or NN maps to bridge modalities / spaces.\nThen, they tag / translate based on neighborhood structure of mapped vectors f (X\nResearch question: Is the neighborhood structure of f (X similar to that of Y? Or rather to X?\nHow to measure similarity of 2 sets of vectors from different spaces? Idea: mean nearest neighbor overlap\n\nSlide Topic: General Setting\nSummary:\n3. Conclusions and Future Work\nMappings f X Y to bridge modalities X and Y:\n\nSlide Topic: Experiment 1\nSummary:\n1. Motivation and Setting\n3. Conclusions and Future Work\nNearest Neighbor Overlap (NNOK (vi zi)) = number of K nearest neighbors that two paired data points vi zi share in their respective spaces.\nThe mean NNO is:\nmNNOK (V ,Z KN NNOK (vi zi)\nNN3(vcat) = {vdog vtiger vlion}\nNN3(zcat) = {zmouse, ztiger zlion} NNO3(vcat zcat) =\nGoal: Learn map f X Y and calculate mNNO(Y f (X )). Compare it with mNNO(X f (X\nDatasets: (i) ImageNet ; (ii) IAPR TC-12; (iii) Wikipedia\nVisual features: VGG-128 and ResNet.\nText features: ImageNet (GloVe and word2vec); IAPR\nLoss: MSE 12f (x) y2. We also tried max-margin and cosine.\n\nSlide Topic: Experiment 1 Results\nSummary:\n1. Motivation and Setting\n3. Conclusions and Future Work\nX f (X Y f (X X f (X Y f (X\nI T T I lin nn\nTable: X f (X and Y f (X denote mNNO10(X f (X and\n\nSlide Topic: Experiment 2\nSummary:\n1. Motivation and Setting\n3. Conclusions and Future Work\nGoal: Map X with an untrained net f and compare performance of X with that of f (X\n(i) Semantic similarity: SemSim, Simlex-999 and\n(ii) Relatedness: MEN and WordSim-353.\n(iii) Visual similarity: VisSim.\n\nSlide Topic: Experiment 2 Results\nSummary:\n1. Motivation and Setting\n3. Conclusions and Future Work\nCos Eucl Cos Eucl Cos Eucl\nTable: Spearman correlations between human ratings and similarities (cosine or Euclidean) predicted from embeddings.\n\nSlide Topic: Conclusions and Future Work\nSummary:\n1. Motivation and Setting\nNeighborhood structure of f (X more similar to X than Y\nNeighborhood structure of embeddings not significantly disrupted by mapping them with an untrained net.\nFuture Work: How to mitigate the problem?\nDiscriminator (adversarial) trying to guess whether the sample is from Y or f (X\nIncorporate pairwise similarities into loss function.\n\n",
    "Slide Topic: Complaints\nSummary:\n| wish | had more time to tell you about complaining, but the organizers only allocated 15 minutes for this talk.\n2019 Bloomberg Finance L.P. All rights reserved. . : Engineering\n\nSlide Topic: Complaints Applications\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Data Annotation\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Data Sampling\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Data Statistics\nSummary:\nNot complaints, random tweets\nNot complaints, sent to other handles\nNot complaints, sent to customer support\n2019 Bloomberg Finance L.P. All rights reserved.\nweets matched to a domain based on the customer support handle\nFood Apparel Retail Cars Services Software Transport Electronics Other Complaint Not Complaint\n\nSlide Topic: Features\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Analysis Complaints\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Analysis Not Complaints\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Prediction\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\nMost Freq Class Complaint Specific Sentiment Emotions POS Tags LIWC Word2Vec Clusters Unigrams Combined MLP BiLSTM\n\nSlide Topic: Prediction Other Experiments\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\nSlide Topic: Takeaways\nSummary:\n2019 Bloomberg Finance L.P. All rights reserved.\n\n",
    "Slide Topic: Objective\nSummary:\nWhat geometric properties of an embedding space are important for performance on a given task?\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\nUnderstand utility of embeddings as input features.\nProvide direction for future work in training and tuning embeddings.\n\nSlide Topic: Embedding space\nSummary:\nIn NLP, the term embedding is often used to denote both a map and (an element of) its image.\nWe define an embedding space as a set of word vectors in Rd.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Geometric properties\nSummary:\nWe consider the following attributes of word embedding geometry: position relative to the origin; distribution of feature values in Rd; global pairwise distances; local pairwise distances.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Our approach\nSummary:\nWe transform the embedding space such that we expose only a subset of the stated properties to downstream models.\nposition relative to the origin; distribution of feature values in Rd; global pairwise distances; local pairwise distances.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Affine\nSummary:\npos. relative to the origin distribution of features global distances local distances\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Cosine distance embedding CDE\nSummary:\nd embedding dimension (300);\n|V distance vector dimension (104 most\npos. relative to the origin distribution of features global distances local distances\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Nearest neighbor embedding NNE\nSummary:\npos. relative to the origin distribution of features global distances local distances\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Hierarchy of transformations\nSummary:\nOrdering is with respect to number of properties ablated.\nWe include a random baseline of meaningless vectors.\nArrow length does not mean anything.\nTransformations are applied independently to the original embeddings.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Embeddings and Tasks\nSummary:\nWord2Vec on Google news;\nGloVe on common crawl;\n10 standard intrinsic tasks.\n5 extrinsic tasks (embeddings plugged into a downstream machine learning model).\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Tasks\nSummary:\nWord Similarity and Relatedness via cosine distance\nSentence-level sentiment polarity classif. on MR movie reviews\nSentiment classif. on IMDB reviews\nSubj./Obj. classif. on Rotten\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Results intrinsic tasks\nSummary:\nWe see the lowest performance on thresholded-NNE.\nLargest drop in performance at\nCDE (written as distAE on the\nRotations, dilations, and reflections are innocuous.\nDisplacing the origin has a nontrivial effect.\nNNE causes a significant drop in performance as well.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Results extrinsic tasks\nSummary:\nCDE is still the largest drop.\nNNE recover most of the losses, and are on par with affines.\nExtrinsic tasks are more robust to translations, but not homotheties.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Discussion\nSummary:\nDrop due to CDE likely associated with the importance of locality in embedding learning.\nWith thresholded-NNE, high out-degree words are rare words, introducing noise during node2vecs random walk.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Takeaways\nSummary:\nWe find that in general, both intrinsic and extrinsic models rely heavily on local similarity, as opposed to global distance information.\nWe also find that intrinsic models are more sensitive to absolute position than extrinsic ones.\nMethods for tuning and training should focus on local geometric structure in Rd.\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\nSlide Topic: Questions\nSummary:\nWhitaker, Newman-Griffis, Haldar, et al. Characterizing Embedding Geometry June 4, 2019\n\n",
    "Slide Topic: Twitter for Public health\nSummary:\nMany users tweet when they caught a disease\n# of tweets is in proportion to # of flu patients\n# of flu related tweets\n\nSlide Topic: Noise included in tweets\nSummary:\nFor more information about bird flu link\nI got a flu I couldnt do anymore Only counts this type of tweets\nIve never caught a flu\nI got a flu shot yesterday\n\nSlide Topic: Our lab runs flu surveillance system\nSummary:\nae 47S A ~ NLP Flu Warning ~\nBIvILY FOr a-ARAMHSRORVRERN TS.\n() (CELA(5) RE MROKIAAORS MR, TLIL-mROMBAIC https://t.co/CsRvdgSL4v #RB HAR #Xvb A/09-7 IDR HAY DID\ne>) AVINLYEOANW ADT, MMA CEAN?HHBAY FIDL BN EY +\nAv FAI tiweett MRE AREY YAN\nAramaki, Eiji, Sachiko Maskawa, and Mizuki Morita. \"Twitter catches the flu: detecting influenza epidemics using Twitter.\" In Proc of EMNLP 2011. http://mednlp.jp/influ_map/\n\nSlide Topic: Similarity between Tweets and Patients\nSummary:\nME X2ieys(fh):Ad Tweets about flu\nis slightly earlier than reports of flu in patients\n\nSlide Topic: Each word has a specific time lag\nSummary:\nCounts of flu related tweets of flu patients\nThe word Fever The word Injection days time lag days time lag\n# of the word fever # of the word Injection\nTime shifted Time shifted\n\nSlide Topic: What is Forecasting Words\nSummary:\nTwitter tends to be an early indicator of actual condition\nWe observed that each word has a specific time lag with actual condition\nOur objective: more flexible modeling\nExtend future forecasting model\n\nSlide Topic: Training data Twitter Corpus\nSummary:\nQuery: The word flu in Japanese\n\nSlide Topic: Gold standard IDSC reports\nSummary:\nInfectious Disease Surveillance Center (IDSC) reports\n# of flu patients once a week\nThey gather the number of flu patients during the period of epidemic\nWe split IDSC reports into three seasons as follows:\n\nSlide Topic: Time lag measure Cross Correlation\nSummary:\nCross Correlation is used to search for the most suitable time shift width for each word frequency as between # of tweets days before and # of actual patients\nThe cross correlation is exactly the same as the Pearsons correlation when\n\nSlide Topic: Motivating examples\nSummary:\nWhen = 16, r is 0.95 B/T tweet and IDSC reports\n# of the word fever\n# of flu patients\nWhen increases, word counts moves to right side:\n\nSlide Topic: Estimate optimal time lag\nSummary:\nWe define optimal time-lag by maximizing the cross correlation\n\nSlide Topic: Heatmap representation of Matrix\nSummary:\nR a w w o rd c o un t s # of patients\nApply t i m e s hift\nX y X y\n\nSlide Topic: Effectiveness of time shift\nSummary:\nRegression for nowcasting with applying time-shift or not:\nThe searching range of time shift is in [0, , 60]\nTrain Season 2 Season 3 Season 1 Season 3 Season 1 Season 2 Avg. Test Season 1 Season 2 Season 3\n\nSlide Topic: Limitation\nSummary:\nTo estimate specific day of the epidemic through\nTwitter, we need to gather same days tweet\nHow to predict future disease outbreaking?\n# of flu related tweets\n# of flu patients\n\nSlide Topic: Restrict time shift estimation\nSummary:\nIn order to forecast t days future epidemics,\nwe restrict searching interval of time shift at least t days\n\nSlide Topic: Motivating example\nSummary:\nNowcasting case: [0, max]\n# of the word fever (10 days shifted) # of the word # of flu fe patients ver (10 d ays shifted)\n# of the word fever (16 days shifted) # of flu patients\n# of the word # of th fever word fever\nof the # of word the word fever fever of the # of wo fl u rd patients fever 30 days shifted) of flu patients\n# of the word Injection (30 days shifted) # of the word # of flu In patients jection (3 0 days shifted)\n# of the word # of the Injection word Injection\n# of the word Injection (55 days shifted) # of flu patients\n\nSlide Topic: Forecasting Modeling\nSummary:\nIn each t, we search optimal time shift for all words.\nEstimate model by Lasso & ENet using these features.\n\nSlide Topic: Our model beyonds baseline\nSummary:\nCorrelation b/w model and IDSC Correlation b/w model and IDSC Correlation b/w model and IDSC\nMinimum Time-Lag Tnin Minimum Time-Lag Tinin Minimum Time-Lag Tinin\ng Lasso 3 B06 Lasso c Elastic-Net Elastic-Net 5 BaseLine 5 5 0.44 BaseLine c c Lasso c % Elastic-Net 0.2 o o BaseLi 3 5 aseLine 5 oO oO oO 0.0\nBase Line: Yrest (t) = Ytrain(t) * Higher is better\n\nSlide Topic: Summary\nSummary:\nWe discovered the time difference between twitter and actual phenomena.\nWe proposed but handling such difference to improve the nowcasting performance and extend for forecasting model.\nOur method is widely applicable for other time series data which has time-lag between response and predictors.\nCode and Data available at http://sociocom.jp/~iso/forecastword\n\n",
    "Slide Topic: Motivation\nSummary:\nModeling coherence in linguistics theory into computational task (Barzilay & Lapata,\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Coherence\nSummary:\nCoherent text is integrated as a whole, rather than a series\nEvery sentence in a coherent text has relation(s) to each other (Halliday and Hasan, 1976; Mann and Thompson,\nEvaluate coherence through cohesion\nLexical and semantic (meaning) continuity are indispensable\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: RelatedWork Entity Graph\nSummary:\nEntity graph was introduced by Guinaudeau &\nText -> Bipartite Graph -> Projection Graphs\nCoherence is achieved by cohesion: considers repeated mention of entities and their syntactical role (weight)\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\nGraph data structure can represent the structure of text and relations among sentences\nCoherence is achieved through lexical cohesion: repeated mention of entities.\nDisadvantage: cannot capture the relation between related-yet-not identical entities (Li and Hovy,\nSolution: use distributed representation of words/sentences\nRelation between vertices in projection graph has to satisfy surface sequential ordering\nProposal: allows two directions (omit the constraint)\n\nSlide Topic: Proposed Method\nSummary:\nFormally, text is a graph , where\nis a set of vertices, represents i-th sentence. is a set of edges, represents relation (cohesion) from i-th to j-th sentence (weighted & directed).\nEvaluate the coherence through cohesion\nSentences are encoded into their meaning form\nAverage of summation of word vectors (distributed representation of words)\nAn edge represents cohesion among sentences\nEstablishment of edge is decided as the operation of vectors representation of sentences\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\nAn edge is established from the sentence vertex in question to the other vertex with the weight calculated by\nText coherence measure (higher is better) is calculated by averaging the averaged weight of outgoing edges from every vertex in the graph as\n# vertices # outgoing edges of vertex vi\n\nSlide Topic: Propose Method\nSummary:\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Evaluation\nSummary:\nTask 1: Discrimination (Barzilay and Lapata, 2008)\nTask 2: Insertion (Eisner and Charniak, 2011)\nBoth tasks evaluate how well the methods in comparing coherence between texts\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Evaluation Discrimination Task\nSummary:\nThe goal is to compare original vs. permutated text S4\nProgram is considered successful when giving greater score to the more coherent (original) text\nDataset: 683 WSJ (LDC) texts, permutations (avg. 24 sentences, 521 tokens)\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Result Discrimination Task\nSummary:\nDifference of performance is statistically significant at\nPAV > MSV > Entity Graph\nCohesion is not only about repeating mention of entities\nPAV MSV pair shares 88.3% same judgement\nLocal (adjacent) cohesion is possibly more important than long-distance cohesion\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Evaluation Insertion Task\nSummary:\nInsertion task is more important than discrimination task\nIt was proposed by Eisner and Charniak (2011):\nGiven a text, take out a sentence (randomly), then place it into other positions\nProgram is considered successful if it prefers to insert take-out-sentence at its original position rather than arbitrary (distorted) positions\nOur Proposal: useTOEFL iBT insertion-type questions\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: TOEFL iBT Insertion type Question\nSummary:\nA text is coherent even without the insertion sentence\nPreservation of coherence is achieved when the question-sentence is inserted\n(A) The raising of livestock is a major economic activity in semiarid lands, where grasses are generally the dominant type of natural vegetation.\n(B) The consequences of an excessive number of livestock grazing in an area are the reduction of the vegetation cover and trampling and pulverization of the soil. (C) This is usually followed by the drying of the soil and accelerated erosion. (D)\nin the correct place coherence otherwise but disrupt\nQuestion: Insert the following sentence into one of\nquestion sentence = \"This economic reliance on livestock in certain regions makes large tracts of land susceptible to overgrazing.\ncorrect answer = B\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Result Insertion Task\nSummary:\nDifference in every pair of methods is not statistically significant at p < 0.05\n14 questions are answered incorrectly by PAV, but correctly by SSV.\nIn these questions, SSV tends to establish the relationship between distance sentences (dist = 2.8). For example, exemplification text\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\nSlide Topic: Conclusion and FutureWork\nSummary:\nCoherence can be achieved through cohesion (lexical and semantic continuity)\nLocal cohesion is more important than long-distance cohesion coherence, but long-distance cohesion can also contribute as well in evaluating\nWe need to introduce a more refined mechanism for incorporating distant sentence relations.\nThe representation of sentences and method to establish edges would be direct targets of the refinement\nSemantic Similarity Graph | wiragotama.github.io TextGraph-11, ACL 2017\n\n",
    "Slide Topic: Motivation\nSummary:\nNeural question answering (QA) systems are end-to-end trainable machine learning models which achieve top performance in domains with large training datasets\nWe apply an extractive neural QA system (FastQA [1]) to BioASQ 5B\nPhase B (list & factoid questions)\nExtractive QA: Answer is given as start and end pointers in the context (snippets)\n\nSlide Topic: Network Architecture\nSummary:\nOriginal FastQA [1] Our Architecture\nChange start probab ility activation from softma x to sig moid\nMultiple s tarts ca n be selected for list questions\nFor each sele cted st art, select the correspondin g end p ointer via softmax\n\nSlide Topic: Training Procedure\nSummary:\nProblem: Neural QA typically requires ~105 questions to train\nDatasets of such scale exist in the open domain, e.g. SQuAD [2] with factoid questions on Wikipedia articles\nWe train in two steps:\nPre-training on a large (~105 questions) open-domain dataset (SQuAD)\nFine-tuning on BioASQ (~103 questions)\n\nSlide Topic: Systems\nSummary:\nWe trained five models using 5-fold cross validation on all available training data\nWe submitted two systems:\nSingle: Best single model according to its respective development set\nEnsemble: Ensemble of all five models (averaging scores before sigmoid/softmax activation)\n\nSlide Topic: Results\nSummary:\nOur system won 3/5 batches\nAveraged over the five batches, our system\npercentage points above the best competitor\nOn average, the best competitor performed 3.4 percentage points better than our ensemble model\n\nSlide Topic: Discussion\nSummary:\nStrengths: Competitive performance, despite:\nLess feature engineering than traditional QA systems\nA less domain-dependent architecture, because we dont rely on domain-specific structured resources\nExtractive QA cannot generate answer which are not explicitly mentioned in the snippets\nNo yes/no & summary questions\n\n",
    "Slide Topic: Stance Classification in Tweets\nSummary:\nAutomatically identify users positions on a pre-chosen target of interest (e.g., public issues) from text\nTarget (given): Climate Change is Real Concern\nTweet (given): We need to protect our islands and stop the destruction of coral reef.\n(Output) Stance label (to be predicted): Favour\n\nSlide Topic: Cross Target Stance Classification\nSummary:\nGeneralise user stance on unseen targets\nTarget: A mining project in Australia (Destination)\nTweet: Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef.\nApply classifiers trained on a source target to the destination target\nTarget: Climate Change is Real Concern (Source)\nTweet: We need to protect our islands and stop the destruction of coral reef.\n\nSlide Topic: Our Approach Basic Idea\nSummary:\nFor targets both related to a common domain, stance generalisation is possible via domain-specific information that reflects users major concerns\nTweet: Environmentalists warn the $16 billion coal facility will damage the Great Barrier Reef.\nTweet: We need to protect our islands and stop the destruction of coral reef.\nTarget: A mining project in Australia\nTarget: Climate Change is Real Concern\nDestination target Source target\nDomain aspects: e.g., reef, destruction/damage\n\nSlide Topic: Extraction of Domain Aspects\nSummary:\nKey properties of domain aspects\nThey tend to be mentioned by multiple users in a corpus\nThey tend to carry the core meaning of a stance-bearing tweet\nIn our project dataset, 3776 our of 41805 tweets mentioned the aspect reef\nwhy fund Adani #Coal Mine and destroy our Reef when theres so much sun in Queensland?\nAnd your massive polluting Carmichael mine will do its bit to kill Australia's great barrier reef?\nAnd thousands of jobs will be lost in reef tourism when Adani goes ahead.\nThe coral reef crisis is actually a crisis of governance.\n\nSlide Topic: A Self Attention Neural Model Overview\nSummary:\nAspect-aware & target-dependent sentence encoding\nThe simplest case: source- side-only model\n\nSlide Topic: Context Encoding Layer\nSummary:\nConditional sentence encoding [Augenstein et al., 2016]: Learn a target-dependent representation for the sentence\n\nSlide Topic: Aspect Attention Layer\nSummary:\nExtract domain aspect words using self-attention weighting\nAttention weights on word positions : the importance in carrying the sentence meaning\nsentence We need to protect destruction of coral reef\nCompatibility function semantic similarity\nword position We need to of coral reef\n\nSlide Topic: Experiments\nSummary:\nSemEval 2016 Task 6: Twitter stance detection\nClimate Change is Concern\n1. Womens Rights: Feminist Movement Legalisation of Abortion\n2. American Politics: Hillary Clinton Donald Trump\n\nSlide Topic: Classification performance\nSummary:\nExtracted domain aspects benefit cross-target task more\nBetter performance on both tasks across almost all targets\nLA: Legalization of Abortion HC: Hillary Clinton DT: Donald Trump CC: Climate Change is Concern\n\nSlide Topic: Visualisation of attention\nSummary:\nThe heatmap of the attention weights assigned to some tweet examples\nFM: Feminist Movement A: Against\nLA: Legalization of Abortion F: Favour\nHC: Hillary Clinton Words central to expressing stances\nDT: Donald Trump CC: Climate Change is Concern AMP: Australian mining project\nare highlighted by our model!\n\nSlide Topic: Conclusion\nSummary:\nA self-attention model which can attend high-level information about the domain for stance generalisation\nDomain aspect words are useful to determine the user stance\nIncorporation of target divergence into our modelling.\nLearning aspects from multiple sources (e.g., environment, community, and economics aspects for mining projects)\n\n",
    "Slide Topic: Introduction\nSummary:\nI Sentence acceptability: the extent to which a sentence is natural to native\nI It encompasses semantic, syntactic and pragmatic plausibility and other\nnon-linguistic factors such as memory limitation.\nI Grammaticality, by contrast, is a theoretical concept that measures the syntactic\nwell-formedness of a sentence.\nI Here we are interested in predicting acceptability judgements.\n\nSlide Topic: Motivation\nSummary:\nI We previously explored using unsupervised probabilistic methods to predict\nsentence acceptability, and found some success.\nI It provides evidence that linguistic knowledge can be represented as a probabilistic\nsystem, addressing foundational questions concerning the categorical nature of grammatical knowledge.\n\nSlide Topic: Acceptability in Context\nSummary:\nI In previous experiments sentence acceptability was judged (by humans) or\npredicted (by models) independently of context.\nI Here we extend the research to investigate the impact of context on acceptability.\nI Context is defined as the full document environment surrounding a sentence.\nI Specifically, we want to understand the influence of context on:\nI Human acceptability ratings\nI Model prediction of acceptability\n\nSlide Topic: Human Acceptability Ratings in Context\nSummary:\nI We perform round-trip translation of sentences (e.g. ENFREN) from English\nWikipedia to generate a set of sentences with varying degrees of acceptability.\nI We use MTurk to collect acceptability judgements (rated on a 4-point scale).\nI Annotation task was run twice: first without context, and second within the\nI We collect multiple ratings for a sentence and take the mean.\nI Human acceptability ratings:\nI without context = h;\nI with context = h+\n\nSlide Topic: With context h Against Without context h Ratings\nSummary:\nmean h per sentence\n\nSlide Topic: Observations\nSummary:\nI Pearsons r = 0.80 between h+ and h.\nI Context boosts acceptability ratings most for ill-formed sentences.\nI Surprisingly, context reduces acceptability for the most acceptable sentences.\nI Context compresses distribution of ratings.\nI One-vs-rest correlation, performance of a single annotator against the rest: 0.628\nI Low correlation is explained by the compression effect of context - good and bad\nsentences are now less separable.\n\nSlide Topic: Modelling Acceptability with Unsupervised Models\nSummary:\nI lstm: standard LSTM language model\nI tdlm: a topically driven language model; language model is driven by a topic\nvector automatically learnt on the document context.\nI 4 variants at test time:\nI Use only the sentence as input: lstm and tdlm;\nI Use both sentence and context as input: lstm+ and tdlm+.\nI lstm+ incorporates context by feeding it to the LSTM network and taking the\nf inal state as the initial state for the current sentence.\nI Models trained on 100K English Wikipedia articles (40M tokens).\n\nSlide Topic: Acceptability Measures\nSummary:\nI P = probability of the sentence given by a model;\nI U = unigram probability of the sentence;\nI L = sentence length\n\nSlide Topic: Results\nSummary:\nlstm lstm+ tdlm tdlm+\nI Across all models (lstm or tdlm) and human ratings (h or h+), using context at\ntest time improves performance.\nI tdlm consistently outperforms lstm (even tdlm lstm+).\nI Lower correlation when predicting sentence acceptability judged with context.\nI It suggests h+ ratings are more difficult to predict than h, which corresponds to\nthe low one-vs-rest human performance.\n\nSlide Topic: Summary\nSummary:\nI Context positively influences acceptability, particularly for ill-formed sentences.\nI But it also has the reverse effect for well-formed sentences.\nI Incorporating context (during training or testing) helps modelling acceptability.\nI Prediction performance declines when tested on acceptability ratings judged with\ncontext, due to the compression effect of ratings.\nI Future work: investigate why context reduces acceptability for highly acceptable\n\n",
    "Slide Topic: STARTING FROM THE END spoiler\nSummary:\nthe Indo-European phylogenetic tree\n(the ground truth) phylogenetic tree reconstructed from monolingual English texts translated from\nFrench Italian Spanish Portuguese Latvian Lithuanian Polish Slovak Czech Slovenian Bulgarian\nSwedish Danish Romanian Lithuanian Portuguese Czech Slovak Bulgarian Latvian Polish Slovenian\n\nSlide Topic: Background the features of translationese\nSummary:\nTranslators (almost) always tried to remain invisible\nTranslations have unique characteristics that set them apart from originals\nUniversals (simplification, standardization, explicitation)\nInterference (the fingerprints of a source language on the translation product)\nLanguages closer to each other are likely to share more\nfeatures in the target language of translation\nThe distance between languages is retained and can be recovered when assessed through these features in\n\nSlide Topic: Dataset\nSummary:\nEuroparl (the proceedings of the European Parliament)\nMembers are allowed to speak in any of the EU languages\nAll parliament speeches were translated from the original language into other EU languages using English as a pivot\nDirect translations into English, indirect translations into all other languages\nWe explore indirect translations into French in this work\nWe focus on 17 source languages, grouped into 3 language families\nGermanic, Romance, and Balto-Slavic\n\nSlide Topic: Reconstruction of language trees\nSummary:\nPOS-trigrams, reflecting shallow syntactic structures\n(strongly associated with interference)\nFunction words, reflecting grammar (associated with interference)\nCohesive markers (associated with a translation universals)\nAGGLOMERATIVE (HIERARCHICAL) CLUSTERING OF FEATURE VECTORS\nUsing the variance minimization algorithm (Ward, 1963)\nPhylogenetic language trees generated with translated text\nRomanian Lithuanian Portuguese Czech Slovak Bulgarian Latvian Polish Slovenian\nSlovak Lithuanian Latvian Bulgarian Romanian Slovenian Portuguese Polish Czech\nENGLISH translations FRENCH translations\n\nSlide Topic: Identification of translationese and its source language\nSummary:\nMATRIX source-language classification (POS-trigrams)\n\nSlide Topic: Evaluation methodology\nSummary:\nMEASURE SIMILARITY TO THE GOLD STANDARD\n(topological) similarity assessing similarity based on both structure and branching length\nAdaptation of the L2-norm to leaf-pair distance\nSuitable for both weighted and unweighted evaluation\ng t the gold tree a tree subject to evaluation\nC Dt(li, lj) distance between two leaves in a tree D\n\nSlide Topic: Evaluation results\nSummary:\nDISTANCE OF A RECONSTRUCTED TREE FROM THE GOLD STANDARD\n(using various feature sets)\nHTD VA ON D E\nfeature AVG STD AVG STD feature AVG STD AVG STD\nPOS-trigrams + FW POS-trigrams + FW\nFunction words Function words\nCohesive markers Cohesive markers\ntrees built from English translations are systematically closer to the gold standard than trees built from translations into French (done via a third language) the quality of trees increases for feature sets associated with interference the worst tree is generated using cohesive markers\nFOUD IN RAN N: R RUCTNG P NSLAT\n\nSlide Topic: Analysis\nSummary:\nIndefinite (a, an) and definite (the)\nWith clitic s (the guests room)\nWith a prepositional phrase containing of (the room of the guest)\nWith noun compounds (guest room)\nVerbs that combine with a particle to create a new meaning (MWEs), e.g., turn down, get over\nWith the auxiliary verbs have (present) or be (progressive), e.g., have done, was going\nFREQUENCIES reflecting various linguistic phenomena in English translations\ndefine articles (per 10 tokens) of' constructions (per 25 tokens) verb-particle (per 250 tokens) perfect (per 100 tokens) progressive (per 500 tokens)\n\nSlide Topic: Summary\nSummary:\nTranslation does not distorts the original text randomly\nA phylogenetic language tree can be reconstructed from monolingual texts translated from various languages\nFeatures associated with interference (POS-ngrams, FWs) yield more accurate phylogenetic language trees\nTranslations impact the evolution of languages\nIt is estimated that for certain languages up to 30% of published texts are mediated through translations (Pym and Chrupaa, 2005)\nAre translations likely to play a role in language change?\n\nSlide Topic: Starting from the end\nSummary:\nphylogenetic tree reconstructed from monolingual English texts translated from\n17 IE languages phylogenetic tree reconstructed from monolingual French texts translated indirectly from 17 IE languages via English pivot\nDanish Romanian Lithuanian Portuguese Czech Slovak Bulgarian Latvian Polish Slovenian\nEnglish Slovak Lithuanian Latvian Bulgarian Romanian Slovenian Portuguese Polish Czech\n\n",
    "Slide Topic: Motivation Model senses instead of only words\nSummary:\nHe withdrew money from the bank.\n\nSlide Topic: Related Work\nSummary:\nLearn sense embeddings exploiting text corpora only (Huang et al.\nand Jurafsky, EMNLP 2015...). Easily adaptable to new domains.\nSenses not interpretable (+change from model to model)\nKnowledge from resources cannot be easily exploited\nSenses (esp. not frequent ones) not easy to discriminate\nModel senses as defined on a sense inventory.\nUsually obtained as a postprocessing of word embeddings\nInfrequent senses not accurately captured\nKnowledge-based sense embeddings (Our approach)\n\nSlide Topic: Idea\nSummary:\nA word is the surface form of a sense: we can exploit this intrinsic relationship for jointly training word and sense embeddings.\nUpdating the representation of associated senses interchangeably. the word and its\n\nSlide Topic: Methodology\nSummary:\nGiven as input a corpus and a semantic network:\n1. Use a semantic network to link to each word its associated senses in context.\nHe withdrew money from the bank.\n2. Use a neural network where the update of word and sense embeddings is linked, exploiting virtual connections.\nHe withdrew from the bank\nIn this way it is possible to learn word and sense/synset embeddings jointly on a single training.\n\nSlide Topic: Methodology Linking words and senses in context\nSummary:\nHe withdrew money from the bank\ntake out financial institution\nGraph-based representation of the sentence using semantic networks\n\nSlide Topic: Methodology Joint training of words and sense embeddings\nSummary:\nOnce each word is connected to its set of senses in context, it is possible to modify standard word embedding architectures to take into account this information.\nIn this work we explore the CBOW architecture of Word2Vec\nOther neural network architectures could be explored as well\n(Skip-gram also included in the code).\n\nSlide Topic: Full architecture of W2V Mikolov et al 2013\nSummary:\nWords and associated senses used both as input and output.\n\nSlide Topic: Full architecture of SW2V this work\nSummary:\nWords and associated senses used both as input and output.\n\nSlide Topic: Output layer alternatives only words\nSummary:\nThe architecture does not try to predict senses. No loss contribution from them.\n\nSlide Topic: Output layer alternatives only senses\nSummary:\nThe architecture does not try to predict words. No loss contribution from them.\n\nSlide Topic: Input layer alternatives only words\nSummary:\nSenses are not included in the input layer. Only words contribute to the hidden state.\nThis way, during backpropagation sense embeddings do not receive any gradient.\nDuring backpropagation, sense embeddings will receive the same gradient of the word they are associated with.\n\nSlide Topic: Input layer alternatives only senses\nSummary:\nWords are not included in the input layer. Only senses contribute to the hidden state.\nThis way, during backpropagation word embeddings do not receive any gradient.\nDuring backpropagation, their embeddings will receive the same gradient of their associated senses.\n\nSlide Topic: Analysis Model configurations\nSummary:\nWe used word similarity for analyzing the performance of sense embeddings on each of the nine configurations.\n- Best configuration -\nInput layer: Only senses\nOutput layer: Both words and senses\nWhy? (Intuition) Co-occurrence information gets duplicated if both words and senses are included in the input layer.\n\nSlide Topic: Evaluation Experimental setting\nSummary:\nBest configuration used in all experiments\nSemantic networks used: WordNet and BabelNet\nCorpora used: UMBC and Wikipedia\nWord and sense interconnectivity (qualitative)\n\nSlide Topic: Evaluation Comparison systems\nSummary:\nAutoExtend (Rothe and Schutze, 2015)\nSensEmbed (Iacobacci et al. 2015)\nNASARI (Camacho-Collados et al. 2016)\nRetrofitting (Faruqui et al. 2015)\n\nSlide Topic: Evaluation Word and sense interconnectivity\nSummary:\nHow coherent is the shared vector space of word and sense embeddings?\nIntuition: the Most Frequent Sense (MFS) should be close to the word embedding -> Reasonably strong MFS baseline for WSD\nEvaluation on two WSD datasets using the embeddings as a\nMFS baseline (closest sense embedding to its associated word embedding is selected).\n\nSlide Topic: Word and sense interconnectivity Example I\nSummary:\nTen closest word and sense embeddings to the sense company (military unit)\n\nSlide Topic: Word and sense interconnectivity Example II\nSummary:\nTen closest word and sense embeddings to the sense school (group of fish)\n\nSlide Topic: Evaluation Word similarity\nSummary:\nAll models using Wikipedia corpus (Pearson correlation)\nAll models using UMBC corpus (Pearson correlation)\n\nSlide Topic: Evaluation Sense clustering\nSummary:\nSome sense inventories make a fine-grained distinction between senses, which can be harmful on downstream applications (Hovy\nEvaluation datasets (Dandala et al. 2013): Highly ambiguous words from past SemEval competitions.\nEmbedding Words and Senses Together via Joint Knowledge-Enhanced training Massimiliano Mancini, Jose Camacho-Collados, Ignacio lacobacci and Roberto Navigli\n\nSlide Topic: Conclusion\nSummary:\nW e presented SW2V: a neural architecture for jointly learning word and sense embeddings in the same vector space using text corpora and knowled ge obtained from semantic networks.\nFu ture wor k:\nExploitin g our model for other linked representations such as multilingual or Image-to-Text embeddings.\nWord Sense Disambiguation and Entity Linking.\n- Integrating our embeddings into downstream NLP applications, following the lines of Pilehvar et al. (ACL 2017).\n\n",
    "Slide Topic: Introduction\nSummary:\nTheres always a bone to pick on MT evaluation metrics (Babych and Hartley, 2004; Callison-\nAppeared calm when he was taken to the American plane , which will to\nReference: Orejuela appeared calm as he was led to the American plane which will take him to Miami , Florida .\nwhich will he was , when taken Appeared calm to the American plane to\nlower BLEU not necessarily worse translation\nBut is higher BLEU = better translation true?\nCallison-Burch et al. (2006) meta-evaluation on 2005 NIST MT Eval\n\nSlide Topic: Bleu\nSummary:\nPenalize if the length of the Count the proportion of n-grams that hypothesis is too long appears in hypothesis and reference\n\nSlide Topic: BLEU in practice\nSummary:\nPenalize if the length of the Count the proportion of n-grams that hypothesis is too long appears in hypothesis and reference\n\nSlide Topic: Segment Level HUMAN\nSummary:\nBaseline: #Hyp 7 #Base\nSource/Reference English Gloss: Tmelt (DSC) = 8 9. 9 C; Teryst (DSC) = 7 C (measured using DSC at 5 C / min) re\n\nSlide Topic: Results\nSummary:\nhigher BLEU = better translation is not always true.\n\nSlide Topic: Segment level Meta Evaluation\nSummary:\nDifference in Segment level RIBES (Hypothesis - Baseline RIBES)\nAn interactive graph can be found here: https://plot.ly/171/~alvations/ (Hint: click on the bubbles here on the interactive graph\nGenerally, -BLEU or RIBES from baseline means worse translations\nNote that the grey bubbles are the same as the previous graph\nIts more prominent here since there are many more instances of +BLEU with 0 HUMAN score than negative HUMAN score\nWith regards to positive HUMAN scores, it fits the conventional wisdom that\nlower BLEU/RIBES = worse translation\nHigher BLEU/RIBES = better translation\nWhen it comes to negative HUMAN scores, it is inconsistent with the conventional wisdom\n\nSlide Topic: Conclusion\nSummary:\nHigher BLEU and RIBES doesnt necessary mean better translations\nAt segment level, >+30 BLEU might not be reliable\nPossible reasons for BLEU/RIBES to not correlate with human judgments includes:\nMinor lexical differences -> huge difference in n-gram precision\nMinor MT evaluation metric differences not reflecting major translation inadequacy\n\n",
    "Slide Topic: Introduction What is BioASQ\nSummary:\nI BioASQ is a series of challenges on biomedical semantic\nindexing and question answering (QA).\nI Participants are required to semantically index content from\nlarge-scale biomedical resources (e.g. MEDLINE) and/or\nI to assemble data from multiple heterogeneous sources (e.g.\nscientific articles, knowledge bases, databases)\nI to compose informative answers to biomedical natural\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Presentation of the challenge Tasks\nSummary:\nTask A: Hierarchical text classification\nI Organizers distribute new unclassified MEDLINE articles.\nI Participants have 21 hours to assign MeSH terms to the articles.\nI Evaluation based on annotations of MEDLINE curators.\n1st batch 2nd batch 3rd batch End of Task5a\nry ry ry rch rch rch rch rch ril ril br br br Ma Ma Ma Ma ua ua ua Ma y 0 Ma y 0 Ma Ap y 1 Ma Ap Ap ril y 2 Ma Fe Fe Fe\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\nTask B: IR, QA, summarization\nI Organizers distribute English biomedical questions.\nI Participants have 24 hours to provide: relevant articles,\nsnippets, concepts, triples, exact answers, ideal answers.\nI Evaluation: both automatic (GMAP, MRR, Rouge etc.) and\nmanual (by biomedical experts).\n1st batch 2nd batch 3rd batch 4th batch 5th batch\nrch rch Ma Ma rch Ma Ma Ap Ap ril ril Ap Ap ril ril rch y 3 Ma y 4 Ma\nPhase A Phase B\n\nSlide Topic: Presentation of the challenge New task\nSummary:\nTask C: Funding Information Extraction\nI Organizers distribute PMC full-text articles.\nI Participants have 48 hours to extract: grant-IDs, funding\nagencies, full grants (i.e. the combination of a grant-ID and the corresponding funding agency).\nI Evaluation based on annotations of MEDLINE curators.\nDry Run Test Batch\nril Ap ril Ap\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Presentation of the challenge BioASQ ecosystem\nSummary:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Presentation of the challenge Per task\nSummary:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5A Hierarchiacal text classification\nSummary:\nWeek Batch 1 Batch 2 Batch 3\nThe numbers in parentheses are the annotated articles for each test dataset.\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5A System approaches\nSummary:\nI Feature Extraction: Representing each abstract\nI tf-idf of words and bi-words\nI doc2vec embeddings of paragraphs\nI Concept Matching: Finding relevant MeSH labels\nI k-NN between article-vector representations\nI Linear SVM binary classifiers for each MESH label\nI Recurrent Neural Networks for sequence-to-sequence prediction\nI UIMA-ConceptMapper and MeSHLabeler tools for boosting NER\nI Latend Dirichlet Allocation and Labeled LDA utilizing topics found\nI Ensemble methodologies and stacking\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5A Evaluation Measures\nSummary:\nFlat measures Hierarchical measures\nI Example Based Precision (EBP)\nI Example Based Recall (EBR)\nI Example Based F-Measure (EBF)\nI Micro Precision/Recall/F-Measure (MiP,MIR,MiF)\nI Hierarchical Precision (HiP)\nI Hierarchical Recall (HiR)\nI Hierarchical F-Measure (HiF)\nI Lowest Common Ancestor Precision\nI Lowest Common Ancestor Recall (LCA-R)\nI Lowest Common Ancestor F-measure (LCA-F)\nA. Kosmopoulos, I. Partalas, E. Gaussier, G. Paliouras and I. Androutsopoulos: Evaluation Measures for Hierarchical Classification: a unified view and novel approaches. Data Mining and Knowledge Discovery, 29:820-865, 2015.\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5A results Evaluation\nSummary:\nI Systems ranked using MiF (flat) and LCA-F (hierarchical).\nI Results, in all batches and for both measures :\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5A results\nSummary:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5B Statistics on datasets\nSummary:\nBatch Size of documents # of snippets\nThe numbers for the documents and snippets refer to averages\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5B Training Dataset Insights\nSummary:\nAverage of items per question\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\nI Broad terms (e.g. proteins, syndromes)\nI More specific terms (e.g. cancer, heart, thyroid)\nI Number of questions related to cancer vs thyroid per year\nI The numbers on top of the bars denote the contributing experts\n\nSlide Topic: Task 5B Evaluation measures\nSummary:\nI Evaluating Phase A (IR)\nRetrieved items Unordered retrieval measures Ordered retrieval measures\nMean Precision, Recall, F-Measure MAP, GMAP snippets\nI Evaluating the exact answers for Phase B (Traditional QA)\nQuestion type Participant response Evaluation measures\nyes/no yes or no Accuracy\nfactoid up to 5 entity names strict and lenient accuracy, MRR\nlist a list of entity names Mean Precision, Recall, F-measure\nI Evaluating the ideal answers for Phase B (Query-focused Summarization)\nany paragraph-sized text ROUGE-2, ROUGE-SU4, manual scores*\n(Readability, Recall, Precision, Repetition)\n*with the help of BioASQ Assessment tool.\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5B System approaches\nSummary:\nI Question analysis: Rule-based, regular expressions, ClearNLP,\nSemantic role labeling (SRL), Stanford Parser, tf-idf, SVD, word embeddings.\nI Query expansion: MetaMap, UMLS, sequential dependence\nI Document retrieval: BM25, UMLS, SAP HANA database, Bag\nof Concepts (BoC), statistical language model.\nI Snippet selection: Agglomerative Clustering, Maximum\nMarginal Relevance, tf-idf, word embeddings.\nI Exact answer generation: Standford POS, PubTator, FastQA,\nSQuAD, Semantic role labeling (SRL), word frequencies, word embeddings, dictionaries, UMLS.\nI Ideal answer generation: Deep learning (LSTM, CNN, RNN),\nneural nets, Support Vector Regression.\nI Answer ranking: Word frequencies.\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5B Results\nSummary:\nI Our experts are currently assessing systems responses\nI The results will be announced in autumn\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5C Statistics on datasets\nSummary:\nI unique grant IDs\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\nNumber of articles per agency in training dataset\n\nSlide Topic: Task 5C Evaluation measures\nSummary:\nI A subset of the Grant IDs and Agencies mentioned in full text\nare available in ground truth data Micro-Recall\nI Each Grant ID (or lone Agency) must exist verbatim in the text\nI Different scores for each subtask:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5C System approaches\nSummary:\nI Grant Support Sentences: Identifying sentences containing\nI Features: tf-idf of n-grams\nI Techniques: SVM and Naive Bayes for scoring, specific XML fields\nI Grant Information Extraction: Detecing Grant-IDs and\nI Manually crafted Regular Expressions\nI Sequential Learning Models, such as Conditional Random Fields,\nHidden Markov Models, Max Entropy Models\nI Ensemble of classifiers for pairing Grant-IDs to Agencies\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Task 5C Results\nSummary:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Challenge Participation Overall\nSummary:\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\n\nSlide Topic: Conclusions and Prespectives\nSummary:\nI BioASQ will run in 2018.\nI Continuous development of benchmark datasets.\nG. Paliouras. Results of the fifth edition of the BioASQ Challenge, 4th of August 2017\nOracle for continuous testing\n\n",
    "Slide Topic: Simultaneous Interpretation SI\nSummary:\nTranslation of the spoken word in real time\n\nSlide Topic: Computer Assisted Interpretation CAI\nSummary:\nHow do we ensure\nmaximum utility with minimum distraction?\n\nSlide Topic: Estimating Interpreter Performance\nSummary:\nDont offer help when they dont need it!\nEstimate how well the interpreter is doing\n\nSlide Topic: Quality Estimation\nSummary:\nWe already do this in Machine Translation!\nCan we apply it to Simultaneous Interpretation?\nQuEst++ is an existing framework for QE (Specia et al., 2015)\n\nSlide Topic: Method\nSummary:\nQuEst++ baseline features Apply\nFeatures tailored to interpretation (METEOR)\nTest using 10-fold cross-validation\n\nSlide Topic: Features of interpretation\nSummary:\nSOURCE: Will the Parliament grant President Dilma Rousseff, on the very first occasion after her groundbaking groundbreaking election and for no sound formal reason, the kind of debate that we usually reserve for people like Mugabe? So, I ask you to remove Brazil from the agenda of the urgencies. (48 words)\nINTERP: Ehm il Parlamento... dopo le elezioni... darem- dar spazio a un dibattito sul ehm sul caso per esempio del presidente Mugabe invece di mettere il Brasile allordine del giorno? (27 words)\nGLOSS: Ehm the Parliament... after the elections... well gi- will give way to a\ndebate on the ehm on the case for example of President Mugabe instead of putting Brazil on the agenda?\n\nSlide Topic: SI Model Features\nSummary:\nNon-specific words - is the interpreter avoiding specific terminology?\nCognates/loan words - if a word is almost identical in both languages an interpreter shouldnt struggle with it (unless its a false friend!)\n\nSlide Topic: Analysis\nSummary:\nSOURCE: Will the Parliament grant President Dilma Rousseff, on the very first occasion after her groundbaking groundbreaking election and for no sound formal reason, the kind of debate that we usually reserve for people like Mugabe? So, I ask you to remove Brazil from the agenda of the urgencies. (48 words)\nINTERP: Ehm il Parlamento... dopo le elezioni... darem- dar spazio a un dibattito sul ehm sul caso per esempio del presidente Mugabe invece di mettere il Brasile allordine del giorno? (27 words)\n\nSlide Topic: Future Work\nSummary:\nEvaluation Metric - finding a metric better aligned with the uniqueness of strategies in SI\nLive system integration - streamlining the system to provide instantaneous feedback\nASR - evaluate the model on ASR output\nSpeech model - enhance the model using prosodic speech features\n\n",
    "Slide Topic: The Word Embedding Pipeline\nSummary:\nU nlabeled corpus c or pus Unlabeled c orpus corpus\nW2V GloVe Polyglot FastText\nUnlabeled Supervised task corpus\nU nlabeled corpus c or pus c orpus\nPenn TreeBank SemEval OntoNotes Univ. Dependencies\nTagging Parsing Sentiment NER\n\nSlide Topic: Actual Pattern\nSummary:\nAffects supervised tas ks\nOur method - compositional\n\nSlide Topic: Sources of OOVs\nSummary:\nNames Chalabi has increasingly marginalized within Iraq, ...\nDomain-specific jargon Important species (...) include shrimp, (...) and some varieties of flatfish.\nForeign words This term was first used in German (Hochrenaissance),\nWithout George Martin the Beatles would have been just another\nuntalented band as Oasis.\nWhat if Google morphed into GoogleOS?\nWell have four bands, and Big D is cookin. lots of fun and great prizes.\nTypos and other errors\nI dislike this urban society and I want to leave this whole enviroment.\n\nSlide Topic: Common OOV handling techniques\nSummary:\ntask corpus U nlabeled c o rpus Unlabeled c orpus corpus\nOne UNK to rule them all\nTrained with embeddings (stochastic unking)\nAdd subword model during WE training\nWhat if we dont have access to the original corpus? (e.g. FastText) OOV\n\nSlide Topic: Char2Tag\nSummary:\nUnlabeled U nlabeled corpus Supervised task corpus U nlabeled c o rpus Unlabeled c orpus corpus\nAdd subword layer to supervised task\nOOVs benefit from co-trained character model\nRequires large supervised training set for efficient transfer to test set OOVs\n\nSlide Topic: Enter MIMICK\nSummary:\n(No context) Unlabeled U nlabeled corpus Supervised task corpus U nlabeled c o rpus Unlabeled c orpus Subword units as inputs corpus\nWhat data do we have, post-unlabeled corpus?\nOrthography (the way words are spelled)\nUse the former as training objective, latter as input\nPre-trained vectors as target\nNo need to access original unlabeled corpus\n\nSlide Topic: MIMICK Training\nSummary:\nm a k e\n\nSlide Topic: MIMICK Inference\nSummary:\nb l a h\n\nSlide Topic: Observation Nearest Neighbors\nSummary:\nEnglish (OOV Nearest in-vocab words)\nMCT AWS, OTA, APT, PDM\npesky euphoric, disagreeable, horrid, ghastly\nlawnmower tradesman, bookmaker, postman, hairdresser\ngeometric (m.pl., nontrad. spelling) geometric (m.pl.)\nSurface form Syntactic properties Semantics\n\nSlide Topic: Intrinsic Evaluation RareWords\nSummary:\nRareWords similarity task: morphologically-complex, mostly unseen words\nForeign words Rare(-ish) morphological derivations Nonce words Nonstandard orthography Typos and other errors\n\nSlide Topic: Extrinsic Evaluation POS Attribute Tagging\nSummary:\nUD is annotated for POS and morphosyntactic attributes\nCze: his stated goals\nosoby v pokrocilem veku\npeople of advanced age\nRare(-ish) morphological derivations Nonce words Nonstandard orthography Typos and other errors\nDT NN VBZ VBG POS\nthe cat is sitting\nAttributes - same as POS layer\nNegative effect on POS\nBackward LSTM Micro F1\n\nSlide Topic: Language Selection\nSummary:\n13 Indo-European (7 different branches)\n10 from 8 non-IE branches\nMRLs (e.g. Slavic languages)\nRelatively free word order\n\nSlide Topic: Language Selection contd\nSummary:\n7 in non-alphabetic scripts\nIdeographic (Chinese) - ~12K characters\nHebrew, Arabic - no casing, no vowels, syntactic fusion\nVietnamese - tokens are non-compositional syllables\nOOV rate (UD against Polyglot vocabulary)\n\nSlide Topic: Evaluated Systems\nSummary:\nNONE: Polyglots default UNK embedding\nthe flatf ish is sitt ing\nCHAR2TAG - additional RNN layer\nChar- LSTM Char- LSTM Char- LSTM Char- LSTM\nBOTH: MIMICK + CHAR2TAG\n\nSlide Topic: Results Full Data\nSummary:\nPOS tags (accuracy) Morpho. Attributes (micro F1)\n\nSlide Topic: Results 5000 training tokens\nSummary:\nPOS tags (accuracy) Morpho. Attributes (micro F1)\n\nSlide Topic: Results Language Types 5000 tokens\nSummary:\nSlavic languages POS Agglutinative languages morpho. attribute F1\n\nSlide Topic: Results Chinese\nSummary:\nPOS tags (accuracy) Morpho. Attributes (micro F1)\n\nSlide Topic: A Word Model from our Sponsor\nSummary:\nOur extrinsic results are on tagging\nPlease consider us for all your WE use cases!\nIE! Code & models:\nCode compatible with w2v, Polyglot, FastText\nModels for Polyglot also on github\n<1MB each, dynet format\nLearn all OOVs in advance and add to param table, or\nLoad into memory and infer on-line\n\nSlide Topic: Conclusions\nSummary:\nMIMICK: an OOV-extension embedding processing step for downstream tasks\nCompositional model complementing distributional artifact\nPowerful technique for low-resource scenarios\nSore spots and Future Work\nVietnamese - syllabic vocabulary\nHebrew and Arabic - nontrivial tokenization, no case\nTry other subword levels (morphemes, phonemes, bytes)\nImprove morphosyntactic attribute tagging scheme\n\n",
    "Slide Topic: Span Parsing is SOTA in Constituency Parsing\nSummary:\nCross+Huang 2016 introduced Span Parsing\nBut with greedy decoding.\nStern et al. 2017 had Span Parsing with Exact Search and Global Training\nBut was too slow: O(n3)\nCan we get the best of both worlds? Cross Huang\nSomething that is both fast and accurate?\nSpeed New at ACL 2018! Also Span Parsing!\n\nSlide Topic: Both Fast and Accurate\nSummary:\nBaseline Chart Parser (Stern et al. 2017a)\nOur Linear Time Parser\n\nSlide Topic: In this talk we will discuss\nSummary:\nLinear Time Constituency Parsing using dynamic programming\nGoing slower in order to go faster: O(n3) O(n4) O(n)\nCube Pruning to speed up Incremental Parsing with Dynamic Programming\nFrom O(n b2) to O(n b log b)\nAn improved loss function for Loss-Augmented Decoding\n2nd highest accuracy among single systems trained on PTB only\n\nSlide Topic: Span Parsing\nSummary:\nSpan differences are taken from an encoder\n(in our case: a bi-LSTM)\nA span is scored and labeled by a feed-forward network. s\nThe score of a tree is the sum of all the labeled span scores\n(i,j,X)2t s You should eat ice cream /s\n\nSlide Topic: Incremental Span Parsing Example\nSummary:\nEat ice cream after lunch VB NN NN IN NN Cross + Huang 2016\nEat NN NN IN NN\nS-VP ice cream after NN\nNP PP Shift NP\nS Action Label Stack\nEat ice cream after lunch Reduce S-VP VB NN NN IN NN Cross + Huang\n\nSlide Topic: How Many Possible Parsing Paths\nSummary:\n2 actions per state.\n\nSlide Topic: Equivalent Stacks\nSummary:\nObserve that all stacks that end with (i, j) will be treated the same!\nUntil (i, j) is popped off.\nSo we can treat these as temporarily equivalent, and merge.\nThis is our new stack representation. Left Pointers\nGraph-Structured Stack (Tomita Huang Sagae 2010)\n\nSlide Topic: Dynamic Programming Merging Stacks\nSummary:\nTemporarily merging stacks will make our state space polynomial.\n\nSlide Topic: Becoming Action Synchronous\nSummary:\nShift-Reduce Parsers are traditionally action synchronous.\nThis makes beam-search straight forward.\nWe will also do the same\nBut will show that this will slow down our DP (befo re applying beam -search)\n\nSlide Topic: Action Synchronous Parsing Example\nSummary:\nGold: (0,1) Shift Shift Shift Reduce Reduce Shift Shift Reduce Reduce\nsh sh sh sh sh r r r r\nr r sh r r r\nr r r r r r\nsh r r r sh sh sh\nr r r sh r sh r r r r\n\nSlide Topic: Runtime Analysis\nSummary:\nsh sh sh sh sh r r r r\nr r r r r r\nsh r r r sh sh sh\nr r r sh r sh r r r r\n#left pointers per state: O(n)\nCheck out the paper for our new theorem: r sh sh\nThanks to Dezhong Deng!\n\nSlide Topic: Going slower to go faster\nSummary:\nOur Action-Synchronous algorithm has a slower runtime than CKY!\nHowever, it also becomes straightforward to prune using beam search.\nSo we can achieve a linear runtime in the end.\nsh sh sh sh sh sh sh sh sh sh r r r r r r r r O(n4) O(n4) r r r r r r r r r r r r sh sh r r r r r r sh sh sh sh sh sh r r r r r r sh sh r r sh sh r r r r r r r r r r\nsh sh sh sh sh sh sh sh sh sh r r r r r r r r O(n) O(n) r r r r r r r sh r sh sh r sh r r (approx. (approx. DP) DP) sh r sh r sh r sh\n\nSlide Topic: Now our runtime is On\nSummary:\nsh sh sh sh sh r r r r\nr r r r r\nr sh r sh\n\nSlide Topic: But this On is hiding a constant\nSummary:\nb states per action step\nO(b) left pointers per state\n\nSlide Topic: Cube Pruning\nSummary:\nWe can apply cube pruning to make O(nb log b)\nsh sh sh sh sh r r r r\nr r r r r\nr sh r sh\nBy pushing all states and their left pointers into a heap\nAnd popping the top b unique subsequent states\n\nSlide Topic: Training\nSummary:\nGoal: Score the gold tree higher than all others by a margin:\n\nSlide Topic: Loss Function\nSummary:\nCounts the incorrectly labeled spans in the tree (Stern et al. 2017)\nHappens to be decomposable, so can even be used to compare partial trees.\n\nSlide Topic: Novel Cross Span Loss\nSummary:\nWe observe that the null label is used in two different ways:\nTo facilitate ternary and n-ary branching trees.\nAs a default label for incorrect spans that violate other gold spans.\ni j i j\nWe modify the loss to account for incorrect spans in the tree.\nIndicates whether (i, j) is crossing a span in the gold tree\nStill decomposable over spans, so can be used to compare partial trees.\n\nSlide Topic: Max Violation Updates\nSummary:\nTake the largest augmented loss value across all time steps.\nThis is the Max-Violation, that we use to train.\nbest in the beam full\ncorrect sequence early max- violation latest\nworst in the beam last valid falls off the beam update invalid update! biggest violation\n\nSlide Topic: Comparison with Baseline Chart Parser\nSummary:\nModel Note F1 (PTB test)\nStern et al. (2017a) Baseline Chart Parser\n\nSlide Topic: Comparison to Other Parsers\nSummary:\nPTB only, Single Model, End-to-End Reranking, Ensemble, Extra Data\nModel Note F1 Model Note F1\nDurett + Klein 2015 Vinyals et al. 2015 Ensemble\nCross + Huang 2016 Original Span Parser Dyer et al. 2016 Generative Reranking\nDyer et al. 2016 Discriminative Fried et al. Reranking Ensemble\nStern et al. 2017a Chart Baseline Parser\nStern et al. 2017c Separate Decoding\nOur Work Beam 20\n\nSlide Topic: Conclusions\nSummary:\nLinear-Time, Span-Based Constituency Parsing with Dynamic Programming\nCube-Pruning to speedup Incremental Parsing with Dynamic Programming\nCross-Span Loss extension for improving Loss-Augmented Decoding\nResult: Faster and more accurate than cubic-time Chart Parsing\n2nd highest accuracy for single-model end-to-end systems trained on PTB only\nStern et al. 2017c is more accurate, but with separate decoding, and is much slower\nAfter this ACL, definitely no longer true. (e.g. Joshi et al. 2018, Kitaev+Klein 2018)\nBut both are Span-Based Parsers and can be linearized in the same way!\n\n",
    "Slide Topic: Query Auto Completion\nSummary:\nSearch engine suggests queries as the user types\nan LSTM to generate completions\nMemory savings over most popular completion\nHandles previously unseen prefixes\nCan we do better by adapting the LM to provide personalized suggestions?\n\nSlide Topic: RNN Language Model Adaptation\nSummary:\nLearn an embedding, c, for each user and use it to adapt the predictions\nword embedding user embedding\nMethod #1: Concatenate the user embedding with the input at each step*\nSame as applying a constant linear shift to the bias vector (in recurrent & output layers)\nLeaves most of the recurrent model parameters unchanged\nMethod #2: Low-rank adaptation of recurrent weight matrix (FactorCell model)\nConcatenating the user embedding is the same as shifting the bias.\nAdjust b and W! * Referred to here as ConcatCell (Mikolov& Zweig, 2012)\n\nSlide Topic: FactorCell Model\nSummary:\nAdapted weights Generic weights Low-rank adaptation\nThe adaptation matrix is formed from a product of the context embedding with left and right bases.\nThe two bases tensors (L and R) hold k different rank r matrices, each the same size as W. Context vectors give a weighted combination.\n\nSlide Topic: Learning\nSummary:\nUser embeddings, recurrent layer weights and {L, R} tensor learned jointly\nNeed online learning to adapt to users that were not previously seen\nIn joint training, learn a cold-start embedding for set of infrequent users\nInitialize each users embedding with learned cold-start vector\nAfter user selects a query, back-propagate and only update the user embedding\n\nSlide Topic: Data and Experiments\nSummary:\nusers and 12 million queries for training\nUser embedding size = 32, LSTM size = 600\nEvaluate on 500K queries with disjoint user population\nMean reciprocal rank (MRR) as a metric\n\nSlide Topic: Experimental Results\nSummary:\nBenefit improves over time!\nPerformance for users with > 50 queries\n\nSlide Topic: Qualitative Comparison\nSummary:\nWhat queries are boosted the most after searching for high school\nsoftball and math homework help?\nhigh school musical horoscope\nchris brown high school musical\nfunnyjunk.com homes for sale\nchat room hair styles\nQueries that most decrease in likelihood with the\nFactorCell include travel agencies and plane tickets.\n\nSlide Topic: Recent Related Work Florini and Lu NAACL 2018\nSummary:\nAlso personalized LSTM for query prediction\nUser embedding learned separately\nAssessed on two datasets, but different split of AOL data\nConfirms benefit of adapted LM\n\nSlide Topic: Conclusions\nSummary:\nPersonalization helps and the benefit increases as more queries are seen\nStronger adaptation of the recurrent layer (FactorCell) gives better results than concatenating a user vector\nNo extra latency/computation due to caching of adapted weight matrix\nTry out the FactorCell on your data\n\n",
    "Slide Topic: Introduction\nSummary:\nEncode a variable-length input sentence into a constant size vector\nBased on Word Embeddings:\n(II) Concatenated P-Mean Embeddings (Ruckle et al. 2018)\n(I) SkipThought Vectors (Kiros et al. 2015)\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\nSlide Topic: Goal\nSummary:\nExploring what specific semantic properties are directly reflected by such embeddings.\nFocusing on a few select aspects of sentence semantics.\nConcurrent related work: Conneau et al. ACL 2018\n(i) Their work studies what you can learn to predict using 100,000 training instances\n(ii) Our goal: Directly study the embeddings (via cosine similarity)\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\nSlide Topic: Approach Contrastive Sentences\nSummary:\nMinor alterations of a sentence may lead to notable shifts in meaning.\n(i) A rabbit is jumping over the fence ( S\n(ii) A rabbit is hopping over the fence ( S=\n(iii) A rabbit is not jumping over the fence S*\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\nSlide Topic: Sentence Modification Schemes\nSummary:\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\nSlide Topic: Negation Detection\nSummary:\nA person is slicing an onion.\nA person is cutting an onion.\nA person is not slicing an onion.\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\nAverage of Word Embeddings is more easier misled by negation.\nBoth InferSent and SkipThought succeed in distinguishing unnegated sentences from negated ones.\nGlove Avg P Means Sent2Vec SkipThought InferSent\n\nSlide Topic: Negation Variant\nSummary:\nA man is not standing on his head under water.\nThere is no man standing on his head under water.\nA man is standing on his head under water.\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\nBoth averaging of word embeddings and SkipThought are dismal in terms of the accuracy.\nInferSent appears to have acquired a better understanding of negation quantifiers, as these are commonplace in many NLI datasets.\nGlove Avg P Means Sent2Vec SkipThought InferSent\n\nSlide Topic: Clause Relatedness\nSummary:\nOctel said the purchase was expected.\nOctel said the purchase was not expected\nGlove Avg P Means Sent2Vec SkipThought InferSent Zhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\nBoth SkipThought vectors and InferSent works poorly when sub clause is much shorter than original one.\nSent2vec best in distinguishing the embedded clause of a sentence from a negation of that sentence.\n\nSlide Topic: Argument Sensitivity\nSummary:\nFrancesca teaches Adam to adjust the microphone on his stage\nAdam is taught to adjust the microphone on his stage\nAdam teaches Francesca to adjust the microphone on his stage\nGlove Avg P Means Sent2Vec SkipThought InferSent Zhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\nNone of the analyzed approaches prove adept at distinguishing the semantic information from structural information in this case.\n\nSlide Topic: Fixed Point Reordering\nSummary:\nA black dog in the snow is jumping off the ground and catching a stick.\nFixed Point Inversion(Corrupted Sentence):\nIn the snow is jumping off the ground and catching a stick a black dog.\nGlove Avg P Means Sent2Vec SkipThought InferSent Zhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\nMethods based on word embeddings do not encode sufficient word order information into the sentence embeddings.\nSkipThought and InferSent did well when the original sentence and its semantically equivalence share similar structure\n\nSlide Topic: Models and Dataset\nSummary:\nDataset Embedding Dim of Sentences From\nGlove Avg Common Crawl Negation Detection SICK, SNLI\nP Means Common Crawl Negation Variant SICK, SNLI\nSent2Vec English Wiki Clause Relatedness TreebankMSR Penn Paraphrase\nSkipThought Book Corpus Argument Sensitivity SICK, MS Paraphrase\nInferSent SNLI Fixed Point Reordering SICK\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\nSlide Topic: Conclusion\nSummary:\nRNN based sentence embeddings better at identifying negation compared with word embedding based models\nBoth SkipThought and InferSent distinguish negation of a sentence from synonymy.\nInferSent better at identifying semantic equivalence regardless of the order of words and copes better with quantifiers.\nSkipThoughts is more suitable for tasks in which the semantics of the sentence corresponds to its structure\nZhu, Li & de Melo. Exploring Semantic Properties of Sentence Embeddings\n\n",
    "Slide Topic: Overview of pre reordering systems\nSummary:\nReorder input text before translation\nJohn hits a ball\nJohn va_nsubj a ball va_obj hits\n\nSlide Topic: Approaches of pre reordering\nSummary:\nSyntactic pre-reordering without parse tree\nHead-finalization (Isozaki et al., 2010)\nSupervised learning with word alignments\nAutomatically learning Rewrite Patterns (Xia and\n\nSlide Topic: Overview of our pre reordering system\nSummary:\nHead-restructured CFG Parse Tree\n\nSlide Topic: Head restructured CFG Parse Tree\nSummary:\nProblem of CFG parse tree\nHard to capture long-distance reordering patterns\nProblem of Dependency parse tree\nFully lexicalized parse tree leads to a sparse reordering model\nRestructure a CFG parse tree to inject head information into it\nHead word is always lexicalized\n\nSlide Topic: Learning reordering model based on LM\nSummary:\nExtract tag sequences in golden order\nHead-restructured CFG parse tree\nnsubj prep_by calculated auxpass aux\nAlignments Train a language model on\n\nSlide Topic: Finding golden order with word alignments\nSummary:\nGiven a bilingual sentence pair, source-side parse tree and word alignments, the golden order of a node layer is defined as\nAverage position (Ranked) a1 a3 a2\n\nSlide Topic: Reordering a input parse tree\nSummary:\n1.List all possible orders for a treelet 3. Select the best order to adjust the treelet\nnsubj dobj hits dobj nsubj hits hits nsubj dobj hits dobj nsubj dobj hits nsubj nsubj hits dobj nsubj dobj hits\n2.Score them with language model\n\nSlide Topic: N best reordering\nSummary:\nReordered treelets with LM scores\nAll 12 possible combinations here\nSelected N-best results by accumulated scores (Cube Pruning is applied in the practice)\n\nSlide Topic: In house experiments\nSummary:\nN-best parse + N- best reorder\nFor N-best reorder, 10 candidate reordering results are considered.\nFor N-best parse, 30 candidate parse trees are considered.\nWe select the final translation by the sum of translation score (given by decoder) and the score of pre-reordering.\n\nSlide Topic: N best reordering and N best parse tree inputs\nSummary:\nIncorporating multiple reordering results and parse trees benefits automatic scores.\n\nSlide Topic: Official evaluation results\nSummary:\nN-best reorder + N-best parse\n\nSlide Topic: Effect of pre ordering\nSummary:\nIdentical ordered sentences increases to 15%\n\nSlide Topic: Example of pre reordering\nSummary:\nthe improvement of the life is a large problem of the practical application.\nReordered input the life of the improvement va_nsubjpass the practical application of a large problem is .\n\nSlide Topic: Review\nSummary:\nLanguage model is just a quick solution to the reordering problem, sometimes it fails in simple cases.\nTo gain more from forest input, its necessary to integrate it inside the pre-reordering model.\n\n",
    "Slide Topic: What is Cross Language Plagiarism Detection\nSummary:\nCross-Language Plagiarism is a plagiarism by translation, i.e. a text has been plagiarized while being translated (manually or automatically).\nFrom a text in a language L, we must find similar passage(s) in other text(s) from among a set of candidate texts in language L (cross-language textual similarity).\n\nSlide Topic: Why is it so important\nSummary:\n- McCabe, D. (2010). Students cheating takes a high-tech turn. In Rutgers Business School. - Josephson Institute. (2011). What would honest Abe Lincoln say?\n\nSlide Topic: Research Questions\nSummary:\nAre Word Embeddings useful for cross-language plagiarism detection?\nIs syntax weighting in distributed representations of sentences useful for the text entailment?\nAre cross-language plagiarism detection methods complementary?\n\nSlide Topic: State of the Art Methods\nSummary:\nLength Model, CL-CnG [Mcnamee and Mayfield, 2004, Potthast et al., 2011], Cognateness\nMT-Based Models Translation + Monolingual Analysis [Muhr et al., 2010, Barron-Cedeno, 2012]\n\nSlide Topic: Augmented CL CTS\nSummary:\nWe use DBNary [Serasset, 2015] as linked lexical resource.\n< ) Le chat boit du lait Sp the cat drinks milk\nCL-CTS-WE uses the top 10 closest words in the embeddings model to build the\nBOW of a word;\nA BOW of a sentence is a merge of the BOW of its words;\nJaccard distance between the two BOW.\n\nSlide Topic: CL WES Cross Language Word Embedding based Similarity\nSummary:\nThis feature is available in MultiVec [Berard et al., 2016] (https://github.com/eske/multivec)\nThe similarity between two sentences S and S is calculated by Cosine Distance between the two vectors V and V , built such as:\nui is the ith word of S; vector is the function which gives the word embedding vector of a word.\n\nSlide Topic: CL WESS Cross Language Word Embedding based Syntax Similarity\nSummary:\nThis feature is available in MultiVec [Berard et al., 2016] (https://github.com/eske/multivec)\nui is the ith word of S; pos is the function which gives the universal part-of-speech tag of a word; weight is the function which gives the weight of a part-of-speech; vector is the function which gives the word embedding vector of a word; is the scalar product.\n\nSlide Topic: Evaluation Dataset\nSummary:\nFrench, English and Spanish;\nParallel and comparable (mix of Wikipedia, conference papers, product reviews,\nDifferent granularities: document level, sentence level and chunk level;\nHuman and machine translated texts;\nObfuscated (to make the similarity detection more complicated) and without added noise;\nWritten and translated by multiple types of authors;\n1A Multilingual, Multi-style and Multi-granularity Dataset for Cross-language Textual Similarity\nDetection. In Proceedings of LREC 2016.\n\nSlide Topic: Evaluation Protocol\nSummary:\nWe compared each English textual unit to its corresponding\nFrench unit and to 999 other units randomly selected;\nWe threshold the obtained distance matrix to find the threshold giving the best F1 score;\nWe repeat these two steps 10 times, leading to a 10 folds:\n- 2 folds for tuning (CL-WESS) and fusion (Decision Tree)\n- 8 folds for validation\n\nSlide Topic: Results\nSummary:\nDecision Tree fusion significantly improves the results.\nCL-CTS-WE: Cross-Language Conceptual Thesaurus-based Similarity with Word-Embedding Table: Average F1 scores of methods applied\nTable: Average F1 scores of methods applied on ENFR sub-corpora.\nCL-WES: Cross-Language Word Embedding-based Similarity\nCL-WESS: Cross-Language Word Embedding-based Syntax Similarity\nCL-C3G: Cross-Language Character 3-Gram\n\nSlide Topic: Conclusion\nSummary:\nAugmentation of several baseline approaches using word embeddings instead of lexical resources;\nCL-WESS beats in overall the precedent best state-of-the-art methods;\nMethods are complementary and their fusion significantly helps cross-language textual similarity detection performance;\nWinning method at SemEval-2017 Task 1 track 4a, i.e. the task on\nSpanish-English Cross-lingual Semantic Textual Similarity detection.\n\nSlide Topic: Complementarity\nSummary:\nFigure: Distribution histograms of CL-CNG (left) and CL-ASA (right) for 1000 positives and\n\nSlide Topic: Fusions\nSummary:\nWeighted Average Fusion Decision Tree Fusion C4.5 [Quinlan, 1993]\n\nSlide Topic: Weighted Fusion\nSummary:\nM is the set of the scores of the methods for one match; mj and wj are the score and the weight of the jth method respectively.\n\nSlide Topic: Results at Chunk Level\nSummary:\nMethods Wikipedia TALN (%) JRC (%) APR (%) Europarl Overall (%)\nTable: Average F1 scores of cross-language similarity detection methods applied on chunk-level\nENFR sub-corpora 8 folds validation.\n\nSlide Topic: Results at Sentence Level\nSummary:\nMethods Wikipedia TALN (%) JRC (%) APR (%) Europarl Overall (%)\nTable: Average F1 scores of cross-language similarity detection methods applied on sentence-level ENFR sub-corpora 8 folds validation.\n\n",
    "Slide Topic: Objectives\nSummary:\nStudy the practices of the NLP (Spoken,\nWritten and Sign Language) community regarding reuse and plagiarism\nCheck whether there is a meaningful difference in taking the verbatim raw word strings compared with applying natural language processing methods to detect possible cases of reuse and plagiarism?\n\nSlide Topic: NLP4NLP Corpus\nSummary:\nPresently conduct large scholar analysis of NLP domain\nProduction, Collaboration, Citation, Innovation\nMajor conferences (ACL, IEEE-ICASSP, ISCA-Interspeech,\nELRA-LREC, etc.) and Journals (IEEE-TASLP, CL,\nSpeechCom, CSAL, LRE, etc.)\n558 Venues (conferences) / Issues (journals)\nshort name # docs format long name language access to content period # venues\nacl conference Association for Computational Linguistics Conference English open access *\nacmtslp journal ACM Transaction on Speech and Language Processing English private access\nalta conference Australasian Language Technology Association English open access *\nanlp conference Applied Natural Language Processing English open access *\ncath journal Computers and the Humanities English private access\ncl journal American Journal of Computational Linguistics English open access *\ncoling conference Conference on Computational Linguistics English open access *\nconll conference Computational Natural Language Learning English open access *\ncsal journal Computer Speech and Language English private access eacl conference European Chapter of the ACL English open access * emnlp conference Empirical methods in natural language processing English open access * hlt conference Human Language Technology English open access *\nicassps conference IEEE International Conference on Acoustics, Speech and Signal Processing - Speech Track English private access ijcnlp conference International Joint Conference on NLP English open access * inlg conference International Conference on Natural Language Generation English open access * isca conference International Speech Communication Association English open access jep conference Journees d'Etudes sur la Parole French open access * lre journal Language Resources and Evaluation English private access lrec conference Language Resources and Evaluation Conference English open access * ltc conference Language and Technology Conference English private access modulad journal Le Monde des Utilisateurs de L'Analyse des Donnees French open access mts conference Machine Translation Summit English open access muc conference Message Understanding Conference English open access * naacl conference North American Chapter of ACL English open access *\npaclic conference Pacific Asia Conference on Language, Information and Computation English open access * ranlp conference Recent Advances in Natural Language Processing English open access * sem conference Lexical and Computational Semantics / Semantic Evaluation English open access * speechc journal Speech Communication English private access tacl journal Transactions of the Association for Computational Linguistics English open access * tal journal Revue Traitement Automatique du Langage French open access taln conference Traitement Automatique du Langage Naturel French open access * taslp journal IEEE/ACM Transactions on Audio, Speech and Language Processing English private access tipster conference Tipster DARPA text program English open access * trec conference Text Retrieval Conference English open access Total incl. duplicates Total excl. duplicates\n\nSlide Topic: Definitions\nSummary:\nSelf-plagiarism: copy & paste when the source of the copy has at least one author who belongs to the group of authors of the text of the paste, but when the source is not cited.\nReuse: copy & paste when the source of the copy has no author in the group of authors of the paste and when the source is cited.\nPlagiarism: copy & paste when the source of the copy has no author in the group of the paste and when the source is not cited.\nSource paper is quoted Source paper is not quoted\nAt least one author in common\nNo author in common Reuse Plagiarism\n\nSlide Topic: Each year Papers of the focus borrowing papers of the search\nSummary:\nFocus NLP4NLP (Same year or previous years)\nSelf-Reusing Self-Plagiarizing Reusing Plagiarizing\n\nSlide Topic: Each year Papers of the focus being borrowed by papers of\nSummary:\nFocus NLP4NLP (Same year or following years)\nSelf-Reused Self-Plagiarized Reused Plagiarized\n\nSlide Topic: Algorithm\nSummary:\nBased on comparison of word sequences, had to be optimized:\nFor each pair of documents D1 of the focus (LREC) and D2 of the search space\neither raw text or text after LP (Tagparser [Francopoulo 2007] with Global Atlas + LRE Map)\nOrthographic variations (British English versus American English)\nSpelling errors Abbreviations (BNC versus British National Corpus)\nCompare 2 texts D1 / D2 using sliding windows of (5-7) lemmas (excluding punctuations)\nCompute a similarity overlapping score [Lyon et al 2001] between documents D1 and D2, with (a variant of) the Jaccard similarity coefficient Score (D1,D2) = #shared windows / #union (D1 windows, D2 windows) Filter the pairs of documents D1 / D2 according to a threshold of (0.03-0.04) coverage) to retain only significantly similar pairs\n\nSlide Topic: Raw text versus LP\nSummary:\nStrategy Backward study Forward study document pairs# after document pairs# document pairs# duplicate pruning\n2. Linguistic processing (LP)\n\nSlide Topic: Tuning Parameters\nSummary:\n+ Number of shared windows > 50\n\nSlide Topic: Self Reuse Plagiarism\nSummary:\nIn 61% of the cases, authors do not quote the source paper\n130 papers have both the same title and the same list of authors\n205 papers have the same title\nSome specific cases (largest similarities)\nRepublishing the corrigendum of a previously published paper\nRepublishing a paper with a small difference in the title and one missing author in the authors list\nSame research center described by the same author in two different conferences, with an overlapping of 90%\n2 papers presented by the same author in 2 successive conferences, the difference being primarily in the name of the 2 systems being presented, that have been funded by the same project agency in 2 different contracts, with an overlapping of 45%\nUsed Using acl acmtslp alta anlp cath cl coling conll csal eacl emnlp hlt icassps ijcnlp inlg isca jep lre lrec ltc modulad mts muc naacl paclic ranlp sem speechc tacl tal taln taslp tipster trec\nTotal used Total using Difference\nanlp anlp cath cath cl cl coling coling conll conll csal csal eacl eacl emnlp emnlp hlt hlt icassps icassps ijcnlp ijcnlp inlg inlg isca isca jep jep lre lre lrec lrec ltc ltc modulad modulad mts mts muc muc naacl naacl paclic paclic ranlp ranlp sem sem speechc speechc tacl tacl tal tal taln taln taslp taslp tipster tipster trec trec Total using\n\nSlide Topic: Reuse and Plagiarism\nSummary:\n261 cases : manual checking\n25 have a least one author in common, but with a somehow different spelling, and should therefore be placed in the Self- plagiarism category\n14 correctly quote the source paper, but with variants in the spelling of the authors names, of the papers title or of the conference or journal source, or correctly citing the source paper but forgetting to place it among the references, and should therefore be placed in the Reuse category.\nAfter manual corrections: 224 cases (0.33% of papers)\nIn 52% of the cases, authors do not quote the source paper\nThis results in 117 possible cases of plagiarism (0.17%):\nThe copying paper cites another reference from the same authors of the source paper\n(typically a previous reference, or a paper published in a Journal) (46 cases)\nBoth papers use extracts of a third paper that they both cite (31 cases)\nAuthors of the two papers are different, but from same laboratory (typical in industrial laboratories or funding agencies) (11 cases)\nAuthors of the two papers previously co-authored papers (typically as supervisor and PhD student or postdoc) but are now in different laboratories (11 cases)\nAuthors of the papers are different, but collaborated in the same project which is presented in the two papers (2 cases) The two papers present the same short example, result or definition coming from another source (13 cases) Only 3 remaining cases of possible plagiarism: same paper as a patchwork of 3 other papers, while sharing several references with them.\nUsed Using acl acmtslp alta anlp cath cl coling conll csal eacl emnlp hlt icassps ijcnlp inlg isca jep lre lrec ltc modulad mts muc naacl paclic ranlp sem speechc tacl tal taln taslp tipster trec\nTotal used Total using Difference\nanlp anlp cath cath cl cl coling coling conll conll csal csal eacl eacl emnlp emnlp hlt hlt icassps icassps ijcnlp ijcnlp inlg inlg isca isca jep jep lre lre lrec lrec ltc ltc modulad modulad mts mts muc muc naacl naacl paclic paclic ranlp ranlp sem sem speechc speechc tacl tacl tal tal taln taln taslp taslp tipster tipster trec trec Total using\n\nSlide Topic: Variants in Spelling Authors Name\nSummary:\nNon-Linear Probability Estimation Method\nUsed in HMM for Modeling Frame Correlation\nQing Guo, Fang Zheng, Jian Wu, and Wenhu Wu\nAn New Method Used in HMM for Modeling\nGuo Qing, Zheng Fang, Wu Jian and Wu Wenhu\n\nSlide Topic: Variants in Spelling References\nSummary:\nQuoted Reference: Graham W. (2007) an OWL\nOntology for HPSG proceeding of the ACL 2007\nCorrect Reference: Graham Wilcock (2007), An\nOWL Ontology for HPSG\nQuoted Reference: Li Liu, Jianglong He, On the use of orthogonal GMM in speaker verification\nCorrect Reference: Li Liu and Jialong He, On the use of orthogonal GMM in speaker recognition\n\nSlide Topic: Self Plagiarism or Fair Use\nSummary:\n(Pamela Samuelson, Comm. of ACM 1994)\nThe previous work must be restated to lay the groundwork for a new contribution in the second work,\nPortions of the previous work must be repeated to deal with new evidence or arguments,\nThe audience for each work is so different that publishing the same work in different places is necessary to get the message out,\nThe authors think they said it so well the first time that it makes no sense to say it differently a second time.\n30% as an upper limit in the reuse of parts of a previously published paper.\nOnly 1.3% of NLP4NLP papers go beyond this limit\n\nSlide Topic: Plagiarism Right to Quote\nSummary:\nNational legislations usually embody the Berne convention limits in one or more of the following requirements:\nthe cited paragraphs are within a reasonable limit,\n<= 10% of the copied / copying papers in France / Canada\nOnly 0.05% of NLP4NLP papers go beyond this limit\nthe cited paragraphs are clearly marked as quotations and fully referenced, the resulting new work is not just a collection of quotations, but constitutes a fully original work in itself.\nthe copied paragraphs must have a function in the goal of the copying paper.\n\nSlide Topic: Conclusions\nSummary:\nProduce results on the study of copy & paste operations on corpora of NLP archives of very large size, using NLP methods\nstill represents a practical computing limitation.\nSelf-reuse and self-plagiarism are common practices (18%)\n40% happen on same year (no way to detect beforehand)\nNo quote of source paper in 60% of the cases (75% if same year)\nNatural flow from conferences to journals\nCurrent tendency for salami-slicing publications caused by the publish-and-perish demand\nPlagiarism very uncommon in the NLP community (<0.05%)\nEthically acceptable if principles are respected\n\nSlide Topic: Further developments\nSummary:\nProcess rogeting: replacing words with synonymous alternatives\nStudy the position and rhetorical structure of the copy & paste in order to identify and justify their function.\nExplore whether copy & paste is more common for non native-English speakers\npublish first in their native language, then in English in an international conference or an international journal, in order to broaden their audience\n\n",
    "Slide Topic: Motivation\nSummary:\nProcessing long, complex sentences is hard!\nChildren, people with reading disabilities, L2 learners\nSentence level NLP systems:\nKoehn & Knowles, 2017 Can we automatically break a complex sentence into several simple ones while preserving its meaning?\n\nSlide Topic: The Split and Rephrase Task\nSummary:\nNarayan, Gardent, Cohen & Shimorina, EMNLP 2017\nDataset, evaluation method, baseline models\nTask definition: complex sentence -> several simple sentences with the same meaning\nAlan Bean joined NASA in 1963 where he became a member of the Apollo 12 mission along with Alfred Worden as back up pilot and David Scott as commander .\nAlan Bean served as a crew member of Apollo 12 . Alfred Worden was the backup pilot of Apollo 12 . Apollo 12 was commanded by David Scott . Alan Bean was selected by Nasa in 1963\nRequires (a) identifying independent semantic units (b) rephrasing those units to single sentences\n\nSlide Topic: This Work\nSummary:\nWe show that simple neural models seem to perform very on the original benchmark due to memorization of the training set\nWe propose a more challenging data split for the task to discourage memorization\nWe perform automatic evaluation and error analysis on the new benchmark, showing that the task is still far from being solved\n\nSlide Topic: WebSplit Dataset Construction\nSummary:\n<Alan_Bean | nationality | United_States, Alan_Bean | mission | Apollo_12, Alan_Bean | NASA selection | 1963> A lan Bean, born in the United St tes, was selected Alan Bean, born in the United States, was selected Alan Bean, born in the United States, was selected by NASA in 1963 and served as a crew member of by NASA in 1963 and served as a crew member o f by NASA in 1963 and served Apollo as 12. a crew member of Apollo 12. Apollo 12.\n(facts from DBpedia) Simple Sentences\nA lan Bean is a US national. A lan Bean is a US national. Alan Bean is a US national.\n<Alan_Bean | mission | Apollo_12> Alan Bean was on the crew of Apollo 12. A lan Bean was on the crew of Apollo 12. Alan Bean was on the crew of Apollo 12.\n<Alan_Bean | NASA selection | 1963> A lan Bean was hired by NASA in 1963. Alan Bean was hired by NASA in 1963. Alan Bean was hired by NASA in 1963.\nSets of RDF triples Complex Sentences\nMatching via RDFs ~1M examples\n\nSlide Topic: Preliminary Experiments\nSummary:\nVanilla LSTM seq2seq with attention\nsim ple sim ple simple\ncomp lex sen ten ce\nShared vocabulary between the encoder and the decoder\nSimple sentences predicted as a single sequence\nEvaluated using single-sentence, multi-reference BLEU as in Narayan et al. 2017\n\nSlide Topic: Preliminary Results\nSummary:\nOur simple seq2seq baseline outperform all but one of the baselines from\nseq2seq (ours) seq2seq split-multi hybrid multi-seq2seq split-seq2seq\nText Only Text + RDFs\nTheir best baselines were using the RDF structures as additional information\nDo the simple seq2seq model really performs so well? seq2seq (ours) seq2seq split-multi\n\nSlide Topic: BLEU can be Misleading\nSummary:\nIn spite of the high BLEU scores, our neural models suffer from:\nMissing facts - appeared in the input but not in the output\nUnsupported facts - appeared in the output but not in the input\nRepeated facts - appeared several times in the output\n\nSlide Topic: A Closer Look\nSummary:\nVisualizing the attention weights we find an unexpected pattern\nThe network mainly attends to a single token instead of spreading the attention\nThis token was usually a part of the first mentioned entity\nConsistent among different input examples\n\nSlide Topic: Testing for Over Memorization\nSummary:\nIn this stage we suspect that the network heavily memorizes entity-fact pairs\nWe test this by introducing it with inputs consisting of repeated entities alone\nThe network indeed generates facts it memorized about those specific entities\n\nSlide Topic: Searching for the Cause Dataset Artifacts\nSummary:\nThe original dataset included overlap between the training/development/test sets\nWhen looking at the complex sentences side, there is no overlap\nOn the other hand, most of the simple sentences did overlap (~90%)\nDev Dev Complex Simple\nComplex Train Simple target\nTest Test Complex Simple\nMakes memorization very effective leakage from train on the target side\n\nSlide Topic: New Data Split\nSummary:\nTo remedy this, we construct a new data split by using the RDF information:\nEnsuring that all RDF relation types appear in the training set (enable generalization)\nEnsuring that no RDF triple (fact) appears in two different sets (reduce memorization)\nThe resulting dataset has no overlapping simple sentences\nOriginal Split New Split unique dev simple sentences in train unique test simple sentences in train\n% dev vocabulary in train\n% test vocabulary in train\nHas more unknown symbols in dev/test need better models!\n\nSlide Topic: Copy Mechanism\nSummary:\nTo help with the increase in unknown words in the harder split, we incorporate a copy mechanism\nUses a copy switch - feed-forward NN component with a sigmoid-activated scalar output\nControls the interpolation of the softmax probabilities and the copy probabilities over the input tokens in each decoder step\ncopy switch attention weights (copy) 1 - copy switch softmax output\n\nSlide Topic: Results New Split\nSummary:\nBaseline seq2seq models completely break (BLEU < on the new split\noriginal split new split\nCopy mechanism helps to generalize\nMuch lower than the original benchmark - memorization was crucial for the high BLEU\n\nSlide Topic: Error Analysis\nSummary:\nOn the original split the models did very well (due to memorization) with up to 91% correct simple sentences correct missing repeated unsupported\noriginal split new split\nOn the new benchmark the best model got only up to\nThe task is much more challenging then previously demonstrated\n\nSlide Topic: Conclusions\nSummary:\nSimple neural models seem to perform well due to memorization\nWe propose a more challenging data split for the task to discourage this\nA similar update was proposed by Narayan et al. in parallel to our work\nWe perform automatic evaluation and error analysis on the new benchmarks, showing that the task is still far from being solved\n\nSlide Topic: More Broadly\nSummary:\nCreating datasets is hard!\nThink how models can cheat\"\nCreate a challenging evaluation environment to capture generalization\nLook for leakage of train to dev/test\nNumbers can be misleading!\nLook at the data\nLook at the model\n\n",
    "Slide Topic: Overview Research question\nSummary:\nCan orthographic (spelling) information enable better word translations in low-resource contexts?\nLanguages with common ancestors and/or borrowing exhibit increased lexical similarity\nSpelling of words can carry signal for translation\nLow-resource pairs are most in need of additional signal\n\nSlide Topic: Overview Task and general approach\nSummary:\nBilingual lexicon induction: single-word translations\nOperate on word embeddings\nHaghigi et al. (2008): orthographic features\nMikolov et al. (2013): word2vec, linear mapping\n\nSlide Topic: Baseline Artetxe et al 2017\nSummary:\nStart with dictionary D (inferred from numerals)\nLearn matrix W minimizing Euclidean distance between target (Z) and mapped source (XW) embeddings of pairs in D\nUse nearest neighbors as entries in new dictionary\n\nSlide Topic: Baseline Artetxe et al 2017 Problems\nSummary:\nLanguage English Word Baselines Prediction Reference\nGerman unevenly gleichmaig (evenly) ungleichmaig\nGerman Ethiopians Afrikaner (Africans) Athiopier\nItalian autumn primavera (spring) autunno\nFinnish Latvians ukrainalaiset (Ukrainians) latvialaiset\nSuffers from clustering problems present in word2vec\nSimilar distributions similar embeddings\nHints of correct translation present in spelling\n\nSlide Topic: Proposed modifications\nSummary:\nUse normalized edit distance in nearest-neighbor calculation\nDuring dictionary induction, distances between similarly-spelled words are reduced\nExtend embedding vectors with character counts\nExtend vectors with scaled counts of letters in both languages alphabets (scale constant k\nWord d1 d2 aba\n\nSlide Topic: Quantitative results\nSummary:\nEnglish Word Translation Accuracy\nGerman Italian Target Language Finnish\nBest when combined; largest contribution from embedding extension\nImprovement less pronounced for English-Finnish (linguistic dissimilarity)\n\nSlide Topic: Qualitative results\nSummary:\nLanguage English Word Baselines Prediction Our Prediction\nGerman unevenly gleichmaig (evenly) ungleichmaig\nGerman Ethiopians Afrikaner (Africans) Athiopier\nItalian autumn primavera (spring) autunno\nFinnish Latvians ukrainalaiset (Ukrainians) latvialaiset\nUse orthographic information to disambiguate semantic clusters\nSignificant gains in adequacy\n\nSlide Topic: Conclusion\nSummary:\nOrthographic information can improve unsupervised bilingual lexicon induction, especially for language pairs with high lexical similarity.\nThese techniques can be incorporated into other embedding-based frameworks.\n\nSlide Topic: Results with Identity\nSummary:\nEnglish Word Translation Accuracy w/ Identity\nGerman Italian Target Language Finnish\n\nSlide Topic: Proof of optimal W\nSummary:\nW = arg min DijXiW Zj2\n= arg min XiW (DZ )i2\n= arg max Tr(XWZD) W\n\nSlide Topic: Proof of optimal W continued\nSummary:\nW = arg max Tr(XWZD)\n= arg max Tr(ZDXW\n= arg max Tr(UV W [UV SVD(ZDX\n= arg max Tr(V WU)\n\n",
    "Slide Topic: Semantic Graphs\nSummary:\nWordNet-like resources are curated to describe relations between word senses\nThe graph is directed\nEdges have form <S, r, T>: <zebra, is-a, equine>\nStill, some relations are symmetric\n\nSlide Topic: Semantic Graphs Relation Prediction\nSummary:\nThe task of predicting relations (zebra is a <BLANK>)\nLocal models use embeddings-based composition for scoring edges\nTranslational Embeddings (transE) [Bordes et al. 2013]\nProblem: task-driven method can learn unreasonable graphs\n\nSlide Topic: Incorporating a Global View\nSummary:\nWe want to avoid unreasonable graphs\nImposing hard constraints isnt flexible enough\nOnly takes care of impossible graphs\nWe still want the local signal to matter - its very strong.\nOur solution: an additive, learnable global graph score\nScore(<zebra, hypernym, equine>| WordNet) =\nslocal(edge) + (sglobal(WN edge), sglobal(WN))\n\nSlide Topic: Global Graph Score\nSummary:\nBased on a framework called Exponential Random Graph Model (ERGM)\nThe score sglobal(WN) is derived from a log-linear distribution across possible graphs that have a fixed number n of nodes\nOK. What are the features?\n\nSlide Topic: Graph Motifs multiple relations\nSummary:\n(some) joint blue/orange motifs:\n\nSlide Topic: ERGM Training\nSummary:\nEstimating the scores for all possible graphs to obtain a probability distribution is implausible\nNumber of possible directed graphs with n nodes: O(exp(n2))\nEstimation begins to be hard at ~n=100 for R=1. In WordNet: n = 40K, R\nUnlike other structured problems, theres no known dynamic programming algorithm either\nWhat can we do?\nDecompose score over dyads (node pairs) in graph\nDraw and score negative sample graphs\n\nSlide Topic: Max Margin Markov Graph Model M3GM\nSummary:\nSample negative graphs from the local neighborhood of the true WN\nLoss = Max {0, 1 + score(negative sample)\nIts important to choose an appropriate proposal distribution (source of the negative samples)\nWe want to make things hard for the scorer\n\nSlide Topic: Evaluation\nSummary:\nNo reciprocal relations (hypernym hyponym)\nStill includes symmetric relations\nRule baseline - take symmetric if exists in train\nUsed in all models as default for symmetric relations DistMult\nSynset embeddings - averaged from FastText\nM3GM (re-rank top 100 from local)\nMetrics - MRR, H@10 transE\n\nSlide Topic: Relation Prediction WN18RR\nSummary:\nRule-based ComplEx Conv* transE | w/ M3GM\n\nSlide Topic: Feature Analysis\nSummary:\nMotifs with heavy positive weights:\nMotifs with heavy negative weights:\nTarget of both has_part and verb_group\nSeen in training data\nDerivations occur in the abstract parts of the graph\n(bodega canteen vs. shop)\n\nSlide Topic: Future Work\nSummary:\nMultilingual transfers of semantic graphs align embeddings / translate concepts\nCan we introduce global features to help?\n\nSlide Topic: Conclusion\nSummary:\nGlobal reasoning of graph features is beneficial for relation prediction\nWorks well on top of strong local models\nApplicable to large graphs with dozens of relation types M3GM\nOrthogonal of word / synset embedding techniques\nFinds a wide variety of linguistic patterns in semantic graphs\n\n",
    "Slide Topic: Geolocation Prediction\nSummary:\nPredict a loca8on of a person\nMy house is at\nVancouver. Vancouver geoloca8on predic8on city name an SNS message\n\nSlide Topic: Our Approach\nSummary:\nGeoloca8on predic8on with neural networks\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Embedding\u0001\nCity User W ord Em bedding\u0001 Embedding\u0001 Embedding\u0001\nInpu ts m essages, metadata, and u ser network\nAtte ntionM\u0001 Text processes with AttentionL\u0001 AttentionD\u0001 AttentionN\u0001\nR NN+APen8on Timezone Embedding\u0001\nAttentionU\u0001 with APen 8on\nTEXT\u0001 Quite a comp lex\n\nSlide Topic: Geolocation Prediction Target\nSummary:\nA popular target in previous works (Cheng et al.,\nGround-truth loca8ons with geotags\n\nSlide Topic: Metadata\nSummary:\nDescrip8on, loca8on, 8mezone, etc.\nState-of-the-art performances combined with texts\nDescrip0on I work as a researcher at XXX\nLoca0on I live in Canada.\nTwiPer user Timezone America/Vancouver\n\nSlide Topic: User Network\nSummary:\nMen8on network, friend network, etc.\nPredic8on with label propaga8on\nState-of-the-art performances combined with texts\nMen8on user 1 Men8on user 2\n\nSlide Topic: Model\nSummary:\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Embedding\u0001\nCity User Word Em bedding\u0001 Embedding\u0001 Embedding\u0001\nM essages Metadata User network\n\nSlide Topic: TEXT Component 1\nSummary:\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Embedding\u0001\nCity User Word Em bedding\u0001 Embedding\u0001 Embedding\u0001\n\nSlide Topic: TEXT Component 2\nSummary:\nfeatures\u0001 user network representation\u0001\nFigure 2: Overview of the text component with\nrepresentation\u0001 nent with a detail\nLayers wise addition and\nand h are concatenated to form g where gt\nht ht and are passed to the first attention layer\nAttentionM. Recurrent Neural Network similarly to messa\nAttentionM computes a message representa- Specifically, GRU (Cho et al., 2014) AttentionM\u0001 attention layer. B\n(Self) APen8on tion m as a weighted sum of gt with weight t: tion and one descr\ntion laye r is not re RNNM\u0001\nAPen8onM m tgt RNNM output pon ent. We also c\nexp vTut Mul8-layer Word Em bedding\u0001 amo ng the m essag\nSofmax t Perceptron tion processes bec\nt exp (vT ut) information. For\nut = tanh (W gt b) assigned f or each\nwhere v is a weight vector, W is a weight ma- and a description\n\nSlide Topic: TEXT and META Component\nSummary:\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Text processes Embedding\u0001\n(singl e te xt) Embedding RNNM\u0001 RNNL\u0001 RNND\u0001\nCity User Word Em bedding\u0001 Embedding\u0001 Embedding\u0001\nM essages Metadata (loca8on, d escrip8on mezone)\n\nSlide Topic: USERNET Component 1\nSummary:\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Embedding\u0001\nCity User Word Em bedding\u0001 Embedding\u0001 Embedding\u0001\n\nSlide Topic: USERNET Component 2\nSummary:\nEmbedding for each user\nEmbedding for each ground-truth city of a user City User\nunknown for unavailable cases\nAPen8on over N users\nlinked user 1\u0001 User current Network\u0001 linked user\u0001 user N\u0001\n\nSlide Topic: Unified Processes\nSummary:\nAttentionTL\u0001 USERNET\u0001 User network\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 Timezone Embedding\u0001\nCity User Word Em bedding\u0001 Embedding\u0001 Embedding\u0001\n\nSlide Topic: Data 1\nSummary:\nUni-direc8onal men8on network (Rahimi et al.,\nDataset users + one-hop users\nSet undirected edges for men8ons\nBob set edge Bob\n\nSlide Topic: Data 2\nSummary:\n* Restricted edges to sa8sfy one of the following condi8ons:\nBoth users have ground truth loca8ons.\nOne user has a ground truth loca8on and another user is men8oned 5 8mes or more.\n\nSlide Topic: Baselines and Sub models\nSummary:\nLR X logistic (Rahimi regression, et al. 2015a) k-d tree\nMADCEL-B-LR X X logistic regression, k-d tree,\nLR-STACK X X X logistic regression, k-d tree, stacking, Modified Adsorption\nSUB-NN-UNET X X TEXT, USERNET\nSUB-NN-META X X TEXT&META\nProposed Model X X X Full model\n\nSlide Topic: Metrics\nSummary:\nAccuracy The percentage of correctly predicted cities.\nA within relaxed 161 accuracy km as correct that takes predictions. prediction errors\nMedian Error Distance* Median value of error distances in predictions.\nError Distance = 94km\n* Error distance evalua8ons are excluded from this presenta8on for simplifica8on.\n\nSlide Topic: Results TwiPerUS\nSummary:\naccuracy@161 * significant improvement with 5% significance level\n\nSlide Topic: Results W NUT\nSummary:\nBaselines (reported)\u0001 Jayasinghe et al. (2016)\naccuracy@161 ** significant improvement with 1% significance level\n\nSlide Topic: Analysis of Attention layers 1\nSummary:\nAnalysis of APen8on layers (1)\nTwiPerUS pro bability FCUN\u0001 densit y func8ons AttentionUN\u0001 FCU\u0001\nAttentionTL\u0001 Timeline & USERNET\u0001\nAtte ntionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezo ne description\u0001 timezone\u0001 Metadata Embedd ing\u0001 RNNM\u0001 RNNL\u0001 RNND\u0001 City User W ord Em bedding\u0001 Embedding\u0001 Embedding\u0001\n\nSlide Topic: Analysis of Attention layers 2\nSummary:\nAnalysis of APen8on layers (2)\nprobability FCUN\u0001 density func8on\nAttention TL\u0001 User&UserNetw oUSrEkRNET\u0001\nAttentionM\u0001 AttentionL\u0001 AttentionD\u0001 AttentionN\u0001 Timezone Embedding\u0001 RNNM\u0001 RNNL\u0001 RNND\u0001\nCity User W ord Em bedding\u0001 Embedding\u0001 Embedding\u0001\n\nSlide Topic: Conclusion\nSummary:\nProposed a neural network model for geoloca8on predic8on\ntext metadata user network\nImprovement over previous state-of-the art models\nAnalysis of aPen8on probabili8es:\nCapturing sta8s8cal characteris8cs of the datasets\n\nSlide Topic: Future Works\nSummary:\nAn extension of the proposed model\nIntroduc8on of temporal state\nCapture loca8on changes like travel\nApplica8on to different tasks\nFor example, gender analysis or age analysis\nSome metadata may not be effec8ve\n\nSlide Topic: Attention Patterns\nSummary:\nClustering of APen8onU and APen8onUN\nk-means with 9 clusters\nCluster 2 and Cluster 3 balancing Timeline and Loca8on\nTwitterUS\u0001 TwitterUS\u0001 W-NUT\u0001 W-NUT\u0001\nTwitterUS\u0001 TwitterUS\u0001 W-NUT\u0001 TwitterUS\u0001 W-NUT\u0001\n\nSlide Topic: Errors with High Confidences\nSummary:\nLoca8on Field: Hong Kong\nPerhaps, a house move\nTwee8ng about San Francisco\n\nSlide Topic: Model Configuration 1\nSummary:\nAttention context vector size\u0001\nMax tweet number per user\u0001\n\nSlide Topic: Training Time\nSummary:\nGeForce GTX Titan X\n\n",
    "Slide Topic: Structured Prediction Reviewed\nSummary:\nz Shareholders took their money\ns s their money took their took money their took s , s s\ns.t. z forms a tree\n\nSlide Topic: Linear Programming Formulation\nSummary:\nz Shareholders took their money\ns.t. z forms a tree\ns s took took money their\n\nSlide Topic: Backprop\nSummary:\ns s took took money their\ns.t. z forms a tree\nz Shareholders took theirmoney rzL\ns their took rsL\nWe have: rzL We need: rsL\nz = argmax z>s\n\nSlide Topic: Some Geometry\nSummary:\nStraight-through Estimator (STE): rsL rzL\nq Shareholders took theirmoney\n\nSlide Topic: Algorithm\nSummary:\nz Shareholders took their money rzL\nParser z argmax z> s\ns s took took money their s.t. z forms a tree rsL\ns their took rsL z\u0000 q\n\nSlide Topic: Connections to Related Work\nSummary:\nz \u0000rzL \u0000rzL z\nPipeline STE Structured Att. SPIGOT\nHard decision on z\nStructured Attention: Kim et al., 2017\n\nSlide Topic: Applications\nSummary:\nShareholders took theirmoney L1 Shareholders took theirmoney\nJoint learning Induce latent structures\nTraining data Training data\nargmax Parser rL1 argmax Parser\nDownstream task r\u0000L2 Downstream task r\u0000L\nLoss L2 Loss L\n\nSlide Topic: Experiments Syntactic then semantic Parsing\nSummary:\narg2 Semantic graph arg1 poss\nKiperwasser and Goldberg, 2016\nShareholders took theirmoney NeurboParser\nEisner, 1996 argmax Syntactic Parser\ntook root money took Peng et al., 2017\nConcat head token embedding Semantic Parser\n\nSlide Topic: SemEval 15 Micro averaged labeled F1\nSummary:\nNeurbo Pipeline STE Structured Att. SPIGOT\nHard decision z N/A\n\nSlide Topic: Semantic Parsing for Sentiment Classification\nSummary:\nSemantic graph arg1 poss\nMartins et al., arg max Semantic Parser\nShareholders took theirmoney BiLSTM+MLP\ntook: arg1 took:arg2; their:poss\nConcat head token and role Classifier\n\nSlide Topic: Stanford Sentiment Treebank accuracy\nSummary:\nBiLSTM Pipeline STE SPIGOT\n\n",
    "Slide Topic: Inferring Character State\nSummary:\nThe band instructor told the band to Players\nHe often stopped the music when players were off-tone. frustrated\nannoyed They grew tired and started playing\nworse after a while.\nThe instructor was furious and threw\nangry his chair. afraid\nHe cancelled practice and expected us to perform tomorrow. stressed\n\nSlide Topic: Reasoning about Naieve Psychology\nSummary:\nNew Story Commonsense Dataset:\nOpen text + psychology theory\nComplete chains of mental states of characters\nImplied changes to characters\n\nSlide Topic: How do we represent naive psychology\nSummary:\nThe band instructor told the band to start playing.\nHe often stopped the music when players were off-tone.\nTo create a good harmony\nAnger feels feels frustrated\n\nSlide Topic: Naieve Psychology Annotations\nSummary:\nCausal source to actions\n\nSlide Topic: Motivation Maslow Hierarchy of Needs 1943\nSummary:\nShe sat down on the couch and instantly fell asleep.\nShe sat down to eat lunch.\n\nSlide Topic: Motivation Reiss Categories 2004\nSummary:\nEsteem She sat down on the couch\nand instantly fell asleep.\nFood She sat down to eat lunch.\n\nSlide Topic: Emotional Reaction Plutchik 1980\nSummary:\nTheir favorite uncle died.\nSuddenly, they heard a loud noise.\n\nSlide Topic: Implicit Mental State Changes\nSummary:\nThe band instructor told the band to start playing.\nHe often stopped the music when players were off-tone.\nThey grew tired and started playing worse after a while.\nThe instructor was furious and threw his chair.\nHow are players affected? implicitly involved inference in these cases\n\nSlide Topic: Tracking Mental States\nSummary:\nThe band instructor told the band to start playing.\nHe often stopped the music when players were off-tone.\nThey grew tired and started playing worse after a while.\nThe instructor was furious and threw his chair.\nHe cancelled practice and expected us to perform tomorrow.\nWhy does the instructor cancel practice? based on previous info need to incorporate context\n\nSlide Topic: Related Work\nSummary:\nReasoning about narratives (Mostafazadeh et al 2016)\nDetecting emotional content (Mohammad et al 2013) or stimuli (Gui et al 2017) of a statement\nBoth motivation and emotion for a characters outlook\nLeverage psychology theories and natural language explanations\n\nSlide Topic: Full Annotation Chain\nSummary:\nSarah gets attacked by a shark.\nSarah fights off the shark.\nSarah escapes the attack.\nIs Sarah taking action: Yes\nStability to escape to safety\nDoes the Shark have a reaction?\n\nSlide Topic: Data Collection Summary\nSummary:\nOver 300k low-level annotations for 15k stories from ROC training set\nOpen-text Open-text + categories\n\nSlide Topic: Annotated Data Distributions Motivation\nSummary:\nFair amount of diversity in the open-text\n~1/3 have positive motivation change:\nSampled Explanations Open-text % Annotations where selected\nmeet goal; to look nice\nto support his friends\nbe employed; stay dry\n\nSlide Topic: Annotated Data Distributions Emotion\nSummary:\nLots of happy stories\n~2/3 have positive emotion change:\nExplanations % Annotations where selected\n\nSlide Topic: New Tasks\nSummary:\nGiven a story excerpt and a character can we explain the mental state:\nExplanation Generation: Generate open-text explanation of motivation/emotional reaction\nState Classification: Predict Maslow/Reiss/Plutchik category\n\nSlide Topic: Task 1 Explanation Generation\nSummary:\nExplain mental state of character using natural language\nThe band instructor told the band to start playing.\nStory Text Excerpt + Character\n\nSlide Topic: Modeling\nSummary:\nStory Text + Character\nEncoders - LSTM, CNN, REN, NPN\nDecoder for generation: single layer\nDecoder for categorization: logistic regression cat = !$`abb()\n\nSlide Topic: Encoding Modules\nSummary:\nGiven entity and line (and entity-specific context sentences\nCNN, LSTM: encode last line and context -- concatenate\n\nSlide Topic: Entity Modeling\nSummary:\nRecurrent Entity Networks (Henaff et al 2017)\nStore separate memory cells for each story character\nUpdate after each sentence with sentence-based hidden states\nNeural Process Networks (Bosselut et al 2018)\nAlso has separate representations for each character\nUpdates after each sentence using learned action embeddings\n\nSlide Topic: Explanation Generation Set up\nSummary:\nEvaluation: Cosine similarity of generated response to reference\nRandom baseline: Select random answer from dev set\nWords for describing intent/emotion are close in embedding space\n\nSlide Topic: Explanation Generation Results\nSummary:\nCos. Similarity to Reference\nMotivation (VE) Emotion (VE)\nRandom LSTM CNN REN NPN\n\nSlide Topic: Predicting psychological categories for mental state\nSummary:\nTask 2 Mental State Classification\nThe band instructor told the band to start playing. anticipation\nStory Text Excerpt + Character Theory categories\n\nSlide Topic: State Classification Results\nSummary:\nLSTM perform best on motivation categories\nEntity modeling has slight improvement in Plutchik Maslow Reiss Plutchik\nRandom LSTM CNN REN NPN\n\nSlide Topic: Further Improvement\nSummary:\nRandom LSTM CNN REN NPN\n\nSlide Topic: Effect of Entity Specific Context\nSummary:\nIncluding previous lines from context that include entity\nF1 w/ and w/o context\nEntity specific context: improves all models F1\nCNN CNN w/ context\n\nSlide Topic: Pre training Encoders\nSummary:\nWe have more open-text explanations than category annotations:\n1. Pre-train encoders on open- text explanations\n2. Fine-tune with the categorical labels\nStory Text + Character\n\nSlide Topic: Effect of Pretrained Encoders\nSummary:\nF1 w/ and w/o Pretrained Encoders\n\nSlide Topic: Performance Per Category\nSummary:\nVery concrete sets of actions (physiological F1: 40% )\n\nSlide Topic: Future Work\nSummary:\nOutside Knowledge: Help with infrequent classes and subtle implied changes\nSocial Commonsense: Help with inferring mental state especially in more contextual cases\nPotential Applications: Improving language models, chat systems, natural language understanding\n\nSlide Topic: Conclusions\nSummary:\n15k roc stories annotated per character\n\n",
    "Slide Topic: Motivation 1 Satire or not\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nAfter years of ghting there\nnally is a settlement\nbetween the Gema and\nYoutube . It became known today , that in future every music video is allowed to be played back in Germany again, as long as the audio is removed\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Motivation 2 Satire or not\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nErfurt ( dpo ) It is an organization which operates outside of law and order, funds numerous NPD operatives and is to a not inconsiderable extent involved in the series of murders of the so-called\nDPA is a German news agency\nDPO does not exist (in this context). University of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Satire\nSummary:\nForm of art to critize in an entertaining manner\nStylistic devices include humor, irony, sarcasm\nGoal: Mimic regular news in diction\nIts not misinformation or desinformation (fake news):\nArticles typically contain satire markers\n(similar to irony or sarcasm)\nAutomatically distinguish satirical news from regular news\nChallenging task (even for humans)\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Previous Work\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nCreated data sets which are automatically labeled from publication source\nPotential limitation: Models might learn characteristics of publication sources instead of actual characteristics of satire\n(evaluation is not faulty, they use dierent publication sources for validation than for training)\nBad generalization to unseen publication sources?\nInterpretation of models (regarding concepts of satire) misleading?\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Our Contributions\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nWe propose adversarial training: Improve robustness of model against confounding variable of publication sources\nWe show that adversarial training is crucial for the model to pay attention to satire instead of publication characteristics\nWe publish a large German data set for satire detection.\nFirst dataset in German\nFirst dataset including publication sources\nLargest resource for satire detection so far\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Model\nSummary:\nsatire detector publication identifier\nJ s J p\ns p satire? (yes/no) publication name\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Data Collection and Selection\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nDer Spiegel, Der Standard, Die Zeit, Suddeutsche Zeitung\nDer Enthuller, Eulenspiegel, Nordd. Nach., Der Postillon,\nSatirepatzer, Die Tagespresse, Titanic, Welt (Satire), Der\nZeitspiegel, Eine Zeitung, Zynismus24\nPublication #Articles Article Sent. Title\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Research Question 1 Performance\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nHow does a decrease in publication classication performance through adversarial training aect the satire classication performance?\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Research Question 2 Attention Weights\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nIs adversarial training eective for avoiding that the model pays most attention to the characteristics of publication source rather than actual satire?\nErfurt ( dpo ) - It is an organization which operates outside of law and order , funds numerous NPD operatives and is to a not inconsiderable extent involved in the series of murders of the so called Zwickauer Zelle .\nnumerous NPD operatives and is to a not inconsiderable extent involved in the series of murders of the so called Zwickauer Zelle .\nAfter discussed all , , the whereof proposal the to Union allow hopes family for reunion an off-putting only inclusive effect mothers-in-law . is being\nadvAfter all , the proposal to allow family reunion only inclusive mothers-in-law is being discussed , whereof the Union hopes for an off-putting effect .\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\nSlide Topic: Conclusion and Availability\nSummary:\nSatire & Research Goals Model/Data Experiments & Results Conclusion\nObservation: Satire detection models learn characteristics of publication sources\nAdversarial training to control for this confounding variable\nConsiderable reduction of publication identication performance while satire detection remains on comparable levels\nAttention weights show eectiveness of our approach\nFirst German dataset for satire detection\nDataset and code available at: http://www.ims.uni-stuttgart.de/data/germansatire\nUniversity of Stuttgart McHardy/Adel/Klinger June 3rd, 2019\n\n",
    "Slide Topic: Motivation\nSummary:\nGood translation preserves the meaning of the sentence.\nNeural MT learns to represent the sentence.\nIs the representation meaningful in some sense?\n\nSlide Topic: Gist of our idea\nSummary:\n1. Train variants of NMT to obtain sentence representations.\n2. Evaluate all such representations semantically.\n3. Relate performance in MT and in semantics.\n\nSlide Topic: Evaluating sentence representations\nSummary:\nprediction tasks for evaluating sentence embeddings\nfocus on semantics (recently, linguistics task added, too).\nHyTER paraphrases (Dreyer and Marcu, 2014)\n\nSlide Topic: Evaluation through classification\nSummary:\nSentEval Classification Tasks an ambitious and moving but bleak film . | and that makes all the difference . | rarely , a movie is more than a movie . +> H | the movie is well done , but slow . | | | the pianist is polanski 's best film . i ||\nand that makes all the difference .\nrarely , a movie is more than a movie . +> H | > 1*x C4)\nthe movie is well done , but slow .\nthe pianist is polanski 's best film .\ne Solo: movies sentiment, product review polarity, question type...\nA square full of people and life . is . =>. E v\nThe square is busy . Hifi\nThe couple is at a restaurant . > LM ~. N Xx\nA cute couple at a club Hl |\nA white dog bounding through snow Ew > F Cv\ne Paired: natural language inference, semantic equivalence e 10 classification tasks in total, we report them as AvgAcc\nA cute couple at a club || 7\noO 4k-55k training examples, with testset or 10-fold crosseval.\n\nSlide Topic: Evaluation through similarity\nSummary:\n7 similarity tasks: pairs of sentences + human judgement\nI think it probably depends on your money. It depends on your country.\nYes, you should mention your experience. Yes, you should make a resume\nHope this is what you are looking for. Is this the kind of thing you're looking for?\nwith training set, sent. similarity predicted by regression, without training set, cosine similarity used as sent. sim., ultimately, the predicted sent. similarity is correlated with the golden truth.\nIn sum, we report them as AvgSim.\n\nSlide Topic: Evaluation using paraphrases the data\nSummary:\n500 translations each captions each\nKhe NaS AKSR AAR, BN-TAAM. the deep cut and halter golden swimwear weighs half kilogram selling at ten million JPY.\n10,000,000 is the retail value for the low-cut gold bathing suit with a low back, and the weight is 5 hundred g. at the weight of five hundred grams, the low cut, halter swimsuit made up of gold will sell at ten million Japanese Yen (JPY).\na person is feeding a donut to the cat. a cat being fed a donut by someone in a grey shirt. a cat nibbles on a sprinkled donut that is being fed by the owner. a grey cat biting into a frosted donuts a cat is eating a donut from a\n\nSlide Topic: Cluster separation Davies Bouldin index\nSummary:\nfind the least well-\n\nSlide Topic: Paraphrase retrieval task NN\nSummary:\nee ----@ nearest neighbor a KON and check\ne oe the same cluster\n\nSlide Topic: Classification task\nSummary:\nRemove some points from the clusters.\nTrain an LDA classifier with the remaining points.\nClassify the removed points back.\n\nSlide Topic: Sequence to sequence with attention\nSummary:\nij: weight of the jth encoder state for the ith decoder state\n\nSlide Topic: Multi head inner attention\nSummary:\nij: weight of the jth encoder state for the ith column of MT\nconcatenate columns of MT\nlinear projection of columns to control embedding size\n\nSlide Topic: Proposed NMT architectures\nSummary:\nATTN-CTX ATTN-ATTN (compound att.)\ndecoder operates on entire embedding decoder selects components of embedding\n\nSlide Topic: Evaluated NMT models\nSummary:\nFINAL, FINAL-CTX: no attention\nAVGPOOL, MAXPOOL: pooling instead of attention\nATTN-CTX: inner attention, constant context vector\nATTN-ATTN: inner attention, decoder attention\nTRF-ATTN-ATTN: Transformer with inner attention\ntranslation from English (to Czech or German), evaluating embeddings of English (source) sentences\n\nSlide Topic: Sample Results translation quality encs\nSummary:\nSelected models trained for translation from English to Czech. The embedding size is 1000 (except ATTN).\nBLEU is consistent with human evaluation.\nAttention in the encoder helps translation quality.\n\nSlide Topic: Sample Results representation eval encs\nSummary:\nSelected models trained for translation from English to Czech. InferSent and GloVe- BOW are trained on monolingual (English) data.\nBaselines are hard to beat.\nAttention harms the performance.\n\nSlide Topic: Full Results correlations excluding Transformer\nSummary:\nBLEU vs. other metrics:\n\nSlide Topic: Compound attention interpretation\nSummary:\nATTN-ATTN en-cs model with 8 heads\n\nSlide Topic: Summary\nSummary:\nProposed NMT architecture combining the benefit of attention and one $&!#* vector representing the whole sentence.\nEvaluated the obtained sentence embeddings using a wide range of semantic tasks.\nThe better the translation, the worse performance in meaning representation.\nHeads divide sentence equidistantly, not logically.\nHeads divide sentence eJoqiuni doisutr antly, not logically.\nJNLE Special Issue on Sentence Representations:\n\nSlide Topic: InferSent multi task training in OCs thesis only\nSummary:\ne |dea: produce better representations by jointly training\nNMT with other tasks\ne Proxy: predict InferSent embeddings as the auxiliary task\nL= Ly + alomse\nL tar mT, target\n\nSlide Topic: Multi task training results encs\nSummary:\n<I Multitask Inactive, a\nSmall loss in BLEU xc. maxeooy, Sometimes gain iN AVGACC cexe. 4000, 4h)\n<4 Multitask Active, a\n\nSlide Topic: Multi task training results ende\nSummary:\nSEE Ne Een Sennen aoe\nen-de results less stable (much smaller vocabulary).\n<1 Multitask Inactive, a <4 Multitask Active, a=\ne Generally promising. L\ne Further exploration of a values |...\nand datasets needed. L\n\n",
    "Slide Topic: Introduction\nSummary:\nMachine Translation Dialogue Text to Code\nthe big black cat A: What do you want to do tonight? sort a list of numbers\nB: Lets go for a movie!\nfor i in range(len(A)): min_idx = i\nInput Sentence Compression Reconstruction\n\nSlide Topic: Unsupervised Models for Language\nSummary:\nDiscrete Latent Variable Autoencoders\nModel the discreteness of language\nSampling is not differentiable\nREINFORCE: sample inefficient and unstable\n\nSlide Topic: Contributions\nSummary:\nModel Supervision Abstractive Differentiable Latent\nFully unsupervised and abstractive\nFully differentiable (continuous approximations)\nHuman-readable compressions via LM prior\nUser-defined flexible compression ratio\nSOTA in unsupervised sentence compression\n\nSlide Topic: SEQ3 Overview\nSummary:\nReconstruction loss: distill input into the latent sequence\nLM Prior loss: human-readable compressions\nCompressor Minimize DKL between Compressor and LM:\nTopic loss: similar topic as input\nvx: IDF-weighted average of esi\nReconstructor vy: average of eci\nLength constraints: user-defined shorter length\nLength-aware decoder initializat ion\nCountdown Encoder inputs Decoder\n3. Explicit length penalty\n\nSlide Topic: Differentiable Sampling\nSummary:\nForward-pass: Discrete embedding (Gumbel-max trick)\nBackward-pass: Mixture of embeddings (Gumbel-softmax approx.)\n\nSlide Topic: Experimental Setup\nSummary:\nTrain LM (LM prior) Train seq3\nNever exposed to target sentences (compressions)\nVocabulary: 15K most frequent words in source sentences\nAverage F1 of ROUGE-1, ROUGE-2, ROUGE-L\n\nSlide Topic: Results on Gigaword\nSummary:\nSupervision Model R-1 R-2 R-L\nUnsupervised Pretrained Generator (Wang & Lee,2018)\nTable: Results on (English) Gigaword for sentence compression.\n\nSlide Topic: Ablation\nSummary:\nBoth topic and LM losses work in synergy\nLM prior loss: how words should be included\nTopic loss: what words to include\n\nSlide Topic: Model Outputs\nSummary:\nthe central election commission ( cec ) on monday decided that taiwan will hold another election of national assembly members\nnational <unk> election scheduled for may\nthe central election commission ( cec ) announced elections\nINPUT dave bassett resigned as manager of struggling english pre-\nmier league side nottingham forest on saturday after they were\nknocked out of the f.a. cup in the third round according to local reports on saturday\nforest manager bassett quits\ndave bassett resigned as manager of struggling english premier league side UNK forest on knocked round press\n\nSlide Topic: Conclusions and Future Work\nSummary:\nFully differentiable seq2seq2seq (seq3) autoencoder\nSOTA in unsupervised abstractive sentence compression\nTopic loss is essential for convergence\nLM prior improves readability\nNext Step: unsupervised machine translation\nMachine Translation Dialogue Text to Code\nthe big black cat A: What do you want to do tonight? sort a list of numbers\nB: Lets go for a movie!\n\nSlide Topic: Differentiable Sampling Extended\nSummary:\nSoft-argmax: Weighted sum of embeddings from peaked softmax\n= argmax(ai i), i Gumbel\ny = softmax(ai i), i Gumbel\nGumbel-softmax: Differentiable approximation to sampling\nStraight-Through: forward-pass: one-hot, backward-pass: soft\n\nSlide Topic: Out of Vocabulary OOV Words\nSummary:\nWe copy OOV words using the approach of Fevry and Phang (2018).\nSimpler alternative to pointer networks (See et al., 2017).\nWe use a set of special OOV tokens: oov1, oov2, . . . , oovN\nWe replace the ith unknown word in the input with the oovi token.\nIf all the OOV tokens are used, we use the generic UNK token.\nIn inference, we replace the special tokens with the original words.\nRAW John arrived in Rome yesterday. While in Rome, John had fun.\nINPUT oov1 arrived in oov2 yesterday. While in oov2, oov1 had fun.\n\nSlide Topic: Temperature for Gumbel Softmax\nSummary:\nTemperature does not affect the forward pass, but it affects gradients.\nHavrylov & Titov (2017) tune bound\nIn our experiments the learned temperature lead to instability.\n\nSlide Topic: Implementation Details\nSummary:\nDecoders: 2-layer unidirectional LSTM with size\nEmbedding: initialize with 100d GloVe (Pennington et al., 2014)\nTied encoders of the compressor and reconstructor.\nShared embedding layer for all encoders and decoders.\nTied embedding-output layers of both decoders.\n\nSlide Topic: Length Control\nSummary:\nSample target length M.\nDecoders state length-aware initialization.\n\nSlide Topic: Results on DUC Shared Tasks\nSummary:\nTable: Results on the DUC-2004\n\nSlide Topic: Model Output Extra\nSummary:\nINPUT the american sailors who thwarted somali pirates flew home\nto the u.s. on wednesday but without their captain , who was still aboard a navy destroyer after being rescued from the hijackers\nGOLD us sailors who thwarted pirate hijackers fly home\nSEQ3 the american sailors who foiled somali pirates flew home\n\n",
    "Slide Topic: Power laws of natural language\nSummary:\n2. Burstiness About how the words are aligned\nWords occur in clusters These can be analyzed through power laws\nOccurrences of words fluctuate\nTodays talk is about quantifying the degree of fluctuation.\nHow these could be useful will be presented at the end.\n\nSlide Topic: Fluctuation underlying text Look at variance in t\nSummary:\nAny words (any word, any set of words) occur in clusters\nOccurrences of rare words in Moby Dick (below 3162th)\nTwo ways of analysis\nLong range correlation weaknesses\n\nSlide Topic: Any words any word any set of words occur in clusters\nSummary:\nFluctuation underlying text Look at variance in\nOccurrences of rare words in Moby Dick (below 3162th)\nVariance is larger when events are clustered vs. random\nTwo ways of analysis\nFluctuation Analysis (Ebeling 1994) variance w.r.t.\nTaylors analysis Our achievements Long range correlation variance w.r.t. mean\n\nSlide Topic: Taylors law Smith 1938 Taylor 1961\nSummary:\nPower law between standard deviation and mean of event occurrences within (space or) time\nEmpirically (but is of course possible, too)\nEmpirically known to hold in vast fields (Eisler, 2007) ecology, life science, physics, finance, human dynamics\nThe only application to language is\nGerlach & Altmann (2014) not really Taylor analysis\nWe devised a new method based on the original concept of Taylors law\n\nSlide Topic: Our method\nSummary:\n1 For every word kind C count its number of occurrence within given length\nEstimate using the least squares method in log scale\n2 Obtain mean C and standard deviation C of C.\n@ log C log C'\n3 Plot C and C for all words. CF0\n\nSlide Topic: Taylors law of natural language\nSummary:\n- - Moby Taylors Here, Every English, vocabulary Dick point law 250k in\n5000. is size log a words, word 20k scale words kind\nTaylor exponent corresponds to gradient of log -log plot.\nTaylors law in log scale\n\nSlide Topic: Theoretical analysis of the exponent\nSummary:\nif all words are independent and identically distributed (i.i.d.).\nTaylor Exponent because shuffled text is equivalent to i.i.d. process.\nif words always co-occur with the same proportion. ex) Suppose that = {0,1}, and occurs always twice as\n\nSlide Topic: Taylors law for other data\nSummary:\nLisp, crawled and parsed\ndear this platform truck insert up xload things and hand unless let\n\nSlide Topic: Datasets\nSummary:\nNewspapers 3 (En,Zh,Ja) WSJ\nTagged Wiki 1 (En+tag) enwiki8\nCHILDES 10(En, Fr, Thomas (English)\nProgram Codes C++, Lisp, Haskell,\n\nSlide Topic: Taylor exponents of various data kind\nSummary:\nNone of the real texts showed the exponent 0.5\n\nSlide Topic: Summary thus far\nSummary:\nTaylors law holds in vast fields including natural/social science\nTaylors law also holds in languages and other linguistic related sequential data\nTaylor exponent shows the degree of co-occurrence among words\nTaylor exponent differs among text categories\n(No such quality for Zipfs law, Heaps law)\nHow can our results be useful?\nDo machine generated texts produce\n\nSlide Topic: Machine generated text by n grams\nSummary:\nbigrams of Moby Dick\n\nSlide Topic: Machine generated texts by character based LSTM language model\nSummary:\nStacked LSTM (3 LSTM layers)\nDistribution of following character\nLearning: Shakespeare by naive setting\nGeneration: Probabilistic generation of succeeding characters\n128 preceding characters State-of the art models present different results\n\nSlide Topic: Texts generated by machine translation\nSummary:\nLes Miserables translated by\nGoogle translator (in English)\nFluctuation that derives from the context is provided by the source text\n\nSlide Topic: Conclusion\nSummary:\nTaylors law holds in vast fields including natural/social science\nTaylors law also holds in languages and other linguistic related sequential data\nTaylor exponent shows the degree of co-occurrence among words\nTaylor exponent differs among text categories\n(No such quality for Zipfs law, Heaps law)\nHow can our results be useful?\nDo machine generated texts produce\nThe nature of : context and long memory one limitation of CL\nTaylor analysis would possibly evaluate machine outputs\nKnowing mathematical characteristic of texts serve for language engineering\n\n",
    "Slide Topic: Tabular QA Visual QA Reading Comprehension\nSummary:\nPeyton Manning became the first quarterback ever to lead two different teams to multiple Super Bowls. He is also the oldest quarterback ever to play in a\nSuper Bowl at age 39. The past record was held by John Elway, who led the Broncos to victory in Super Bowl XXXIII at age 38 and is currently Denvers Executive Vice\nPresident of Football Operations and\nQ: How many medals did India win? Q: How symmetrical are the white Q: What is the name of the\nA: bricks on either side of the building?\nA: very quarterback who was 38 in Super Bowl\nNeural Programmer (2016) Kazemi and Elqursh (2017) model. Yu et al (2018) model. accuracy on WikiTableQuestions (state of the art) on VQA 1.0 dataset (state of the art = 66.7%) F-1 score on SQuAD (state of the art)\nHave the models read the question carefully?\n\nSlide Topic: Visual QA\nSummary:\nKazemi and Elqursh (2017) model.\nQ: How asymmetrical are the white bricks on either side of the building?\nQ: How big are the white bricks on either side of the building?\nQ: How fast are the bricks speaking on either side of the building? A: very\n\nSlide Topic: QA over tables\nSummary:\n33.5% validation accuracy on WikiTableQuestions dataset (state of the art)\nQ: Which country won the most medals?\nNeural Programmer: max(total), print(nation)\nQ: Which country won the most number of medals?\nNeural Programmer: max(bronze), print(nation)\n\nSlide Topic: Jia and Liang 2017 Adversarial Attacks on Reading Comprehension Models\nSummary:\nEMNLP 2017 Outstanding Paper Award\nAdd an adversarial sentence to the paragraph to fool the model\nHighly successful attacks: over 16 models, F1 score drops from 75% to 36%\nTheir takeaway: reading comprehension models are overly stable; unable to distinguish a sentence that answers the question from one that merely has words common with the question\nQuestion for us: How does overstability manifest? Why do their attacks work?\n\nSlide Topic: Our contributions\nSummary:\nA workflow based on attributions (word-importances) to understand\ninput-output behavior of networks\nIdentify weaknesses in the networks as suggested by attributions\nCraft adversarial examples by exploiting the weaknesses\nExplain and improve Jia and Liang (2017)s attacks\n\nSlide Topic: Attributions\nSummary:\nProblem statement: Attribute a complex deep networks prediction to input\nfeatures, relative to a certain baseline (informationless) input\nE.g. : attribute an object recognition networks prediction to its pixels,\na text sentiment networks prediction to individual words\nExplain F(input) - F(baseline) in terms of input features\n\nSlide Topic: Visual QA attributions\nSummary:\nQ: How symmetrical are the white bricks on either side of the building?\nred: high attribution blue: negative attribution gray: near-zero attribution\n\nSlide Topic: Overstability\nSummary:\nDrop all words from the dataset except ones which are frequently top attributions\nE.g. How many players scored more than 10 goals? How many\nVisual QA Neural Programmer\ncolor, many, what, how, doing, or, where, there, many, tm_token, how, number, total, after,\n\nSlide Topic: Stopword deletion attack\nSummary:\nDelete contentless words from the question\nshow, tell, did, me, my, our, are, is, were, this, on, would, and, for, should, be,\nby, based, in, of, bring, with, to, from, whole, being, been, want, wanted, as, can, see, doing, got, sorted, draw, listed, chart, only\nNeural Programmers accuracy falls from 33.5% to 28.5%\n\nSlide Topic: Subject ablation attack\nSummary:\nReplace the subject of a question with a low-attribution noun from the vocabulary\nWhat is the man doing? What is the tweet doing?\nHow many children are there? How many tweet are there?\nVQ A models response remains same 75.6% of the time on questions that it originally answered correctly\n\nSlide Topic: Question concatenation attacks\nSummary:\nPrefix a content-free phrase to the question\nNeural Programmer Visual QA\nOriginal accuracy: Original accuracy:\n\nSlide Topic: Predicting the effectiveness of Jia and Liang 2017s adversarial attacks\nSummary:\nAttacks are more likely to be effective when\nHigh-attribution words are present in the adversarial sentence\nOnly low-attribution words are mutated\nred: high attribution, blue: negative attribution, gray: near-zero attribution\n\nSlide Topic: Summary\nSummary:\nAn attribution-based workflow to look inside and understand weaknesses of a model\nExplained how overstability manifests - QA networks do not focus on the right words!\nCrafted adversarial examples and improved Jia and Liang (2017)s attacks\nDeep learning practitioners can easily use attributions to look inside models\nAdding soft network constraints\nE.g. add bias to attention vector so as to limit the influence of how, what, etc.\nInformed enrichment of datasets\nE.g. add more questions with word symmetrical such that answer is not very\n\n",
    "Slide Topic: ICLR 2018 Neural Language Modeling by Jointly Learning Syntax and Lexicon\nSummary:\nSupervised Constituency Parsing with Syntactic Distance?\n\nSlide Topic: Chart Neural Parsers Transition based Neural Parsers\nSummary:\nComplexity of CYK is O(n^3).\nIncompleted tree (the shift and reduce steps may not match).\nThe model is never exposed to its own mistakes during training\n\nSlide Topic: Intuitions\nSummary:\nOnly the order of split (or combination) matters for reconstructing the tree.\nCan we model the order directly?\n\nSlide Topic: Tree to Distance\nSummary:\nThe height for each non-terminal node is the maximum height of its children plus 1\n\nSlide Topic: Distance to Tree\nSummary:\nSplit point for each bracket is the one with maximum distance.\n\nSlide Topic: Framework for inferring the distances and labels\nSummary:\nLabels for non-leaf nodes\nLabels for leaf nodes\n\nSlide Topic: Inferring the distances\nSummary:\n<s> She enjoys| | playing tennis a </s>\n\nSlide Topic: Pairwise learning to rank loss for distances\nSummary:\na variant of hinge loss\nWhile di > dj While di < dj\n\nSlide Topic: Inferring the Labels\nSummary:\non to fu fu te\n\nSlide Topic: Experiments Chinese Treebank\nSummary:\nModel LP LR Fil Semi-supervised\n\nSlide Topic: Conclusions and Highlights\nSummary:\nA novel constituency parsing scheme: predicting tree structure from a set of real-valued scalars (syntactic distances).\nCompletely free from compounding errors.\nStrong performance compare to previous models, and\nSignificantly more efficient than previous models\nEasy deployment: The architecture of model is no more than a stack of standard recurrent and convolutional layers.\n\nSlide Topic: One more thing\nSummary:\nThe research in rank loss is well-studied in the topic of\nModels that are good at learning these syntactic distances are not widely known until the rediscovery of LSTM in 2013 (Graves 2013).\nEfficient regularization methods for LSTM didnt become mature until\n\n",
    "Slide Topic: Sentiment to Sentiment Translation\nSummary:\nThe movie is amazing! - The movie is boring!\n2) I went to this restaurant last weak, the staff was friendly, and I were so happy to have a great meal! - I went to this restaurant last weak, the staff was rude, and I were so angry to have a terrible meal!\n\nSlide Topic: Applications Dialogue Systems\nSummary:\nI am sad about the failure of the badminton player A.\nThe badminton player B defeats A. Congratulations!\nRefined Answer: Im sorry to see that the badminton player B defeats A.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 5 of 34\n\nSlide Topic: Applications Personalized News Writing\nSummary:\nSentiment-to-sentiment translation can save a lot of human labor!\nThe visiting team defeated the home team\nNews for fans of the visiting team: The players of the home team performed badly, and lost this game.\nNews for fans of the home team: Although the players of the home team have tried their best, they lost this game regretfully.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 6 of 34\n\nSlide Topic: Challenge Can a sentiment dictionary handle this task\nSummary:\nThe simple replacement of emotional words causes low-quality sentences.\nThe food is terrible like rock The food is delicious like rock\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 7 of 34\nFor some emotional words, word sense disambiguation is necessary.\nFor example, good has three antonyms: evil, bad, and ill in WordNet. Choosing which word needs to be decided by the semantic meaning of good based on the given content.\nSome common emotional words do not have antonyms.\nFor example, we find that WordNet does not annotate the antonym of delicious.\n\nSlide Topic: Background State of the Art Methods\nSummary:\nThey first separate the non-emotional information from the emotional information in a hidden vector.\nThey combine the non-emotional context and the inverse sentiment to generate a sentence.\nAdvantage: The models can automatically generate appropriate emotional antonyms based on the non- emotional context.\nDrawback: Due to the lack of supervised data, most existing models only change the underlying sentiment and fail in keeping the semantic content.\nThe food is delicious What a bad movie\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 11 of 34\n\nSlide Topic: Approach Overview\nSummary:\nExtract non-emotional semantic information\nAdd sentiment to the neutralized semantic content\nCombine and train two modules.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 14 of 34\n\nSlide Topic: Neutralization Module\nSummary:\nLong-Short Term Memory Network\nGenerate the probability of being neutral or being polar\nThe learned attention are the supervisory signal.\nThe cross entropy loss is computed as\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 15 of 34\n\nSlide Topic: Emotionalization Module\nSummary:\nBi-decoder based encoder-decoder network\nThe encoder compresses the context\nThe decoder generates sentences\nThe input is the neutralized input sequence\nThe supervisory signal is the original sentence\nThe cross entropy loss is computed as\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 16 of 34\n\nSlide Topic: Cycled Reinforcement Learning\nSummary:\n1) Neutralize an emotional sentence to non-emotional\n2) Reconstruct the original sentence by adding the source\n3) Train the emotionalization module using the\n4) Train the neutralization module using reinforcement\n2) Force the emotionalization module to reconstruct the\noriginal sentence by adding the source sentiment.\n3) The reconstruct loss is used to train the\n\nSlide Topic: Reward\nSummary:\nAdd different sentiment to the semantic content\nUse the quality of the generated text as reward\nThe confidence score of a sentiment classifier\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 21 of 34\n\nSlide Topic: Dataset\nSummary:\nProvided by McAuley and Leskovec (2013). It consists of amounts of food\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 23 of 34\n\nSlide Topic: Baselines\nSummary:\nRefined alignment of latent.\nMulti-Decoder with Adversarial Learning (MDAL)\nA multi-decoder model with adversarial.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 24 of 34\n\nSlide Topic: Evaluation Metrics\nSummary:\nThe annotators are asked to score the transformed text in terms of sentiment and semantic\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 25 of 34\nsentiment and semantic similarity.\n\nSlide Topic: Results\nSummary:\nYelp ACC BLEU G-score\nAmazon ACC BLEU G-score\nAutomatic evaluations of the proposed method and baselines.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 27 of 34\nYelp Sentiment Semantic G-score\n\nSlide Topic: Generated Examples\nSummary:\nInput: I would strongly advise against\nCAAE: I love this place for a great\nMDAL: I have been a great place was\nProposed Method: I would love using\nInput: Worst cleaning job ever!\nCAAE: Great food and great service!\nMDAL: Great food, food!\nProposed Method: Excellent outstanding job ever!\nInput: Most boring show Ive ever been.\nCAAE: Great place is the best place in town.\nMDAL: Great place Ive ever ever had.\nProposed Method: Most amazing show Ive ever been.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 29 of 34\n\nSlide Topic: Analysis of the neutralization module\nSummary:\nMichael is absolutely wonderful.\nI would strongly advise against using this company.\nWorst cleaning job ever!\nMost boring show i ve ever been.\nHainan chicken was really good.\nI really dont understand all the negative reviews for this dentist.\nSmells so weird in there.\nThe service was nearly non-existent and extremely rude.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 31 of 34\n\nSlide Topic: Error Analysis\nSummary:\nThe service here is very good Outstanding and bad service\nIts our first time to the bar and it is totally amazing Its our first time to the bar\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 32 of 34\n\nSlide Topic: Conclusion\nSummary:\nA. Enable training with unpaired data.\nB. Tackle the bottleneck of keeping semantic.\nUnpaired Sentiment-to-Sentiment Translation: A Cycled Reinforcement Learning Approach 33 of 34\n\n",
    "Slide Topic: Motivation\nSummary:\n- Getting manually labeled data in each domain for sentiment analysis is always an expensive and a time consuming task, cross-domain sentiment analysis provides a solution.\n- However, polarity orientation (positive or negative) and the significance of a word to express an opinion often differ from one domain to another.\nChanging Significance: Entertaining, boring, one-note, etc. are classification in the movie domain. significant for\nChanging Polarity: Unpredictable plot of a movie //Positive sentiment\nUnpredictable behaviour of a machine //Negative sentiment\n\nSlide Topic: Problem Definition\nSummary:\nSignificant Consistent Polarity (SCP) words represent the transferable (usable) information across domains.\nWe present an approach based on test and cosine-similarity between context vector of words to identify polarity preserving significant words across domains.\nFurthermore, we show that a weighted ensemble of the classifiers enhances the cross-domain classification performance.\n\nSlide Topic: Technique Find SCP\nSummary:\nSignificant Consistent Polarity (SCP): S T\n//Transferable information from the source (S) to the target (T) for cross-domain SA.\nS: Significant words with their polarity orientation in the labeled source domain: 2 test\nH0 : unpredictable has equal distribution in the positive and negative corpora\nHa : unpredictable has significantly different count in either positive or negative corpus\nIf X2 score is greater than\nProbability of the observed value given null hypothesis is true is less than\n=> Reject the Null hypothesis\n=> unpredictable has occurred significantly more often in one of the class with a 2 score of\nCwP > CwN , hence unpredictable is po sitiveraksha.sharma1@tcs.com\n\nSlide Topic: Technique Find SCP 2\nSummary:\nT: Significant words with their polarity orientation in the unlabeled target domain:\nSignificance: NormalizedCountt(Significants(w)) > Significantt(w)\nNote: We construct a 100 dimensional vector for each candidate word from the unlabeled target domain data.\nSignificant Consistent Polarity (SCP): S T\n//Transferable information from the source to the target for cross-domain SA.\n\nSlide Topic: Example Inferred polarity orientation in the Target Domain\nSummary:\nCosine-similarity score with the Pos-pivot (great) and Neg-pivot (bad), and inferred polarity orientation of words in the movie domain.\n\nSlide Topic: F score for SCP words identification task\nSummary:\nAvailable at: http://www.cs.jhu.edu/~mdredze/datasets/sentiment/ind ex2.html\nGold standard SCP words: Application of test in both the domains considering target domain is also labeled gives us gold standard SCP words from the corpus. No manual annotation.\nSCL: Structured Correspondence Learning (Bhatt et al., 2015)\n\nSlide Topic: Domain Adaptation Algorithm\nSummary:\nCs(exampleDoc) = -0.07 (wrong prediction, negative)\nCt(exampleDoc) = 0.33 (correct prediction, positive)\n\nSlide Topic: Cross domain Results\nSummary:\nSys1 Sys2 Sys3 Sys4 Sys5 Sys6\nSystem Name: Transferred Info\nSystem-4: System-1 + iterations\nWe obtained a strong positive\ncross-domain between and accuracy\n\nSlide Topic: Conclusion\nSummary:\n- Significant Consistent Polarity (SCP) words shows a strong positive correlation of 0.78 with the sentiment classification accuracy achieved in the unlabeled target domain.\n- Essentially, a set of less erroneous transferable features lead to a more accurate classification system in the unlabeled target domain.\n\n",
    "Slide Topic: Classification vs Structured Prediction\nSummary:\nI like this book Classifier\nPredictor I like this book\n\nSlide Topic: Search based Structured Prediction\nSummary:\nI like this book\n\nSlide Topic: Problems of the Generic Learning Algorithm\nSummary:\nAmbiguities in training data Training and test discrepancy both this and the seems reasonable What if I made wrong decision?\nI like this book\n\nSlide Topic: Knowledge Distillation\nSummary:\nbook I like love the this book I like love the this\nis the output distribution of a teacher model (e.g. ensemble)\nOn supervised data argmax0\n\nSlide Topic: Knowledge Distillation from Where\nSummary:\nLearning from knowledge distillation\nargmax sumy q(y) p(y |I, like)\nbook I like love the this\nAmbiguities in training data\nWe use ensemble of M structure predictor as the teacher q\n\nSlide Topic: KD on Supervised reference Data\nSummary:\nbook I like love the this book I like love the this\nI like this book\n\nSlide Topic: KD on Explored Data\nSummary:\nbook I like love the this\nI like book this the\nTraining and test discrepancy Search Space\nExplore (Ross and Bagnell, 2010)\nWe use teacher q to explore the search space & learn from KD on the explored data\n\nSlide Topic: Experiments\nSummary:\nPenn Treebank (Stanford dependencies)\nLAS Neural Machine Translation\nMIXER (Ranzato et al.\n\nSlide Topic: Analysis Why the Ensemble Works Better\nSummary:\nExamining the ensemble on the problematic states.\nTestbed: Transition-based dependency parsing.\nTools: dynamic oracle, which returns a set of reference actions for one state.\nEvaluate the output distributions against the reference actions.\n\nSlide Topic: Analysis Is it Feasible to Fully Learn from KD w o NLL\nSummary:\nTransition-based Parsing Neural Machine Translation\nFully learning from KD is feasible\n\nSlide Topic: Conclusion\nSummary:\nWe propose to distill an ensemble into a single model both from reference and exploration states.\nExperiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single models performance.\nAnalysis gives empirically guarantee for our distillation method.\n\n",
    "Slide Topic: Machine learning can help you\nSummary:\n***If you have enough training data\n\nSlide Topic: Traditional Labeling\nSummary:\nTom Brady was spotted in New York City on Monday with his wife Gisele Bundchen amid rumors of\nBradys alleged role in Deflategate.\nIs person 1 married to person 2?\n\nSlide Topic: Higher Bandwidth Supervision\nSummary:\nTom Brady was spotted in New York City on Monday with his wife Gisele Bundchen amid rumors of\nBradys alleged role in Deflategate.\nIs person 1 married to person 2?\nWhy do you think so?\nBecause the words his wife are right before person 2.\n\nSlide Topic: Explanations Encode Labeling Heuristics\nSummary:\nWhy did you label True?\nBecause the words his wife are right before person 2.\nBarack batted back tears as he thanked his wife, Michelle, for all her help.\nBoth Bill and his wife Hillary smiled and waved at reporters as they rode by.\nGeorge attended the event with his wife, Laura, and their two daughters.\nBig Idea: Instead of collecting labels, collect labeling heuristics (in the form of explanations) that can be used to label more examples for free.\n\nSlide Topic: Babble Labble Framework\nSummary:\nINPUT SEMANTIC PARSER FILTER BANK LABEL AGGREGATOR DISC. MODEL\ny x y e1\nUnlabeled Examples + Explanations Labeling Functions Filters Label Matrix\nLabel whether person 1 is married to person 2\nx1 Tom Brady and his wife Gisele Bundchen were\nspotted in New York City on Monday amid rumors\nof Bradys alleged role in Deflategate.\nreturn if his wife in left(x.person2, dist==1) else Correct x1 x2 x3 x4\ndef LF_1b(x): return if his wife in right(x.person2) else\n(inconsistent) True, because the words his wife are right before person 2. def LF_2a(x): return if x.person1 in x.sentence and x.person2 in x.sentence else x2 None of us knows what happened at Kanes home Aug. 2, but it is telling that the NHL has not suspended Kane.\nPragmatic Filter LF4c (always true)\ndef LF_2b(x): return if x.person1 x.person2) else Correct False, because person 1 and person in the sentence are identical. y\nNoisy Labels Classifier x3 Dr. Michael Richards and real estate and insurance businessman Gary Kirke did not attend the event. Correct\nFalse, because the last word of person 1 is different than the last word of person 2.\nPragmatic Filter (duplicate of LF_3a)\ny x y PRAGMATIC\nTrue, because LF1A x1 x2 x3\nx1 x2 x3 y False, because\nEXPLANATIONS LABELING FUNCTIONS LABEL MATRIX PROBABILISTIC LABELS TRAINED MODEL\nIMPORTANT: No Babble Labble components require no labeled training data!\n\nSlide Topic: Explanations Encode Heuristics\nSummary:\nWhy did you label True?\nBecause the words his wife are right before person 2.\ndef f(x): return if (his wife in left(x.person2, dist==1)) else 0 #abstain\n\nSlide Topic: Semantic Parser\nSummary:\nSTART LABEL FALSE BECAUSE ARG AND ARG IS EQUAL STOP\n<START> label false because X and Y are the same person <STOP>\nLexical Rules Unary Rules Compositional Rules Ignored token\n<START> START FALSE BOOL TRUE BOOL START LABEL BOOL BECAUSE CONDITION STOP LF label false LABEL FALSE INT NUM ARGLIST ISEQUAL ARG AND ARG CONDITION ARGLIST\ndef LF(x): return [label] if [condition] else [abstain]\n\nSlide Topic: Semantic Parser I O\nSummary:\n1 Explanation 1 Parse\nTrue, because def f(x): return 1 if\nGoal: produce the correct parse\n1 Explanation Many Parses\nGoal: produce useful parses (whether theyre correct or not)\n\nSlide Topic: Filter Bank\nSummary:\nTrue, because def f(x): return 1 if def f(x): return 1 if\ndef f(x): return 1 if False, because\n\nSlide Topic: Semantic Filter\nSummary:\nExample x1: Tom Brady was spotted in New York City on\nMonday with his wife Gisele Bundchen amid rumors of Bradys alleged role in Deflategate.\nExplanation True, because the words his wife\nare right before person 2.\nright before = to the right of right before = immediately before\ndef LF_1b(x): return if his wife in right(x.person2) else def LF_1a(x): return if his wife in left(x.person2, dist==1) else\n(his wife is not to the right of person 2) (his wife is, in fact, 1 word to the left of person 2)\n\nSlide Topic: Pragmatic Filters\nSummary:\nHow does the LF label our unlabeled data?\nUniform labeling signature xN x1\n\nSlide Topic: Label Aggregator\nSummary:\nHigh correlation; not independent?\nHigh conflict; low accuracy?\nLow coverage, high accuracy?\nHow should I break this tie?\nData Programming: (Ratner, et al. NIPS 2016)\nAs implemented in: snorkel.stanford.edu\n\nSlide Topic: Discriminative Classifier\nSummary:\nLabeling functions generate noisy, conflicting votes\nResolve conflicts, re-weight & combine\nGeneralize beyond the labeling functions\n\nSlide Topic: Generalization\nSummary:\nTask: identify disease-causing chemicals\nKeywords mentioned in LFs:\ntreats, causes, induces, prevents,\nHighly relevant features learned by discriminative model:\ncould produce a, support diagnosis of,\nTraining a discriminative model that can take advantage of additional useful features not specified in labeling functions boosted performance by 4.3 F1 points on average (10%).\n\nSlide Topic: Datasets\nSummary:\nName Unlabeled Sample Explanations\nSpouse 22k Label true because \"and\" occurs between X\nand Y and \"marriage\" occurs one word after person1.\nDisease 6.7k Label true because the disease is immediately\nafter the chemical and \"induc\" or \"assoc\" is in the chemical name.\nProtein 5.5k Label true because \"Ser\" or \"Tyr\" are within\n10 characters of the protein.\n\nSlide Topic: Results\nSummary:\nClassifiers trained with Babble Labble and explanations achieved the same F1 score as ones trained with traditional labels while requiring 5100x fewer user inputs\n\nSlide Topic: Utilizing Unlabeled Data\nSummary:\nWith labeling functions, training set size (and often performance) scales with the amount of unlabeled data we have.\n\nSlide Topic: Filter Bank Effectiveness\nSummary:\nBabble Labble % Incorrected\nThe filters removed almost of incorrect parses. Without the filters removing bad parses, F1 drops by 15 F1 points on average.\n\nSlide Topic: Perfect Parsers Need Not Apply\nSummary:\nTask Babble Labble Babble Labble\nUsing perfect parses yielded negligible improvements. In this framework, for this task, a naive semantic parser is good enough!\n\nSlide Topic: Limitations\nSummary:\nAlice beat Bob in the annual office pool tournament.\nNo, because it sounds like theyre just co-workers. Prefers\n(e.g., it says so) (e.g., keywords, word distance, capitalization, etc.)\nUsers reasons for labeling are sometimes high-level concepts that are hard to parse.\n\nSlide Topic: Related Work Data Programming\nSummary:\nUse weak supervision (e.g., labeling functions) to generate training sets\nFlagship platform for dataset creation from weak supervision\nStructure Learning (Bach et al., ICML 2017)\nLearning dependencies between correlated labeling functions\nReef (Varma and Re, In Submission)\nAuto-generating labeling functions from a small labeled set\n\nSlide Topic: Related Work Explanations as Features\nSummary:\nWhat if we use our explanations to make features instead of training labels?\nUse as features for classifier\nLABEL AGGREGATOR DISC. MODEL Exp 4:\nExp 5: y x y Use as labels for training set\nUsing the parses to label training data instead of as features boosts 4.5 F1 points.\n\nSlide Topic: Related Work Highlighting\nSummary:\nHighlight key phrases in text:\nMark key regions in images:\nLabel key features directly:\nTom Brady was spotted in New York City on Monday with his wife Gisele Bundchen amid rumors of Bradys alleged role in Deflategate.\nBenefits of natural language approach: more options: e.g., X is not in the sentence, X or Y is in the sentence more direct credit assignment (compared to highlighting) no feature set required a priori\n\nSlide Topic: Summary\nSummary:\nWe need more efficient ways to collect supervision\nWe can collect labeling heuristics instead of labels\nUsing this approach, training set size grows with the amount of unlabeled data we have\n\nSlide Topic: Babble Labble\nSummary:\nTom Brady was spotted in New York City on Monday with his wife Gisele Bundchen amid rumors of\nBradys alleged role in Deflategate.\nLF3 Is person 1 married to person 2?\nWhy do you think so? Aggregated Labels\nBecause the words his wife are right before person 2. y\nx y def LF1(x): return if his wife in left(x.person2, dist==1) else\n\n",
    "Slide Topic: Background Dialog\nSummary:\nPersonal assistant, helps people complete specific tasks\nCombination of rules and statistical components\nNo specific goal, attempts to produce natural responses\nUsing variants of seq2seq model\n\nSlide Topic: Background Neural Model\nSummary:\nutterance-response: n-to-1 relationship e.g., the response Must support! Cheer! is used for 1216 different input utterances\nMy friends and I are shocked! pre-defined a set of topics\nfrom an external corpus rely on external corpus\ntreat all the utterance-response pairs uniformly employ a single model to learn the mapping between utterance and response\nintroduce latent responding factors to model multiple responding mechanisms lack of interpretation\nfavor such general responses with high frequency\n\nSlide Topic: How to capture different utterance response relationships\nSummary:\nOur motivation comes from\n\nSlide Topic: Human Conversation Process\nSummary:\nDo you know a good eating place for Australian special food?\nknowledge state dialogue partner\nGood Australian eating places include steak, seafood, cake, etc. What do you want to choose?\n\nSlide Topic: Model Architecture\nSummary:\nintroduce an explicit specificity control variable s to represent the response purpose\ns summarizes many latent factors into one variable s has explicit meaning on specificity actively controls the generation of the response\nknowledge state dialogue partner\n\nSlide Topic: Model Encoder\nSummary:\nthe specificity control variable is introduced into the Seq2Seq model single model -> multiple model\ndifferent <utterance, response>, different , different models\nsemantic representation: relates to the semantic meaning usage representation: relates to the usage preference\nmy name is John Semantic-based & Specificity-based Generation P(John) = ()(John) + (John) Response Decoder <eos> my name is Attentive Read Gaussian Kernel Layer Specificity Control Variable Utterance Encoder U\nSemantic Representation Usage Representation what is your name\n\nSlide Topic: Model Decoder\nSummary:\nBi-RNN: modeling the utterance from both forward and backward directions\npredict target word based on a mixture of two probabilities: the semantic-based and specificity-based generation probability\ndecides what to say next given the input\nhidden state semantic representation\n\nSlide Topic: Model Training\nSummary:\ndecides how specific we should reply\nthe specificity control variable interacts with the usage representation of words through the layer let the word usage representation regress to the variable through certain mapping function (sigmoid) , )U\nspecificity control variable 2U exp(\n0 denotes the most general response\n1 denotes the most specific response variance usage representation\n\nSlide Topic: Distant Supervision\nSummary:\nObjective function log likelihood\nTraining data: triples s is not directly available in the raw conversation corpus\nHow to obtain s to learn our model?\nWe propose to acquire distant labels for\n\nSlide Topic: Specificity Controlled Response Generation\nSummary:\nNormalized Inverse Response Frequency (NIRF)\na response is more general if it corresponds to more input utterances the Inverse Response Frequency (IRF) in a conversation corpus\nNormalized Inverse Word Frequency (NIWF)\na response is more specific if it contains more specific words the maximum of the Inverse Word Frequency (IWF) of all the words in a response\n\nSlide Topic: Experiments Dataset\nSummary:\nSpecificity Controlled Response Generation\nGiven a new input utterance, we can generate responses at different specificity levels by varying the control variable s\nDifferent s, different models, different responses\n= 1: the most informative response\n: more dynamic , enrich the styles in the response\n= 0: the most general response\nGeneral response Specific response\n\nSlide Topic: Experiments Model Analysis\nSummary:\nShort Text Conversation (STC) dataset\nreleased in NTCIR-13 a large repository of post-comment pairs from the Sina Weibo\n3.8 million post-comment pairs\nJieba Chinese word segmenter\n\nSlide Topic: Experiments Comparisons\nSummary:\nPRA She SaueNSSRAS ASS\nKey Laboratory of Network Data Science & Technology, CAS\nModels distinct-1 distinct-2 BLEU-1 BLEU-2 Average Extrema\nTable 3: Comparisons between our SC-Seq2Seq and the baselines under the automatic evaluation.\nWhen s = 1, our SC-Seq2Seqnywr model can achieve the best specificity performance\nBLEU-1 BLEU-2 Average Extrema Models distinct-1 distinct-2\n1. our SC-Seq2Seqyywr model can best fit the ground truth data\n2. there are diverse responses in real data in terms of specificity\n\n",
    "Slide Topic: Novelty\nSummary:\n1. Identify and paraphrase metaphors in\nwhole sentences from unrestricted\n2. Using word embedding input and output\nvectors to model a word and its context\nTranslation. bi | ING\n3. Metaphor processing for Machine G.\n\nSlide Topic: The definition of metaphor\nSummary:\nLinguistically, metaphor is defined as a language expression that uses one or several words to represent another concept, rather than taking their literal meanings of the given words in the context (Lagerwerf\n\nSlide Topic: Metaphors are widespread in natural language\nSummary:\nOne third of sentences in typical corpora contain metaphors.\n\nSlide Topic: Contexts help to find anomalies and identify metaphors\nSummary:\nShe devoured his sandwiches.\nShe devoured his novels.\ndevoured means enjoyed avidly. Interpretation\nitently and enjoyed are different concepts identification\n\nSlide Topic: Motivation\nSummary:\nMany previous metaphor processing methods are domain dependent\nMany works simply use input vectors\nMetaphor processing has rarely been applied to a real-world NLP task, instead mostly reporting accuracy on metaphor identification or interpretation.\n\nSlide Topic: Contribution\nSummary:\n1. Metaphor detection and interpretation in sentences from\n2. Investigate the effectiveness of input and output vectors of word embedding.\n3. Apply metaphor detection and G. Google\ninterpretation to improve Machine Translate\n\nSlide Topic: 1 Metaphor detection and interpretation in whole sentence from unrestricted domains\nSummary:\nShe devoured his novels.\nSentence level Phrase level\nThis young man knows how\nto climb the social ladder. T ladder\n\nSlide Topic: 2 Investigate the effectiveness of input and output vectors of word embeddings\nSummary:\nOutput vector Input vector of of enjoyed enjoyed\n\nSlide Topic: 3 Apply metaphor detection and interpretation to improve Machine Translation\nSummary:\nChinese English Spanish English -detected ~\nShe devoured his novels.\nSb eSMET thet) Vi.\nChinese (Simplified) English Spanish - RT\nxoo< ile te de xidoshud.\n* | ennese Smpites ) English\neo =n | ( ) Translator\n* SOEERR AT iE\nSRE BE She enjoyed his novels.\n4 o wrOoo< sin te de xi\n\nSlide Topic: One of novelties of our work is to model co occurrence between words with input and output vectors\nSummary:\nCBOW word2vec Input Hidden Output\nContext words c, O T Target words\nO ff Ban Output vector\nOB ME Abandoned ii\nSona aaa (e.g., gensim word2vec\nInput vec Output vec\n\nSlide Topic: The interaction between input and output vectors represents the co occurrence of words and contexts\nSummary:\nInput vec Output vec apple\n500 iterations on wevi https://ronxin.github.io/wevi/\norange drink input vec\n\nSlide Topic: Summary\nSummary:\nInput vectors can better model the similarity between words with similar semantics and syntax;\nOutput vector can better model the co-occurrence between words with different Part-of-Speech\n\nSlide Topic: The co occurrence between a target word and its context is measured by\nSummary:\nSCOTEcooccur = cos( vp, Veontext )\nVeontext mL , Yen\n\nSlide Topic: Hypotheses\nSummary:\nH11. Literal sense is more common that metaphorical.\nOne third of sentences in typical corpora contain metaphors.\nH2. A metaphorical word can be identified, if the sense the Identify a word takes within its context and its literal sense come from metaphor\n\nSlide Topic: Framework\nSummary:\ncos( w;, context) cos( s;, context) argmax COS( Sz, context)! 5 w* ew\ncos(h;, context) | Best fit word\nliteral, if S > threshold 4 . S = cos(w*, w;) metaphoric, otherwise\n\nSlide Topic: Step 1 training word embedding models on Wikipedia so that we can model the common expressions\nSummary:\nW, Ox ZO W,\nTrain W2 O PS > LZ 0 Wa\nWord Embedding XK OK\nWikipedias language could be more literal.\nWe model the literal so that we can identify the anomalies in metaphor in next steps. (H1)\n\nSlide Topic: Step 2 look up WordNet to list all possible senses of a target word\nSummary:\nCandidate word set W\nSeparate context words and a target word.\nAcandidate word set consists of hypernyms and synonyms of the target word, which represents all possible senses of the target word.\n\nSlide Topic: Step 3 identify the most likely sense from the candidate set\nSummary:\nCandidate word set W\nargmax cos(s,,context) | w*EWw\ncos( h;, context ) Best fit word\n* Compute the most likely word appearing in the context.\nThe best fit word is interpreted as the sense that metaphor\n\nSlide Topic: Step 4 identify the metaphoricity of a target word\nSummary:\nliteral, if S > threshold metaphoric, otherwise\nA metaphor could be identified as the real sense and its literal sense come from different domains. (H2)\n\nSlide Topic: An example in Step 2\nSummary:\nShe devoured his novels.\nContext words: {She, his, novels}\n{devour, devoured, devours, devouring}\n\nSlide Topic: An example in Step 3\nSummary:\nVeontext = m Ven 3 (Vsne + Vhis + Vnovels)\ncos(Vgestroyed Veontext ) = 0.04\narg max O i _\nBest fit word = enjoyed\n\nSlide Topic: An example in Step 4\nSummary:\n: literal, if S > threshold\nS= COS(Ven joy Vaevour) . .\nBest fit word Target word\n\nSlide Topic: Examine on Machine Translation before paraphrasing\nSummary:\nTranslate Tum off instant translation @\nChinese English Spanish English - detected ~ 4 Chinese (Simplified) English Spanish ~ Ea\nShe devoured his novels. * thaSIti-7-AhAO/) i. * She (physically) swallowed his novels.\nTA tinshile ta de xidoshud.\nHE Microsoft b 2\nShe voraciously wrote novels.\n1 ling ton ha yan 0 8 xo stud,\nShe devoured his novels. HORA Alte S 1) ik. : )\n\nSlide Topic: Examine on Machine Translation after paraphrasing\nSummary:\nTranslate Tum off instant translation |\ncm i ha - (C ae\nShe enjoyed his novels. <p EEPR HAO) iE. ) * She enjoyed his novels.\nTa thud t de aidoshus\nWx hun t Ge xido shud,\nShe enjoyed his novels. - PhS RAJ ii , )\n\nSlide Topic: Experiment setup\nSummary:\nMetaphor identification: e Sentence level: inputs are original sentences e Phrase level: inputs are parsed phrases\n\nSlide Topic: Dataset and baselines\nSummary:\nShutova et al. (2016) used Skip-gram input vectors to model the similarity between two component\n* Rei et al. (2017) used sigmoid function, projecting\nSkip-gram input vectors into another space, then\nSentence Phrase training a deep neural network based classifier.\n\nSlide Topic: Metaphor identification results\nSummary:\nMethod P R Fl\n\nSlide Topic: Evaluation with different thresholds\nSummary:\nP R Fl Fl gim-csow, o Filsim-se, O\nTable 2: Model performance vs. different threshold (7) settings. | NB: the sentence level results are based on\n\nSlide Topic: Experiment design for Machine Translation evaluation\nSummary:\nThe ex-boxer's job is to bounce people who want to enter this private club. bounce: eject from the premises Good / Bad\nBB ARENT EE BRA EAN HEA EL ERB APB EFAS TEE LAR BEE A EL ER BA SB AB RAST ea HEA BEA AL ER PB A AS eGR eT BI AE A ER A BPR RNS TET A fea EEA XL ERB. BB EASE eT aa BE AEE A LER A,\nGoogle translation on the original sentence.\nBing Translation on the original sentence.\nGoogle translation on our model paraphrased sentence.\nGoogle translation on Context2Vec paraphrased sentence. Bing Translation on Context2Vec paraphrased sentence.\n\nSlide Topic: Metaphor interpretation results\nSummary:\nL) Paraphrased by our model\nA Paraphrased by the baseline (Melamud et al. 2016)\nLiteral Metaphoric Overall Literal Metaphoric Overall\n\nSlide Topic: Takeaway\nSummary:\nA novel model for metaphor identification and interpretation on sentence level.\nA metaphor could be identified by its interpretation.\nInput and output vectors could better model the\nco-occurrence between two words.\nEffective paraphrasing of metaphors could improve\n\n",
    "Slide Topic: Utility to site moderators and administrators\nSummary:\nControversy (as we have defined it) is not necessarily a bad thing.\nMonitoring for bad controversy can prevent harm to the group\nBringing productive controversy to the communitys attention can help the group solve problems\n\nSlide Topic: Observation controversy is community specific\nSummary:\nbreak up: controversial in the Reddit group on relationships, but not in the group for posing questions to women\nmy parents: controversial for personal-finance group\n(example: live with my parents) but not in the relationships group\n\nSlide Topic: Observation we can also use early reactions\nSummary:\nEarly opinions can greatly affect subsequent opinion dynamics\n(Salganik et al. MusicLab experiment, Science 2006, inter alia)\nBoth the content and structure of the early discussion tree may prove helpful.\n\nSlide Topic: Our datasets derived from Baumgartner\nSummary:\n6 communities on www.reddit.com: two QA subreddits: AskMen, AskWomen a special interest community: Fitness three advice communities:\nPosts and comments mostly web-English\nUp/downvote information: eventual percent-upvoted\n(we cant use early votes: no timestamps)\n\nSlide Topic: Data selection\nSummary:\nAll posts with %- upvoted Filtered Posts no edits, stable %-upvoted\nLabel validation steps (details in paper): 1) high-precision overlap (>88 F-measure) with reddits low-recall rank-by-controversy 2) we ensure popularity prediction != controversy prediction\n\nSlide Topic: Labeled Dataset Statistics\nSummary:\nBalanced, binary classification with controversial/non-controversial labeling\n\nSlide Topic: Some posting time text only results\nSummary:\n(this, plus timestamp, is our baseline)\no Rather than passing BERT vectors to a bi-LSTM, it\nworks about as well and faster to mean-pool, dimension-reduce, and feed to a linear classifier\no Our hand-crafted features + word2vec match BERT- based algorithms on 3 of 6 subreddits\n\nSlide Topic: Does the shape of the tree predict controversy\nSummary:\nUsually yes, even after controlling for the rate of incoming comments.\nmax depth/total comment ratio proportion of comments that were top-level (i.e., made in direct reply to the original post) average node depth average branching factor proportion of top-level comments replied to Gini coefficient of replies to top-level comments (to measure how clustered the total discussion is) Wiener Index of virality (average pairwise pathlength between all pairs of nodes)\ntotal number of comments logged time between OP and the first reply average logged parent-child reply time (over all pairs of comments)\n[binary logistic regression, LL-Ratio test p<.05 in 5/6 communities]\n\nSlide Topic: Prediction results incorporating comment features\nSummary:\n4 comments, on average\n\nSlide Topic: Takeaways modulo caveats see paper\nSummary:\nWe advocate an early-detection, community-specific approach to controversial-post prediction\nWe can use features of the content and structure of the early discussion tree\nEarly detection outperforms posting-time-only features in 5 of 6\nReddit communities tested, even for quite small early-time windows\nEarly content is most effective, but tree-shape and rate features transfer across domains better\n\n",
    "Slide Topic: Trending of Social Media\nSummary:\nFacebook YouTube Instagram Twitter Snapchat Reddit Pinterest Tumblr Linkedin\nNumber of active users (millions)\nON a ts 200 croc\n\nSlide Topic: Name Tagging\nSummary:\n[ORG France] defeated [ORG Croatia] in [MISC\nWorld Cup] final at [LOC Luzhniki Stadium].\nProvide inputs to downstream applications\n\nSlide Topic: Challenges of Name Tagging in Social Media\nSummary:\nReal Madrid midfielder Toni Kroos has revealed why he snubbed Cristiano Ronaldo's birthday party, following their humiliating derby defeat to Atletico Madrid. W. W\nRead: Khedira Doesn't Regret Attending CR7's Party Ronaldo received a lot of criticism for hosting his birthday party just hours after his side lost 4-0 to Atletico, and although Kroos understands it was difficult to cancel the party, he feels the tim- ing wasn't right. was invited to Cristiano Ronaldo's party. | didn't go because I knew what could happen he told German TV station ZDF. It wasn't the moment to have a party after losing 4-0 against Atletico. It's also true that many people had been invited and cancelling it wouldn't have been easy.\" R 7 Oo r T K8\nThe 25-year-old, who won the World Cup with Germany in Brazil, also insisted that recent media reports of a Real Madrid crisis' were thrown out of proportion. \"We should take a step back and look at the whole picture in the face of what is being said. We have only lost the one game, Kroos added. e Limited Textual Context I think that many teams would love to suffer a crisis like ours. Of course we should be criticised if we play a bad game, as we did that day, without doubt.\" e Performs much worse on Read: Barca In Trouble For Drunk Ronaldo Chants? : . social media data Kroos joined Los Blancos for 30 million last summer and started all but two games for Real, as- sisting 12 goals and scoring one. Do you think Real Madrid will return to their form from before Christmas? Have your say in the comments section below.\nSocial Media eLanguage Variations\nAlison wonderlandxDiploxjuaz B2B ayee\nWithin word white spaces\n\nSlide Topic: Utilization of Vision\nSummary:\nKarl-Anthony Towns named unanimous intimate surprise set at Shea 2015-2016 NBA Rookie of the Year\nDifficult cases based on text only\n\nSlide Topic: Task Definition\nSummary:\nMultimedia Input: image-sentence pair\nColts Have 4th Best QB Situation in NFL with Andrew Luck #ColtStrong\n[ORG Colts] Have 4th Best QB Situation in [ORG\nNFL] with [PER Andrew Luck] #ColtStrong\nOutput: tagging results on sentence\n\nSlide Topic: Our work\nSummary:\nState-of-the-art for news articles (\nVisual attention model (Bahdanau et al.,\nExtract visual features from image regions that are most related to accompanying sentence\nModulation Gate before CRFs\nCombine word representation with visual features based on their relatedness\n\nSlide Topic: Overall Framework\nSummary:\nMultimodal Input : B-PER |-PER I- t I-PER\nFlorence and the Machine ~ text\nsurprises ill teen with _ LSTM\nprivate concert Hl 4 CRE\n/ a dk -p/ /isual - > L od Modulation\nf } G j m\\ \\ Gate | Gate \\ Gate ate / Gate\nceo I Forward Attention Model ! LSTM I\nGee fF Aw we = = = LI 114 Ss word SQ embedding ; ie char e representations \\ Florence and the Machine\n_ Visual Attention Model :\n\nSlide Topic: Sequence LabelingBLSTM CRE Lample et al 2016\nSummary:\nand a re the input, memory and hidden state at time t respectively. and are weight matrices. is the element-wise product functions and is the element-wise sigmoid function\n\nSlide Topic: Attention Model for Text Related Visual Features Localization\nSummary:\nV= CNN(I) Outputs from convolutional layer\nSs Florence and the Machine fe surprises ill teen with private concert QU\ne,= W pa; + by Attention\nI Input image C= S a,V; Context Vector\n\nSlide Topic: Modulation Gate\nSummary:\nUV C visual context\n() Multiplication word _\n* representations ( a ) activation function\nf_ ; / \\ (tanh } activation function { tanh } (tanh)\nvisual gate visual context word representations\nBy o(Wyh; + Uyve + by) Uc Visual context\nBw = o(Wwh; + UO wve + by) h; Word representation\nWm bw . h; + By -M Wm, _ Visually tuned word representation\n\nSlide Topic: Dataset\nSummary:\nTopics: Sports, concerts and other social events\nNamed Entity Types: Person, Organization, Location and MISC\nSize of the dataset in numbers of sentences and tokens\n\nSlide Topic: Results\nSummary:\nPrecision Recall F1 Precision Recall F1\nGate controlled visual attention\n\nSlide Topic: Attention Visualization\nSummary:\n(a). [PER Kiay Thompson} [ORG (b). [PER Radiohead] offers old and (c). [MISC Cannes} just became the\nWarriors} overwhelm [ORG new at first concert in four years. [PER Blake Lively] show\n(d). #iPhoneAtt0: How [PER Steve (e). [PER Florence and the Machine] (f). [ORG Warriorette) Basketball Jobs} and [ORG Apple] changed surprises ill teen with private concert Campers ready tor Day 2 modern society\n(g). ts defending champ [PER Sandeul] (h). Shirts at the ready for our (i). ARMY put up a huge ad in [LOC able to win for the third time on (MISC hometown game today #[{ORG Times Square] for [PER BTS} 4th Duet Song Festival)'? Leicester] #pgautomotive 4[(ORG anniversary! premierteague]\n\nSlide Topic: Error analysis\nSummary:\nNice sine of [PER Kevin Love] and [PER Kyle Very drunk in a #magnum concert Looking forward to editing some SBU baseball\nKorver] during 1st half #NBAFinals #Cavsin9 # shots from Saturday. [LOC Cleveland]\nPoor Alignment between Image and Blur Images Wrong Visual Attention Sentence\n\n",
    "Slide Topic: Motivation\nSummary:\nOnline adaptation is a key feature of modern computer-aided translation (CAT)\nSource #1: Der Terrier beit die Frau\nHypothesis #1: The dog bites the lady\nThe terrier bites the woman\nSource #2: Der Mann beit den Terrier\nThe dog bites the man\nThe man bites the terrier\nTranslators have a reasonable expectation that . . .\nNew vocabulary (in context) gets quickly picked up by the system, ideally right away\nThe system generally adapts to new domains\nWith neural machine translation fine-tuning can readily be used [Turchi et al.,\n\nSlide Topic: Approach\nSummary:\nf ine-tuning is evaluated in a batch setting\nCorpus BLEU or isolated sentence-wise metrics are often used\nThese do not necessarily express how fast a system adapts\nAs we will show this is not good enough\nWe seek to measure perceived, immediate adaptation performance\nCalculate recall on the set of all words that are not stopwords, ignoring\n1In each of the data sets considered in this work, the average number of occurrences of content\nwords ranges between 1.01 and 1.11 per sentence\nSince the task is online adaptation - specifically focus on few-shot learning:\nConsider only first and second occurrences of words!\n\nSlide Topic: One Shot Recall R1\nSummary:\nAfter seeing a word exactly once before in a reference/confirmed translation, is it correctly produced the second time around?\nHi Content words in the hypothesis i th example\nR1,i Content inthe reference words for whose\ni th example second occurrence is\n\nSlide Topic: One Shot Recall R1 Example\nSummary:\nSource #1: Der Terrier beit die Frau\nHypothesis #1: The dog bites the lady\nThe terrier bites the woman\nSource #2: Der Mann beit den Terrier\nThe man bites1 the terrier1\n\nSlide Topic: Zero Shot Recall R0\nSummary:\nNot having seen a word before, is it still correctly produced? Is the system adapting\nto the domain at hand?\nHi Content words in the hypothesis for i th example\nR0,i Content thereference words for\ni th that example occur for the first time in\n\nSlide Topic: Zero and One Shot Recall R01\nSummary:\nHi Content words in the hypothesis for i th example\nR0,i R1,i secondtime Content in the words reference that occur for\ni for th the example first or\n\nSlide Topic: Corpus Level Metric\nSummary:\nG: Corpus of |G| source, reference/confirmed seg-\n\nSlide Topic: Complete Example\nSummary:\nDer Terrier beit die Frau\nThe dog bites the lady\nThe terrier0 bites0 the woman0\nSource #2: Der Mann beit den Terrier\nThe terrier bites the man\nThe man0 bites1 the terrier1\n\nSlide Topic: Evaluation Adaptation Methods\nSummary:\nThe task is online adaptation to the Autodesk data set [Zhechev, 2012]. The background model is an English-to-German Transformer, trained on about 100M segments.\nFour methods for comparison: bias Add an additional bias to the output projection [Michel and Neubig, 2018] full Fine-tuning of all weights top Adapt top encoder/decoder layers only lasso Dynamic selection of adapted tensors with group lasso regularization [Wuebker\n\nSlide Topic: Results\nSummary:\nRelative differences for adaptive systems, positive results highlighted with green color.\nSystem Metric BLEU TER R1 R0 R0+1\nbias full top lasso\n\nSlide Topic: Results Novel Content Words\nSummary:\nResults when calculating the metrics only for truly novel content words, i.e. ones that do not occur in the training data.\nSystem Metric R1 R0 R0+1\n\nSlide Topic: Conclusion\nSummary:\nImmediate adaptation performance is important for adaptive MT in CAT\nWe proposed three metrics for measuring immediate and possibly perceived adaptation performance\nR1 for one-shot recall, quantifying pick up of new vocabulary\nR0 for zero-shot recall, quantifying general domain adaptation performance\nThe combined metric R0+1\nThese metrics give a different signal than the MT metrics that are traditionally used\nZero-shot recall R0 suffers from unregularized adaptation!\nCareful regularization can mitigate this effect, while retaining most of the one-shot recall R1\n\nSlide Topic: Bibliography I\nSummary:\nN. Bertoldi, P. Simianer, M. Cettolo, K. Waschle, M. Federico, and S. Riezler. Online adaptation to post-edits for phrase-based statistical machine translation. Machine\nS. S. R. Kothur, R. Knowles, and P. Koehn. Document-level adaptation for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine\nP. Michel and G. Neubig. Extreme adaptation for personalized neural machine\nK. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting on association for computational linguistics, pages 311318. Association for\nA. Peris, L. Cebrian, and F. Casacuberta. Online learning for neural machine\n\nSlide Topic: Bibliography II\nSummary:\nM. Turchi, M. Negri, M. A. Farajian, and M. Federico. Continuous learning from human post-edits for neural machine translation. The Prague Bulletin of\nJ. Wuebker, P. Simianer, and J. DeNero. Compact personalized models for neural machine translation. In Proceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, 2018.\nV. Zhechev. Machine translation infrastructure and post-editing performance at autodesk. In AMTA 2012 workshop on post-editing technology and practice\n\nSlide Topic: Results Subwords\nSummary:\nResults when calculating the metrics with subwords.\nSystem Metric R1 R0 R0+1\n\n",
    "Slide Topic: Current systems\nSummary:\nSpanish text ola mi nombre es hodor\nEnglish text: hi my name is hodor Machine\n\nSlide Topic: Unwritten languages\nSummary:\nBantu language, Republic of Congo, ~160K speakers\n~3000 languages with no writing system\nMboshi text: not available Recognition\npaired with French translations (Godard et al. 2018)\nEfforts to collect speech and translations using mobile apps\n\nSlide Topic: Haiti Earthquake 2010\nSummary:\nSurvivors sent text messages to helpline\nInternational rescue teams face language barrier\nNo automated tools available\nVolunteers from global Haitian diaspora help create parallel text corpora in short time\n\nSlide Topic: Are we better prepared in 2019\nSummary:\nMoun kwense nan Sakre\nPeople trapped in Sacred\n\nSlide Topic: Can we build a speech to text translation ST system\nSummary:\ngiven as training data:\nTens of hours of speech paired with text translations\nNo source text available\n\nSlide Topic: Neural models\nSummary:\nSequence-to-Sequence Weiss et al. (2017)\nEnglish text: hi my name is hodor\n\nSlide Topic: Spanish speech to English text\nSummary:\nEncoder telephone speech (unscripted) realistic noise conditions multiple speakers and dialects crowdsourced English text translations\nCloser to real-world conditions\n\nSlide Topic: But\nSummary:\nPoor performance in low-resource settings\n# hours of training data (log scale)\n\nSlide Topic: Why Spanish English\nSummary:\nsimulate low-resource settings and test our method\nLater: results on truly low-resource language ---\n\nSlide Topic: Method\nSummary:\nSame model architecture for ASR and ST\nAttention *randomly initialized parameters\n\nSlide Topic: Pretrain on high resource\nSummary:\n300 hours of English audio and text\nAttention *train until convergence\n\nSlide Topic: Fine tune on low resource\nSummary:\nEnglish audio Spanish audio\ntransfer from English ASR\nEnglish text English text\n*train until convergence Attention\n\nSlide Topic: Ablation model parameters\nSummary:\nSpanish to English, N = 20 hours\n+English ASR: encoder English text English text\n+English ASR: decoder Decoder Decoder\ntransferring encoder only parameters works well!\ncan pretrain on a language different from both source and target in ST pair\n\nSlide Topic: Pretraining on French\nSummary:\nSpanish to English, N = 20 hours\n+English ASR: encoder Decoder Decoder\n+French ASR: encoder French text English text\n*only 20 hours of French ASR\nFrench ASR helps Spanish-English ST\n\nSlide Topic: Takeaways\nSummary:\nPretraining on a different language helps\ntransfer all model parameters for best gains\nencoder parameters account for most of these\nuseful when target vocabulary is different\n\nSlide Topic: Mboshi French ST\nSummary:\nST data by Godard et al. 2018\n~4 hours of speech, paired with French translations\nBantu language, Republic of Congo\n\nSlide Topic: Mboshi French Results\nSummary:\nMboshi to French, N = 4 hours\n*outperformed by a naive baseline\n\nSlide Topic: Pretraining on French ASR\nSummary:\nMboshi to French, N = 4 hours\nFrench text French text\nFrench ASR helps Mboshi-French ST\n\nSlide Topic: Pretraining on English ASR\nSummary:\nMboshi to French, N = 4 hours\n+English ASR: encoder Decoder Decoder\nEnglish text French text\nusing encoder trained on a lot more data\nEnglish ASR helps Mboshi-French ST\nbaseline Encoder From English ASR\n+French ASR: all Attention\n+French ASR: remaining French text\ncombining gives the best gains\nBLEU score is still low but above naive baseline\n\nSlide Topic: Pretraining on French and English ASR\nSummary:\nFrench text French text English text\n\nSlide Topic: Conclusions\nSummary:\nPretraining on high-resource ASR improves low-resource ST\nPotentially useful for endangered and/or unwritten languages\nBootstrap ST in time-critical scenarios\nFuture work: experiments on more languages, multilingual\ntraining with joint vocabulary\n\nSlide Topic: Why does pretraining help\nSummary:\nASR data contains audio from 100s of speakers\nLearning to factor out background noise (?)\nBLEU Baseline +English ASR\n\nSlide Topic: Spanish English ST\nSummary:\n*results on Fisher test set ...\nSpanish to English, N = 20 hours\n+En ASR: 20h English text\n\nSlide Topic: Neural model\nSummary:\nyo vive en bronx\nbi-LSTM 1 LSTM 2\nbi-LSTM 2 LSTM 3\n\n",
    "Slide Topic: Semantic Parsing\nSummary:\nh h h ?\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Training with Full Supervision\nSummary:\ny: CapitalOf.argmaEx (XT yPpe .State LocatedIn.US,Population)\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Training with Weak Supervision\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Problems with Weak Supervision\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\nSpurious programs (Pasupat and Liang, 2016; Guu et al.,\n\nSlide Topic: CNLVR Cuhr et al 2017\nSummary:\nxz :There is a small yellow item not touching any wall\nIntroduction Semantic parser Abstract examples > Results Conclusions\n\nSlide Topic: Insight\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Contributions\nSummary:\nData augmentation Abstract cache\nhelps search tackles spuriousness\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Logical Program\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Model\nSummary:\nThere is a yellow triangle\nTraining maximizes log-likelihood of correct programs\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Abstraction\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Analysis\nSummary:\nOne square.. There is.. One of the. There are.. There is.... Two towers..\nOne tower.. There are. C-Num C-Shape There is. C-Num towers.. Another last There are.. There is.. One circle. There is.. Last one..\n~150 abstract sentences cover of CNLVR.\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Data Augmentation\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Training Procedure\nSummary:\n~6000 Instantiated examples Supervised model\n3163 CNLVR training examples Weakly-supervised model\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Abstract Cache\nSummary:\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Reward Tying\nSummary:\nsize: 20}, . xz: There is a oi yellow item not touching any wall\n50% Spurious amp Y :True\nIntroduction Semantic parser Abstract examples Results Conclusions 21\na :There is a small yellow item * [Hytoe: . a Black, ty) oA\n\nSlide Topic: Models\nSummary:\nMax Entropy classifier on extracted features\nWeakly supervised trained model (+Re-ranker)\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Results Public test set\nSummary:\nTest-P Accuracy Test-P Consistency\nMajority MaxEnt Sup. Sup.+Rerank W.Sup. W.Sup.+Rerank\nIntroduction Semantic parser Abstract examples Results Conclusions\n\nSlide Topic: Ablations\nSummary:\nAbstract weakly supervised parser\nIntroduction Semantic parser Abstract examples Results Conclusions\nDev Accuracy Dev Consistency\n-Abstraction -Data augment. -Beam cache W.Sup.+Rerank\n\nSlide Topic: Conclusions\nSummary:\nSimilar ideas in: Dong and Lapata (2018) and Zhang et al.\nAutomation would be useful\n\n"
]