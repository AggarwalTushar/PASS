[
    "```json\n{\n  \"Introduction to Abstract Meaning Representation (AMR)\": \"Abstract Meaning Representation (AMR) is a semantic graph representation that abstracts away from the syntactic realization of a sentence. In AMR, nodes represent concepts and edges represent semantic relations. AMRs are graphs rather than trees because co-references and control structures result in nodes with multiple parents, known as reentrancies. AMR-to-text generation involves automatically generating natural language from AMR graphs, utilizing encoder/decoder architectures similar to those used in Neural Machine Translation.\",\n  \n  \"Challenges in AMR-to-Text Generation\": \"Reentrancies, which are central to AMR, pose significant challenges in both parsing and generation. Previous work has either removed reentrancies to obtain sequential or tree-structured data or maintained them without analyzing their impact on performance. It remains unclear whether explicit encoding of reentrancies is beneficial for generation, as state-of-the-art parsers struggle with predicting reentrant structures.\",\n  \n  \"Comparison of Encoder Types for AMR\": \"This paper compares three types of encoders for AMR: sequential encoders, which reduce AMR graphs to sequences; tree encoders, which ignore reentrancies; and graph encoders. The study highlights the benefits of graph encoders over sequential and tree encoders, particularly in handling reentrancies and long-range dependencies. The best model, based on a graph encoder, achieves state-of-the-art results on the LDC2015E86 and LDC2017T10 datasets.\",\n  \n  \"Graph-Structured AMRs and Their Encoding\": \"Graph-structured AMRs are represented as rooted and directed graphs with labeled edges. To reduce model parameters, labeled edges are transformed into unlabeled edges, creating an equivalent Levi graph. This transformation allows for more efficient encoding of AMR graphs, as demonstrated in previous studies.\",\n  \n  \"Tree-Structured and Sequential AMR Representations\": \"Tree-structured AMRs are obtained by discarding reentrancies, replacing nodes with multiple incoming edges with identically labeled nodes. Sequential AMRs involve linearizing and anonymizing AMR graphs, converting them into sequences through depth-first traversal. This process loses reentrancy information but reduces data sparsity.\",\n  \n  \"Stacking Encoders: Structure and Sequence Integration\": \"The study explores stacking structural encoders like TreeLSTMs and GCNs with sequential encoders like BiLSTMs. Two approaches are tested: structure on top of sequence and sequence on top of structure. The latter, where structural encoders refine input embeddings before feeding them to BiLSTMs, proves more effective, highlighting the importance of integrating structural information.\",\n  \n  \"Experimental Results and Evaluation Metrics\": \"The study uses BLEU and Meteor as evaluation metrics, reporting results on the LDC2015E86 and LDC2017T10 datasets. The GCNSEQ model, which integrates graph encoders with BiLSTMs, achieves state-of-the-art results, demonstrating the efficacy of structural encoders over purely sequential ones. The graph encoder outperforms other systems, particularly in handling reentrancies and long-range dependencies.\",\n  \n  \"Handling Reentrancies and Long-Range Dependencies\": \"Graph encoders explicitly model reentrancies, providing an advantage over tree and sequential encoders. The study shows that graph models perform better in examples with more reentrancies and longer dependencies. Manual inspection and contrastive analysis reveal that graph models handle pronouns and control structures more accurately, benefiting from the explicit encoding of reentrancies.\"\n}\n```",
    "```json\n{\n  \"Introduction to Word Embeddings and Their Analysis\": \"Learned vector representations of words, known as word embeddings, have become ubiquitous throughout natural language processing (NLP) applications. As a result, analysis of embedding spaces to understand their utility as input features has emerged as an important avenue of inquiry, in order to facilitate proper use of embeddings in downstream NLP tasks. Many analyses have focused on nearest neighborhoods, as a viable proxy for semantic information. However, neighborhood-based analysis is limited by the unreliability of nearest neighborhoods.\",\n  \"Limitations of Neighborhood-Based Analysis\": \"Neighborhood-based analysis is limited by the unreliability of nearest neighborhoods. Further, it is intended to evaluate the semantic content of embedding spaces, as opposed to characteristics of the feature space itself. Nearest neighbors in sets of embeddings are commonly used as a proxy for qualitative semantic information. However, their instability across embedding samples is a limiting factor, and they do not necessarily correlate with linguistic analyses.\",\n  \"Geometric Analysis of Word Embeddings\": \"Geometric analysis offers another recent angle from which to understand the properties of word embeddings, both in terms of their distribution and correlation with downstream performance. Through such geometric investigations, neighborhood-based semantic characterizations are augmented with information about the continuous feature space of an embedding. Geometric features offer a more direct connection to the assumptions made by neural models about continuity in input spaces.\",\n  \"Transformations of Word Embeddings\": \"We consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec, GloVe, and FastText, and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance. Our findings suggest that further development of embedding learning and tuning methods should focus explicitly on local similarity.\",\n  \"Affine Transformations and Their Impact\": \"Affine transformations have been previously utilized for post-processing of word embeddings. For example, Artetxe et al. learn a matrix transform to align multilingual embedding spaces, and Faruqui et al. use a linear sparsification to better capture lexical semantics. Affine transformations change positions of points relative to the origin. While prior work has typically focused on linear transformations, which fix the origin, we consider the broader class of affine transformations, which do not.\",\n  \"Cosine Distance Encoding and Its Implications\": \"Our cosine distance encoding transformation obfuscates the distribution of features by representing a set of word vectors as a pairwise distance matrix. Such a transformation might be used to avoid the non-interpretability of embedding features and compare embeddings based on relative organization alone. We use a subset consisting of the distance from each point to the embeddings of the 10K most frequent words from each embedding set.\",\n  \"Nearest Neighbor Encoding and Graph Structures\": \"Our nearest neighbor encoding transformation discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. The output of this transformation is a sparse vector. This transformation relates to the common use of nearest neighborhoods as a proxy for semantic information. We take the previously proposed approach of combining the output for each word vector to form a sparse adjacency matrix, which describes a directed nearest neighbor graph.\",\n  \"Evaluation of Transformed Embeddings\": \"In order to measure the contributions of each geometric aspect to the utility of word embeddings as input features, we evaluate embeddings transformed using our sequence of operations on a battery of standard intrinsic evaluations, which model linguistic information directly in the vector space; and extrinsic evaluations, which use the embeddings as input to learned models for downstream applications. Our intrinsic evaluations include word similarity and relatedness, using cosine similarity.\",\n  \"Analysis and Discussion of Results\": \"Comparing within the set of affine transformations, the innocuous effect of rotations, dilations, and reflections on both intrinsic and extrinsic tasks suggests that the models used are robust to simple linear transformations. Extrinsic evaluations are also relatively insensitive to translations, which can be modeled with bias terms, though the lack of learned models and reliance on cosine similarity for the intrinsic tasks makes them more sensitive to shifts relative to the origin.\",\n  \"Conclusion and Future Directions\": \"Analysis of word embeddings has largely focused on qualitative characteristics such as nearest neighborhoods or relative distribution. In this work, we take a quantitative approach analyzing geometric attributes of embeddings in order to understand the impact of geometric properties on downstream task performance. Our findings suggest that work on tuning and improving these embeddings should focus explicitly on local geometric structure in sampling and evaluation methods.\"\n}\n```",
    "```json\n{\n  \"Introduction to Geolocation Prediction in Social Media\": \"Social media sites have become a popular source of information to analyze current opinions of numerous people. Many researchers have worked to realize various automated analytical methods for social media because manual analysis of such vast amounts of data is difficult. Geolocation prediction is one such analytical method that has been studied widely to predict a user location or a document location. Location information is crucially important for analyses such as disaster analysis, disease analysis, and political analysis. Such information is also useful for sentiment analysis and user attribute analysis to undertake detailed region-specific analyses. Geolocation prediction has been performed for platforms like Wikipedia, Flickr, Facebook, and Twitter, with Twitter often preferred due to its characteristics suited for geolocation prediction.\",\n  \n  \"Proposed Neural Network Model for Geolocation Prediction\": \"Herein, we propose a neural network model to tackle geolocation prediction in Twitter. Our model combines text, metadata, and user network information using a complex neural network. Neural networks have recently shown effectiveness to capture complex representations combining simpler representations from large-scale datasets. We intend to obtain unified text, metadata, and user network representations with an attention mechanism that is superior to the earlier ensemble approaches. The contributions of this paper include proposing a neural network model that learns unified text, metadata, and user network representations with an attention mechanism, showing that the proposed model outperforms previous ensemble approaches in two open datasets, and analyzing some components of the proposed model to gain insight into the unification processes of the model.\",\n  \n  \"Text-Based Approaches in Geolocation Prediction\": \"Probability distributions of words over locations have been used to estimate the geolocations of users. Maximum likelihood estimation approaches and language modeling approaches minimizing KL-divergence have succeeded in predicting user locations using word distributions. Topic modeling approaches to extract latent topics with geographical regions have also been explored considering word distributions. Supervised machine learning methods with word features are also popular in text-based geolocation prediction, including Multinomial Naive Bayes, logistic regression, hierarchical logistic regression, and a multilayer neural network with stacked denoising autoencoder.\",\n  \n  \"User-Network-Based Approaches and Their Integration\": \"Social media often include interactions of several kinds among users, which can be regarded as links that form a network among users. Several studies have used such user network information to predict geolocation. For instance, a probabilistic model was introduced to predict the location of a user using friendship information in Facebook. Friend and follower information in Twitter were used to predict user locations with various algorithms. Mention information in Twitter is also used with label propagation models and an energy and social local coefficient model. Comparisons with the proposed model show that it integrates user network information along with other information, combining text and metadata with an attention mechanism.\",\n  \n  \"Metadata-Based Approaches and Combinatory Methods\": \"Metadata such as location fields are useful as effective clues to predict geolocation. Decent accuracy of geolocation prediction can be achieved using location fields. Approaches to combine metadata with texts are also proposed to extend text-based approaches. Combinatory approaches such as a dynamically weighted ensemble method, polygon stacking, stacking, and average pooling with a neural network have strengthened geolocation prediction. Several attempts have been made to combine user-network-based approaches with other approaches, enhancing geolocation prediction.\",\n  \n  \"Evaluation and Comparison with Baseline Models\": \"The proposed model outperforms earlier models. In terms of machine learning methods, our model is a neural network model that shares some similarity with previous neural network models. Our model and these previous models have two key differences: integrating user network information along with other information and combining text and metadata with an attention mechanism. Evaluation metrics include accuracy, accuracy@161, median error distance, and mean error distance. The proposed model achieved significant improvements in accuracy and accuracy@161 against baseline models.\",\n  \n  \"Analysis of Attention Mechanisms in the Proposed Model\": \"The proposed model has implicitly shown effectiveness at unifying text, metadata, and user network representations through improvements in the four metrics. To gain insight into the unification processes, we analyzed the states of two attention layers. The model assigns higher probabilities to timeline representations than to other three representations in TwitterUS compared to W-NUT. Strong preference of network representation for TwitterUS against W-NUT is found, which is intuitive due to more user network edges in TwitterUS. Clustering attention probabilities revealed typical attention patterns, matching with previously estimated probability density functions.\"\n}\n```",
    "```json\n{\n  \"Introduction to Presuppositions and Triggers\": \"In pragmatics, presuppositions are assumptions or beliefs shared between discourse participants when an utterance is made. They are crucial for smooth communication and are indicated by presupposition triggers such as definite descriptions, factive verbs, and certain adverbs. For instance, the statement 'John is going to the restaurant again' presupposes that 'John has been to the restaurant' due to the trigger 'again'. Presuppositions are unique as they remain unaffected by the negation of the presupposing context. This paper focuses on adverbial presupposition triggers like 'again', 'also', and 'still', which indicate recurrence or continuation of events in discourse. Despite their frequency, these triggers have been underexplored in computational literature, and understanding them can enhance language generation applications like summarization and dialogue systems.\",\n  \"Adverbial Presupposition Triggers in Language\": \"Adverbial presupposition triggers such as 'again', 'also', and 'still' are prevalent in language, indicating the recurrence or continuation of events. A study on English journalistic texts found adverbial triggers to be the most common after existential triggers. These triggers are complex, often spanning multiple sentences, and understanding them is crucial for pragmatic reasoning. They have practical applications in language technology systems, improving readability and coherence in generated outputs. The paper proposes investigating the detection of contexts where these triggers can be used, forming a binary classification task using datasets from the Penn Treebank and English Gigaword corpora.\",\n  \"Dataset Creation and Extraction Methodology\": \"The datasets for this study are extracted from the Penn Treebank and English Gigaword corpora, focusing on five target adverbs: 'too', 'again', 'also', 'still', and 'yet'. The task is framed as a binary classification to predict the presence of an adverbial presupposition. The data extraction process involves creating samples with a label, a list of tokens, and corresponding POS tags. Positive contexts contain a triggering adverb, while negative contexts do not, ensuring models cannot rely on the identity of the governor alone. The extraction process controls for potential confounding factors, balancing the number of positive and negative samples.\",\n  \"Attention-Based Deep Learning Models\": \"Attention-based deep learning models are explored for detecting adverbial presupposition triggers. Attention mechanisms allow models to weigh information from multiple points in the context, inferring long-range dependencies. This approach is promising as it can detect instances involving recurring events, such as multiple mentions of 'John' and 'restaurants', indicating the appropriateness of 'again'. An attention-based RNN has shown success in predicting article definiteness, another class of presupposition triggers, highlighting the potential of attention mechanisms in this task.\",\n  \"Weighted Pooling Attention Mechanism\": \"A new weighted pooling attention mechanism is introduced for predicting adverbial presupposition triggers. This mechanism allows for a weighted averaging of RNN hidden states, informed by the inputs, without introducing new parameters compared to the LSTM baseline. The model uses self-attention, where the input sequence acts as both the query and key/value. Unlike other attention models, this approach aggregates learned attention scores through another level of attention, enhancing prediction performance without additional computational impact.\",\n  \"Experimental Setup and Baseline Comparisons\": \"The experimental setup involves comparing the weighted pooling model against several baselines, including a most-frequent-class baseline, logistic regression, and standard LSTM and CNN models. The experiments are conducted on datasets from the Penn Treebank and English Gigaword corpora. The impact of POS tags and attention mechanisms on prediction accuracy is also investigated. The weighted pooling model uses the same bidirectional LSTM as the baseline LSTM, allowing for a fair comparison, and demonstrates superior performance in predicting adverbial presupposition triggers.\",\n  \"Analysis of Model Performance and Results\": \"The weighted pooling model outperforms baseline models, achieving 82.42% accuracy on predicting the adverb 'also' on the Gigaword dataset. The model's performance is evaluated using confusion matrices, showing balanced misclassifications between the presence and absence of presupposition triggers. The analysis includes examples from the PTB dataset, illustrating the model's ability to detect presuppositions despite complex coreferences. The results highlight the model's effectiveness in capturing long-range dependencies and interactions within sentences, demonstrating the utility of attention mechanisms in this task.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Role Labeling and Current Challenges\": \"Semantic role labeling (SRL) captures predicate-argument relations, such as 'who did what to whom.' Recent high-performing SRL models are BIO-taggers, labeling argument spans for a single predicate at a time. These models are typically evaluated with gold predicates and require pipelining with error-prone predicate identification models for deployment. A key challenge is the inability to model overlapping spans across different predicates in the same output structure, which our proposed span-graph formulation aims to overcome.\",\n  \n  \"Proposed End-to-End SRL Model and Its Advantages\": \"We propose an end-to-end approach for predicting all predicates and their argument spans in one forward pass. Our model builds on a recent coreference resolution model by using learned, contextualized span representations to predict SRL graphs directly over text spans. This approach allows for modeling overlapping spans and does not assume given predicates, achieving state-of-the-art performance on PropBank. It also suggests potential applications in other span-span relation tasks like syntactic parsing and relation extraction.\",\n  \n  \"Model Architecture and Span Representation\": \"The model considers all tokens in the input sentence as possible predicates and all continuous spans as arguments. It predicts relations between each predicate-argument pair, including no relation. The scoring function for a possible (predicate, argument, label) combination is decomposed into unary scores on the predicate and argument, as well as a label-specific score for the relation. This architecture allows for dynamic decision-making on which spans and roles to include.\",\n  \n  \"Learning and Beam Pruning Techniques\": \"For each input, the model minimizes the negative log likelihood of the gold structure. Beam pruning is used to handle the computational complexity of considering O(n^3 |L|) possible relations. Two beams are defined for storing candidate arguments and predicates, ranked by their unary scores. This reduces the number of relational factors evaluated to O(n^2 |L|) and limits the maximum width of spans, further reducing computational load.\",\n  \n  \"Neural Architecture and Contextualized Representations\": \"The model builds contextualized representations for argument spans and predicate words based on BiLSTM outputs. Word-level contexts are created using pre-trained word embeddings concatenated with character-based representations, which are then contextualized using a bidirectional LSTM. Argument representations include endpoints from BiLSTM outputs, a soft head word, and embedded span width features, while predicate representations are derived from BiLSTM outputs at specific positions.\",\n  \n  \"Experimental Setup and Results\": \"Experiments were conducted on the CoNLL 2005 and CoNLL 2012 benchmarks using end-to-end and gold predicate setups. The model outperformed previous systems by an F1 difference of 1.3 to 6.0, particularly on out-of-domain and nominal predicate datasets. ELMo embeddings were added to improve performance, showing orthogonal improvements to the model's contributions. The model predicted over 40% of sentences completely correctly across datasets.\",\n  \n  \"Analysis of Model Performance and Beam Pruning Effectiveness\": \"The model's architecture differs from previous BIO systems in input and decision space. Beam pruning effectively reduces the number of edges considered by efficiently pruning unlikely argument spans and predicates. The model performs better at predicting arguments farther from predicates, likely due to direct connections between predicate-argument pairs. It achieves comparable syntactic agreement to previous syntax-based systems but struggles with global consistency due to strong independence assumptions.\",\n  \n  \"Conclusion and Future Work\": \"We proposed a new SRL model that jointly predicts all predicates and argument spans, supporting joint predicate identification and incorporating span-level features. The model excels at long-range dependencies and syntactic boundary agreement but is weaker at global consistency. Future work could involve higher-order inference methods to relax independence assumptions and combining the span-based architecture with self-attention layers for more effective contextualization.\"\n}\n```",
    "```json\n{\n  \"Introduction to Machine Translation and Semantic Representations\": \"The goal of machine translation is to convert source language utterances into fluent target language utterances with the same meaning. Recent approaches use statistical techniques on parallel data to learn transformations. Unified semantic representations, such as Abstract Meaning Representation (AMR), have gained interest for their potential to influence natural language understanding and generation tasks, including machine translation. These deeper models allow for a more parsimonious translation model by handling morphologically related words in a unified manner and making semantically related concepts immediately available for modeling.\",\n  \"Related Work and Historical Context\": \"There is a significant body of work on utilizing deep language representation in NLP and MT. Early systems from the 1950s used a form of transfer, where source sentences were parsed and translated into target representations. Examples include KANT, SysTran, and TectoMT. AMR has been used for abstractive summarization, and approaches like Jones et al. (2012) exploit semantic graphs for MT. Discriminative models have been used in statistical MT, with features derived from deep representations capturing semantic relations rather than surface correspondences.\",\n  \"Semantic Representation and Logical Form\": \"Our representation of sentence semantics is based on Logical Form (LF), which are labeled directed graphs with nodes corresponding to content words. Edge labels describe semantic relations, and additional linguistic information is stored in graph nodes. The logical form can be converted to a representation conforming to the AMR specification. We retain bits as features for discriminative modeling of translation, positioning our semantic graphs between LF and AMR.\",\n  \"Graph-to-String Translation Model\": \"We develop models for semantic-graph-to-string translation, viewing translation as a process of selecting target words conditioned on source language representation. Our model assumes an alignment between target words and source semantic graph nodes. The generative process involves transitioning through graph nodes to generate target-side words, stopping once the target sentence is complete. The model uses a feature-rich approach for translation reranking, conditioned on source and target context.\",\n  \"Alignment of Semantic Graph Nodes\": \"We experimented with techniques for aligning source-side semantic graph nodes to target-side surface words. Using a generative HMM-style model, we align target words to source nodes based on graph paths. We employ Gibbs sampling to incorporate a sparse prior in parameter estimation. Direct GIZA++ aligns linearized semantic graph nodes to target sentences, and composed alignments involve two stages: aligning semantic graph nodes to source words and aligning source and target words. The composition of gold alignment and GIZA++ produced superior results.\",\n  \"Discriminative Model and Feature Set\": \"Our discriminative model assumes given alignments, using a maximum entropy model to learn conditional probabilities. Features include lemma, part of speech, and bits from graph nodes, capturing agreement phenomena and semantic context. Additional features handle multiple target word generation from source nodes. On the target side, previous tokens act as discriminative language model features. The model maintains state during MT decoding, with complexity similar to conventional MT state.\",\n  \"Experiments and Results\": \"We tested our model in an n-best re-ranking experiment using a phrase-based MT system for English\u2192French. The system was optimized with minimum error rate training, and reranking involved adding scores from our model to n-best lists. The resulting feature weights were used to rescore test n-best lists, evaluated using BLEU. We obtained a modest but consistent improvement, suggesting potential gains with direct decoder integration.\",\n  \"Conclusion and Future Directions\": \"We presented an initial attempt at including semantic features in a statistical MT system, using discriminative training and a broad set of features. While gains are modest, further feature engineering and decoder integration could lead to substantial improvements. The approach depends on semantic parser accuracy, and exploring robust features or combining multiple parsers could be beneficial. Evaluating morphological, syntactic, and semantic features separately could identify promising research areas. Integrating models of semantic plausibility could prevent 'word salad' outputs, enhancing MT system outputs.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Problem and Solution\": \"Teaching machines to answer arbitrary user-generated questions is a long-term goal of natural language processing. Existing information retrieval methods can locate documents likely to contain the answer, but extracting the answer automatically remains a challenge. Recent successes with neural models suggest they could be key to solving this problem. However, most neural models struggle to scale beyond short paragraphs, necessitating adaptations for document-level input. This paper proposes an improved pipelined method achieving state-of-the-art results and introduces a method for training models to produce accurate per-paragraph confidence scores, enhancing performance further.\",\n  \n  \"Pipelined Method for Question Answering\": \"The pipelined QA system selects a single paragraph to pass to a paragraph-level model. For a single source document, the paragraph with the smallest TF-IDF cosine distance to the question is selected. For multiple documents, a linear classifier using TF-IDF scores and other features is employed. This approach improves the chance of selecting the correct answer from 83.1% to 85.1% on TriviaQA web, compared to previous methods.\",\n  \n  \"Handling Noisy Labels in Distant Supervision\": \"Noisy supervision can lead to models selecting unwanted answer spans. To address this, a summed objective function optimizes the negative log-likelihood of selecting any correct answer span. This approach allows the model to focus on the most relevant spans, improving its ability to handle noise in distantly supervised setups.\",\n  \n  \"Model Architecture and Design\": \"The model uses layers including embedding, pre-processing with a bi-directional GRU, attention mechanisms, self-attention, and prediction layers. It employs variational dropout and uses a linear layer with ReLU activations. The attention mechanism builds a query-aware context representation, and self-attention is applied between the passage and itself. The prediction layer computes answer start and end scores, with softmax applied to produce probabilities.\",\n  \n  \"Confidence Method for Multi-Paragraph Settings\": \"The model adapts to multi-paragraph settings by using un-normalized scores as confidence measures. Errors occur when models trained only on answer-containing paragraphs produce poor confidence scores. The shared-normalization approach forces the model to produce comparable scores across paragraphs, improving performance in multi-paragraph settings.\",\n  \n  \"Experimental Setup and Datasets\": \"The approach is evaluated on datasets including TriviaQA unfiltered, TriviaQA wiki, TriviaQA web, and SQuAD. Preprocessing involves restructuring documents and marking answer spans. Sampling involves selecting paragraphs from the context during training. The model is trained with the Adadelta optimizer, using GloVe word vectors and specific dimensionalities for GRUs and linear layers.\",\n  \n  \"Results and Performance Analysis\": \"An ablation study on TriviaQA web shows the effectiveness of the proposed methods. The shared-norm approach outperforms others, especially on datasets with many paragraphs. The model achieves significant improvements over prior work on TriviaQA web and wiki test sets. On SQuAD, the shared-norm model adapts well to document-level tasks, outperforming previous results.\",\n  \n  \"Discussion and Error Analysis\": \"Models trained only on answer-containing paragraphs perform poorly in multi-paragraph settings. The shared-norm approach resolves issues of distraction by irrelevant text. Error analysis on TriviaQA web dev-set shows errors due to insufficient evidence, complex syntactic structures, and multi-sentence reading errors. Improvements could focus on sentence and paragraph-level comprehension and handling document-level coreferences.\"\n}\n```",
    "```json\n{\n  \"Introduction to Query Auto-Completion and Personalization\": \"Query auto-completion (QAC) is a feature used by search engines that provides a list of suggested queries for the user as they are typing. This feature can save the user time and reduce cognitive load. Most approaches to QAC are extensions of the Most Popular Completion (MPC) algorithm, which suggests completions based on the most popular queries in the training data that match the specified prefix. Personalization in QAC relies on the fact that query likelihoods are drastically different among different people depending on their needs and interests.\",\n  \n  \"Advancements in Language Models for QAC\": \"Recently, Park and Chiba suggested a significantly different approach to QAC by using a character LSTM language model instead of ranking completions retrieved from a database. This approach is able to complete queries whose prefixes were not seen during training and has significant memory savings over having to store a large query database. Building on this work, the task of personalized QAC is advanced by combining the advantages of personalization with the effectiveness of a language model in handling rare and previously unseen prefixes.\",\n  \n  \"FactorCell Model for Personalized QAC\": \"The FactorCell model uses an embedding vector to additively transform the weights of the language model's recurrent layer with a low-rank matrix. By allowing a greater fraction of the weights to change during personalization, the FactorCell model has advantages over the traditional approach to adaptation. The novel aspects of this work are the application of an adaptive language model to the task of QAC personalization and the demonstration of how RNN language models can be adapted to contexts not seen during training.\",\n  \n  \"Learning and Adapting User Embeddings\": \"During training, an embedding for each user is learned, which holds latent demographic factors for each user. Users with less than 15 queries are grouped together as a single entity. The user embeddings matrix is learned via back-propagation as part of the end-to-end model. Online updating of the user embeddings is done during evaluation, allowing the model to apply to users not seen during training.\",\n  \n  \"Recurrent Layer Adaptation Techniques\": \"Three model architectures are considered for adapting the recurrent layer. The unadapted LM does no personalization. The ConcatCell model works by concatenating a user embedding to the character embedding at every step of the input to the recurrent layer. The FactorCell model allows the user embedding to transform the weights of the recurrent layer via a low-rank adaptation matrix, providing a stronger adaptation than the ConcatCell model.\",\n  \n  \"Experimental Setup and Data Utilization\": \"Experiments use the AOL Query data collected over three months in 2006, with approximately 12 million queries from 173,000 users. Models were trained for six epochs using the Adam optimizer. The language model is a single-layer character-level LSTM with coupled input and forget gates and layer normalization. Experiments were conducted on two model configurations: small and large, with different hidden state sizes and user embedding dimensions.\",\n  \n  \"Results and Performance Analysis\": \"Results are reported using mean reciprocal rank (MRR), the standard method of evaluating QAC systems. Neural models outperform the MPC baseline, with personalized models performing better than the unadapted one. The FactorCell model is the best overall, especially for seen prefixes. Figure 1 shows the relative improvement in MRR over an unpersonalized model versus the number of queries seen per user, with the FactorCell outperforming the ConcatCell over time.\",\n  \n  \"Comparative Analysis with Existing Models\": \"The FactorCell model outperforms the ConcatCell by adapting more quickly to repetitive search behaviors. Case studies show that the FactorCell model examples are more semantically coherent than the ConcatCell examples. The FactorCell model identifies queries that are more relevant to the user's context, while the ConcatCell seems to focus more on orthography than semantics.\",\n  \n  \"Conclusion and Future Directions\": \"Experiments show that the LSTM model can be improved using personalization, with the FactorCell model providing an advantage. The adapted weight matrix only needs to be computed once per query, making it efficient. Personalization is effective even for users not seen during training, with benefits increasing over time. Future work could explore combining the language model with MPC and including time as an additional conditioning factor.\"\n}\n```",
    "```json\n{\n  \"Introduction to NLP Pipelines and Neural Architectures\": \"Learning methods for natural language processing are increasingly dominated by end-to-end differentiable functions that can be trained using gradient-based optimization. Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semantically analyzed. Pipelines, which make 'hard' (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing. Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing, we argue that pipelines can be treated as layers in neural architectures for NLP tasks.\",\n  \n  \"Challenges with Discrete Decisions in Neural Networks\": \"Several solutions are readily available: Reinforcement learning (most notably the REINFORCE algorithm) and structured attention. These methods replace argmax with a sampling or marginalization operation. We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvement, is more straightforward for hard decisions than for posteriors. The straight-through estimator (STE) treats discrete decisions as if they were differentiable and simply passes through gradients. While fast and surprisingly effective, it ignores constraints on the argmax problem, such as the requirement that every word has exactly one syntactic parent.\",\n  \n  \"Structured Projection of Intermediate Gradients Optimization Technique (SPIGOT)\": \"This paper introduces a new method, the structured projection of intermediate gradients optimization technique (SPIGOT), which defines a proxy for the gradient of a loss function with respect to the input to argmax. Unlike STE's gradient proxy, SPIGOT aims to respect the constraints in the argmax problem. SPIGOT can be applied with any intermediate layer that is expressible as a constrained maximization problem, and whose feasible set can be projected onto. We show empirically that SPIGOT works even when the maximization and the projection are done approximately.\",\n  \n  \"Application of SPIGOT in NLP Architectures\": \"We offer two concrete architectures that employ structured argmax as an intermediate layer: semantic parsing with syntactic parsing in the middle, and sentiment analysis with semantic parsing in the middle. These architectures are trained using a joint objective, with one part using data for the intermediate task, and the other using data for the end task. The datasets are not assumed to overlap at all, but the parameters for the intermediate task are affected by both parts of the training data.\",\n  \n  \"Empirical Evaluation of SPIGOT\": \"Our experiments show that our architecture improves over a state-of-the-art semantic dependency parser, and that SPIGOT offers stronger performance than a pipeline, SA, and STE. On sentiment classification, we show that semantic parsing offers improvement over a BiLSTM, more so with SPIGOT than with alternatives. Table 1 compares the semantic dependency parsing performance of SPIGOT to all five baselines, showing that SPIGOT outperforms STE on DM by more than 0.3% absolute labeled F1, both in-domain and out-of-domain.\",\n  \n  \"Analysis of Intermediate Model Behavior\": \"We examine here how the intermediate model is affected by the end-task training signal. Is the end-task signal able to 'overrule' intermediate predictions? We use the syntactic-then-semantic parsing model as a case study. Table 3 compares a pipelined system to one jointly trained using SPIGOT. We consider the development set instances where both syntactic and semantic annotations are available, and partition them based on whether the two systems' syntactic predictions agree (SAME), or not (DIFF).\",\n  \n  \"Related Work in Joint Learning and Differentiable Optimization\": \"Joint learning in NLP pipelines. To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines. However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters. Differentiable optimization. Gould et al. review the generic approaches to differentiation in bi-level optimization. Amos and Kolter extend their efforts to a class of subdifferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application in NLP.\",\n  \n  \"Conclusion and Future Directions\": \"We presented SPIGOT, a novel approach to backpropagating through neural network architectures that include discrete structured decisions in intermediate layers. SPIGOT devises a proxy for the gradients with respect to argmax's inputs, employing a projection that aims to respect the constraints in the intermediate task. We empirically evaluate our method with two architectures: a semantic parser with an intermediate syntactic parser, and a sentiment classifier with an intermediate semantic parser. Experiments show that SPIGOT achieves stronger performance than baselines under both settings, and outperforms state-of-the-art systems on semantic dependency parsing.\"\n}\n```",
    "```json\n{\n  \"Introduction to Similarity Search and Semantic Hashing\": \"The problem of similarity search, also called nearest-neighbor search, consists of finding documents from a large collection of documents, or corpus, which are most similar to a query document of interest. Fast and accurate similarity search is at the core of many information retrieval applications, such as plagiarism analysis, collaborative filtering, content-based multimedia retrieval, and caching. Semantic hashing is an effective approach for fast similarity search. By representing every document in the corpus as a similarity-preserving discrete (binary) hashing code, the similarity between two documents can be evaluated by simply calculating pairwise Hamming distances between hashing codes.\",\n  \"Challenges in Existing Text Hashing Approaches\": \"While considerable research has been devoted to text (semantic) hashing, existing approaches typically require two-stage training procedures. These methods can be generally divided into two categories: binary codes for documents are first learned in an unsupervised manner, then binary classifiers are trained via supervised learning to predict the l-bit hashing code; continuous text representations are first inferred, which are binarized as a second step during testing. Because the model parameters are not learned in an end-to-end manner, these two-stage training strategies may result in suboptimal local optima.\",\n  \"Proposed Neural Architecture for End-to-End Text Hashing\": \"We propose a simple and generic neural architecture for text hashing that learns binary latent codes for documents in an end-to-end manner. Inspired by recent advances in neural variational inference for text processing, we approach semantic hashing from a generative model perspective, where binary (hashing) codes are represented as either deterministic or stochastic Bernoulli latent variables. The inference (encoder) and generative (decoder) networks are optimized jointly by maximizing a variational lower bound to the marginal distribution of input documents.\",\n  \"Neural Variational Inference Framework\": \"Inspired by the recent success of variational autoencoders for various NLP problems, we approach the training of discrete (binary) latent variables from a generative perspective. We define a generative model that simultaneously accounts for both the encoding distribution and decoding distribution by defining approximations via inference and generative networks. This cyclic strategy provides the latent variable with a better ability to generalize.\",\n  \"Rate-Distortion Theory and Data-Dependent Noise\": \"Motivated by the rate-distortion theory, we propose to inject data-dependent noise into the latent codes during the decoding stage, which adaptively accounts for the tradeoff between minimizing rate and distortion during training. The connection between the proposed method and rate-distortion theory is further elucidated, providing a theoretical foundation for the effectiveness of our framework.\",\n  \"Supervised Hashing with NASH\": \"The proposed Neural Architecture for Semantic Hashing (NASH) can be extended to supervised hashing, where a mapping from latent variable to labels is learned, here parametrized by a two-layer MLP followed by a fully-connected softmax layer. To allow the model to explore and balance between maximizing the variational lower bound and minimizing the discriminative loss, a joint training objective is employed.\",\n  \"Experimental Setup and Evaluation Metrics\": \"We use three standard publicly available datasets for training and evaluation: Reuters21578, 20Newsgroups, and TMC. For the inference networks, we employ a feedforward neural network with 2 hidden layers using the ReLU non-linearity activation function. Our model is trained using Adam, with a learning rate of 1 \u00d7 10\u22123 for all parameters. To evaluate the hashing codes for similarity search, we consider each document in the testing set as a query document. The performance is measured with precision.\",\n  \"Ablation Study and Qualitative Analysis\": \"The effect of stochastic sampling shows stronger retrieval results on all three datasets, indicating that endowing the sampling process of latent variables with more stochasticity improves the learned representations. For the encoder networks, increasing the non-linearity by stacking MLP layers leads to better empirical results. However, when we employ a non-linear network for the decoder, the retrieval precision drops dramatically. This observation may be attributed to the fact that the decoder networks can be considered as a similarity measure between latent variable and the word embeddings for every word.\"\n}\n```",
    "```json\n{\n  \"Introduction to Cross-Lingual Transfer Learning\": \"Supervised learning remains dominant in natural language processing, requiring large annotated corpora. However, most of the world's 6,000+ languages lack such resources. Cross-lingual transfer learning addresses this by transferring knowledge from high-resource to low-resource languages. Traditional methods include annotation projection over parallel corpora or using transferable representations like phonetic transcriptions and bilingual dictionaries. Most methods rely on a single source language, limiting the transfer potential. Some approaches use multiple source languages, but often fail to account for transfer quality, treating all sources equally.\",\n  \"Challenges in Multilingual Transfer\": \"Most cross-lingual transfer methods depend on a single source language, which restricts the transfer of knowledge. The target language might share similarities with multiple source languages, such as script or word order, which could enhance transfer. However, existing methods do not adequately account for the quality of transfer from multiple sources, often weighting each language's contribution uniformly. This can lead to suboptimal performance, especially when the source languages have diverse characteristics.\",\n  \"Proposed Zero-Shot Multilingual Transfer Method\": \"We propose a novel zero-shot multilingual transfer method inspired by truth inference in crowd-sourcing. This method estimates each model's reliability and patterns of mistakes, adapting these ideas to a multilingual transfer setting. The approach involves learning the quality of transfer and language-specific errors to infer the best labeling in the target language using a Bayesian graphical model. This unsupervised method allows for accurate inferences despite substantial noise, outperforming uniform ensembling and single best models.\",\n  \"Bayesian Graphical Model for Transfer Quality\": \"Our probabilistic graphical model, inspired by Kim and Ghahramani (2012), is designed for unsupervised inference in zero-shot transfer settings. The model generates predictions based on a 'true' label corrupted by each transfer model, described by a confusion matrix. Inference involves explaining observed predictions efficiently, differentiating reliable from unreliable models. This model, referred to as BEA, uses mean-field variational Bayes for inference, optimizing the evidence lower bound. The model's effectiveness is demonstrated in NER tasks, where it filters out unreliable transfer models.\",\n  \"Few-Shot Transfer with Limited Annotations\": \"In scenarios where limited annotations are available, we propose two models: BEA sup and RaRe. BEA sup uses labeled data to estimate the Bayesian model's parameters, while RaRe ranks systems and retrains a model on the target language using top-ranked models' outputs. This approach captures regularities like common words or character patterns, which are often missed by cross-lingual transfer. RaRe involves a distillation step using a large unannotated dataset and fine-tuning with the small supervised dataset, improving performance in low-resource settings.\",\n  \"Experimental Setup and Data\": \"Our primary evaluation uses a subset of the Wikiann NER corpus with 41 languages, chosen for their overlap with multilingual word embedding resources. We also evaluate on CoNLL 2002 and 2003 datasets for comparison with prior work. We use fastText 300-dimensional Wikipedia embeddings, mapped to English embedding space using the Procrustes rotation method. The sequential tagger is a BiLSTM-CRF model, with hyperparameters tuned for high and low-resource settings. Experiments involve training models on all 41 languages in both high-resource and naive low-resource settings.\",\n  \"Results and Analysis of Transfer Methods\": \"Our proposed multilingual methods outperform majority voting, with unsupervised zero-shot learning showing significant improvements. BEA ent uns outperforms BEA tok uns, highlighting the effectiveness of entity-level inference. Limited annotation access in the target language further enhances performance, with RaRe and BEA ent sup achieving F1 scores of 77.4 and 74.8, respectively. Majority voting performs well for Romance and Germanic languages but fails for Slavic languages. RaRe outperforms BEA ent sup for isolated languages, demonstrating the limitations of relying solely on data aggregation.\",\n  \"Comparison with Existing Methods\": \"Our methods are competitive with benchmark methods on the CoNLL-02/03 datasets, with RaRe l showing improvements over unsupervised models. Majority voting performs well, especially MV ent, comparable to BEA's performance. The use of 100 annotated sentences in the target language provides significant improvements. Our work demonstrates that multilingual transfer outperforms state-of-the-art unsupervised single-source models like BWET, and our methods can combine multiple source models to outperform majority voting.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Parsing and Graph-Structured Representations\": \"The recent years have seen an increased interest as well as rapid progress in semantic parsing and surface realization based on graph-structured semantic representations, such as Abstract Meaning Representation (AMR), Elementary Dependency Structure (EDS), and Dependency-based Minimal Recursion Semantics (DMRS). These representations are grounded under type-logical semantics, which treats linguistic elements as roots in semantic graphs, making the structure flat rather than hierarchical. This distinction is essential in modeling natural language semantics.\",\n  \n  \"Challenges in Existing DAG Transducers for NLG\": \"The only existing DAG transducer for NLG, proposed by Quernheim and Knight, is designed for DAG-to-tree transformation and is unsuitable for meaning graphs transformed from type-logical semantics. It lacks the ability to reverse edge directions and handle multiple roots, which are necessary for type-logical semantic graphs. Additionally, it struggles with unconnected nodes that have direct dependencies, making it difficult to generate natural language sentences from such graphs.\",\n  \n  \"Proposed Novel DAG Transducer Design\": \"We propose a novel DAG transducer that performs graph-to-program transformation. The input is a semantic graph, and the output is a program licensed by a declarative programming language. By executing this program, we can easily generate a surface string. This approach allows for more flexibility in handling diverse internal structures of semantic graphs and can be extended to other linguistic structures.\",\n  \n  \"Declarative Programming for Graph-to-String Transformation\": \"Our transducer leverages declarative programming, which describes what a program must accomplish rather than how to accomplish it. This approach allows for the transformation of type-logical semantic graphs into strings by executing a program. The syntax of our declarative programming language involves string concatenation and variable assignments, enabling the construction of target structures through program execution.\",\n  \n  \"DAG Transduction-based Natural Language Generation\": \"Different languages exhibit varying morphosyntactic properties, affecting NLG. Our DAG transducer translates semantic graphs into sequential lemmas, considering syntactico-semantic properties. A neural sequence-to-sequence model then generates final surface strings, addressing morpho-syntactic properties. This two-step strategy unifies a general framework for DAG transduction-based NLG.\",\n  \n  \"Inducing Transduction Rules from Semantic Graphs\": \"We present an empirical study on DAG transduction-based NLG using variable-free MRS representations, specifically EDS. The data set used is DeepBank 1.1. We introduce constraints to ensure reasonable string generation and a fine-to-coarse strategy to enhance robustness. Induced rules are extracted from training data, while extended and dynamic rules address data sparseness and decoding challenges.\",\n  \n  \"Evaluation and Analysis of the DAG Transducer\": \"Our experiments on DeepBank 1.1 demonstrate the effectiveness of our DAG transducer for NLG. The system achieves a BLEU-4 score of 68.07, with high accuracy and efficiency, generating more than 5 sentences per second on average. The transducer outperforms a pure sequence-to-sequence model, confirming its utility in handling EDS graphs and achieving full-coverage robustness.\",\n  \n  \"Conclusion and Future Implications\": \"We extend the work on DAG automata and propose a flexible DAG transducer leveraging declarative programming to minimize computation burden. Our system is accurate, efficient, and robust, confirming the usefulness of a DAG transducer for NLG. This design may benefit many NLP tasks involving graph manipulation, exemplified by our semantic-graph-to-string task.\"\n}\n```",
    "```json\n{\n  \"Introduction to Social Media as a Data Source for Epidemic Detection\": \"The increased use of social media platforms has led to wide sharing of personal information. Twitter, a micro-blogging platform, has attracted researchers because it can be a valuable personal information resource. The feasibility of using social media as 'social sensors' has been demonstrated in various event detection systems, including disease outbreaks. This study examines the detection of seasonal influenza epidemics using Twitter, as more than 30 Twitter-based influenza detection systems have been developed worldwide. These systems operate on the assumption that flu spreading in the real world is reflected in tweets, aggregating counts of daily flu-related tweets to estimate current patient status.\",\n  \"Challenges in Using Twitter for Influenza Surveillance\": \"Although Twitter provides valuable data for influenza surveillance, it also presents challenges. Tweets can express actual illness or mere suspicion, which introduces noise into the data. Earlier studies have focused on removing such noise, including suspicion, negation, and news-related tweets. This study employs noisy tweets, assuming that words like 'fever' can indicate an upcoming influenza outbreak. The challenge lies in distinguishing between tweets that reflect actual patient status and those that do not, which is crucial for precise influenza surveillance.\",\n  \"Forecasting Words and Time Lag Analysis\": \"The study identifies 'forecasting words' like 'fever' that appear before the onset and diagnosis of influenza. A time lag exists between the frequency of these words and the number of patients. For example, a 16-day time lag is observed for 'fever'. If known in advance, this time lag can help approximate patient numbers. The study also examines flu prevention words like 'shot' and 'injection', which have longer time lags. The problems to solve include estimating the optimal time lag for each word and incorporating these lags into the model.\",\n  \"Methodology: Time-Shifted Word Matrix and Nowcasting Model\": \"To estimate current influenza epidemics and forecast future ones, a linear model is constructed using a time-shifted word matrix. The optimal time shift for each word is determined using cross-correlation between word frequency and patient numbers. A matrix is then constructed with shifted word frequencies. The nowcasting model is built from this matrix, with parameters estimated using regularization methods like Lasso and Elastic Net. This model can be extended to a forecasting model by shifting word frequencies further into the future.\",\n  \"Parameter Estimation Using Lasso and Elastic Net\": \"The nowcasting model's parameters are estimated by minimizing the squared error with a penalty term. Lasso, a well-known method for parameter selection, is used, but it can overfit data when vocabulary size is large. Therefore, Elastic Net, which combines L1 and L2 penalties, is employed. Elastic Net selects groups of correlated variables and provides better performance than Lasso alone. It is used to estimate parameters for the nowcasting model, allowing for automatic variable selection and continuous shrinkage.\",\n  \"Experimentation and Results: Nowcasting and Forecasting Performance\": \"The nowcasting model's performance was assessed using actual influenza reports. Time-shifted models demonstrated about a 0.1 point improvement over baseline models, indicating the contribution of time shift features. The forecasting model showed superior accuracy to the baseline up to 3 weeks into the future. The model's performance decreased with longer time shifts, but it demonstrated equivalent performance up to 10 days into the future. The results highlight the model's capability to predict future epidemics effectively.\",\n  \"Discussion on Model Limitations and Improvements\": \"The proposed approach demonstrated basic feasibility, but exceptions were noted, such as inconsistencies in time shifts between training and testing seasons. Words like 'vaccination' showed high correlation in one season but not in others, indicating annual cycling. The model overestimated before outbreaks and underestimated after peaks. A technique to distinguish actual forecasting words from noise is needed. Despite these limitations, the time shift was effective for social media-based surveillance, and the model enables prediction.\",\n  \"Conclusion and Future Applications of the Model\": \"This study proposed a novel social media-based influenza surveillance system using forecasting words from Twitter. The model achieved the best nowcasting performance to date and sufficient forecasting performance. It is the first model enabling prediction of future epidemics. The approach has potential applications for predicting other events, demonstrating the utility of social media data in epidemic detection and forecasting.\"\n}\n```",
    "```json\n{\n  \"Introduction to Satire and Satire Detection\": \"Satire is a form of art used to criticize in an entertaining manner, utilizing stylistic devices such as humor, irony, sarcasm, exaggerations, parody, or caricature. Satirical news mimics regular news in diction but does not aim to deceive readers into believing false information. The task of satire detection is to automatically distinguish satirical news from regular news, which is challenging even for human readers. An automatic system for satire detection is relevant for downstream applications and could help humans better understand satire's characteristics.\",\n  \"Challenges in Automatic Satire Detection\": \"Solving satire detection computationally is challenging because even human readers sometimes struggle to recognize satire. Previous work often relies on corpora labeled based on publication sources, which may lead models to learn publisher characteristics rather than satire. This approach can mislead model interpretation and harm generalization to unseen sources.\",\n  \"Limitations of Previous Satire Detection Methods\": \"Previous methods often used handcrafted features and focused on English satire detection. These methods may enable classifiers to learn which articles belong to which publication, potentially classifying everything else as satire. The lack of publication source information in datasets limits the analysis of these methods' effectiveness.\",\n  \"Proposed Model with Adversarial Training\": \"We propose a new model with adversarial training to control for the confounding variable of publication sources, aiming to debias the model. The model includes a satire detector and a publication identifier, sharing a feature extractor. Adversarial training is used to update parameters to fool the publication identifier, ensuring the model focuses on satire characteristics.\",\n  \"Adversarial Training and Its Impact on Satire Detection\": \"Adversarial training is crucial for the model to pay attention to satire instead of publication characteristics. Our experiments show that satire detection performance remains comparable with the adversarial component, while publication identification performance drops, indicating the model can still learn to identify satire.\",\n  \"Creation and Utilization of a New German Satire Dataset\": \"We publish a large German dataset for satire detection, the first of its kind, including publication sources. This dataset enables experiments to investigate the hypothesis that models learn publisher characteristics rather than satire. The dataset consists of almost 330k articles, with a realistic representation of publication-specific phrases and common words.\",\n  \"Evaluation Metrics and Results Analysis\": \"For evaluating satire detection, we use precision, recall, and F1 score. The model with adversarial training achieves comparable satire classification performance to the baseline, with a significant drop in publication identification precision. This suggests overlapping features for satire and publication classification, indicating the tasks cannot be entirely untangled.\",\n  \"Future Directions and Potential Confounding Variables\": \"Future work could explore other potential confounding variables in satire detection, such as the distribution of time and region of articles. More quantitative and qualitative analyses are proposed to better understand the behavior of different classifier configurations.\"\n}\n```",
    "```json\n{\n  \"Introduction to Deep Learning in Question Answering\": \"Recently, deep learning has been applied to a variety of question answering tasks, including answering questions about images, tabular data, and passages of text. Developers, end-users, and reviewers in academia are interested in understanding the capabilities of these models. The standard way of measuring the effectiveness of a system is by evaluating its error on a test set. High accuracy is indicative of a good model only if the test set is representative of the underlying real-world task. However, it is challenging to manually ensure that test and training sets are representative of real-world scenarios.\",\n  \n  \"Sensitivity Analysis of Deep Learning Models\": \"In this paper, we propose techniques to analyze the sensitivity of a deep learning model to question words by applying attribution and generating adversarial questions. For example, in Visual Question Answering, the system correctly answers 'how symmetrical are the white bricks on either side of the building?' as 'very'. However, it also answers 'how spherical are the white bricks on either side of the building?' as 'very', indicating reliance on a few words. This sensitivity analysis helps identify important question words and anticipate their function in question answering.\",\n  \n  \"Integrated Gradients for Attribution\": \"We employ an attribution technique called Integrated Gradients (IG) to isolate question words that a deep learning system uses to produce an answer. IG attributes the prediction at input x relative to a baseline input x0, which is an empty question in our case. IG aggregates the gradients of the probability with respect to the input along a trajectory using a path integral. This method is chosen for its ease and efficiency of implementation and its axiomatic justification.\",\n  \n  \"Overstability in Question Answering Networks\": \"A key contribution of this work is an overstability test for question answering networks. We find that reading comprehension networks are overly stable to semantics-altering edits to the passage, and this overstability also applies to questions. This behavior is observed in visual and tabular question answering networks as well. We define a general-purpose test for measuring the extent of overstability by measuring how a network's accuracy changes as words are systematically dropped from questions.\",\n  \n  \"Visual Question Answering Analysis\": \"The Visual Question Answering Task requires a system to answer questions about images. We analyze the deep network from Kazemi and Elqursh (2017), which achieves 61.1% accuracy on the validation set. We find that the network ignores many question words, relying largely on the image to produce answers. For instance, the model retains more than 50% of its original accuracy even when every word that is not 'color' is deleted from all questions in the validation set. This indicates a reliance on the image rather than the question.\",\n  \n  \"Question Answering on Tables\": \"We analyze a system called Neural Programmer (NP) that answers questions on tabular data. NP determines the answer by selecting a sequence of operations to apply on the accompanying table. We find that these operation selections are more influenced by content-free words in questions than important words such as nouns or adjectives. Dropping all content-free words reduces the validation accuracy of the network from 33.5% to 28.5%. This indicates a reliance on irrelevant words rather than the content of the question.\",\n  \n  \"Reading Comprehension and Adversarial Attacks\": \"The reading comprehension task involves identifying a span from a context paragraph as an answer to a question. We analyze the network by Yu et al. (2018) and find that the network often ignores words that should be important. Jia and Liang (2017) proposed attacks wherein sentences are added to paragraphs that ought not to change the network's answers, but sometimes do. Our main finding is that these attacks are more likely to succeed when an added sentence includes all the question words that the model found important.\",\n  \n  \"Conclusion and Implications for QA Models\": \"We analyzed three question answering models using an attribution technique. Attributions helped us identify weaknesses of these models more effectively than conventional methods. A workflow that uses attributions can aid the developer in iterating on model quality more effectively. While the attacks in this paper may seem unrealistic, they expose real weaknesses that affect the usage of a QA product. Our attribution-based methods can be directly used to gauge the extent of such problems.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Network Mappings in Cross-Modal Retrieval\": \"Neural network mappings are widely used to bridge modalities or spaces in cross-modal retrieval, zero-shot learning, and building multimodal representations. Typically, a neural network is trained to predict the distributed vectors of one modality from another. At test time, operations such as retrieval or labeling are performed based on the nearest neighbors of the predicted vectors. The success of these systems relies on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure. However, whether neural networks achieve this goal in general has not been thoroughly investigated.\",\n  \"Challenges in Achieving Semantic Similarity in Neural Network Mappings\": \"Recent work evidences that considerable information about the input modality propagates into the predicted modality. The predicted vectors of cross-modal neural net mappings are still largely informative about the input vectors. Counterintuitively, it has been found that the vectors 'imagined' from a language-to-vision neural map outperform the original visual vectors in concept similarity tasks. This suggests that the mapped vectors often resemble the input vectors more than the target vectors in semantic terms, which goes against the goal of a cross-modal map.\",\n  \"Proposed Measure for Quantifying Neighborhood Structure Similarity\": \"To measure the similarity between the neighborhood structure of two sets of paired vectors, the mean nearest neighbor overlap measure (mNNO K) is proposed. This measure quantifies how semantically similar two sets of paired vectors are by calculating the overlap of their nearest neighbors. The normalizing constant K scales mNNO K between 0 and 1, making it independent of the choice of K. This measure implicitly performs retrieval for some similarity measure and provides a more realistic estimate of semantic similarity.\",\n  \"Experimental Setup and Datasets for Cross-Modal Tasks\": \"To test the generality of the claims, a rich diversity of cross-modal tasks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki) are selected. Various image and text features of varied dimensionality are used to ensure results are independent of feature choice. The experiments involve both text-to-image and image-to-text directions, and the extracted features are made publicly available.\",\n  \"Results and Analysis of Neighborhood Structure in Mapped Vectors\": \"The results reveal that the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the target vectors. This finding is consistent across different mappings, embeddings, and directions. The test mean nearest neighbor overlap shows that the semantic structure of the mapped vectors is more akin to the input vectors, highlighting a phenomenon neglected so far in cross-modal mappings.\",\n  \"Impact of Untrained Networks on Semantic Structure Preservation\": \"Untrained linear and neural net mappings preserve the semantic structure of the input, complementing the findings of the trained mappings. This suggests a natural tendency of neural nets to preserve semantic information about the input, regardless of the choice of target vectors and loss function. The results indicate that even without training, the semantic structure of the input is maintained.\",\n  \"Conclusions and Future Directions for Cross-Modal Neural Network Research\": \"Overall, the study uncovers a phenomenon where neural net cross-modal mappings produce mapped vectors more akin to the input vectors than the target vectors in terms of semantic structure. This finding has been possible thanks to the proposed measure that quantifies similarity between the neighborhood structures of vector sets. The study advocates for the development of better architectures to bridge modalities and the use of semantic-based criteria to evaluate the quality of predicted vectors.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Scientific Publication Lifecycle\": \"The Scientific Publication Lifecycle comprises different activities carried out by researchers, including literature review, actual research work, and dissemination of results through conferences and journals. These activities involve multiple sub-activities requiring specific expertise and experience. Researchers with low experience often face difficulties in completing research-related activities and rely on assistance from supervisors, experts, and librarians to learn the required skills.\",\n  \n  \"Challenges Faced by Novice Researchers\": \"Novice researchers, such as Ph.D. and Masters students, often encounter difficulties in research execution due to a lack of experience. They typically require assistance in selecting information sources, formulating search queries, browsing retrieved results, and judging the relevance of retrieved articles. These challenges highlight the expertise gap between novice and experienced researchers.\",\n  \n  \"Existing Academic Assistive Systems\": \"Academic assistive systems have been developed to bridge the expertise gap between experts and novices. These systems include search systems with faceted user interfaces, bibliometric tools for visualizing citation networks, and scientific paper recommender systems. Techniques have also been proposed to recommend articles for citation contexts in manuscripts and to suggest prospective conference venues for manuscript publication.\",\n  \n  \"Unexplored Areas in Manuscript Preparation\": \"One unexplored area in manuscript preparation is helping researchers identify important and unique papers for citation in their manuscripts. This task is influenced by the type of research and the type of article being written. The article-type places constraints on citations in terms of dimensions such as recency and quantity. Our research addresses this task by shortlisting papers from researchers' reading lists based on article-type preference.\",\n  \n  \"Overview of the Rec4LRW System\": \"The Rec4LRW system is designed to assist researchers in literature review and manuscript preparation. It helps in building an initial reading list, finding similar papers, and shortlisting papers for manuscript inclusion based on article-type choice. The system uses a corpus of papers from the ACM Digital Library and is particularly beneficial for novice researchers and those exploring new research topics.\",\n  \n  \"Technical Details and Dataset of Rec4LRW\": \"The Rec4LRW system uses a snapshot of the ACM Digital Library as its dataset, comprising 103,739 articles and 2,320,345 references. The system's recommendation mechanisms are based on seven features representing the characteristics of the bibliography. The Girvan-Newman algorithm is used for clustering in the citation network to identify important papers. The system's UI includes unique features like information cue labels and options to view papers in parent clusters.\",\n  \n  \"User Evaluation Study and Results\": \"A user evaluation study was conducted with 116 participants, including research students and staff, to evaluate the Rec4LRW system. The study aimed to assess the usefulness and effectiveness of the system's tasks. Results showed high agreement percentages for measures like Importance and Shortlisting_Feature, indicating the task's usefulness in identifying important papers. Participants appreciated features like information cue labels and the option to view papers in parent clusters.\",\n  \n  \"Conclusion and Future Work\": \"The Rec4LRW system addresses the gap in assistive systems for manuscript preparation by providing a novel task for shortlisting articles. The user evaluation study validated the system's effectiveness, with participants acknowledging the quality of the shortlisted papers. Future work will focus on expanding the task's scope, including variations for different article-type choices, integrating with citation context recommendation tasks, and providing more user control.\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentence Representations in Deep Learning\": \"Deep learning has enabled the automatic learning of continuous sentence representations, which can be tailored for specific tasks like sentiment analysis or semantic similarity. These representations can be learned in both unsupervised and supervised ways. A notable approach involves training neural machine translation (NMT) models, which have seen significant advancements, particularly with the introduction of attention mechanisms. This paper explores how these improvements impact sentence representations.\",\n  \"Advancements in Neural Machine Translation and Attention Mechanisms\": \"Since the work of Hill et al. (2016), NMT has experienced substantial improvements in translation quality, largely due to the introduction of attention mechanisms. Attention allows NMT systems to dynamically focus on important parts of the source sentence, eliminating the need for a static vector representation. This paper proposes a novel encoder-decoder architecture that maintains a structured fixed-size representation while allowing dynamic attention.\",\n  \"Proposed Encoder-Decoder Architecture with Compound Attention\": \"The proposed architecture incorporates compound attention, which includes attention in both the encoder and decoder. The encoder uses a bidirectional recurrent network with gated recurrent units (GRU) to process input sequences, and inner attention to compute weighted averages of encoder states. The decoder applies attention to these components, creating an implicit alignment between source and target sequences. This model is a variation of traditional attention mechanisms, aiming to improve sentence representation while maintaining translation quality.\",\n  \"Evaluation of Sentence Representations Across Tasks\": \"Continuous sentence representations are evaluated using classification and similarity tasks from SentEval. The evaluation includes 10 classification and 7 similarity tasks, with metrics like Pearson and Spearman correlation used to assess performance. Additionally, paraphrase representation is evaluated using datasets like COCO and HyTER Networks, with metrics such as cluster classification accuracy and nearest-neighbor retrieval accuracy.\",\n  \"Experimental Setup and Training Details\": \"The experimental setup involves training English-to-German and English-to-Czech NMT models using Neural Monkey. The models are trained on datasets like Multi30K and CzEng 1.7, with specific preprocessing steps such as truecasing and byte-pair encoding. Training details include the number of epochs, batch size, and vocabulary size, with evaluation performed on respective development and test sets.\",\n  \"Translation Quality and BLEU Score Analysis\": \"Translation quality is assessed using BLEU scores, with results indicating that the ATTN model performs best, followed by the Transformer and ATTN-ATTN models. The analysis shows a correlation between the number of attention heads and BLEU scores, suggesting that larger representations and more heads generally lead to higher BLEU scores. Manual ranking confirms these findings, highlighting the impact of model configurations on translation quality.\",\n  \"Comparison with Existing Models and Baselines\": \"The proposed models are compared with existing approaches like InferSent, GloVe-BOW, and SkipThought. While InferSent outperforms other models in most tasks, the proposed models show competitive performance, particularly in NLI tasks. The comparison highlights the strengths and weaknesses of different models in various evaluation tasks, providing insights into their effectiveness in sentence representation.\",\n  \"Discussion on Representation Metrics and Translation Quality\": \"The relationship between representation metrics and translation quality is explored, revealing a negative correlation between BLEU scores and representation metrics. This suggests that models focusing on translation quality may not effectively capture sentence meaning. The analysis includes a heatmap of Pearson correlations, illustrating the complex interplay between different metrics and highlighting the challenges in achieving both high translation quality and meaningful sentence representations.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Vector Representations\": \"Neural vector representations have become ubiquitous in all subfields of natural language processing. For word vectors, important properties have been studied, such as linear substructures and the nexus to pointwise mutual information scores. However, little is known about sentence embeddings, which encode variable-length input sentences into fixed-length vectors. These embeddings are mainly evaluated based on how well their cosine similarities mirror human judgments of semantic relatedness, often using datasets like SICK. Despite the effectiveness of shallow methods, the specific semantic properties reflected by sentence embeddings remain largely unexplored.\",\n  \n  \"Properties and Evaluation of Sentence Embeddings\": \"Sentence embeddings have been evaluated in terms of their ability to predict the occurrence, order, and length of words in the original sentence. They are also used in classification tasks such as sentiment polarity or question type classification. Conneau et al. investigated the classification of syntactic and semantic properties using large datasets. Despite these efforts, little is known about the specific semantic properties directly reflected by embeddings. Our framework generates triplets of sentences to explore how changes in syntactic structure or semantics affect the similarities between their embeddings.\",\n  \n  \"Sentence Modification Schemes\": \"We describe transformations applied to generate altered sentences, which are then assembled into sentence triplets to assess semantic properties of embeddings. These transformations include Not-Negation, Quantifier-Negation, Synonym Substitution, Embedded Clause Extraction, Passivization, Argument Reordering, and Fixed Point Inversion. Each transformation aims to test different aspects of sentence semantics, such as negation detection and argument sensitivity, by altering the sentence structure while maintaining or changing its meaning.\",\n  \n  \"Sentence Triplet Generation and Evaluation Datasets\": \"Using modified sentences, we induce five evaluation datasets: Negation Detection, Negation Variants, Clause Relatedness, Argument Sensitivity, and Fixed Point Reorder. Each dataset is designed to test specific semantic properties of sentence embeddings, such as distinguishing between similar structures with opposite meanings or assessing semantic relatedness despite structural differences. These datasets are created using sentences from sources like SICK and Penn Treebank WSJ, with human annotators ensuring grammaticality and semantics.\",\n  \n  \"Embedding Methods Compared\": \"We compare three prominent sentence embedding methods: GloVe Averaging, Concatenated P-Mean Embeddings, and Sent2Vec. GloVe Averaging involves taking the average of word vectors, while P-Means concatenates different p-means of word vectors. Sent2Vec learns word and n-gram embeddings for high-quality sentence vectors. Additionally, Skip-Thought Vector and InferSent are discussed, with Skip-Thought applying neighbour prediction intuitions at the sentence level and InferSent using supervision from auxiliary tasks like the Stanford NLI dataset.\",\n  \n  \"Results and Discussion on Negation Detection\": \"Table 2 shows results for the Negation Detection dataset, where S, S+, S* refer to the original, Synonym Substitution, and Not-Negation versions. GloVe Avg. often misleads with synonyms, while InferSent and SkipThought distinguish unnegated from negated sentences. Accuracy is computed as the percentage of triplets where the original and synonym-substituted versions have higher similarity than the original and Not-Negation version. InferSent and SkipThought perform well in distinguishing negation.\",\n  \n  \"Analysis of Negation Variants and Clause Relatedness\": \"Table 3 presents results for Negation Variants, with S, S+, S* as original, Not-Negation, and Quantifier-Negation versions. InferSent shows better understanding of negation quantifiers. Table 4 discusses Clause Relatedness, with Sent2Vec best distinguishing embedded clauses from negations. SkipThought's accuracy varies based on sentence structure, performing better on sentences with similar word order. InferSent also shows varied performance based on sentence structure.\",\n  \n  \"Argument Sensitivity and Fixed Point Reorder\": \"Table 5 discusses Argument Sensitivity, with S, S+, S* as original, Passivization, and Argument Reordering versions. None of the approaches effectively distinguish semantic from structural information. Table 6 covers Fixed Point Reorder, with S, S+, S* as original, semantically equivalent, and Fixed Point Inversion versions. Sentence embeddings based on means or concatenation fail to distinguish fixed point inverted sentences, while SkipThought and InferSent perform better with similar structures.\",\n  \n  \"Conclusion on Sentence Embedding Methods\": \"This paper proposes a method to inspect sentence embeddings for semantic properties, analyzing three popular methods. SkipThought and InferSent distinguish negation from synonymy, with InferSent better at identifying semantic equivalence regardless of word order. SkipThought is suitable for tasks where sentence semantics correspond to structure but struggles with different word orders. Dedicated sentence embeddings from neural networks outperform simple word embedding averaging.\"\n}\n```",
    "```json\n{\n  \"Introduction to Knowledge Base Question Answering\": \"Large-scale knowledge bases (KB) like DBPedia and Freebase have become crucial for open-domain question answering (QA). State-of-the-art approaches to KB-QA often rely on semantic parsing, where a question is mapped to a formal meaning representation and then translated into a KB query. This process allows for retrieving answers by executing the query and provides a deeper understanding of the question, aiding in error analysis and answer justification. However, traditional semantic parsing approaches face challenges due to their decoupling from the knowledge base, such as ontology matching problems and difficulties in finding correct predicates from a large vocabulary.\",\n  \"Challenges in Semantic Parsing for QA\": \"Traditional semantic parsing approaches are largely decoupled from the knowledge base, leading to challenges in QA applications. A generic meaning representation may face ontology matching problems when logical forms use predicates differing from those in the KB. Even when the representation language is closely related to the KB schema, finding the correct predicates remains difficult. These challenges necessitate a more integrated approach to semantic parsing that leverages the knowledge base more tightly.\",\n  \"Proposed Semantic Parsing Framework\": \"Inspired by previous works, we propose a semantic parsing framework that tightly integrates the knowledge base when forming the parse for an input question. The framework involves defining a query graph that can be mapped to a logical form in \u03bb-calculus, semantically related to \u03bb-DCS. Semantic parsing is reduced to query graph generation, formulated as a search problem with staged states and actions. This approach allows for efficient search by focusing on promising areas in the space that likely lead to the correct query graph.\",\n  \"Query Graph Design and Execution\": \"Our query graph design is inspired by previous works but with key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base, allowing for straightforward translation to a logical form query that is directly executable. The query graph consists of grounded entities, existential variables, lambda variables, and aggregation functions, connected by directed edges labeled with predicates. This design allows for efficient execution against the knowledge base to retrieve answers.\",\n  \"Staged Query Graph Generation\": \"We focus on generating query graphs with specific properties, such as having a topic entity as the root and a core inferential chain describing the main relationship between the answer and topic entity. The process is formalized as a search problem with staged states and actions, allowing for efficient exploration of possible query graphs. The staged design helps in pruning the search space and biasing the exploration order, improving the likelihood of generating the correct semantic parse.\",\n  \"Deep Convolutional Neural Networks for Relation Matching\": \"To handle the variety of semantically equivalent ways of stating questions and the mismatch between natural language utterances and KB predicates, we use Siamese neural networks for identifying the core inferential chain. The model maps questions to patterns and compares them with candidate chains using a convolutional neural network (CNN) framework. This approach offers advantages over lexical matching by using continuous-space representations, improving relation matching accuracy.\",\n  \"Augmenting Constraints and Aggregations\": \"A graph with just the inferential chain forms the simplest legitimate query graph, but additional constraints and aggregation functions can further restrict the set of answer entities. Constraints are added by attaching entities to variable nodes, while aggregation functions handle constraints over the entire answer set. Simple rules are employed to retain only legitimate constraints, improving the precision of the query graph.\",\n  \"Experimental Results and Analysis\": \"Our semantic parsing approach improves the state-of-the-art result on the WEBQUESTIONS dataset to 52.5% in F1, a 7.2% absolute gain compared to the best existing method. The system performance is measured by the ratio of correctly answered questions, with precision, recall, and F1 computed for each question. The results demonstrate the effectiveness of our approach, with significant improvements over previous methods.\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentence Acceptability and Grammaticality\": \"Sentence acceptability is defined as the extent to which a sentence is well-formed or natural to native speakers of a language. It encompasses semantic, syntactic, and pragmatic plausibility and other non-linguistic factors such as memory limitation. Grammaticality, by contrast, is the syntactic well-formedness of a sentence. Grammaticality as characterized by formal linguists is a theoretical concept that is difficult to elicit from non-expert assessors. In the research presented here, we are interested in predicting acceptability judgments. Lau et al. (2015, 2016) present unsupervised probabilistic methods to predict sentence acceptability, where sentences were judged independently of context. In this paper, we extend this research to investigate the impact of context on human acceptability judgments, where context is defined as the full document environment surrounding a sentence.\",\n  \"Impact of Document Context on Acceptability Judgements\": \"Our goal is to construct a dataset of sentences annotated with acceptability ratings, judged with and without document context. To obtain sentences and their document context, we extracted 100 random articles from the English Wikipedia and sampled a sentence from each article. We found a strong correlation of Pearson's r = 0.80 between the two sets of ratings. We see that adding context generally improves acceptability, but the pattern reverses as acceptability increases, suggesting that context boosts sentence ratings most for ill-formed sentences. The trend persists throughout the whole range of acceptability, so that for the most acceptable sentences, adding context actually diminishes their rated acceptability.\",\n  \"Experimental Design and Data Collection\": \"To generate a set of sentences with varying degrees of acceptability, we used the Moses MT system to translate each sentence from English to four target languages and then back to English. We gathered acceptability judgments using Amazon Mechanical Turk and asked workers to judge acceptability using a 4-point scale. We ran the annotation task twice: first where we presented sentences without context, and second within their document context. We collected 20 judgments per sentence, giving us a total of 20,000 annotations. To ensure annotation reliability, sentences were presented in groups of five, one from the original English set, and four from the round-trip translations.\",\n  \"Analysis of Human Judgements with and without Context\": \"We first look at the correlation between without-context (h\u2212) and with-context (h+) mean ratings. We found a strong correlation of Pearson's r = 0.80 between the two sets of ratings. Adding context 'compresses' the distribution of (mean) ratings, pushing the extremes to the middle. The net effect is that it lowers correlation, as the good and bad sentences are now less separable. One possible explanation for this compression is that workers focus more on global semantic and pragmatic coherence when context is supplied.\",\n  \"Modelling Sentence Acceptability with Enriched Language Models\": \"Lau et al. (2015, 2016) explored a number of unsupervised models for predicting acceptability, including n-gram language models, Bayesian HMMs, LDA-based models, and a simple recurrent network language model. We experiment with neural models in this paper. We use lstm and tdlm models, where tdlm is a joint model of topic and language. The topic model component produces topics by processing documents through a convolutional layer and aligning it with trainable topic embeddings. The language model component incorporates context by combining its topic vector with the LSTM's hidden state.\",\n  \"Performance Evaluation of Language Models\": \"To assess the performance of the acceptability measures, we compute Pearson's r against mean human ratings. We achieved a correlation of 0.584 when we compared lstm\u2212 against h\u2212, which is similar to the previously reported performance. SLOR outperforms all other measures, which is consistent with the findings in Lau et al. (2015). Across all models and human ratings, using context at test time improves model performance. This suggests that taking context into account helps in modeling acceptability, regardless of whether it is tested against judgments made with or without context.\",\n  \"Comparison with Related Work\": \"There is an extensive literature on automatic detection of grammatical errors, but limited work on acceptability prediction. Heilman et al. (2014) trained a linear regression model that uses features such as spelling errors, sentence scores from n-gram models, and parsers. Lau et al. (2015, 2016) experimented with unsupervised learners and found that a simple RNN was the best performing model. Both works predict acceptability independently of any contextual factors outside the target sentence.\",\n  \"Conclusions and Future Work\": \"We found that context positively influences acceptability, particularly for ill-formed sentences, but it also has the reverse effect for well-formed sentences. Incorporating context when modeling acceptability improves model performance. Prediction performance declines when tested on judgments collected with context, overturning our original hypothesis. We discovered that human agreement decreases when context is introduced, suggesting that ratings are less predictable in this case. We will investigate this question in future work and experiment with a wider range of models, including sentence embedding methodologies.\"\n}\n```",
    "```json\n{\n  \"Introduction to Taylor's Law in Text Analysis\": \"Taylor's law characterizes how the variance of the number of events for a given time and space grows with respect to the mean, forming a power law. It is a quantification method for the clustering behavior of a system. Since the pioneering studies of this concept, a substantial number of studies have been conducted across various domains, including ecology, life science, physics, finance, and human dynamics. Despite such diverse application across domains, there has been little analysis based on Taylor's law in studying natural language. For the work described in this article, we applied Taylor's law for texts, in a manner close to the original concept, considering lexical fluctuation within texts.\",\n  \n  \"Application of Taylor's Law to Natural Language\": \"We found that the Taylor exponent is indeed a characteristic of texts and is universal across various kinds of texts and languages. These results are shown here for data including over 1100 single-author texts across 14 languages and large-scale newspaper data. Moreover, we found that the Taylor exponents for other symbolic sequential data, including child-directed speech, programming language code, and music, differ from those for written natural language texts, thus distinguishing different kinds of data sources. The Taylor exponent in this sense could categorize and quantify the structural complexity of language.\",\n  \n  \"Measuring the Taylor Exponent in Texts\": \"Given a set of elements W (words), let X = X1, X2, ..., XN be a discrete time series of length N, where Xi \u2208 W for all i = 1, 2, ..., N, i.e., each Xi represents a word. For a given segment length \u2206t \u2208 N (a positive integer), a data sample X is segmented by the length \u2206t. The number of occurrences of a specific word wk \u2208 W is counted for every segment, and the mean \u00b5k and standard deviation \u03c3k across segments are obtained. Following a previous work, Taylor's law is defined to hold when \u00b5 and \u03c3 are correlated by a power law in the following way: \u03c3 \u221d \u00b5\u03b1.\",\n  \n  \"Taylor Exponents Across Different Data Types\": \"Analysis of different kinds of data showed how the Taylor exponent differed according to the data source. The distributions appear different from those for the natural language texts, and the exponents were significantly larger. This means that these data sets contained expressions with fixed forms much more frequently than did the natural language texts. The kind of data thus might be distinguishable using the Taylor exponent.\",\n  \n  \"Impact of Data and Segment Size on Taylor Exponent\": \"The Taylor exponent depended only slightly on the data size. When the data size was increased, the exponent exhibited a slight tendency to decrease. The Taylor exponent also differed according to \u2206t, and the exponent is known to grow when the segment size gets larger. This phenomenon suggests how word co-occurrences in natural language are self-similar.\",\n  \n  \"Comparison of Taylor Exponents in Machine-Generated Text\": \"We showed that a character-based LSTM language model generated text with a Taylor exponent of 0.5. This indicates one limitation of that model. State-of-the-art word-level language models can generate text whose Taylor exponent is larger than 0.50 but smaller than that of the dataset used for training. This indicates both the capability of modeling burstiness in text and the room for improvement.\",\n  \n  \"Implications for Language Model Evaluation\": \"The Taylor exponent can reasonably serve for evaluating machine-generated text. In contrast to character-level neural language models, neural-network-based machine translation models are capable of maintaining the burstiness of the original text. The translated text retains the clustering behavior of the original text, as the Taylor exponent is equivalent to that of the original text.\"\n}\n```",
    "```json\n{\n  \"Introduction to Language Identification\": \"Language identification is crucial in natural language processing, with applications such as improving letter-to-phoneme conversion and machine transliteration. Traditional methods use character-level n-gram language models, but this paper proposes using support vector machines (SVMs) for identifying languages in very short texts like proper nouns. SVMs have shown to outperform language models on datasets of personal names, and the hypothesis is that language identification can enhance transliteration by pre-processing source data and training separate models.\",\n  \"Previous Work on Language Identification\": \"N-gram approaches have been popular for language identification. Cavnar and Trenkle (1994) achieved 99.8% accuracy using n-gram models for texts of 300 characters or more. However, accuracy drops with shorter texts, as reported by Kruengkrai et al. (2005), where accuracy was 90.2% for texts averaging 50 bytes. SVMs, however, achieved 99.7% accuracy on the same task. Konstantopoulos (2007) found that n-gram models yielded low F1 scores for proper nouns, but training on smaller datasets improved results. Li et al. (2007) achieved 94.8% accuracy for single-name identification using n-gram models, benefiting from the dissimilarity of the languages involved.\",\n  \"SVMs for Language Identification\": \"The paper proposes using SVMs with character n-gram counts as features for language identification. SVMs can handle a large number of features and weigh them appropriately. Experiments tested various kernels, with linear, sigmoid, and radial basis function (RBF) kernels performing best. Optimal n-gram lengths were determined through cross-validation, and models were constructed using the entire training data. The LIBLINEAR and LIBSVM packages were used for different kernels, and preprocessing steps included converting letters to lowercase and removing short names.\",\n  \"Intrinsic Evaluation of SVM Approach\": \"The SVM-based approach was tested on the Transfermarkt corpus of soccer player names and the CEJ corpus of first names and surnames. The Transfermarkt corpus consists of names from 13 national languages, and SVMs outperformed language models, especially on last names. The CEJ corpus includes names classified as Chinese, English, or Japanese, and the linear kernel reduced the error rate by over 50% compared to language models. The RBF and sigmoid kernels were slower due to the corpus size, so only the linear kernel was tested.\",\n  \"Application to Machine Transliteration\": \"Machine transliteration benefits from language identification as it influences pronunciation. The English-Hindi corpus was used to test this application, with names classified as Indian or non-Indian. The language identification model achieved 80% accuracy with the linear kernel and 84% with the RBF kernel. However, using language identification as a preprocessing step did not improve transliteration accuracy, likely due to reduced data size for training separate models. An experiment confirmed that data size reduction was a significant factor, suggesting that language identification provides useful information for transliteration.\",\n  \"Experimental Setup and Results\": \"A simple method combined language identification with transliteration by splitting data into disjoint classes and training separate models. The DIRECTL transliteration system was used, and the language identification model was trained on 1000 tagged names. Splitting the data yielded a combined top-1 accuracy of 46.0%, compared to 47.0% for a single model. The lack of improvement was attributed to reduced data size, and an experiment confirmed this hypothesis. The transliteration system may implicitly leverage language origin information, and closer integration of the modules could improve accuracy.\",\n  \"Conclusion and Future Work\": \"The paper presents a novel approach to language identification using SVMs with n-gram counts, outperforming traditional language models. While language identification did not significantly improve transliteration accuracy, it showed potential compared to random data splits. Future work will explore other methods of incorporating language identification in transliteration, such as using language identification probabilities as features and allowing non-disjoint data sets for model training.\"\n}\n```",
    "```json\n{\n  \"Introduction to Machine Translation Challenges and Solutions\": \"Recently, statistical machine translation (SMT) has been broadly developed and successfully used in practicable systems. However, it is costly to create a large volume of parallel corpora across various domains for commercial use. To address this, a rule-based machine translation (RBMT) system using a monolingual corpus in the target language has been developed. This system allows for target word selection based on co-occurrence relationships extracted from a monolingual corpus. Additionally, word sense disambiguation has been developed for Japanese-Korean and Korean-Japanese translation systems.\",\n  \n  \"Overview of Toshiba's Rule-Based Machine Translation System\": \"The RBMT system is fundamentally a transfer-based machine translation framework. It includes morphological analysis, syntactic/semantic analysis, target word selection, structural transfer, syntactic generation, and morphological generation. The system is supported by a vast number of rules and word dictionaries, enabling high translation performance and customization flexibility. For Japanese-Korean translation, syntactic analysis and transfer are omitted due to grammatical similarities between the languages.\",\n  \n  \"Statistical Post Editing and Its Role in Domain Adaptation\": \"Statistical Post Editing (SPE) using phrase-based SMT is an efficient framework for adapting translation output to target domains. Initially, source sentences from training data in ASPEC and JPO Patent Corpus were translated using RBMT. A phrase-based model was then trained between translated and reference sentences using the Moses toolkit. The training involved 1M sentences for ja-en, en-ja, JPCzh-ja, and JPCko-ja, and 0.67M for ja-zh and zh-ja. Language models were trained using KenLM, and specific distortion limits were set for different language pairs.\",\n  \n  \"System Combination Using Recurrent Neural Network Language Models\": \"Although both SPE and SMT are based on statistical models, they generate different translation candidates. A system combination between SPE and SMT was realized as n-best reranking using a Recurrent Neural Network Language Model (RNNLM). This involved combining 100-best candidates from both SPE and SMT for the dev-set and tuning with MERT by adding the RNNLM score. RNNLMs were trained with 500 hidden layers and 50 classes using the RNNLM toolkit, utilizing 500k sentences from training data.\",\n  \n  \"Preprocessing and Postprocessing Techniques for Improved Translation\": \"Technical term dictionaries were selected for each task, and additional patent dictionaries were used for specific language pairs. English word correction was applied to improve translation of sentences with misspelled words, and Japanese KATAKANA normalization was performed to standardize notation. Other post-processing techniques included translating unknown words in SMT results using RBMT dictionaries, which helped reduce unknown words and improve translation quality.\",\n  \n  \"Experimental Results and Performance Metrics\": \"Experimental results showed that SPE improved translation results of RBMT on BLEU and RIBES scores. In most tasks, reranking improved BLEU scores by about 0.3-0.5, and system combination (COMB) showed better performance than other systems. In JPOko-ja, SMT, SPE, and COMB achieved very high performances close to 70 BLEU, with SMT and reranking achieving the highest scores. Table 1 and 2 provide BLEU and RIBES scores for different subtasks, highlighting the effectiveness of the system combination.\",\n  \n  \"Official Results and Human Evaluation Insights\": \"Official results indicated that Toshiba systems were ranked among the top three in human evaluation for ja-en, ja-zh, and JPOzh-ja tasks. Despite a lower BLEU score, ja-zh achieved the highest human evaluation score. In JPOko-ja, a high BLEU score was achieved, but the HUMAN score was comparatively low. The system combination improved human evaluation scores for Chinese translations in scientific documents, suggesting complementary translation capabilities.\",\n  \n  \"Discussion on Evaluation Discrepancies and System Improvements\": \"Analysis of crowdsourcing results revealed discrepancies between system evaluations. Although Online A achieved a high HUMAN score, superior results of COMB over Online A were shown in pair-wise evaluation. Differences in evaluation may arise from the treatment of number expressions, such as brackets in JPOko-ja. The need to establish a combination of multi-systems for practical use, leveraging their unique characteristics, was highlighted.\"\n}\n```",
    "```json\n{\n  \"Introduction to Phrase-based Statistical Machine Translation\": \"Phrase-based Statistical Machine Translation (PB-SMT) is a data-oriented approach that has been widely used for over a decade. The Moses toolkit, developed by the Statistical Machine Translation Group at the University of Edinburgh, is a prominent open-source tool in this field. The training process in PB-SMT is crucial as it forms the core knowledge for machine translation, with word or phrase alignment being a key component that affects translation quality.\",\n  \n  \"Word and Phrase Alignment in PB-SMT\": \"Word or phrase alignment is a critical step in the training process of PB-SMT, as it establishes translation relationships among words or phrases in a sentence-aligned bi-corpus. This step is not only essential for translation quality but also one of the most time-consuming processes. The alignment process involves determining the best set of alignment links between source and target words or phrases in parallel sentences.\",\n  \n  \"Probabilistic and Associative Alignment Approaches\": \"Probabilistic approaches, such as IBM models and HMM alignment models, use the EM algorithm to align words based on statistical models. GIZA++ is a well-known implementation of these models. Associative approaches, on the other hand, rely on statistical measures like the Dice coefficient and mutual information, processing each sentence independently. These methods offer different strategies for achieving word alignment in machine translation.\",\n  \n  \"Sampling-based Multilingual Alignment and Hierarchical Sub-sentential Alignment\": \"Sampling-based multilingual alignment, exemplified by Anymalign, and hierarchical sub-sentential alignment, as seen in Cutnalign, are two associative approaches. Anymalign samples sub-corpora to obtain word occurrence distributions, while Cutnalign uses a recursive binary segmentation process for alignment. These methods aim to reduce training time while maintaining alignment quality.\",\n  \n  \"Data and Tools Used for Chinese-Japanese Translation\": \"The Chinese-Japanese JPO Patent Corpus, provided by WAT 2015, was used for training translation models. This corpus includes 1 million parallel sentences across four domains: Chemistry, Electricity, Mechanical engineering, and Physics. Tools like the Stanford Segmenter and Juman were employed for Chinese and Japanese segmentation, respectively. The data was preprocessed through tokenization, lowercase conversion, and cleaning.\",\n  \n  \"Bilingual Hierarchical Sub-sentential Alignment Method\": \"Cutnalign is a bilingual hierarchical sub-sentential alignment method that uses a recursive binary segmentation process. It involves measuring translation link strength, computing optimal joint clustering, and segmenting sentence pairs. The method evaluates translation probabilities and uses a bipartite graph for optimal alignment, as demonstrated in alignment matrices and segmentation examples.\",\n  \n  \"Improvements and Optimizations in Alignment Methods\": \"Improvements in alignment methods include multi-processing and re-implementation of core components in C to accelerate the process. Approximations in N cut computation and search space reduction in hierarchical sub-sentential alignment have led to significant speed-ups. These optimizations aim to enhance the efficiency of alignment methods without compromising translation quality.\",\n  \n  \"Experimental Setup and Evaluation Metrics\": \"Experiments were conducted using GIZA++ or MGIZA, with phrase tables extracted using the grow-diag-final-and heuristic in the Moses toolkit. The systems were evaluated using BLEU and RIBES metrics. Different timeouts and versions of Anymalign and Cutnalign were tested to assess their impact on training time and translation quality.\",\n  \n  \"Results and Analysis of Different Alignment Methods\": \"Evaluation results showed no significant differences in translation quality across different alignment methods, but training times varied. The fastest training time was achieved with a specific configuration of Moses, Anymalign, and Cutnalign, reducing the time to one fifth of that required by GIZA++ or MGIZA. Confidence intervals and p-values indicated the reliability of these results.\",\n  \n  \"Conclusion and Impact on SMT Development\": \"The study demonstrated that significant reductions in training time for SMT systems are possible without affecting translation quality. The methods explored, particularly those involving bilingual hierarchical sub-sentential alignment, offer a promising direction for accelerating SMT development. The findings suggest that efficient alignment methods can maintain steady translation quality even with reduced computation time.\"\n}\n```",
    "```json\n{\n  \"Introduction to Tree-Based NMT Systems\": \"Neural Machine Translation (NMT) has seen significant advancements, with efforts categorized into string-to-string, tree-to-string, and string-to-tree systems. Tree-based systems, which include tree-to-string and string-to-tree methods, leverage syntactic information and prior knowledge, making them a focal point in NMT research. These systems can be represented by tree-structured neural networks or through linearization, with the latter being computationally simpler and preferred for handling larger corpora.\",\n  \"Challenges in Forest-Based NMT\": \"Despite the success of tree-based NMT systems, they are limited by their reliance on the 1-best parse tree, which can lead to translation errors due to parsing inaccuracies. Forest-based methods, which use packed forests to represent multiple parse trees, offer a solution but are complex due to the lack of a fixed traversal order and the hyperedge-attachment nature of weights. This complexity hinders the development of efficient forest-based NMT systems.\",\n  \"Proposed Forest-Based NMT Approach\": \"Inspired by tree-based NMT methods, we propose a forest-based NMT approach that encodes syntactic information from a packed forest using a novel weighted linearization method. This approach allows for decoding under a simple sequence-to-sequence framework, effectively utilizing the syntactic richness of packed forests to improve translation accuracy.\",\n  \"Linearization of Packed Forests\": \"Linearizing a packed forest is challenging due to its structure as a directed acyclic graph (DAG) and the edge-based weight system. We propose a novel linearization algorithm that preserves word order and parent-child relationships, using operators to combine different parts of the forest. This method encodes syntax information effectively, enhancing NMT performance.\",\n  \"Encoding and Decoding in Forest-Based NMT\": \"The linearized packed forest serves as input to the encoder, which differs from traditional sequence-to-sequence models by incorporating both symbol and score sequences. We propose two frameworks, Score-on-Embedding (SoE) and Score-on-Attention (SoA), to integrate these sequences, allowing the NMT system to leverage syntactic information during encoding and decoding.\",\n  \"Experimental Setup and Results\": \"We evaluated our forest-based NMT systems on English-to-Chinese and English-to-Japanese tasks, using character-level BLEU scores for performance assessment. Our models, particularly the SoA framework, outperformed string-to-string and tree-based baselines, demonstrating the utility of syntactic information and packed forests in improving translation accuracy.\",\n  \"Qualitative Analysis of Translation Outputs\": \"Our qualitative analysis shows that using packed forests improves grammatical accuracy and attention in translations. For instance, phrases that were ignored in 1-best tree translations were correctly translated using packed forests, highlighting the advantage of incorporating multiple syntactic structures.\",\n  \"Comparison with Related Work\": \"Our approach stands out by efficiently encoding multiple trees using linearized packed forests, unlike previous methods that rely on single trees or latent graphs. This allows for better utilization of syntactic information, as demonstrated by our superior experimental results compared to state-of-the-art tree-based systems.\",\n  \"Conclusion and Future Work\": \"We introduced a novel NMT framework that encodes packed forests using linear-structured neural networks, offering a significant improvement over traditional NMT systems. Future work will focus on refining the score layer integration and applying our linearization method to other tasks to further enhance translation performance.\"\n}\n```",
    "```json\n{\n  \"Introduction to Metaphor Processing\": \"Metaphor enriches language, playing a significant role in communication, cognition, and decision making. Computational metaphor processing refers to modeling non-literal expressions (e.g., metaphor, metonymy, and personification) and is useful for improving many NLP tasks such as Machine Translation (MT) and Sentiment Analysis. For instance, Google Translate failed in translating 'devour' within a sentence, 'She devoured his novels,' into Chinese, illustrating the challenges in metaphor translation. Metaphor identification approaches based on word embeddings have become popular as they do not rely on hand-crafted knowledge for training.\",\n  \"Challenges in Metaphor Identification\": \"Metaphor identification approaches based on word embeddings follow a paradigm where input sentences are parsed into phrases, and the metaphoricity of the phrases is identified. However, these methods do not tackle word-level metaphor and often discard important contextual information, leading to failures in identifying metaphors when both words in a phrase are metaphorical. For example, 'This young man knows how to climb the social ladder' is a metaphorical expression, but when parsed into a verb-direct object phrase, 'climb ladder,' it appears literal.\",\n  \"Proposed Unsupervised Metaphor Processing Model\": \"We propose an unsupervised metaphor processing model that can identify and interpret linguistic metaphors at the word level. Our model is built upon word embedding methods and uses WordNet for lexical relation acquisition. It is distinguished by not constraining the source domain of metaphor and not relying on labeled data for training. Linguistic metaphors are identified by modeling the distance between the target word's literal and metaphorical senses, allowing the model to operate at the sentence level without preprocessing.\",\n  \"Technical Methodology and Framework\": \"Our metaphor processing framework is based on two hypotheses: (H1) a metaphorical word can be identified if its sense within context and its literal sense come from different domains, and (H2) literal senses of words occur more commonly in corpora than metaphoric senses. The framework involves training word embeddings, separating target words and context words, constructing a candidate word set, identifying the best fit word, and computing cosine similarity to determine metaphoricity. The model uses both input and output vectors of CBOW and Skip-gram embeddings to improve performance.\",\n  \"Evaluation and Experimental Design\": \"We evaluate our model against three strong baselines on the task of metaphor identification. Extensive experimentation conducted on a publicly available dataset shows that our model significantly outperforms the unsupervised learning baselines on both phrase and sentence evaluation. We also conducted experiments to evaluate how metaphor processing can support the task of MT, showing significant improvements in metaphoric translation.\",\n  \"Results and Performance Analysis\": \"Our model outperforms the unsupervised baselines in both sentence and phrase evaluations. For sentence-level metaphor identification, our models outperform the baseline, with SIM-CBOW I+O giving the highest F1 score of 75%. In phrase-level evaluation, SIM-SG I+O performs best, significantly outperforming the word embedding-based approach and matching the performance of a deep supervised method. The results are stable across different thresholds, and the model improves translation accuracy in MT systems.\",\n  \"Application in Machine Translation\": \"We conducted experiments to evaluate how metaphor processing can support English-Chinese machine translation. By paraphrasing metaphorically used target words with the best fit words, the performance of translation engines improved significantly. The translation improvement for metaphorical sentences was dramatic for both Google Translate and Bing Translate, with overall translation performance improved by 11% and 9%, respectively.\",\n  \"Conclusion and Future Work\": \"We proposed a framework that identifies and interprets metaphors at the word level with an unsupervised learning approach. Our model improves translation systems' accuracy and demonstrates the effectiveness of using WordNet for paraphrasing metaphors. Future work will introduce weighted CBOW and Skip-gram to learn positional information within sentences, enhancing metaphor processing in practical tasks.\"\n}\n```",
    "```json\n{\n  \"Introduction to Sentiment-to-Sentiment Translation\": \"Sentiment-to-sentiment translation involves changing the underlying sentiment of a sentence while preserving its non-emotional semantic content. It is a special style transfer task in NLP with applications like review sentiment transformation and news rewriting. The lack of parallel training data is a significant challenge, as existing methods often fail to maintain semantic content while changing sentiment. For instance, transforming 'The food is delicious' to 'What a bad movie' changes the sentiment but not the topic. This issue arises from the difficulty in separating emotional from semantic information in dense hidden vectors.\",\n  \n  \"Challenges in Sentiment-to-Sentiment Translation\": \"The primary challenge in sentiment-to-sentiment translation is the lack of parallel data, which makes it difficult to modify sentiment without losing semantic information. Existing models often fail to preserve non-emotional semantic content, as they attempt to separate emotional and semantic information in a hidden layer where all information is mixed. This results in poor content preservation, as seen in examples where the sentiment is changed but the topic is not preserved.\",\n  \n  \"Proposed Cycled Reinforcement Learning Approach\": \"To address the lack of parallel data, a cycled reinforcement learning approach is proposed, consisting of a neutralization module and an emotionalization module. The neutralization module extracts non-emotional semantic content by filtering out emotional words, while the emotionalization module adds sentiment to this content. This approach allows for training with unpaired data and significantly improves content preservation by explicitly separating sentiment from semantic content.\",\n  \n  \"Neutralization and Emotionalization Modules\": \"The neutralization module identifies non-emotional words using a Long-short Term Memory Network (LSTM) and a self-attention based sentiment classifier. It filters out emotional words to produce a neutralized sequence. The emotionalization module, using a bi-decoder framework, adds sentiment to the neutralized content. Pre-training methods are employed to give these modules initial learning ability, enhancing their performance in sentiment-to-sentiment translation.\",\n  \n  \"Cycled Reinforcement Learning Mechanism\": \"The cycled reinforcement learning mechanism involves training the neutralization and emotionalization modules to improve each other. The neutralization module extracts semantic content, and the emotionalization module reconstructs the original sentence with the source sentiment. Policy gradient is used to train the neutralization module, with rewards based on sentiment confidence and BLEU score, guiding it to better identify non-emotional words.\",\n  \n  \"Experimental Setup and Datasets\": \"Experiments are conducted on two review datasets, Yelp and Amazon, with user ratings indicating sentiment. Reviews are processed to create text-sentiment pairs, focusing on sentence-level sentiment translation. The datasets are split into training, validation, and testing sets, with a sentiment classifier filtering pairs with low confidence. Hyper-parameters are tuned based on validation performance, and modules are pre-trained before cycled training.\",\n  \n  \"Evaluation Metrics and Baselines\": \"Evaluation includes automatic and human assessments. Automatic evaluation uses sentiment accuracy and BLEU score to measure sentiment transformation and content preservation. Baselines include Cross-Alignment Auto-Encoder (CAAE) and Multi-Decoder with Adversarial Learning (MDAL). Human evaluation scores transformed text on sentiment and semantic similarity. The proposed method is compared against these baselines, showing improved performance.\",\n  \n  \"Results and Analysis\": \"The proposed method outperforms baselines in content preservation, achieving higher BLEU scores and overall performance. Automatic evaluations show significant improvements in BLEU scores, indicating better semantic preservation. Human evaluations confirm these findings, with higher semantic scores for the proposed method. The method effectively separates emotional from semantic content, enhancing translation quality.\",\n  \n  \"Error Analysis and Future Work\": \"Error analysis reveals issues with sentiment-conflicted and neutral sentences, where sentiment is not fully removed or added. These errors highlight the need for handling complex sentiment expressions and improving sentiment-aware decoders. Future work aims to explore fine-grained sentiment translation, adjusting sentiment strength in addition to reversal.\"\n}\n```",
    "```json\n{\n  \"Introduction to Domain Adaptation Challenges\": \"Deep neural networks (DNNs) excel at learning from labeled data and have achieved state of the art in a wide array of supervised NLP tasks. However, learning from unlabeled data, especially under domain shift, remains a challenge. This is common in many real-world applications where the distribution of the training and test data differs. Many state-of-the-art domain adaptation approaches leverage task-specific characteristics which do not generalize to other tasks. Other approaches only evaluate on proprietary datasets or on a single benchmark, which carries the risk of overfitting to the task.\",\n  \"Re-evaluation of Bootstrapping Algorithms\": \"We re-evaluate bootstrapping algorithms in the context of DNNs. These are general-purpose semi-supervised algorithms that treat the model as a black box and can thus be used easily with the current generation of NLP models. Many of these methods were originally developed with in-domain performance in mind, so their effectiveness in a domain adaptation setting remains unexplored. We re-evaluate three traditional bootstrapping methods, self-training, tri-training, and tri-training with disagreement for neural network-based approaches on two NLP tasks with different characteristics.\",\n  \"Self-training and Its Variants\": \"Self-training is one of the earliest and simplest bootstrapping approaches. It leverages the model's own predictions on unlabeled data to obtain additional information that can be used during training. Typically, the most confident predictions are taken at face value. Calibration is important as output probabilities in neural networks are poorly calibrated. We select the top n unlabeled examples predicted with the highest confidence after every epoch and add them to the labeled data. This variant, called throttling, outperforms the classic selection in our experiments.\",\n  \"Tri-training and Tri-training with Disagreement\": \"Tri-training is a classic method that reduces the bias of predictions on unlabeled data by utilizing the agreement of three independently trained models. An unlabeled data point is added to the training set of a model if the other two models agree on its label. Tri-training with disagreement adds a simple modification, requiring that for an unlabeled data point on which two models agree, the other model disagrees on the prediction. This method is more data-efficient and has achieved competitive results on part-of-speech tagging.\",\n  \"Proposed Multi-task Tri-training Method\": \"To reduce the time and space complexity of tri-training, we propose Multi-task Tri-training (MT-Tri). MT-Tri leverages insights from multi-task learning to share knowledge across models and accelerate training. All models collaborate on learning a joint representation, which improves convergence. The output softmax layers are model-specific and are only updated for the input of the respective model. An orthogonality constraint is introduced to ensure diversity among models.\",\n  \"Experiments and Evaluation on NLP Tasks\": \"We evaluate on two widely used unsupervised domain adaptation datasets for two tasks, a sequence labeling and a classification task. For POS tagging, we use the SANCL 2012 shared task dataset and compare to top results in both low and high-data conditions. For sentiment analysis, we evaluate on the Amazon reviews dataset. The model is an MLP with one hidden layer, and we compare against several baselines including Variational Fair Autoencoder and domain-adversarial neural networks.\",\n  \"Analysis of POS Tagging and Sentiment Analysis Results\": \"For sentiment analysis, tri-training achieves the best average results on two target domains and outperforms the state of the art on average. MT-Tri outperforms the state of the art on 3/4 domains. For POS tagging, tri-training with disagreement works best in the low-data regime, reaching an overall average accuracy of 89.70. In the high-data setup, classic tri-training is the best method, significantly outperforming the baseline in 4/5 cases.\",\n  \"Comparison with State-of-the-Art and Related Work\": \"We compare our methods to the top results published on both datasets and include several baselines. Our proposed multi-task tri-training model outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis. For POS tagging, classic tri-training is superior. We emphasize the importance of comparing neural approaches to strong baselines and reporting results across several runs.\",\n  \"Conclusions and Future Directions\": \"We re-evaluate a range of traditional general-purpose bootstrapping algorithms in the context of neural network approaches to semi-supervised learning under domain shift. Classic tri-training works the best for the two examined NLP tasks and even outperforms a recent state-of-the-art method. We propose a more efficient multi-task tri-training model, which outperforms both traditional tri-training and recent alternatives in the case of sentiment analysis. Future work could explore shared-private models for high-data setups.\"\n}\n```",
    "```json\n{\n  \"Introduction to Stance Classification\": \"Stance classification is the task of automatically identifying users' positions about a specific target from text. Traditionally, this task is approached by learning a target-specific classifier that is trained for prediction on the same target of interest. An alternative is cross-target classification, where the classifier is adapted from different but related targets, allowing the use of existing knowledge. For example, a classifier adapted from 'climate change is concern' can be used for mining project approvals, as both involve environmental impacts.\",\n  \"Challenges in Cross-Target Stance Classification\": \"Cross-target stance classification is challenging because language models may not be compatible between different targets. However, for targets related to the same domain, it might be possible to generalize through domain-specific aspects reflecting users' major concerns. For instance, both climate change and mining project discussions may involve 'reef destruction', a common environmental concern.\",\n  \"Proposed Model: CrossNet\": \"CrossNet is a novel neural model for cross-target stance classification, leveraging the self-attention mechanism. It aims to learn domain-specific aspects from a source target and apply them to a destination target. Preliminary analysis indicates that CrossNet can extract useful domain-specific information, improving classification performance in certain domains.\",\n  \"Embedding and Context Encoding Layers\": \"CrossNet's Embedding Layer uses word embeddings to represent words in a stance-bearing sentence and a target. The Context Encoding Layer employs a bi-directional Long Short-Term Memory Network (BiLSTM) to capture contextual information, using conditional encoding to model sentence-target dependencies. This involves initializing the sentence BiLSTM with the final states of the target BiLSTM.\",\n  \"Aspect Attention Layer and Domain-Specific Aspects\": \"The Aspect Attention Layer discovers domain-specific aspects crucial for stance inference. It uses self-attention to identify core sentence parts that are semantically compatible with the entire sentence. This layer captures recurring domain aspects, such as 'reef' in discussions about mining projects, which are central to understanding users' stances.\",\n  \"Prediction Layer and Model Training\": \"The Prediction Layer uses a multilayer perceptron (MLP) to predict stance labels based on domain aspect encoding. Model training involves multi-class cross-entropy loss with L2-regularization. The model is trained and validated on a source target and tested on a destination target, using ADAM optimizer and stratified 10-fold cross-validation.\",\n  \"Experimental Setup and Evaluation Metrics\": \"The experimental setup includes using pre-trained GloVe word vectors and training the model on a source target. Evaluation metrics include F1-score, with both micro-averaged and macro-averaged scores considered. The transfer ratio metric assesses the effectiveness of target adaptation by comparing cross-target and in-target performance.\",\n  \"Classification Performance and Results\": \"CrossNet outperforms baseline models in cross-target stance classification, particularly in domains like Women's Rights and American Politics. The aspect attention mechanism enhances target-level generalization, with significant improvements in cross-target tasks. The transfer ratio results suggest the importance of considering destination target-specific information.\",\n  \"Visualization of Attention and Learned Aspects\": \"Visualization of self-attention results shows that CrossNet effectively highlights domain-relevant sentence parts, such as 'feminist' and 'rights' for women's rights. The model captures domain-specific information, aiding stance inference across related targets. It also identifies sentiment-carrying words, contributing to stance prediction.\",\n  \"Conclusion and Future Work\": \"The study proposes a self-attention neural model for cross-target stance classification, achieving superior results in certain domains. Future work includes exploring effective source target selection, considering multiple source targets for knowledge transfer, and evaluating the model's multilingual generalization ability.\"\n}\n```",
    "```json\n{\n  \"Introduction to Domain-Specific Sentiment Analysis\": \"The choice of words to express an opinion depends on the domain, as users often use domain-specific words. For example, 'entertaining' and 'boring' are frequently used in the movie domain to express an opinion, but finding these words in the electronics domain is rare. Moreover, there are words that may change their polarity orientation from one domain to another. For instance, 'unpredictable' is positive in the movie domain but negative in the automobile domain. Due to these differences, a supervised algorithm trained on a labeled source domain does not generalize well on an unlabeled target domain, and the cross-domain performance degrades.\",\n  \"Challenges in Cross-Domain Sentiment Classification\": \"Generally, supervised learning algorithms have to be re-trained from scratch on every new domain using a manually annotated review corpus, which is not practical due to the numerous domains and the expensive and time-consuming nature of obtaining manually annotated data. Existing transfer learning-based domain adaptation algorithms for cross-domain classification have been useful in reducing the labeled data requirement but do not consider words that change polarity orientation across domains. Transfer of changing polarity words affects cross-domain performance negatively.\",\n  \"Significant Consistent Polarity (SCP) Words\": \"In this paper, we propose that words which are equally significant with a consistent polarity across domains represent usable information for cross-domain sentiment analysis. \u03c7\u00b2 is a popularly used and reliable statistical test to identify the significance and polarity of a word in an annotated corpus. However, for an unlabeled corpus, no such statistical technique is applicable. Therefore, identification of words which are significant with a consistent polarity across domains is a non-trivial task.\",\n  \"Proposed SCP Extraction Technique\": \"We present a novel technique based on the \u03c7\u00b2 test and cosine-similarity between context vectors of words to identify Significant Consistent Polarity (SCP) words across domains. The algorithm identifies the significance and polarity of all words individually in their respective domains. Words significant in both domains with consistent polarity orientation are used to initiate the cross-domain adaptation algorithm.\",\n  \"Ensemble-Based Adaptation Algorithm\": \"An ensemble-based adaptation algorithm is proposed where a classifier trained on SCP words in the labeled source domain acts as a seed to initiate a classifier on the target-specific features. These classifiers are then combined in a weighted ensemble to enhance cross-domain classification performance. The ensemble of classifiers trained on SCP features and target-specific features overcomes the errors of individual classifiers.\",\n  \"Comparison with Existing Methods\": \"Our results show that our approach gives a statistically significant improvement over Structured Correspondence Learning (SCL) and common unigrams in the identification of transferable words, which facilitates a more accurate sentiment classifier in the target domain. The SCP words given by our approach represent more accurate transferable information compared to SCL and common unigrams.\",\n  \"Experimental Setup and Results\": \"We show a comparison between SCP-based domain adaptation and SCL-based domain adaptation using four domains: Electronics, Kitchen, Books, and DVD. The SCP words are extracted from the training data, and the weights for the source and target classifiers are accuracies obtained on the validation dataset from the target domain. Our approach performs significantly better than SCL and common unigrams, with a strong positive correlation of 0.78 between F-score and cross-domain accuracy.\",\n  \"Conclusion and Future Work\": \"We proposed that SCP words represent transferable information for cross-domain sentiment classification. A strong positive correlation of 0.78 was observed between SCP words identified by our approach and sentiment classification accuracy in the unlabeled target domain. Our technique based on \u03c7\u00b2 test and cosine-similarity identifies SCP words more accurately than SCL and common unigrams. Future work could explore further enhancements in cross-domain sentiment classification accuracy.\"\n}\n```",
    "```json\n{\n  \"Introduction to Span-Based Neural Constituency Parsing\": \"Span-based neural constituency parsing has gained attention due to its high accuracy and simplicity. Unlike other neural parsers that model tree structures, the span-based framework uses bidirectional RNNs to model the input sequence, allowing for efficient dynamic programming algorithms like CKY. This approach decomposes the output space, making it more manageable and efficient.\",\n  \n  \"Limitations of Existing Span-Based Parsers\": \"Existing span-based parsers face a significant limitation in search efficiency. Greedy span parsers are fast but explore only a single path in a vast search space, while chart-based parsers achieve high accuracy but are too slow for longer sentences. This trade-off limits their application in tasks like end-to-end discourse parsing and integrated sentence boundary detection.\",\n  \n  \"Proposed Linear-Time Span-Based Neural Parser\": \"The proposed parser combines the strengths of greedy and chart-based approaches, achieving linear-time parsing while searching over an exponentially large space. It uses left-to-right dynamic programming in an action-synchronous style, with a practical runtime of O(nb^2) using beam search. Cube pruning further improves runtime to O(nb log b), making it effectively linear in sentence length.\",\n  \n  \"Dynamic Programming and Score Decomposition\": \"The parser decomposes the score of a tree into the sum of span scores, allowing for efficient dynamic programming. Each span's label is chosen independently, simplifying the parsing process. This decomposition is crucial for the parser's efficiency and accuracy.\",\n  \n  \"Graph-Structured Stack without Bookkeeping\": \"A novel formulation of the graph-structured stack is introduced, eliminating the need for extra bookkeeping. This approach maintains a set of predecessor states for each parsing state, allowing for efficient combination of spans without additional complexity. The time complexity remains O(n^4) with an extra O(n) for the step index.\",\n  \n  \"Action-Synchronous Beam Search and Cube Pruning\": \"The parser uses action-synchronous beam search to maintain top parsing states, reducing runtime complexity. Cube pruning is introduced to manage the beam size, further optimizing the runtime to O(nb log b). This method ensures efficient parsing even with larger beam sizes.\",\n  \n  \"Training with Structured SVM and Cross-Span Loss\": \"Training employs a structured SVM approach with a novel cross-span loss function, penalizing incorrect spans that cross gold-tree spans. Max-violation updates are used to optimize the model, ensuring it scores the gold tree higher than any other tree by a specified margin.\",\n  \n  \"Experiments and Results on Penn Treebank and Discourse Parsing\": \"Experiments on the Penn Treebank and PTB-RST discourse treebank demonstrate the parser's efficiency and accuracy. The parser achieves state-of-the-art results with a beam size of 20, showing significant speed improvements over existing methods. Discourse parsing experiments highlight the parser's ability to handle longer sequences effectively.\"\n}\n```",
    "```json\n{\n  \"Introduction to Diversity in Paraphrasing\": \"Inducing diversity in the task of paraphrasing is an important problem in NLP with applications in data augmentation and conversational agents. Previous paraphrasing approaches have mainly focused on generating semantically similar paraphrases, often resulting in structurally similar sentences. This work aims to obtain highly diverse paraphrases without compromising on quality, using a novel formulation based on monotone submodular function maximization. The method is demonstrated to be effective for data augmentation in tasks like intent classification and paraphrase recognition.\",\n  \"Monotone Submodular Function Maximization\": \"Addressing diverse paraphrasing through monotone submodular function maximization alleviates the problem of retaining fidelity with respect to the source sentence. The submodular objective offers flexibility in controlling diversity and fidelity, and a simple greedy algorithm guarantees a solution close to optimal. Submodular functions are relevant in many real-world applications, making them useful in practice. They share commonalities with convex functions, allowing for operations like mixtures and truncation.\",\n  \"Diverse Paraphraser using Submodularity (DiPS)\": \"We introduce Diverse Paraphraser using Submodularity (DiPS), which maximizes a novel submodular objective function specifically for paraphrasing. DiPS generates structurally diverse paraphrases without compromising fidelity, and its utility is demonstrated in data augmentation for tasks like intent and question classification. The method is compared against several diversity-inducing schemes, showing its effectiveness in generating high-quality paraphrases.\",\n  \"Fidelity and Diversity in Paraphrase Generation\": \"Fidelity and diversity are key characteristics in paraphrase generation. Fidelity is ensured by maximizing semantic similarity between generated and source sentences, using monotone submodular functions. Diversity is encouraged by rewarding sequences with distinct n-grams and structural variations. The balance between fidelity and diversity is achieved through a parameterized submodular objective function, allowing for high-quality paraphrase generation.\",\n  \"Methodology and Implementation\": \"The task of paraphrase generation is formulated as a sequence-to-sequence learning problem. The proposed framework combines a sentence encoder with a diversity-inducing decoder. Instead of using greedy decoding or standard beam search, a modified decoder with a submodular objective is used to obtain high-quality paraphrases. The generation process involves selecting probable subsequences based on the optimization of the submodular objective, repeated until the desired output length is achieved.\",\n  \"Experiments and Datasets\": \"The experiments are conducted on multiple datasets, categorized based on the task. The datasets are used to evaluate the proposed method's effectiveness in generating diverse paraphrases. Baseline models are compared, focusing on producing diverse responses in dialog systems. The experiments aim to answer whether DiPS can generate diverse paraphrases without compromising fidelity and if they are useful in data augmentation.\",\n  \"Intrinsic Evaluation and Results\": \"Intrinsic evaluation involves assessing fidelity using metrics like BLEU, METEOR, and TER-Plus, and diversity by calculating distinct n-grams. DiPS outperforms baseline models in fidelity metrics and achieves substantial gains in diversity without compromising fidelity. The accuracy of paraphrase recognition models is evaluated using augmented training data, showing that high-quality paraphrases yield better results. The results demonstrate the effectiveness of DiPS in generating diverse and meaningful paraphrases.\",\n  \"Data Augmentation Applications\": \"Data augmentation is evaluated in intent and question classification tasks. DiPS provides high structural variations without compromising fidelity, outperforming other augmentation schemes like SBS and DBS. The boost in accuracy scores on classification models indicates the importance of high-quality paraphrases for data augmentation. The method's effectiveness is demonstrated through experiments on standard classifier models, showcasing its utility in improving classification performance.\"\n}\n```",
    "```json\n{\n  \"Criticism of BLEU as an MT Evaluation Metric\": \"Automatic Machine Translation (MT) evaluation metrics, including BLEU, have faced criticism for various reasons. Despite the consistent correlation of higher BLEU scores with better human judgments in major MT shared tasks, there are notable failures. Callison-Burch et al. (2006) highlighted scenarios where translations with the same BLEU score differed in quality. They showed high correlations between BLEU and human scores when excluding an outlier system, but poor correlation when included. This indicates that a low BLEU score does not necessarily result in a poor human judgment score.\",\n  \n  \"Real-World Example of BLEU's Limitations\": \"A real-world example from the WAT 2015 shared task demonstrated a machine translation with high BLEU scores but poor manual evaluation. The hypothesis translation had a BLEU score of 64.03, significantly higher than the baseline's 43.29, yet human evaluators preferred the baseline. The example highlighted how minor lexical differences can cause large BLEU score disparities, despite similar translation quality.\",\n  \n  \"Technical Details of BLEU Calculation\": \"BLEU is a precision-based metric defined by n-gram precision, with a brevity penalty to emulate recall. The BLEU score is calculated using the formula: BLEU = BP \u00d7 exp(\u03a3(w_n log p_n)), where BP is the brevity penalty, and p_n is the n-gram precision. The score ranges from 0 to 1, with higher scores indicating closer alignment to reference translations.\",\n  \n  \"RIBES as an Alternative to BLEU\": \"RIBES addresses BLEU's lack of explicit reordering consideration by incorporating a rank correlation coefficient. This metric penalizes wrongly ordered phrases more effectively than BLEU. However, in the WAT 2015 evaluation, RIBES still failed to account for significant adequacy errors in translations, as seen in a case where the hypothesis translation had a higher RIBES score but was less accurate than the baseline.\",\n  \n  \"Other Automatic MT Evaluation Metrics\": \"Other metrics like METEOR and ROUGE-S have been developed to address BLEU's limitations. METEOR allows for paraphrase and stem matches, while ROUGE-S uses skip-grams. These metrics aim to capture syntactic fluency and semantic adequacy beyond surface n-grams. Despite these advancements, they do not fully resolve BLEU's issues, such as the omission of content-bearing materials and the inability to calculate recall.\",\n  \n  \"Experimental Setup for WAT 2015 Shared Task\": \"The WAT 2015 shared task involved translating Korean to Japanese patents using the JPO Patent Corpus. The system used the Moses toolkit with specific settings, including MGIZA++ for word alignment, a bi-directional lexicalized reordering model, and KenLM for language modeling. Minimum Error Rate Training (MERT) was used for tuning, and the system was evaluated against a baseline phrase-based MT system.\",\n  \n  \"Human Evaluation Methodology\": \"Human evaluations for the WAT 2015 task were conducted using the Lancers crowdsourcing platform. Evaluators compared hypothesis and baseline translations, judging which was better or if they were tied. The HUMAN score was calculated based on these judgments, ranging from -100 to +100. The evaluators were non-experts, which may have affected the precision of their judgments, especially for patent translations.\",\n  \n  \"Segment Level Meta-Evaluation and Disparities\": \"A segment-level meta-evaluation revealed disparities between BLEU/RIBES improvements and human judgments. Figures showed that high BLEU improvements did not always correlate with better human scores. Factors contributing to this disparity included minor lexical differences, crowd-sourced versus expert preferences, and metric differences not reflecting major translation inadequacies. These findings highlight the limitations of automatic metrics in capturing true translation quality.\"\n}\n```",
    "```json\n{\n  \"Introduction to Search-based Structured Prediction\": \"Search-based structured prediction models the generation of natural language structures as a search problem. It has gained significant research attention due to its competitive performance in terms of accuracy and running time. The process involves learning a stochastic policy by imitating a reference policy, which can be problematic due to ambiguities and discrepancies between training and testing phases. Techniques like ensemble and exploration during training are used to address these issues.\",\n  \"Challenges in Imitation Learning for Structured Prediction\": \"Imitation learning in search-based structured prediction faces challenges such as ambiguities in the reference policy and discrepancies between training and testing. Ambiguities arise when multiple actions lead to the optimal structure, but only one is chosen for training. Discrepancies occur when the learned policy encounters non-optimal states during testing, which were not learned during training. These challenges hinder the generalization ability and performance of the models.\",\n  \"Knowledge Distillation as a Solution\": \"Knowledge distillation is proposed as a solution to improve the generalization ability of search-based structured prediction models. It involves transferring the knowledge from a cumbersome teacher model, often an ensemble, to a smaller student model. This is achieved by optimizing a distillation loss that uses the teacher model's output distribution as a 'soft target'. The method aims to overcome the limitations of traditional training methods by focusing on both reference and exploration states.\",\n  \"Distillation from Reference and Exploration\": \"The paper proposes a method to distill knowledge from both reference and exploration states. Distillation from reference involves learning from states produced by the reference policy, while distillation from exploration involves learning from states encountered during exploration. Combining these methods enhances the model's performance by leveraging the generalization ability of the ensemble model. The workflow of this method is illustrated in Figure 1.\",\n  \"Experimental Setup and Methodology\": \"Experiments are conducted on two tasks: transition-based dependency parsing and neural machine translation. The parsing experiments use the Penn Treebank dataset, while the translation experiments use the German-to-English portion of the IWSLT 2014 dataset. The models are parameterized using LSTM-based architectures, and performance is evaluated using metrics like LAS and BLEU scores. The experimental setup includes varying hyperparameters and using multiple seeded runs to ensure robust results.\",\n  \"Results and Performance Analysis\": \"The results demonstrate the effectiveness of the proposed knowledge distillation method. In parsing experiments, an improvement of 1.32 in LAS is achieved, while in translation experiments, a 2.65 improvement in BLEU score is observed. The distilled models outperform strong baselines and other greedy structured prediction models. The analysis shows that the ensemble model's output distribution is more informative, leading to better generalization on problematic states.\",\n  \"Ensemble Behavior on Problematic States\": \"The ensemble model shows improved performance on 'problematic' states, which are either ambiguous or non-optimal. The dynamic oracle is used to evaluate the accuracy of parsers' decisions in these states. The ensemble model significantly outperforms the baseline, indicating its ability to generalize well on problematic states. The distillation model, learned from exploration, also performs better than both the baseline and ensemble.\",\n  \"Impact of Distillation Loss and Learning Stability\": \"The impact of distillation loss on learning stability is studied by varying the parameter \u03b1. Models configured with larger \u03b1, which focus more on distillation loss, generally perform better. The distillation method also leads to more stable learning, as evidenced by smaller standard deviations in performance scores across differently seeded runs. This stability is attributed to the less sharp minimizers presented by the distillation loss.\"\n}\n```",
    "```json\n{\n  \"Introduction to Adversarial Attacks in Machine Learning\": \"Adversarial attacks involve making changes to a machine learning model's input to achieve an adversarial goal, such as misclassification or mistranslation. These attacks highlight the vulnerabilities and brittleness of models to samples outside the training distribution, posing security concerns for safety-critical applications like self-driving cars. Initially defined for computer vision systems, adversarial attacks in NLP face challenges due to the discrete nature of text data, where even minor changes are perceptible to humans.\",\n  \"Challenges of Adversarial Attacks in NLP\": \"In NLP, adversarial attacks are more problematic due to the discrete nature of text data, where even a single word or character change is noticeable. Unlike continuous spaces like images, there is no canonical distance metric for text, making it difficult to evaluate semantic similarity between sentences. This raises the question of what constitutes an adversarial perturbation in NLP, especially when meaning preservation is crucial.\",\n  \"Proposed Criterion for Adversarial Examples in NLP\": \"The proposed criterion for adversarial examples in NLP focuses on meaning-preserving perturbations on the source side and meaning-destroying on the target side. This approach contrasts with previous work that did not explicitly evaluate meaning preservation. The method allows for evaluating adversarial attacks using human evaluation or approximations without human intervention.\",\n  \"Framework for Evaluating Adversarial Attacks\": \"A framework for evaluating adversarial attacks on seq2seq models is presented, focusing on the trade-off between source and target similarity. The evaluation includes measures of target similarity and source similarity scores, with a recommendation to report both when presenting results. The framework can be extended to other tasks by adapting the similarity metrics.\",\n  \"Gradient-Based Adversarial Attacks\": \"Gradient-based attacks involve replacing words in a sentence to maximize an adversarial loss function. The approach uses a differentiable function to find optimal word substitutions, with constraints like kNN and CharSwap to preserve meaning. The method is computationally efficient and conducive to adversarial training.\",\n  \"Experiments and Evaluation Metrics\": \"Experiments are conducted on the IWSLT2016 dataset using LSTM and Transformer models. The evaluation framework compares various adversarial attacks, with chrF showing higher correlation with human judgment than BLEU and METEOR. The results highlight the effectiveness of meaning-preserving attacks and the robustness of different model architectures.\",\n  \"Adversarial Training with Meaning-Preserving Attacks\": \"Adversarial training augments training data with adversarial examples, improving robustness without impacting test performance. The study shows that meaning-preserving attacks, like CharSwap, enhance robustness more effectively than unconstrained attacks. The results suggest that adversarial training with constraints is beneficial for model performance.\",\n  \"Conclusion and Future Directions\": \"The paper emphasizes the importance of meaning-preserving adversarial perturbations in NLP models. A general evaluation framework is proposed, and various automatic metrics are compared as proxies for human judgment. The study confirms that naive attacks do not preserve meaning, and adversarial training is useful in this context. Future work should focus on evaluating meaning conservation more consistently.\"\n}\n```",
    "```json\n{\n  \"Introduction to Discourse Phenomena in Translation\": \"Handling discourse phenomena is crucial in translation to ensure coherence and resolve ambiguities. Traditional machine translation systems often ignore these phenomena, translating sentences in isolation. Earlier research focused on specific phenomena like pronoun translation and discourse connectives. With the advent of neural machine translation (NMT), there is potential to incorporate extended context directly into the models, which has shown performance improvements. However, understanding which discourse phenomena are effectively handled by NMT systems remains a challenge.\",\n  \n  \"Context-Aware Neural Machine Translation\": \"Our work aims to enhance understanding of discourse phenomena modeling in NMT by constructing a simple discourse-aware model. This model, based on the Transformer architecture, encodes source and context sentences independently, using a single attention layer to produce a context-aware representation. This approach is more accurate than simply concatenating input sentences, as it ensures contextual information is captured through the attention layer. The model shows improvements over a discourse-agnostic baseline, particularly in pronoun translation.\",\n  \n  \"Transformer Architecture and Context Integration\": \"The Transformer architecture, proposed by Vaswani et al., is a state-of-the-art model for NMT, known for its parallelizability and training efficiency. It uses stacked self-attention and fully connected layers. Our context-aware model modifies the Transformer's encoder to incorporate context information, sharing parameters between source and context encoders. This integration is crucial for effectively handling discourse phenomena in translation tasks.\",\n  \n  \"Experiments and Data Setting\": \"We use the OpenSubtitles2018 corpus for English and Russian, selecting 2 million training instances and subsets for development and testing. Sentences are encoded using byte-pair encoding with vocabularies of about 32,000 tokens. The experiments aim to verify genuine improvements from context integration, using BLEU scores to evaluate performance.\",\n  \n  \"Results and Analysis of Contextual Information\": \"The context-aware model shows a 0.7 BLEU improvement over the discourse-agnostic model when using the previous sentence as context. This suggests that discourse phenomena, rather than topic effects, drive the improvement. The model's reliance on context is confirmed by performance drops when real context is replaced with random sentences. Analysis reveals that the model attends to context for translating ambiguous pronouns, indicating implicit learning of coreference phenomena.\",\n  \n  \"Analysis of Pronoun Translation\": \"The model heavily relies on contextual history for translating pronouns like 'it', 'yours', 'you', and 'I', which are ambiguous out-of-context in Russian. BLEU scores show improvements in test sets containing co-referential pronouns, especially when antecedents are nouns. This suggests that the model effectively uses context to resolve pronoun ambiguities, improving translation quality.\",\n  \n  \"Latent Anaphora Resolution\": \"The context-aware model's attention mechanism can be interpreted as performing latent anaphora resolution. Experiments show that the model's attention weights align well with coreference systems, indicating that it captures anaphora without explicit feature engineering. Human evaluation confirms the model's effectiveness in resolving anaphora, outperforming simple heuristics.\",\n  \n  \"Related Work and Comparison\": \"Previous work on anaphora in statistical machine translation relied on external coreference systems, with mixed results. Our context-aware NMT model implicitly learns coreference phenomena without feature engineering, contrasting with other models that do not show increased attention between pronouns and antecedents. This highlights the effectiveness of our approach in capturing discourse phenomena.\",\n  \n  \"Conclusions and Future Work\": \"We introduced a context-aware NMT system based on the Transformer architecture, outperforming baselines in handling ambiguous pronouns and inducing anaphora relations. Future work could explore specialized features in the attention model to further improve anaphora handling and investigate other discourse phenomena like elliptical constructions and discourse relations.\"\n}\n```",
    "```json\n{\n  \"Introduction to Natural Language Inference (NLI)\": \"In this paper, we focus on the task of Natural Language Inference (NLI), which is known as a significant yet challenging task for natural language understanding. In this task, we are given two sentences which are respectively called premise and hypothesis. The goal is to determine whether the logical relationship between them is entailment, neutral, or contradiction. Recently, performance on NLI has been significantly boosted since the release of some high-quality large-scale benchmark datasets such as SNLI and MultiNLI.\",\n  \"Role of Discourse Markers in NLI\": \"Most state-of-the-art works focus on the interaction architectures between the premise and the hypothesis, while they rarely concerned the discourse relations of the sentences, which is a core issue in natural language understanding. People usually use some certain set of words to express the discourse relation between two sentences. These words, such as 'but' or 'and', are denoted as discourse markers. These discourse markers have deep connections with the intrinsic relations of two sentences and intuitively correspond to the intent of NLI, such as 'but' to 'contradiction', 'so' to 'entailment', etc.\",\n  \"Discourse Marker Augmented Network\": \"In this paper, we propose a Discourse Marker Augmented Network for natural language inference, where we transfer the knowledge from the existing supervised task: Discourse Marker Prediction (DMP), to an integrated NLI model. We first propose a sentence encoder model that learns the representations of the sentences from the DMP task and then inject the encoder to the NLI network. The contributions of this paper can be summarized as follows: Unlike previous studies, we solve the task of the natural language inference via transferring knowledge from another supervised task.\",\n  \"Sentence Encoder Model and Training\": \"Following, we use BookCorpus as our training data for discourse marker prediction, which is a dataset of text from unpublished novels, and it is large enough to avoid bias towards any particular domain or application. We first use Glove to transform sentences into vectors word by word and subsequently input them to a bi-directional LSTM. We apply max pooling on the concatenation of the hidden states from both directions to extract the features of the whole sequences of vectors. Finally, we project the combined representation to a vector of label size and use the softmax function to normalize the probability distribution.\",\n  \"Reinforcement Learning for Label Confidence\": \"Because our NLI datasets are manually annotated, each example from the datasets might get several different labels from the annotators although they will finally come to a consensus and also provide a certain label. In consideration of that different confidence level of the final labels should be discriminated, we employ reinforcement learning with a reward defined by the uniformity extent of the original labels to train the model. We propose a new objective function that combines both the log probabilities of the ground-truth label and a reward defined by the property of the datasets for the reinforcement learning.\",\n  \"Experimental Setup and Datasets\": \"We use the dataset from BookCorpus to pre-train our sentence encoder model. We preprocessed and collected discourse markers from BookCorpus. SNLI is a collection of more than 570k human-annotated sentence pairs labeled for entailment, contradiction, and semantic independence. MultiNLI has 433k sentence pairs and is in the same format as SNLI, but it includes a more diverse range of text, as well as an auxiliary test set for cross-genre transfer evaluation.\",\n  \"Results and Performance Analysis\": \"We present the ensemble results on both datasets. We build an ensemble model which consists of 10 single models with the same architecture but initialized with different parameters. The performance of our model achieves 89.6% on SNLI, 80.3% on matched MultiNLI, and 79.4% on mismatched MultiNLI, which are all state-of-the-art results.\",\n  \"Ablation and Semantic Analysis\": \"We conduct an ablation experiment on SNLI development dataset to evaluate the individual contribution of each component of our model. The result is obviously not satisfactory when only using sentence embedding from discourse markers to predict the answer. We observe that the performance drops significantly when the sentence encoder model is removed, indicating that discourse markers have deep connections with the logical relations between two sentences they link. We also show the performance on the three relation labels when the model is pre-trained on different discourse markers sets.\"\n}\n```",
    "```json\n{\n  \"Introduction to Simultaneous Interpretation Challenges\": \"Simultaneous Interpretation (SI) is an inherently difficult task that carries significant cognitive and attentional burdens. The role of the simultaneous interpreter is to accurately render the source speech in a given target language in a timely and precise manner. Interpreters employ a range of strategies, including generalization and summarization, to convey the source message as efficiently and reliably as possible. Unfortunately, the interpreter is pitched against the limits of human memory and stamina, and after only minutes of interpreting, the number of errors made by an interpreter begins to increase exponentially. We examine the task of estimating simultaneous interpreter performance: automatically predicting when interpreters are interpreting smoothly and when they are struggling.\",\n  \"Quality Estimation in Machine Translation and Its Application to Interpretation\": \"Blatz et al. (2004) first proposed the problem of measuring the quality of MT output as a prediction task, given that existing metrics such as BLEU rely on the availability of reference translations to evaluate MT output quality, which aren't always available. As such, QE has since received widespread attention in the MT community and since 2012 has been included as a task in the Workshop on Statistical Machine Translation, using approaches ranging from linear classifiers to neural models. QuEst++ is a well-known QE pipeline that supports word-level, sentence-level, and document-level QE. Its effectiveness and flexibility make it an attractive candidate for our proposed task.\",\n  \"Adapting Quality Estimation for Interpreter Output\": \"To adapt QE to interpreter output, we augment the baseline feature set with four additional types of features that may indicate a struggling interpreter. An increased number of hesitations or incomplete words in interpreter output might indicate that an interpreter is struggling to produce accurate output. In our particular case, both corpora we use in experiments are annotated for pauses and partial renditions of words. We include features such as the ratio of pauses/hesitations/incomplete words, ratio of non-specific words, ratio of 'quasi-'cognates, and ratio of number of words.\",\n  \"Interpretation-Specific Features for Quality Estimation\": \"Interpreters often compress output by replacing or omitting common nouns to avoid specific terminology, either to prevent redundancy or to ease cognitive load. For example, 'The chairman explained the proposal to the delegates' might be rendered in a target language as 'he explained it to them.' To capture this, we include a feature that checks for words from a pre-determined seed list of pronouns and demonstrative adjectives. Transliterated words in interpreted speech could represent facilitated translation by language proximity, or an attempt to produce an approximation of a word that the interpreter did not know.\",\n  \"Evaluation Metrics for Interpreter Performance\": \"Novice interpreters are assessed for accuracy on the number of omissions, additions, and the inaccurate renditions of lexical items and longer phrases, but recovery of content and correct terminology are highly valued. While no large corpus exists that has been manually annotated with these measures, they align with the phenomena that MT evaluation tries to solve. One important design decision is which evaluation metric to target in our QE system. We use METEOR, a more sophisticated MT evaluation metric that considers paraphrases and content-function word distinctions, and thus should be better equipped to deal with the disparity between MT and SI.\",\n  \"Data and Corpora for Interpretation Quality Experiments\": \"For our EN-JA language data, we train the pipeline on combined data from seven TED Talks taken from the NAIST TED SI corpus. This corpus provides human transcribed SI output from three interpreters of low, intermediate, and high levels of proficiency. Our EN-FR and EN-IT data are drawn from the EPTIC corpus, which provides source and interpreter transcripts for speeches from the European Parliament. The EN-FR and EN-IT datasets contain 739 and 731 utterances respectively.\",\n  \"Results and Analysis of Quality Estimation Models\": \"To evaluate the quality of our QE system, we use the Pearson's r correlation between the predicted and true METEOR for each language pair. Our proposed, interpreter-focused model out-performs in all language settings with notable gains in particular for EN-JA(A-Rank), achieving its highest accuracy on the EN-FR dataset. Over all datasets, the gain of the proposed model is statistically significant at p < 0.05 by the pairwise bootstrap. Our model can better capture the issues in examples because it has many interpretation-specific qualities such as pauses, compression, and omission.\",\n  \"Conclusion and Future Directions in Interpreter Quality Estimation\": \"We introduce a novel and effective application of QE to evaluate interpreter output, which could be immediately applied to allow CAI systems to selectively offer assistance to struggling interpreters. This work uses METEOR to evaluate interpreter output, but creation of fine-grained measures to evaluate various aspects of interpreter performance is an interesting avenue for future work.\"\n}\n```",
    "```json\n{\n  \"Introduction to Zero-Shot Learning in Text Classification\": \"Zero-shot learning (ZSL) for text classification aims to classify documents of classes which are absent from the learning stage. This approach is inspired by the human ability to learn new concepts by transferring knowledge from known to unknown domains using high-level descriptions and semantic representations. The framework is expected to exploit supportive semantic knowledge, such as class descriptions, relations among classes, and external domain knowledge, to infer the features of unseen classes using patterns learned from seen classes. Three main types of semantic knowledge have been employed: semantic attributes, concept ontology, and semantic word embeddings.\",\n  \n  \"Challenges and Limitations of Existing Approaches\": \"Many traditional classification models struggle in dynamic environments where new classes emerge post-learning. Existing zero-shot learning methods often rely on semantic correspondences between training and testing classes, which limits their applicability in strict zero-shot scenarios. Some methods use datasets with similar classes in training and testing, which does not truly test zero-shot capabilities. Additionally, few studies have explored the combination of different types of semantic knowledge in zero-shot text classification.\",\n  \n  \"Proposed Two-Phase Framework for Zero-Shot Text Classification\": \"The paper proposes a novel two-phase framework for zero-shot text classification, which includes coarse-grained and fine-grained classification. The framework does not require semantic correspondence between training and inference classes, allowing for clearly different seen and unseen classes. It utilizes convolutional neural networks and integrates four kinds of semantic knowledge: word embeddings, class descriptions, class hierarchy, and a general knowledge graph (ConceptNet). The first phase determines if a document belongs to seen or unseen classes, while the second phase specifies the class.\",\n  \n  \"Data Augmentation through Topic Translation\": \"A novel data augmentation technique called topic translation is introduced to enhance the framework's ability to detect documents from unseen classes. This process involves translating a document from its original seen class to a new unseen class using analogy. For example, words in a document are translated to corresponding words in the context of a target class by solving analogy questions using word embeddings. This technique helps create augmented documents for unseen classes, which are used as additional negative examples in the training phase.\",\n  \n  \"Feature Augmentation Using Semantic Knowledge\": \"Feature augmentation is performed by using integrated semantic knowledge to transfer knowledge from seen to unseen classes. The relationship vector shows the relatedness between words and classes based on a general knowledge graph, ConceptNet. This vector is constructed by considering connections between words and class-related nodes within a certain number of hops. The augmented features are used in the zero-shot classifier to provide additional information, enhancing the framework's ability to generalize zero-shot reasoning.\",\n  \n  \"Experimental Setup and Datasets\": \"The experiments were conducted using two textual datasets: the DBpedia ontology dataset and the 20newsgroups dataset. The DBpedia dataset includes 14 non-overlapping classes with data from Wikipedia, while the 20newsgroups dataset contains 20 topics. Different rates of unseen classes (50% and 25%) were tested, and the results were averaged over ten random selections of seen and unseen class groups. The framework's classifiers were implemented using TensorLayer and TensorFlow, with GloVe vectors for word embeddings.\",\n  \n  \"Evaluation Metrics and Baseline Comparisons\": \"The framework's performance was compared with several baselines, including DOC, a state-of-the-art open-world text classification approach, and other models like RNN AutoEncoder and CNN+FC. Phase 1 was evaluated using binary classification accuracy, while Phase 2 and the overall framework were assessed using multi-class classification accuracy. The proposed framework demonstrated competitive accuracy on unseen classes while maintaining accuracy on seen classes, achieving the highest overall accuracy compared to baselines.\",\n  \n  \"Results and Discussion on Framework Performance\": \"The evaluation of Phase 1 showed that the proposed framework outperformed DOC in detecting documents from seen classes and improved accuracy on unseen classes with augmented data. Phase 2's zero-shot classifier achieved the highest accuracy when using a combination of word embeddings, class embeddings, and augmented features. The integration of semantic knowledge effectively transferred knowledge from seen to unseen classes. The overall framework achieved the highest accuracy in all settings, demonstrating its effectiveness in zero-shot text classification.\",\n  \n  \"Related Work in Zero-Shot Learning and Data Augmentation\": \"Previous works in zero-shot text classification often relied on semantic relationships between classes and documents via embeddings. However, the proposed framework leverages multiple types of semantic knowledge, including word embeddings and knowledge graphs, for data and feature augmentation. Data augmentation in NLP is less common than in computer vision, but recent works have explored its use in tasks like machine translation and text classification. The proposed technique translates documents to substitute unavailable labeled data for unseen classes.\",\n  \n  \"Conclusion and Future Directions\": \"The proposed CNN-based two-phase framework, with data and feature augmentation, effectively addresses zero-shot text classification. The experiments demonstrated that topic translation improved detection of unseen classes, while feature augmentation enabled knowledge transfer. The framework achieved the highest overall accuracy compared to baselines. Future work includes extending the framework to multi-label classification with larger datasets and exploring the use of semantic units defined by linguists in zero-shot scenarios.\"\n}\n```",
    "```json\n{\n  \"Introduction and Motivation\": \"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters. In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks. Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic damage. We consider the basic task of classifying each incoming tweet during a crisis event into one of the predefined classes of interest in real-time. However, the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not feasible in our setting. We need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.\",\n  \n  \"Proposed Model and Framework\": \"In this paper, we extend recent methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework. The basic task-solving network is put together with two other networks - one for semi-supervised learning and the other for domain adaptation. The semi-supervised component learns internal representations by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances. The domain adaptation is achieved by training the feature extractor in adversary with respect to a domain discriminator. The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.\",\n  \n  \"Model Components and Architecture\": \"The input to the network is a tweet containing words that come from a finite vocabulary defined from the training set. The first layer of the network maps each of these words into a distributed representation by looking up a shared embedding matrix. The output of the look-up layer is a matrix, which is passed through a number of convolution and pooling layers to learn higher-level feature representations. The network splits into three branches each of which serves a different purpose and contributes a separate loss to the overall loss of the model. The first component is a supervised classification loss, the second is a graph-based semi-supervised loss, and the third is an adversary loss to induce domain invariance in the learned features.\",\n  \n  \"Semi-supervised Learning Approach\": \"The semi-supervised branch induces structural similarity between training instances in the source and target events. We adopt a graph-based semi-supervised deep learning framework, which shows impressive gains over existing methods. In this framework, a 'similarity' graph encodes relations between training instances, which is then used by the network to learn internal representations. The semi-supervised branch takes the shared representation as input and learns internal representations by predicting a node in the graph context of the input tweet. The graph is constructed using a k-nearest neighbor-based approach, and the number of nearest neighbors was set to 10.\",\n  \n  \"Domain Adaptation with Adversarial Training\": \"The network described so far can learn abstract features that are discriminative for the classification task. However, our goal is also to make these learned features invariant across domains or events. We achieve this by domain adversarial training of neural networks. We put a domain discriminator that takes the shared internal representation as input and tries to discriminate between the domains of the input. The domain discriminator is defined by a sigmoid function, and we use the negative log-probability as the discrimination loss. In adversarial training, we seek parameters such that the updates of the shared parameters for the discriminator work adversarially to the rest of the network, and vice versa.\",\n  \n  \"Experimental Setup and Datasets\": \"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake and the 2013 Queensland floods. These datasets are comprised of millions of tweets collected through the Twitter streaming API using event-specific keywords/hashtags. We employed paid workers from a crowdsourcing platform to obtain labeled examples for our task. The annotation consists of two classes: relevant and non-relevant. We split the labeled data into 60% as training, 30% as test, and 10% as development. Data preprocessing was performed by following the same steps used to train the word2vec model.\",\n  \n  \"Results and Analysis\": \"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for both datasets. The graph-based semi-supervised approach outperforms the two baselines, showing 4% to 13% absolute improvements in terms of F1 scores. The results with domain adversarial training show improvements across both events, with 1.8% to 4.1% absolute gains in F1. When combining semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.\",\n  \n  \"Related Work and Comparisons\": \"Two lines of research are directly related to our work: semi-supervised learning and domain adaptation. Several models have been proposed for semi-supervised learning, including self-training and co-training approaches. In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances, and edge weights represent the similarity between them. In the literature, proposed approaches for domain adaptation include supervised, semi-supervised, and unsupervised methods. Adversarial training of neural networks has shown a big impact recently, especially in areas such as computer vision.\",\n  \n  \"Conclusions and Future Work\": \"In this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events. Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines. We have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.\"\n}\n```",
    "```json\n{\n  \"Introduction to Human-Computer Conversation\": \"Human-computer conversation is a critical and challenging task in AI and NLP. There have been two major streams of research in this direction, namely task-oriented dialog and general-purpose dialog (i.e., chit-chat). Task-oriented dialog aims to help people complete specific tasks such as buying tickets or shopping, while general-purpose dialog attempts to produce natural and meaningful conversations with people regarding a wide range of topics in open domains. In recent years, the latter has attracted much attention in both academia and industry as a way to explore the possibility of developing a general-purpose AI system in language (e.g., chatbots). A widely adopted approach to general-purpose dialog is learning a generative conversational model from large-scale social conversation data.\",\n  \n  \"Challenges in General Purpose Dialog\": \"General-purpose dialog is intrinsically different from machine translation. In machine translation, every sentence and its translation are semantically equivalent, with a 1-to-1 relationship. However, in general-purpose dialog, a general response could correspond to a large variety of input utterances. Previous Seq2Seq models, which treat all the utterance-response pairs uniformly, will inevitably favor such general responses with high frequency. Although these responses are safe for replying to different utterances, they are boring and trivial since they carry little information and may quickly lead to an end of the conversation.\",\n  \n  \"Existing Solutions and Their Limitations\": \"There have been a few efforts attempting to address this issue in the literature. Li et al. (2016a) proposed to use the Maximum Mutual Information (MMI) as the objective to penalize general responses. Xing et al. (2017) pre-defined a set of topics from an external corpus to guide the generation of the Seq2Seq model. However, it is difficult to ensure that the topics learned from the external corpus are consistent with that in the conversation corpus, leading to the introduction of additional noises. Latent responding factors were introduced to model multiple responding mechanisms, but these are usually difficult in interpretation and hard to decide the number of latent factors.\",\n  \n  \"Proposed Specificity Controlled Seq2Seq Model\": \"We propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity. The key idea is inspired by our observation of everyday conversation between humans. We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent. This variable interacts with the usage representation of words through a Gaussian Kernel layer, guiding the Seq2Seq model to generate responses at different specificity levels. We refer to our model as the Specificity Controlled Seq2Seq model (SC-Seq2Seq).\",\n  \n  \"Model Architecture and Mechanisms\": \"The overall architecture of SC-Seq2Seq is depicted in Figure 2. The encoder maps the input utterance into a compact vector that captures its essential topics using a bi-directional GRU. The decoder generates a response given the hidden representations of the input utterance under some specificity level denoted by the control variable s. The probability of generating any target word is defined by a mixture of semantic-based and specificity-based generation probabilities. A Gaussian Kernel layer is introduced to define the specificity-based generation probability, interacting with the usage representations of words.\",\n  \n  \"Distant Supervision Techniques\": \"We employ distant supervision to train our SC-Seq2Seq model since the specificity control variable is unknown in the raw data. We describe two ways to acquire distant labels for the specificity control variable, namely Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF). By using normalized values, we restrict the specificity control variable to be within a pre-defined continuous value range with each end having a very clear meaning on the specificity.\",\n  \n  \"Experimental Setup and Evaluation\": \"We conduct experiments on the public Short Text Conversation (STC) dataset. We compare our proposed SC-Seq2Seq model against several state-of-the-art baselines. For evaluation, we employ both automatic and human evaluations, including distinct-1 & distinct-2, BLEU, Average & Extrema, and human evaluation. Our model is trained on a Tesla K80 GPU card, and we report results on the test dataset. The SC-Seq2Seq model trained with NIWF can achieve our purpose, generating responses from general to specific as measured by the distinct metrics.\",\n  \n  \"Analysis and Case Studies\": \"We conduct some case studies to better understand how different models perform. The responses generated by the four baselines are often quite general and short. SC-Seq2Seq NIWF with large control variable values can generate very long and specific responses. When we decrease the control variable value, the generated responses become more general and shorter. We also analyze the usage representations of words introduced in our model, finding that the two distributions of usage and semantic embeddings are significantly different.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Parsing and QA-SRL\": \"Learning semantic parsers to predict the predicate-argument structures of a sentence is a long-standing, open challenge. Such systems are typically trained from datasets that are difficult to gather, but recent research has explored training non-experts to provide this style of semantic supervision. In this paper, we show for the first time that it is possible to go even further by crowdsourcing a large-scale dataset that can be used to train high-quality parsers at modest cost. We adopt the Question-Answer-driven Semantic Role Labeling (QA-SRL) annotation scheme, which is appealing because it is intuitive to non-experts and has been used for end tasks such as Open IE.\",\n  \n  \"Crowdsourcing Approach for QA-SRL Annotation\": \"Our first contribution is a new, scalable approach for crowdsourcing QA-SRL. We introduce a streamlined web interface (including an autosuggest mechanism and automatic quality control to boost recall) and use a validation stage to ensure high precision. With this approach, we produce QA-SRL Bank 2.0, a dataset with 133,479 verbs from 64,018 sentences across 3 domains, totaling 265,140 question-answer pairs, in just 9 days. Our analysis shows that the data has high precision with good recall, although it does not cover every possible question.\",\n  \n  \"Data Annotation and Quality Control\": \"A QA-SRL annotation consists of a set of question-answer pairs for each verbal predicate in a sentence, where each answer is a set of contiguous spans from the sentence. We introduce a crowdsourcing pipeline to collect annotations rapidly, cheaply, and at large scale. Our pipeline consists of a generation and validation step. In the generation step, a sentence with one of its verbs marked is shown to a single worker, who must write QA-SRL questions for the verb and highlight their answers in the sentence. The questions are passed to the validation step, where n workers answer each question or mark it as invalid.\",\n  \n  \"Model Architecture and Span Detection\": \"Our proposed parsers construct tuples in a three-step pipeline: verbal predicates are identified, unlabeled span detection selects a set of spans as arguments for a given verb, and question generation predicts a question for each span. We describe two models for unlabeled span detection: a sequence-tagging model with BIO encoding, and a span-based model which assigns a probability to every possible span. The span-based model significantly improves over the BIO model in both precision and recall.\",\n  \n  \"Question Generation Techniques\": \"We introduce two question generation models. Given a span representation, our models generate questions by picking a word for each question slot. The local model predicts the words for each slot independently, while the sequence model uses the machinery of an RNN to share information between slots. The sequential model's exact match accuracy is significantly higher, while word-level accuracy is roughly comparable.\",\n  \n  \"Data Expansion and Bootstrapping\": \"Since our trained parser can produce full QA-SRL annotations, its predictions can be validated by the same process as in our original annotation pipeline, allowing us to focus annotation efforts towards filling potential data gaps. By detecting spans at a low probability cutoff, we over-generate QA pairs for already-annotated sentences. The expansion step increased the number of valid questions in the train and dev partitions by 20%.\",\n  \n  \"Evaluation Metrics and Results\": \"We evaluate span detection using a modified notion of precision and recall. We count predicted spans as correct if they match any of the labeled spans in the dataset. Results show that the span-based model significantly improves over the BIO model in both precision and recall. For question generation, the sequential model's exact match accuracy is significantly higher. Final human evaluation shows that the sequence-based question generation models are much more accurate than the local model.\",\n  \n  \"Conclusion and Future Work\": \"In this paper, we demonstrated that QA-SRL can be scaled to large datasets, enabling a new methodology for labeling and producing predicate-argument structures at a large scale. We presented a new, scalable approach for crowdsourcing QA-SRL, which allowed us to collect QA-SRL Bank 2.0, a new dataset covering over 250,000 question-answer pairs from over 64,000 sentences, in just 9 days. We demonstrated the utility of this data by training the first parser which is able to produce high-quality QA-SRL structures.\"\n}\n```",
    "```json\n{\n  \"Introduction to Constituency Parsing and Its Importance\": \"Devising fast and accurate constituency parsing algorithms is an important, long-standing problem in natural language processing. Parsing has been useful for incorporating linguistic prior in several related tasks, such as relation extraction, paraphrase detection, and more recently, natural language inference and machine translation. Neural network-based approaches relying on dense input representations have recently achieved competitive results for constituency parsing.\",\n  \"Challenges in Transition-Based and Chart-Based Parsing Models\": \"Transition-based models decompose the structured prediction problem into a sequence of local decisions. This enables fast greedy decoding but also leads to compounding errors because the model is never exposed to its own mistakes during training. Solutions to this problem usually complexify the training procedure by using structured training through beamsearch and dynamic oracles. On the other hand, chart-based models can incorporate structured loss functions during training and benefit from exact inference via the CYK algorithm but suffer from higher computational cost during decoding.\",\n  \"Proposed Fully-Parallel Model Using Syntactic Distances\": \"In this paper, we propose a novel, fully-parallel model for constituency parsing, based on the concept of 'syntactic distance'. The model is trained to reproduce the ordering between split points induced by the ground-truth distances by means of a margin rank loss. Crucially, our model works in parallel: the estimated distance for each split point is produced independently from the others, which allows for an easy parallelization in modern parallel computing architectures for deep learning, such as GPUs.\",\n  \"Algorithm for Mapping Syntactic Distances to Parse Trees\": \"We propose two algorithms, one to convert a parse tree into a compact representation based on distances between consecutive words, and another to map the inferred representation back to a complete parse tree. The representation will later be used for supervised training. Mapping from syntactic distances to a tree can be efficiently done in O(n log n), which makes the decoding computationally attractive.\",\n  \"Model Architecture and Learning Syntactic Distances\": \"We use neural networks to estimate the vector of syntactic distances for a given sentence. The syntactic distances and constituent labels are predicted using a neural network architecture that stacks recurrent (LSTM) and convolutional layers. The model architecture includes bidirectional LSTM layers and convolutional layers to capture long-term syntactical relations between words. The final loss for the overall model is just the sum of individual losses for syntactic distances and constituent labels.\",\n  \"Experimental Evaluation on PTB and CTB Datasets\": \"We evaluate our model on the standard Wall Street Journal part of the Penn Treebank (PTB) dataset, and the Chinese Treebank (CTB) dataset. Our model achieves good performance for single-model constituency parsing trained without external data, setting a new state-of-the-art for single-model parsing achieving 86.5 F1 on the CTB test set. The detailed statistics are shown in the tables provided in the document.\",\n  \"Ablation Study and Parsing Speed Comparison\": \"We perform an ablation study by removing components from a network trained with the best set of hyperparameters, and re-train the ablated version from scratch. The prediction of syntactic distances can be batched in modern GPU architectures. The distance to tree conversion is a O(n log n) divide-and-conquer algorithm. Our model achieves a parsing speed of 111.1 sentences per second, outperforming other state-of-the-art neural parsers.\"\n}\n```",
    "```json\n{\n  \"Introduction to Bilingual Lexicon Induction\": \"Over the past few years, new methods for bilingual lexicon induction have been proposed that are applicable to low-resource language pairs, for which very little sentence-aligned parallel data is available. Parallel data can be very expensive to create, so methods that require less of it or that can utilize more readily available data are desirable. One prevalent strategy involves creating multilingual word embeddings, where each language's vocabulary is embedded in the same latent space; however, many of these methods still require a strong cross-lingual signal in the form of a large seed dictionary. More recent work has focused on reducing that constraint.\",\n  \"Challenges in Low-Resource Language Pairs\": \"These recent advances in unsupervised bilingual lexicon induction show promise for use in low-resource contexts. However, none of them make use of linguistic features of the languages themselves. This is in contrast to work that predates many of these embedding-based methods that leveraged linguistic features such as edit distance and orthographic similarity. The addition of linguistic features led to increased performance in these earlier models, especially for related languages, yet these features have not been applied to more modern methods.\",\n  \"Modern Embedding-Based Approaches\": \"In this work, we extend the modern embedding-based approach of Artetxe et al. (2017) with orthographic information in order to leverage similarities between related languages for increased accuracy in bilingual lexicon induction. Artetxe et al. (2017) use a very small, automatically-generated seed lexicon of identical numerals as the initialization in an iterative self-learning framework to learn a linear mapping between monolingual embedding spaces. This formulation requires a seed dictionary, and to reduce the need for a large seed dictionary, they propose an iterative, self-learning framework.\",\n  \"Orthographic Extension of Word Embeddings\": \"This method augments the embeddings for all words in both languages before using them in the self-learning framework of Artetxe et al. (2017). To do this, we append to each word's embedding a vector of length equal to the size of the union of the two languages' alphabets. Each position in this vector corresponds to a single letter, and its value is set to the count of that letter within the spelling of the word. After appending, the resulting augmented vector is normalized to have magnitude 1.\",\n  \"Orthographic Similarity Adjustment\": \"This method modifies the similarity score for each word pair during the dictionary induction phase of the self-learning framework of Artetxe et al. (2017), which uses the dot product of two words' embeddings to quantify similarity. We modify this similarity score by adding a measure of orthographic similarity, which is a function of the normalized string edit distance of the two words. The normalized edit distance is defined as the Levenshtein distance divided by the length of the longer word.\",\n  \"Experimental Setup and Datasets\": \"We use the datasets used by Artetxe et al. (2017), consisting of three language pairs: English-Italian, English-German, and English-Finnish. Each dataset includes monolingual word embeddings for both languages and a bilingual dictionary, separated into a training and test set. We do not use the training set as the input dictionary to the system, instead using an automatically-generated dictionary consisting only of numeral identity translations.\",\n  \"Results and Discussion\": \"For our experiments with orthographic extension of word embeddings, each embedding was extended by the size of the union of the alphabets of both languages. The heuristic identified approximately 2 million word pairs for each language pair out of a possible 40 billion, resulting in significant computation savings. Table 1 compares our methods against the system of Artetxe et al. (2017), using scaling factors selected based on development data results. The extension method is capable of learning a mapping between source and target letters, which could partially explain its improved performance.\",\n  \"Conclusion and Future Work\": \"In this work, we presented two techniques for improving embedding-based bilingual lexicon induction for related languages using orthographic information and no parallel data, allowing their use with low-resource language pairs. These methods increased accuracy in our experiments, with both the combined and embedding extension methods providing significant gains over the baseline system. In the future, we want to extend this work to related languages with different alphabets and to extend other unsupervised bilingual lexicon induction systems.\"\n}\n```",
    "```json\n{\n  \"Introduction to Extractive Summarization Challenges\": \"Extractive summarization involves extracting salient word sequences from a source document to form a summary. Existing supervised approaches often use human abstracts to create annotations for extraction units, but this method faces challenges. A significant issue is that most source words are tagged as 0s, with only a few as 1s, due to the concise nature of human abstracts. Additionally, not all labels are accurate, as some source words may be paraphrases or related to words in the abstracts, leading to mislabeling. These challenges make leveraging human abstracts for extractive summarization supervision difficult.\",\n  \"Neural Abstractive vs. Extractive Summarization\": \"Neural abstractive summarization allows systems to copy words from source texts or generate new words, addressing some issues of extractive summarization. However, it faces challenges in ensuring summaries remain faithful to the original, with factual inaccuracies being a major obstacle. This study focuses on neural extractive summarization, which aims to produce summaries that are semantically close to human abstracts while sharing common words.\",\n  \"Cloze-Style Question Generation for Training\": \"The study introduces a new training paradigm for extractive summarization by converting human abstracts into Cloze-style comprehension questions. These questions encourage system summaries to preserve salient source content relevant to the questions, making them effective document surrogates for predicting correct answers. An attention mechanism is used to locate summary segments relevant to a given question, allowing the summary to answer multiple questions.\",\n  \"Reinforcement Learning for Extractive Summarization\": \"Reinforcement learning is applied to explore the space of possible extractive summaries. The approach rewards summaries that preserve salient source content useful for answering questions and share common words with abstracts. The system is optimized to generate summaries that can answer key questions about the document, using a question-focused reward mechanism.\",\n  \"Question-Focused Reward Mechanism\": \"The question-focused reward mechanism rewards summaries that can serve as document surrogates to answer important questions. A set of question-answer pairs is generated from human abstracts, and the system uses a Bi-LSTM to encode questions and summaries. An attention mechanism locates relevant summary parts, and the QA reward is defined as the log-likelihood of correctly predicting all answers. This mechanism ensures summaries are semantically close to human abstracts.\",\n  \"Experimental Setup and Hyperparameters\": \"The experimental setup involves training, validation, and testing on the CNN dataset, with specific hyperparameters tuned on the validation set. The Bi-LSTM hidden state size is 256, and the single-direction LSTM encoder size is 30. A dropout rate of 0.2 is used, with a minibatch size of 256. Early stopping is applied, and the Adam optimizer is used with an initial learning rate of 1e-4. The source vocabulary contains 150K words, and 100-dimensional word embeddings are initialized by GloVe.\",\n  \"Comparison with State-of-the-Art Systems\": \"The study compares its methods with state-of-the-art systems, including both extractive and abstractive approaches. Two variants, 'EntityQ' and 'KeywordQ', are tested, with both outperforming baseline systems on a benchmark dataset. The R-1, R-2, and R-L scores show that the proposed methods yield superior results, with R-2 F-scores of 11.5 and 11.6, respectively.\",\n  \"Future Directions and Usability Studies\": \"Future work includes investigating approaches that automatically group selected summary segments into clusters, capturing unique document aspects. Inspired by recent work, the study also plans to conduct usability studies to test how well summary highlights help users quickly answer key questions. This will provide an alternative strategy for evaluating the proposed method against extractive and abstractive baselines.\"\n}\n```",
    "```json\n{\n  \"Introduction to Chinese Spelling Check Challenges\": \"Chinese spelling checkers are challenging to develop due to the absence of word delimiters and the vast number of Chinese characters, each with unique contexts. The complexity is further increased by the phonological and visual similarities among multi-character words, which are semantically distinct. An effective spelling checker must identify errors and suggest corrections accurately.\",\n  \"Overview of SIGHAN 2015 Bake-off\": \"The SIGHAN 2015 Bake-off for Chinese Spelling Check provided an evaluation platform for developing automatic Chinese spelling checkers. It followed previous bake-offs, focusing on essays by learners of Chinese as a Foreign Language. The task involved identifying spelling errors in passages and suggesting corrections, building on the experiences of SIGHAN 2013 and 2014.\",\n  \"Task Description and Objectives\": \"The task aimed to evaluate the capability of Chinese spelling checkers by providing passages with or without spelling errors. The checkers needed to identify incorrect characters and suggest corrections, with specific output formats for error-free and erroneous passages. The task focused solely on spelling error correction, excluding grammatical errors or word ordering issues.\",\n  \"Data Preparation and Corpus Details\": \"The learner corpus was sourced from the Test of Chinese as a Foreign Language essays, annotated by native speakers. It included a training set with 970 essays and 3,143 errors, a dryrun set for familiarization, and a test set with 1,100 passages. Participants could use additional resources for error detection and correction, focusing exclusively on spelling errors.\",\n  \"Evaluation Metrics and Performance Criteria\": \"Performance was evaluated at detection and correction levels, using a confusion matrix to measure true positives, false positives, true negatives, and false negatives. Metrics included precision, recall, and F1 score, with a focus on reducing false positives. The CAS team achieved the highest F1 score for error detection and correction.\",\n  \"Results and Performance Analysis\": \"The CAS team excelled in both detection and correction levels, achieving high F1 scores. Their system used an HMM-based approach and a two-stage filter process. NCTU&NTUT also performed well, using a word vector/conditional random field-based detector. No team excelled in all metrics, but CAS and NCTU&NTUT showed strong overall performance.\",\n  \"Approaches and Methodologies of Participant Teams\": \"The CAS team proposed a unified framework using HMM-based sentence segmentation and a two-stage filter for candidate re-ranking. NCTU&NTUT used a word vector/conditional random field-based detector to guide language model rescoring. These approaches aimed to detect and correct potential spelling errors efficiently.\",\n  \"Conclusions and Future Directions\": \"The SIGHAN 2015 Bake-off advanced the development of Chinese spelling checkers, with all submissions contributing valuable insights. The data sets and evaluation tools are publicly available to support future research. Future work will focus on Chinese grammatical error correction and developing new resources for computer-aided language learning.\"\n}\n```",
    "```json\n{\n  \"Introduction to the ACL Anthology\": \"The ACL Anthology is a service provided by the Association for Computational Linguistics (ACL) that offers open access to the proceedings of all ACL-sponsored conferences and journal articles. It also hosts third-party computational linguistics literature from sister organizations. The Anthology supports both text and faceted search of indexed papers and author-specific pages, and it can incorporate third-party metadata and services. As of the document's writing, it hosts over 43,000 papers and serves over 4,500 daily requests. The code for the Anthology is available under a Creative Commons license, and its maintenance is handled by volunteers coordinated by the Anthology editor.\",\n  \n  \"Current State and Technical Infrastructure\": \"The ACL Anthology was first proposed in 2001 and launched in 2002, with a second version developed in 2012. It provides access to papers in PDF format and metadata in multiple formats. The technology stack includes Ruby on Rails, Solr, PostgreSQL, Nginx/Jetty, and Debian GNU-Linux. The Anthology is hosted at Saarland University, with plans for future hosting under the ACL umbrella. The process of importing and indexing new papers is semi-automated, integrated with ACL conference management software, and involves scripts for processing metadata and PDFs. Table 1 in the document details the tech stack used for the Anthology.\",\n  \n  \"Community-Driven Development and Challenges\": \"The Anthology operates as a community project, with volunteers recruited at ACL conferences. New features are often developed by researchers using the Anthology as a resource. The unpredictable nature of volunteer involvement poses challenges in preserving operational knowledge. The Anthology editor plays a crucial role in ensuring continuity, but this creates a potential single point of failure. Efforts are underway to improve documentation to ease the transition for new volunteers. The ACL Executive is considering commercial support for publishing, but the document advocates for maintaining a volunteer-driven development model.\",\n  \n  \"Future Proofing and Technological Upgrades\": \"The Anthology's code, documentation, and feature requests are hosted on GitHub. Documentation has been improved to reduce the learning curve for new volunteers and facilitate server migration. The Anthology's core code has remained stable, but dependency deprecation poses challenges. Efforts are being made to replace deprecated libraries and use Docker containers to manage dependencies and improve security. Docker provides a virtualized environment that simplifies server setup and enhances security. Development versions of the Anthology Docker image are available, allowing for easy mirroring and experimentation.\",\n  \n  \"Security and Stability Concerns\": \"Security is a major concern for the Anthology, as older dependencies increase exposure to unpatched bugs. Although the Anthology does not store personal data, a compromised server could pose risks to maintainers and the community. Docker containers are being used to abstract server setup and tackle dependency issues, allowing hosting institutions to apply their own security policies. This approach enhances the robustness of the Anthology through a larger network of mirrors, reducing the risk of service downtime and unauthorized applications.\",\n  \n  \"Potential Extensions: Anonymous Pre-prints and Reviewer Matching\": \"The Anthology faces challenges with the increasing popularity of pre-prints, which are not anonymous and can introduce bias. The ACL executive has adopted guidelines for double-blinded submissions, and anonymous pre-prints are being considered. Two implementation options are discussed: collaboration with an existing pre-print service or hosting pre-prints directly within the Anthology. Reviewer matching is another challenge, with the potential to leverage Anthology data to support conference organizers. The Anthology could provide data on reviewer interests and expertise, aiding in the assignment of peer reviewers and improving response rates.\",\n  \n  \"Conclusion and Future Directions\": \"The ACL Anthology is a vital resource for the NLP community, supported by software engineering and maintenance efforts. Improved documentation and server setup are expected to ease volunteer recruitment and training. The document invites community members to experiment with Anthology images and contribute to its development. Current needs include system administration, software development, database management, and Docker integration. The document extends an open invitation for community involvement to enhance the Anthology's capabilities and address open challenges.\"\n}\n```",
    "```json\n{\n  \"Introduction to Conversational Interfaces and Use Cases\": \"Due to recent advances in speech recognition and language understanding, conversational interfaces such as Alexa, Cortana, and Siri are becoming more common. They currently have two large use cases. First, a user can use them to complete a specific task, such as playing music. Second, a user can use them to ask questions where the questions are answered by querying a knowledge graph or database back-end. Typically, under a common interface, there exist two disparate systems that can handle each use case. The system underlying the first use case is known as a spoken language understanding (SLU) system.\",\n  \"Executable Semantic Parsing as a Unified Framework\": \"In this work, we propose executable semantic parsing as a common framework for both use cases by framing SLU as executable semantic parsing that unifies the two use cases. For Q&A, the input utterances are parsed into logical forms that represent the machine-readable representation of the question, while in SLU, they represent the machine-readable representation of the user intent and slots. One added advantage of using parsing for SLU is the ability to handle more complex linguistic phenomena such as coordinated intents that traditional SLU systems struggle to handle.\",\n  \"Challenges in Semantic Parsing and Solutions\": \"A major issue with semantic parsing is the availability of the annotated logical forms to train the parsers, which are expensive to obtain. A solution is to rely more on distant supervisions such as by using question-answer pairs. Alternatively, it is possible to exploit annotated logical forms from a different domain or related data set. In this paper, we focus on the scenario where data sets for several domains exist but only very little data for a new one is available and apply transfer learning techniques to it.\",\n  \"Neural Transition-based Parser with Stack-LSTMs\": \"In this paper, we consider the neural executable semantic parser of Cheng et al. (2017), which follows the transition-based parsing paradigm. Its transition system differs from traditional systems as the words are not consumed from the buffer because in executable semantic parsing, there are no strict alignments between words in the input and nodes in the tree. The neural architecture encodes the buffer using a Bi-LSTM and the stack as a Stack-LSTM, a recurrent network that allows for push and pop operations.\",\n  \"Handling Data Sparsity: Delexicalization vs. Copy Mechanism\": \"One common issue in parsing is how to deal with rare or unknown words, which is usually addressed by either delexicalization or by implementing a copy mechanism. We show clear differences in the outcome of these and other techniques when applied to data sets of varying sizes. Regardless of the data set, the copy mechanism generally outperforms delexicalization. For NLmaps and all SLU domains, using a copy mechanism results in an average accuracy improvement of 16% over the baseline.\",\n  \"Transfer Learning Techniques for Low-Resource Domains\": \"We consider the scenario where large training corpora are available for some domains and we want to bootstrap a parser for a new domain where little training data is available. We investigate the use of two transfer learning approaches: pre-training and multi-task learning. For SLU, there is no clear winner between pre-training and MTL. Nevertheless, they always outperform the baseline, demonstrating the importance of transfer learning, especially for smaller domains.\",\n  \"Experiments and Results on Q&A and SLU Data Sets\": \"We first run experiments on single-task semantic parsing to observe the differences among the three different data sources. Specifically, we explore the impact of an attention mechanism on the performance as well as the comparison between delexicalization and a copy mechanism for dealing with data sparsity. The metric used to evaluate parsers is the exact match accuracy, defined as the ratio of sentences correctly parsed. Our baseline outperforms previous results, achieving a score of 0.846. Our best F1 results are also obtained when adding the copy mechanism, achieving a score of 0.874.\",\n  \"Conclusions and Future Work\": \"We framed SLU as an executable semantic parsing task, which addresses a limitation of current commercial SLU systems. By applying our framework to different data sets, we demonstrate that the framework is effective for Q&A as well as for SLU. We explored a typical scenario where it is necessary to learn a semantic parser for a new domain with little data, but other high-resource domains are available. Preliminary experiment results on transfer learning across domains belonging to heterogeneous data sets suggest future work in this area.\"\n}\n```",
    "```json\n{\n  \"Introduction to Cross-Language Plagiarism Detection\": \"Plagiarism is a significant issue, especially in higher education, and while monolingual plagiarism detection is well-researched, cross-language plagiarism remains challenging. This type of plagiarism involves translating a text from one language to another, making detection difficult as the suspicious document is in a different language from its source. The paper explores how distributed representations of words, such as word embeddings, can aid in developing new cross-lingual similarity measures for plagiarism detection.\",\n  \"Contributions and Innovations in Plagiarism Detection\": \"The paper's main contributions include enhancing state-of-the-art methods with word embeddings, introducing syntax weighting in sentence representations for improved textual similarity detection, and combining methods to verify their complementarity. These innovations resulted in an overall F1 score of 89.15% for English-French similarity detection at the chunk level, significantly outperforming individual methods.\",\n  \"Evaluation Conditions and Dataset Characteristics\": \"The study uses a dataset introduced by Ferrero et al. (2016), designed for rigorous cross-language textual similarity evaluation. It is multilingual, containing French, English, and Spanish texts, and offers cross-language alignment at document, sentence, and chunk levels. The dataset includes both human and machine-translated texts, varying percentages of named entities, and a mix of obfuscated and clear documents, written by authors of different expertise levels.\",\n  \"Overview of State-of-the-Art Methods\": \"The paper reviews several state-of-the-art methods for cross-language textual similarity detection. These include Cross-Language Character N-Gram (CL-CnG), Cross-Language Conceptual Thesaurus-based Similarity (CL-CTS), Cross-Language Alignment-based Similarity Analysis (CL-ASA), Cross-Language Explicit Semantic Analysis (CL-ESA), and Translation + Monolingual Analysis (T+MA). Each method has unique approaches, such as using character n-grams, conceptual thesauri, alignment analysis, semantic vectors, and translation for monolingual comparison.\",\n  \"Proposed Methods Using Word Embeddings\": \"The paper proposes using word embeddings for plagiarism detection, leveraging their ability to capture synonymy and morphological closeness. The MultiVec toolkit is used to compute continuous text representations, employing models like word2vec and bilingual distributed representations. The embeddings are trained on the News Commentary parallel corpus, using the CBOW model with specific parameters for vector size, window size, and sampling.\",\n  \"Improving Textual Similarity with Syntax Weighting\": \"Two new methods are introduced: CL-CTS-WE, which replaces lexical resources with word embeddings, and CL-WES, which uses cosine similarity on sentence embeddings. CL-WESS further improves CL-WES by incorporating syntax weighting, using part-of-speech tags to weight word contributions in sentence representations. This syntactic enhancement significantly boosts performance, making CL-WESS the best single method evaluated.\",\n  \"Combining Multiple Methods for Enhanced Detection\": \"The paper explores combining methods to enhance detection performance through weighted and decision tree fusion. Weighted fusion assigns weights to each method's similarity score, while decision tree fusion uses the C4.5 algorithm to optimize method combinations. The decision tree fusion outperforms other methods, achieving an F1 score of 89.15% at the chunk level, demonstrating the complementarity and effectiveness of the combined approaches.\",\n  \"Results and Discussion on Method Performance\": \"The use of word embeddings improves method performance, with CL-CTS-WE showing gains over traditional methods. CL-WESS, with syntax weighting, significantly outperforms other methods, achieving the best results on the corpus. Decision tree fusion further enhances performance, achieving an F1 score of 89.15% at the chunk level and 88.50% at the sentence level, confirming the complementarity of the proposed methods and the utility of embeddings in cross-language similarity detection.\"\n}\n```",
    "```json\n{\n  \"Introduction to Story Understanding and Mental State Reasoning\": \"Understanding a story requires reasoning about the causal links between the events in the story and the mental states of the characters, even when those relationships are not explicitly stated. This reasoning is remarkably hard for both statistical and neural machine readers, despite being trivial for humans. Most powerful language models have been designed to effectively learn local fluency patterns, generally lacking the ability to abstract away from surface patterns in text to model more complex implied dynamics, such as intuiting characters' mental states or predicting their plausible next actions.\",\n  \n  \"Annotation Formalism and Dataset Properties\": \"In this paper, we construct a new annotation formalism to densely label commonsense short stories in terms of the mental states of the characters. The dataset offers three unique properties: it provides a fully-specified chain of motivations and emotional reactions for each story character, includes state changes for entities even when they are not mentioned directly in a sentence, and encompasses both formal labels from multiple theories of psychology as well as open text descriptions of motivations and emotions. Our corpus spans across 15k stories, amounting to 300k low-level annotations for around 150k character-line pairs.\",\n  \n  \"Theories of Motivation and Emotion\": \"We use two popular theories of motivation: the 'hierarchy of needs' of Maslow and the 'basic motives' of Reiss to compile 5 coarse-grained and 19 fine-grained motivation categories. Maslow's 'hierarchy of needs' are comprised of five categories, ranging from physiological needs to spiritual growth. Reiss proposes 19 more fine-grained categories that provide a more informative range of motivations. Among several theories of emotion, we work with the 'wheel of emotions' of Plutchik, using the eight basic emotional dimensions.\",\n  \n  \"Annotation Framework and Pipeline\": \"We introduce a new annotation framework that pipelines a set of smaller isolated tasks. The components and workflow of the full annotation pipeline include entity resolution, action resolution, affect resolution, motivation annotation, and emotional reaction annotation. All annotations were collected using crowdsourced workers from Amazon Mechanical Turk. The pipeline is designed to handle the complexity of annotating full chains of mental state effects for story characters.\",\n  \n  \"Dataset Statistics and Insights\": \"Our dataset includes a total of 300k low-level annotations for motivation and emotion across 15,000 stories. It covers over 150,000 character-line pairs, with 56k character-line pairs having an annotated motivation and 105k having an annotated change in emotion. The tasks corresponding to the theory category assignments are the hardest and most expensive in the pipeline. We obtain theory category labels only for a third of our annotated stories, which we assign to the development and test sets.\",\n  \n  \"Experimental Setup and Baseline Models\": \"The experimental setup involves training state classification models, annotation classification models, and explanation generation models. The state classification tasks involve categorizing the psychological states of story characters for each of the label sets collected for the dev and test splits of our dataset. Baseline models include TF-IDF, GloVe, LSTM, REN, and NPN, each with specific architectures for encoding sentence and character representations.\",\n  \n  \"Experimental Results and Analysis\": \"Results on the test set for categorizing Maslow, Reiss, and Plutchik states show that all models outperform the random baseline. The performance boost from adding entity-specific contextual information indicates that models learn to condition on a character's previous experience to classify its mental state. Pretraining encoder parameters using free response annotations offers a clear performance boost for all models on all three prediction tasks.\",\n  \n  \"Related Work and Conclusion\": \"Incorporating emotion theories into NLP tasks has been explored in previous projects. Our work provides a new dataset for tracking emotional reactions and motivations of characters in stories. We present a large scale dataset as a resource for training and evaluating mental state tracking of characters in short commonsense stories. Importantly, we show that modeling character-specific context and pretraining on free-response data can boost labeling performance.\"\n}\n```",
    "```json\n{\n  \"Introduction to User Profiling in Social Media\": \"The growth of online social networks provides the opportunity to analyze user text in a broader context, including social networks, spatio-temporal information, and personal attributes. Previous research has shown that language use is influenced by user attributes like location, gender, impact, and age. User profiling is important for sociolinguistic studies and applications like recommender systems and text regression tasks. Our hypothesis is that language use in social media can indicate a user's occupational class, with different occupations showing distinct language patterns.\",\n  \n  \"Hypothesis on Language Use and Occupational Class\": \"Social status, reflected through a person's occupation, influences language use. Our hypothesis is that language use in social media can be indicative of a user's occupational class. For example, executives may write more about business, while those in manufacturing might focus on personal interests. We explore this hypothesis by predicting a user's occupational class based on Twitter attributes and content, which has applications in sociological studies and recruitment.\",\n  \n  \"Data Collection and Standard Occupational Classification\": \"To study user occupation, we adopt the Standard Occupational Classification (SOC), a UK system for classifying occupations hierarchically. We created a data set of Twitter users mapped to their occupation using self-disclosed job titles from SOC unit groups. After filtering, we obtained 5,191 users from 55 minor groups across nine major SOC groups. The data set includes 10,796,836 tweets and is openly available. This classification helps in mapping Twitter users to occupations for our study.\",\n  \n  \"Feature Extraction for Occupational Class Prediction\": \"Features for occupational class prediction are divided into user-level and textual features. User-level features include general user information and tweet statistics, while textual features are derived from aggregated tweets. We use SVD word embeddings and NPMI clusters to represent words and topics. Spectral clustering is applied to create word clusters, which are useful for model analysis. Neural embeddings are also used to capture syntactic information, providing a better representation of words.\",\n  \n  \"Gaussian Processes for Classification\": \"Gaussian Processes (GPs) provide a Bayesian non-parametric framework for classification, defining a prior on functions. GPs use a kernel to model covariance in response values. For classification, GPs squash the latent function output through a logistic function. We use Expectation Propagation for inference, which approximates the non-Gaussian joint posterior with a Gaussian one. GPs offer advantages like not using a validation set and providing interpretability through Automatic Relevance Determination.\",\n  \n  \"Experimental Results and Predictive Accuracy\": \"We compare classification methods on held-out data using different feature sets. User-level features are not useful for predicting job class, highlighting the task's difficulty. Textual features improve performance, with clusters outperforming embeddings. GPs usually outperform SVMs, offering interpretability advantages. Our data set allows studying finer occupation class granularities, with prediction performance reaching 33.9% accuracy for sub-major groups and 29.2% for minor groups.\",\n  \n  \"Qualitative Analysis of Language Use Across Occupations\": \"Word clusters used in the GP classifier allow qualitative analysis of language use across occupations. Topics like 'Corporate', 'Software Engineering', and 'Health' are informative. Cumulative Distribution Functions show topic prevalence across classes, indicating divergence in language use. Higher skilled jobs discuss 'Politics' more, while 'Arts' separates class 5. Informal topics like 'Elongated Words' are more common in lower skilled jobs. Jensen-Shannon divergence quantifies topic distribution differences, confirming language divergence between occupational classes.\",\n  \n  \"Related Work and Sociolinguistic Context\": \"Occupational class prediction has been studied in psychology and economics, with studies linking social status to language use. Previous work includes relation extraction for user profiles and classification of users on platforms like Sina Weibo. Our study differs by focusing on Twitter, using a large-scale data set and a model allowing qualitative interpretation. This work builds on sociolinguistic studies that highlight social status as a factor in language use.\",\n  \n  \"Conclusions and Implications for Future Research\": \"This study presents a large-scale analysis of language use on social media for inferring occupational class. We introduced a labeled data set from Twitter and used a GP framework for classification, achieving good predictive accuracy. The study shows that occupation influences text use, with derived topics capturing occupation-specific interests and class-based behaviors. While reflecting the Twitter population, the study's conclusions suggest potential for future applications in user profiling and sociolinguistic research.\"\n}\n```",
    "```json\n{\n  \"Introduction to High-Bandwidth Supervision\": \"The standard protocol for obtaining a labeled dataset involves a human annotator providing a label for each example, which typically yields only one bit of information per example. This work explores how to extract more information per example by using natural language explanations as supervision. Annotators provide explanations for each label, which are parsed into logical forms representing labeling functions. These functions map examples to labels, creating a large, weakly-supervised training set for classifier training. This approach allows for more efficient use of annotator effort and can significantly speed up the training process.\",\n  \n  \"The BabbleLabble Framework\": \"The BabbleLabble framework transforms natural language explanations and unlabeled data into a noisily-labeled training set. It consists of three components: a semantic parser, a filter bank, and a label aggregator. The semantic parser converts explanations into logical forms, the filter bank removes incorrect logical forms, and the label aggregator combines labels into a single probabilistic label per example. This framework allows for the training of a discriminative model using the resulting labeled examples, enhancing the efficiency of the labeling process.\",\n  \n  \"Semantic Parsing and Logical Forms\": \"Semantic parsing of natural language into logical forms is a challenging problem. In this work, a simple rule-based semantic parser is used, which suffices due to its ability to filter out incorrect logical forms and tolerate noise. The parser uses a set of grammar rules to generate logical forms from explanations, allowing for coverage over many potentially useful labeling functions. This approach is domain-independent and can be applied across various tasks without task-specific training.\",\n  \n  \"Filter Bank and Label Aggregation\": \"The filter bank in the BabbleLabble framework discards incorrect logical forms without requiring additional labels. It uses semantic and pragmatic filters to ensure consistency with examples and to remove redundant or correlated logical forms. The label aggregator then combines the remaining labels into a single probabilistic label per example using data programming, which models the relationship between true labels and labeling functions as a factor graph. This process enhances the accuracy and reliability of the generated labels.\",\n  \n  \"Experimental Setup and Results\": \"The BabbleLabble framework was evaluated on three relation extraction tasks: Spouse, Disease, and Protein. The framework demonstrated a significant improvement in supervision efficiency, achieving comparable F1 scores with far fewer user inputs than traditional supervision. The experiments showed that BabbleLabble can effectively utilize unlabeled data, providing a high-bandwidth supervision signal that outperforms traditional methods in domains with abundant unlabeled data.\",\n  \n  \"Utility of Incorrect Parses\": \"The filter bank in the BabbleLabble framework successfully removes over 95% of incorrect logical forms, resulting in a final set that is 86% correct on average. The average difference in end-task accuracy between correct and incorrect parses is less than 2.5%. This indicates that even incorrect parses can provide valuable signals, and the framework's filters are effective in maintaining high accuracy.\",\n  \n  \"Using Logical Forms as Functions or Features\": \"Logical forms derived from natural language explanations can be used as functions for weak supervision or as features in a classifier. The BabbleLabble framework uses them as functions, which allows for the creation of a larger training set via data programming. This approach outperforms using logical forms as features, as it can leverage unlabeled data to improve performance. The data programming method exposes the classifier to additional patterns not directly mentioned in the explanations, enhancing its predictive capabilities.\"\n}\n```",
    "```json\n{\n  \"Introduction and Context of Reuse and Plagiarism in NLP\": \"The ease of copy & paste has significantly contributed to the proliferation of documents, and with the availability of archives, we can now study the extent of reuse and plagiarism in the field of Natural Language Processing (NLP). This research follows previous studies, such as those initiated during the ACL's 50th anniversary, which highlighted the importance of checking the authenticity and novelty of submitted texts. Automatic plagiarism detection applications have become essential in identifying potential sources of submitted work, aiding human judges in decision-making. Our study specifically examines reuse and plagiarism in papers published at the Language Resources and Evaluation conference (LREC) from 1998 to 2014.\",\n  \n  \"Objectives and Methodology of the Study\": \"The primary objective of this study is to explore the entry-level detection of reuse and plagiarism, focusing on whether there is a meaningful difference between using verbatim raw strings and the results of linguistic parsing. The study is positioned as an extrinsic detection effort, aiming to find near-matches between texts. A secondary objective is to present and analyze practices within the NLP field. The methodology involves comparing word sequences to detect copy & paste operations, using a focus index to optimize comparisons, and applying a similarity overlapping score with the Jaccard distance.\",\n  \n  \"Corpus and Data Preprocessing Techniques\": \"The NLP4NLP corpus, collected at IMMI-CNRS and LIMSI-CNRS, contains 65,003 documents from various conferences and journals, spanning 50 years from 1965 to 2015. Preprocessing involved transforming metadata from different formats into a common BibTeX format under UTF8 and extracting text from PDFs using PDFBox and Tesseract OCR. Quality filters were applied to ensure content was at least 900 characters and had a low ratio of out-of-vocabulary words. The corpus includes documents in English and French, with German and Russian texts being ignored due to their low numbers.\",\n  \n  \"Definitions and Terminology Clarification\": \"To avoid misunderstandings, four key terms are defined: 'self-reuse' refers to a copy & paste with proper citation by the same authors, 'self-plagiarism' is similar but without citation, 'reuse' involves citing a source with different authors, and 'plagiarism' is copying without citation from different authors. The study examines both backward and forward directions of copy & paste operations within the NLP4NLP collection.\",\n  \n  \"Algorithm and Technical Implementation\": \"The algorithm involves building sliding windows of lemmas for each document and recording them in a focus index. A similarity score is computed using the Jaccard distance, and pairs of documents are filtered based on a threshold. The implementation considers variations such as hyphenation, case, and spelling differences, and applies linguistic parsing to normalize text. Parameters like window size and threshold were tuned based on PAN contest articles and manual examination of results.\",\n  \n  \"Results and Analysis of Self-Reuse and Self-Plagiarism\": \"The study found that self-reuse and self-plagiarism are common, with 12,493 documents (18% of the total) exhibiting these practices. In 61% of cases, authors did not quote the source paper. The most used sources are large conferences like ISCA and IEEE-ICASSP, and the flow of papers often moves from conferences to journals. The similarity scores range from 4% to 97%, with about 4,500 pairs having a score of 10% or higher.\",\n  \n  \"Findings on Reuse and Plagiarism\": \"Reuse and plagiarism are less common, with only 261 papers reusing fragments from other authors, and 60% of these cases not quoting the source. Manual checking revealed that many cases involved shared authorship or collaboration, reducing the number of true plagiarism cases to just three. The similarity scores for these cases range from 4% to 42%.\",\n  \n  \"Discussion on Ethical Implications and Further Developments\": \"The study highlights that self-reuse is more prevalent than reusing others' content, with source papers quoted in only 39% of cases. Plagiarism raises legal issues, but most cases comply with the right to quote. Self-reuse and self-plagiarism often result from authors' writing styles and the practice of submitting similar papers to different venues. Future developments include addressing 'rogeting' and improving the identification of altered text.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multimodal Social Media and Name Tagging Challenges\": \"Social platforms like Snapchat, Twitter, Instagram, and Pinterest have become integral to our lives, facilitating easier communication. These platforms are increasingly multimodal, combining images, videos, audios, and texts for better expressiveness. As social media posts become more multimodal, understanding the textual components becomes challenging, often requiring visual context for comprehension. Name tagging, crucial for language understanding, is particularly challenging in this context due to short texts, linguistic variations, and colloquial language. This work extends state-of-the-art models to incorporate visual information using an attention mechanism, addressing these challenges.\",\n  \"Proposed Model: Visual Attention and Modulation Gate\": \"The proposed model leverages visual context for name tagging in text by using an attention-based model to extract relevant visual features from images. A modulation gate combines textual features from a Bidirectional Long Short Term Memory (BLSTM) with visual features before feeding them into a Conditional Random Fields (CRF) layer for tag prediction. This architecture modulates word-level multimodal features, enhancing the model's ability to perform name tagging in multimodal social media.\",\n  \"BLSTM-CRF Sequence Labeling and Character-level Representation\": \"Name tagging is modeled as a sequence labeling problem using a Bidirectional LSTM (BLSTM) to capture long-range dependencies. Character-level representation is generated using another BLSTM, combining prefix, suffix, and spelling information. The final word representation is a concatenation of word embedding and character-level representation. Conditional Random Fields (CRFs) are used to learn label constraints and predict the best label sequence.\",\n  \"Visual Feature Representation and Attention Mechanism\": \"Visual features are obtained using Convolutional Neural Networks (CNNs), specifically ResNet, to extract regional and global visual representations. The attention mechanism focuses on image regions related to the text, enhancing the model's decision-making. The attention mechanism maps a query and key-value pairs to an output, using a weighted sum of values. This approach helps the model focus on relevant visual features, improving name tagging accuracy.\",\n  \"Datasets: Twitter and Snapchat Multimodal Data\": \"The model is evaluated on two multimodal datasets from Twitter and Snapchat, containing named entities like Location, Person, Organization, and Miscellaneous. Twitter data includes tweets with images, while Snapchat data consists of snaps with captions. The datasets differ in sentence length and image styles, with Twitter texts being longer and more contextually related to images. These datasets serve as benchmarks for multimodal name tagging.\",\n  \"Experimentation and Model Evaluation\": \"Experiments involve tokenization, labeling schema, word embeddings, character embeddings, and pretrained CNNs. The models are optimized using mini-batch stochastic gradient descent with specific hyperparameters. The proposed model, incorporating visual attention and modulation gate, achieves the best F1 scores on both datasets, demonstrating the effectiveness of visual features in validating entity types.\",\n  \"Results and Attention Visualization\": \"The model performs better on the Twitter dataset due to longer sentences providing more context. Visual features contribute significantly to name tagging, especially in well-aligned multimodal contexts like sports and concerts. Attention visualization shows the model focusing on relevant image regions, aiding in accurate name tagging. However, challenges arise from poor alignment, blur images, and incorrect attention.\",\n  \"Related Work in Name Tagging and Visual Attention\": \"Recent advancements in name tagging involve neural network architectures like BLSTM-CRF, outperforming traditional methods. These models, however, focus on newswire data. For social media, adaptations include leveraging unlabeled data and multi-task approaches. Visual attention mechanisms, widely used in tasks like Image Captioning and Visual Question Answering, are adapted here to enhance name tagging by focusing on relevant image regions.\",\n  \"Conclusions and Future Work\": \"The proposed gated Visual Attention model for name tagging in multimodal social media shows significant improvements. The creation of Twitter and Snapchat datasets encourages further research in this area. Future work includes expanding the model to fine-grained name tagging and entity linking, leveraging visual features for more specific entity types like sports teams and artists.\"\n}\n```",
    "```json\n{\n  \"Introduction to Instruction Execution Challenges\": \"An agent executing a sequence of instructions must address multiple challenges, including grounding the language to its observed environment, reasoning about discourse dependencies, and generating actions to complete high-level goals. For example, consider the environment and instructions in Figure 1, where a user describes moving chemicals between beakers and mixing chemicals together. A key challenge in executing sequences of instructions is considering contextual cues from both the history of the interaction and the state of the world. Instructions often refer to previously mentioned objects or actions, and the world state provides the set of objects the instruction may refer to, implicitly determining the available actions.\",\n  \"Proposed Model and Learning Approach\": \"We describe a model and learning approach to map sequences of instructions to actions. Our model considers previous utterances and the world state to select actions, learns to combine simple actions to achieve complex goals, and can be trained using the ALCHEMY domain. The model focuses on directly generating the sequence of actions, requiring resolving references without explicitly modeling them, and learning the sequences of actions required to complete high-level actions.\",\n  \"Attention-Based Model Architecture\": \"We propose an attention-based model that takes as input the current instruction, previous instructions, the initial world state, and the current state. At each step, the model computes attention encodings of the different inputs and predicts the next action to execute. The model uses attention to process the different inputs and a recurrent neural network (RNN) decoder to generate actions. Figure 2 illustrates the model architecture while generating the third action in the third utterance from Figure 1.\",\n  \"Learning Algorithm: SESTRA\": \"We address the exploration problem with a learning algorithm that observes the reward for all possible actions for each visited state and maximizes the immediate expected reward. The SESTRA algorithm iterates over the training data, sampling an execution from the policy with at most M actions. It computes the entropy and observes the immediate reward for all actions for each step, using RMSPROP to update the parameters.\",\n  \"Evaluation and Results\": \"We evaluate our approach on SCONE, which includes three domains, and is used to study recovering predicate logic meaning representations for sequential instructions. Our approach robustly learns to execute sequential instructions with up to 89.1% task-completion accuracy for single instruction, and 62.7% for complete sequences. Table 3 shows test results, indicating our approach significantly outperforms POLICYGRADIENT and CONTEXTUALBANDIT.\",\n  \"Comparison with Related Work\": \"Executing instructions has been studied using the SAIL corpus with focus on navigation using high-level logical representations and low-level actions. Our work is related to the work of Branavan et al. and Vogel and Jurafsky, which study executing sequences of instructions. In contrast, we focus on environments that change during execution and directly generating environment actions, a scenario more related to robotic agents than database query.\",\n  \"Error Analysis and Model Sensitivity\": \"We analyze the instruction-level errors made by our best models when the agent is provided the correct initial state for the instruction. We study fifty examples in each domain to identify the type of failures. Table 5 shows the counts of major error categories, including state reference errors and multi-turn reference errors. We also find a significant number of errors due to ambiguous or incorrect instructions.\",\n  \"Discussion and Future Directions\": \"We propose a model to reason about context-dependent instructional language that displays strong dependencies on the history of the interaction and the state of the world. Future modeling work may include using intermediate world states from previous turns in the interaction. Our learning approach, SESTRA, is particularly suitable for recovering from biases acquired early during learning. One possible direction for future work is to use an estimator to predict rewards for all actions, rather than observing them.\"\n}\n```",
    "```json\n{\n  \"Introduction to Temporal and Causal Relations\": \"Understanding events is crucial for natural language understanding, particularly in applications like story completion and summarization. This paper focuses on the joint extraction of temporal and causal relations, highlighting their interaction. For instance, the temporal relation between events can be dictated by causal relations, as shown in examples where the occurrence of one event causes another, thus determining their temporal order.\",\n  \"Joint Framework for Temporal and Causal Reasoning (TCR)\": \"The paper introduces a joint framework for Temporal and Causal Reasoning (TCR) using a constrained conditional model (CCM) with an integer linear programming (ILP) objective. This framework enforces constraints such as a cause must temporally precede its effect, symmetry, and transitivity. These constraints leverage the one-dimensional nature of time and causality, making CCM a suitable choice for this problem.\",\n  \"Challenges in Jointly Annotating Temporal and Causal Data\": \"A significant challenge in studying temporal and causal relations is the lack of jointly annotated data. The paper addresses this by developing a dataset with dense temporal annotations, augmenting the Event-Causality dataset. This allows for statistically significant improvements in extracting both relations using the proposed joint framework.\",\n  \"Empirical Results and Improvements in Temporal Extraction\": \"The paper presents empirical results showing improvements in temporal extraction by incorporating explicit time expressions and high-precision rules into the ILP objective. These modifications, tested on the TimeBank-Dense dataset, demonstrate that such enhancements can be beneficial within ILP-based methods, leading to better temporal relation identification.\",\n  \"Related Work and Existing Approaches\": \"Temporal and causal relations have been represented by directed acyclic graphs in previous work. Early methods focused on local classification models, which often broke symmetric and transitive constraints. Recent approaches, like CAEVO, use global reasoning to enforce these constraints. The paper builds on these methods by adopting an ILP approach for joint temporal and causal relation extraction.\",\n  \"Transitivity and Linguistic Constraints in Temporal Reasoning\": \"The paper explains the design of transitivity constraints and the enforcement of linguistic rules to improve temporal reasoning. Transitivity constraints ensure temporal consistency across event pairs, while linguistic rules, derived from syntactic or semantic patterns, are incorporated as high-precision constraints in the global reasoning process.\",\n  \"Joint Performance on New Dataset with Temporal and Causal Annotations\": \"The paper evaluates the joint framework on a new dataset with both temporal and causal annotations. The results show that the joint framework improves both temporal and causal performance, indicating that these signals are mutually beneficial. The dataset was created by augmenting existing data with dense temporal annotations, ensuring a comprehensive evaluation of the joint approach.\",\n  \"Discussion on Temporal and Causal Interactions\": \"The discussion highlights the complementary nature of temporal and causal information. Temporal relations often focus on local context, while causal relations encode global knowledge. The joint framework effectively synthesizes these signals, improving the understanding of event interactions. The paper also notes challenges in handling cases where causes are temporally after effects, suggesting a need for better event definitions.\"\n}\n```",
    "```json\n{\n  \"Introduction to Emotion Classification\": \"Human emotions are complex and unique features that are difficult to define and understand completely. Ekman (1980) proposed six basic emotions (anger, disgust, fear, guilt, joy, and sadness) that have a shared meaning across cultures. In this work, seven types of emotional statements (anger, disgust, fear, guilt, joy, sadness, and shame) were extracted from a psychological corpus. These statements were tokenized, grouped into trigrams, and considered as Context Vectors. The vectors were POS tagged, and TF and TF-IDF scores were measured to determine their importance as features. Affinity Scores were calculated for each pair of Context Vectors using different distance metrics (Chebyshev, Euclidean, and Hamming), leading to the application of various classification methods like NaiveBayes, J48, Decision Tree, and BayesNet.\",\n  \"Related Work and Existing Resources\": \"Strapparava and Valitutti (2004) developed WORDNET-AFFECT, a lexical resource assigning affective labels to WORDNET synsets. Wiebe et al. (2005) described an annotation scheme for opinions and emotions in language. Kobayashi et al. (2004) developed an opinion lexicon from annotated corpora. Esuli and Sebastiani (2006) adapted SentiWord-Net for synset classification using ternary classifiers. Mohammad et al. (2010) analyzed emotion annotations, and Bandyopadhyay (2009, 2010) created emotion lexicons for Bengali. SenticNet and EmotiNet are resources for emotion detection, with EmotiNet being extendable with external sources.\",\n  \"Data Preprocessing and Corpus Preparation\": \"The emotional statements were collected from the ISEAR database, containing reports from 3000 respondents across 37 countries. The statements were tokenized into words and grouped into trigrams to form Context Windows (CWs). The WordNet Affect list was used to identify affect words in the CWs. Statistics showed that the anger class had the most trigrams, while the joy class had the fewest. Patterns like 'advices, about, problems' (Anger) and 'always, joyous, one' (Joy) were identified. Stop words were prevalent in the CW patterns, with fear having the most stop words in certain positions.\",\n  \"Context Vector Formation and Affinity Score Calculation\": \"Context Windows were mapped into a vector space to identify their role in classifying emotions. Vectors were formed based on the occurrence of words in the CW pattern (NAW1, AW, NAW2). Affinity Scores were calculated for each pair of Context Vectors using a formula inspired by Newton's law of gravitation. The scores reflect the affinity between vectors, with higher scores indicating higher affinity. Distance metrics like Chebyshev, Euclidean, and Hamming were used to calculate the proximity of vectors in the space.\",\n  \"Feature Selection and Analysis\": \"Feature selection is crucial for building a good pattern classifier. Sentences were POS tagged, and Context Windows were extracted as PTCWs. The POS tag sequences were named PTWs. The 'fear' emotion class had the most CWs and unique PTCWs, while 'anger' had the most unique PTWs. TF and TF-IDF scores were calculated for CWs, PTCWs, and PTWs. Guilt had the highest Max_TF and variance, while anger and disgust had the lowest. Ranking scores were calculated for CWs using SentiWordnet, identifying important emotional phrases.\",\n  \"Classification Methods and Results\": \"Classifiers like BayesNet, J48, NaiveBayesSimple, and DecisionTree were used. BayesNet achieved 100% accuracy on Test data with Euclidean distance and 97.91% on 10-fold cross-validation. J48 achieved 77% on Test data and 83.54% on cross-validation. NaiveBayesSimple had 92.30% on Test data but only 27.07% on cross-validation. DecisionTree performed well with 98.30% on Test data and 98.10% on cross-validation. BayesNet also performed well with Hamming and Chebyshev distances, achieving high accuracy across different metrics.\",\n  \"Conclusions and Future Work\": \"The study involved vector formation for Context Windows and calculation of TF and TF-IDF measures. Affinity scores were inspired by Newton's law of gravitation. Classifiers like BayesNet, J48, NaiveBayesSimple, and DecisionTable were used. Future work includes incorporating more lexicons and using associative learning to identify classification rules.\"\n}\n```",
    "```json\n{\n  \"Introduction to Controversial Content Detection\": \"Controversial content, which attracts both positive and negative feedback, can be beneficial for community health by sparking debate. Detecting such content is useful for community members and moderators, as it can help users refine their posts and alert moderators to discussions that may need monitoring. Some platforms already sort content by controversy, highlighting its importance. The controversiality of content is context-dependent, varying across different communities. For instance, a topic like 'break up' may be controversial in one subreddit but not in another. This variability underscores the need for community-specific detection methods.\",\n  \n  \"Community-Specific Controversiality\": \"Controversial topics are often specific to the community in which they are discussed. For example, 'crossfit' is controversial in the Fitness subreddit but not in others. Community moderators may struggle to predict which posts will become controversial due to unpredictable factors like current events or the specific users online at a given time. The influence of early opinions can significantly impact the dynamics of a discussion, sometimes more than the content itself. This highlights the importance of considering community-specific factors when detecting controversial content.\",\n  \n  \"Early-Detection Approach and Methodology\": \"We propose an early-detection approach that considers both the content of the initial post and the structure of the initial comments. This method integrates previously separate research streams and shows that incorporating structural and textual features of early comment trees improves predictive performance. For instance, in one community, adding features from the first 15 minutes of discussion significantly increased prediction accuracy. This approach also explores feature transferability across different communities, finding that while text features are strong predictors within a community, structural features transfer better across communities.\",\n  \n  \"Datasets and Data Collection\": \"Our dataset is derived from Reddit, covering posts and comments from 2007 to 2014. We focus on six text-based subreddits, each with a large number of posts and comments. The dataset includes Q&A subreddits like AskMen and AskWomen, a special-interest community like Fitness, and advice communities like LifeProTips, personalfinance, and relationships. This comprehensive dataset allows us to explore community-specific controversiality and test our early-detection approach across different types of communities.\",\n  \n  \"Assigning and Validating Controversy Labels\": \"We assign binary controversy labels to posts based on the proportion of upvotes and downvotes they receive. Posts with a high number of both upvotes and downvotes are considered controversial. Our labeling method aligns well with Reddit's sort-by-controversy function, achieving high precision in identifying controversial posts. We also validate our labels by comparing them to Reddit's controversial ranking, ensuring that our method accurately reflects community perceptions of controversy.\",\n  \n  \"Analysis of Early Discussion Threads\": \"We analyze comments posted within the first hour of a post to understand early discussion dynamics. Comment text and tree structure are examined, revealing that structural features of conversation trees are predictive of controversy. For example, the max-depth ratio and Wiener index are important features in different communities. This analysis shows that early discussion features, beyond just the rate of comments, can provide valuable insights into the potential controversiality of a post.\",\n  \n  \"Early Prediction of Controversy\": \"Our early prediction models use features available shortly after a post is made, such as comment text and tree structure. These models significantly outperform post-time baselines, demonstrating the value of early discussion features. Textual content of comments is a major source of predictive gain, highlighting the importance of considering both post and comment text in controversy prediction. Our results show that controversy prediction models learn distinct patterns from popularity prediction models, emphasizing the unique nature of controversial content.\",\n  \n  \"Domain Transfer and Feature Transferability\": \"We explore the transferability of features across different communities, finding that while text features are strong predictors within a community, they are less effective across communities. In contrast, rate and structural features of comment trees transfer better between communities. This suggests that while text features capture community-specific nuances, structural features provide more generalizable insights. Our findings highlight the potential for developing more robust algorithms that can effectively transfer across domains.\"\n}\n```",
    "```json\n{\n  \"Introduction to Weblio Pre-reordering SMT System\": \"In this paper, we describe the details of Weblio Pre-reordering Statistical Machine Translation (SMT) System, experiments, and some issues we faced. For this SMT system, we applied the pre-reordering method proposed in (Zhu et al., 2014). This method automatically learns pre-reordering models from word alignments and parse trees. The statistical language model is integrated into the pre-reordering model to reorder each node layer in parse trees. In the 1st Workshop on Asian Translation (WAT2014), we mainly applied this method in the English-Japanese translation subtask.\",\n  \"Head-restructured CFG Parse Tree for Reordering\": \"In order to reorder SVO (subject-verb-object) order into SOV (subject-object-verb) order, correctly reordering long-distance words that play important roles in a sentence is crucial. The approach in (Zhu et al., 2014) addresses this problem by injecting sentence-level dependencies into CFG parse trees. This new parse tree is called 'Head-restructured CFG parse tree' or 'HRCFG tree'. An example of an HRCFG tree is shown in Figure 1, where tree components explicitly show subject, object, and verb parts in the sentence.\",\n  \"Integration of Language Model in Reordering\": \"The reordering model we used in our MT systems follows the same fashion as the model in (Zhu et al., 2014). A language model is integrated to identify the best order of a node layer according to the order of the target language. With bilingual training data and automatically learned word alignments given by GIZA++, we find the best order for each node layer in all parse trees. We show an example of this reordering process in Figure 2, where the sequence 'nsubj dobj hits' gets the highest language model score, as it is closest to the order in Japanese.\",\n  \"N-best Reordering and Translation Hypotheses\": \"In the reordering model described, the best order for the whole sentence is comprised of all 1-best orders of every node layer in the parse tree. Considering N-best reordering results is necessary to obtain the best translation result. In our MT system, we implement this feature by collecting N-best reordering results for all node layers and ranking them by accumulated language model score. Table 1 shows two plausible reordering results for the sentence 'The rocket is launched by NASA'.\",\n  \"Experimental Setup and Results\": \"For our baseline system, we use 1-best parse trees for training and testing. We carried out several experiments combining the use of N-best parse trees and N-best reordering results. A list of automatic evaluation scores for different system settings is listed in Table 2. Our experiment results show that incorporating N-best parse tree and reordering results gained improvements for both BLEU and RIBES metrics. In the official human evaluation, although System 4 achieved better results in automatic evaluations, its human evaluation score degraded compared to System 3.\",\n  \"Evaluation of Pre-reordering Performance\": \"In this section, we evaluate the performance of pre-reordering. Following the method described in (Isozaki et al., 2010), we estimate Kendall's \u03c4 from word alignments. A comparison of Kendall's \u03c4 distribution upon the first 1.5M sentences of the ASPEC corpus is shown in Figure 5. Average Kendall's \u03c4 of natural order and adjusted order is 0.30 and 0.71, respectively. The distribution of Kendall's \u03c4 in Figure 5 gives an intuitive picture of the improvements in word order.\",\n  \"Error Analysis and Challenges in Reordering\": \"Although our pre-reordering SMT system produces relatively better translation results compared to baseline SMT systems, the translation results still suffer from the defect of the reordering model. The reordering model is a language model built on sequences mixed with nonterminal tags and words, making it sparse. For some rare or unknown words, the reordering model usually fails to reorder sentences correctly. Table 4 shows two reordering samples, highlighting errors in the reordering results for the ASPEC test corpus.\",\n  \"Context-aware Machine Translation Efforts\": \"In this section, we describe efforts for utilizing context information during translation. We attempted to tackle the phrase selection problem for English-Japanese translation. Many English words have multiple translations in Japanese. Figure 6 shows the distribution of translations across different domains. We carried out experiments that interpolate general and in-domain language models. Figure 7 shows the language model perplexity achieved on domain-specific test data using different settings, indicating that interpolated language models generally achieve the best perplexities.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Machine Translation and Its Advantages\": \"Neural machine translation is a brand-new approach that samples translation results directly from RNNs. Most published models involve an encoder and a decoder in the network architecture, called the Encoder-Decoder approach. This method has shown to outperform phrase-based SMT baselines in tasks such as English-French and English-German translations. The use of stacked Long Short-Term Memory (LSTM) units for both encoder and decoder has been particularly effective in boosting performance.\",\n  \n  \"Recurrent Neural Networks and Their Limitations\": \"Recurrent neural networks (RNNs) are designed for modeling temporal data. They update their internal state based on new inputs and previous states, producing outputs recursively. However, RNNs struggle with capturing long-term dependencies due to the dilution of error information over time. This limitation is addressed by more advanced architectures like LSTM and GRU, which incorporate mechanisms to preserve information over longer periods.\",\n  \n  \"Long Short-Term Memory and Gated Recurrent Units\": \"Long Short-Term Memory (LSTM) units incorporate gates to control information flow, allowing them to capture long-term dependencies. They use memory cells to store information, which can be updated linearly. Gated Recurrent Units (GRU) are similar but maintain only one kind of internal state, reducing computational complexity. Both LSTM and GRU have been shown to improve the performance of NMT models by effectively managing information flow.\",\n  \n  \"Encoder-Decoder Architecture in NMT\": \"The Encoder-Decoder architecture encodes an input sequence into a vector representation and then decodes it to generate an output sequence. This approach uses trainable word embeddings and a softmax function to compute cross-entropy. The architecture is illustrated with stacked multilayer recurrent units, which enhance the model's ability to handle complex translations.\",\n  \n  \"Soft-Attention Mechanism in NMT\": \"The soft-attention mechanism addresses the limitations of the basic Encoder-Decoder approach, particularly with long input sentences. It evaluates weights for all possible positions to attend and makes a weighted summarization of hidden states in the encoder. This mechanism allows the model to focus on relevant parts of the input, improving translation quality without degrading performance on longer sentences.\",\n  \n  \"Handling Unknown Words in NMT\": \"Handling unknown words is a critical challenge in NMT due to the fixed vocabulary size. Solutions include model-specific approaches like noise-contrastive training and translation-specific approaches that use token alignments. Annotating unknown words with aligned source positions helps in recovering missing translations, allowing NMT models to surpass state-of-the-art SMT systems in certain tasks.\",\n  \n  \"Experiments and Evaluation of NMT Models\": \"Experiments were conducted on English-Japanese translation using the ASPEC-JE corpus. Models were evaluated based on perplexity, with soft-attention models outperforming simple Encoder-Decoder models. The experiments highlighted the challenges of translating between languages with different structures and the importance of capturing long-range dependencies.\",\n  \n  \"Qualitative Analysis and Error Patterns in NMT Translations\": \"Qualitative analysis revealed that NMT models often omit information, resulting in translations that are valid but incomplete. This pattern suggests that while NMT models can produce fluent translations, they may fail to preserve the full meaning of the source text. Addressing these error patterns could lead to further improvements in translation quality.\"\n}\n```",
    "```json\n{\n  \"Introduction to Word Embeddings and Their Limitations\": \"Word embeddings, which embed words into low-dimensional vector spaces, have become popular in NLP tasks like Machine Translation and syntactic parsing. Despite their success, they struggle with the inability to discriminate among different meanings of the same word. Previous works have attempted to address this by inducing word senses from corpora or bilingual data, but these approaches often lack interpretability and are not easily mappable to lexical resources.\",\n  \"Existing Approaches to Sense Disambiguation\": \"Various methods have been proposed to overcome the limitations of word embeddings, such as using disambiguation systems like Babelfy to learn sense embeddings. However, these methods often replace words with their intended senses, resulting in sense representations only. Other approaches involve multiple training steps and rely on resources like WordNet, which limits their applicability. Some models aim to align vector spaces of words and entities but are restricted to nominal instances.\",\n  \"Proposed SW2V Model: Integrating Words and Senses\": \"The SW2V model is proposed to learn embeddings for both words and senses by exploiting knowledge from text corpora and semantic networks. It offers three key features: representing word and sense embeddings in the same vector space, flexibility for different predictive models, and scalability for large semantic networks and corpora. The model aims to address the limitations of previous approaches by providing a unified vector space.\",\n  \"Shallow Word-Sense Connectivity Algorithm\": \"The shallow word-sense connectivity algorithm is designed to connect words and senses in context using a semantic network. It is linear in corpus size and allows words to have multiple associated senses. The algorithm calculates connections based on a semantic network and associates words with the most connected senses in a given context. This approach speeds up training and improves accuracy compared to existing disambiguation systems.\",\n  \"Joint Training of Words and Senses\": \"The SW2V model extends conventional word embedding models by integrating explicit knowledge into its architecture. It uses a shared vector space for words and senses, updating embeddings based on their intrinsic relationship. The model can be applied to architectures like CBOW and Skip-Gram, predicting both words and senses in context. This joint training approach aims to create a semantically coherent vector space.\",\n  \"Analysis of Model Components and Configurations\": \"The analysis of SW2V components includes evaluating different model configurations and the shallow word-sense connectivity algorithm. The model is tested on word similarity datasets, showing that configurations with only senses in the input layer perform best. The shallow algorithm outperforms state-of-the-art disambiguation systems, providing better results and faster processing times.\",\n  \"Evaluation of SW2V on Word Similarity and Sense Clustering\": \"SW2V is evaluated on word similarity tasks using datasets like SimLex-999 and MEN, outperforming other sense-based models. It also excels in sense clustering tasks, demonstrating its ability to capture semantics accurately. The model's shared space of words and senses offers advantages over retrofitting approaches, which conflate different meanings into the same vector.\",\n  \"Interconnectivity of Word and Sense Embeddings\": \"The interconnectivity of word and sense embeddings is tested through a Word Sense Disambiguation task. SW2V's shared vector space allows for a strong Most Common Sense baseline, outperforming other models. This experiment highlights the semantic coherence of SW2V's vector space, which is valuable for resource-poor languages and large knowledge resources.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multi-task Learning in NLP\": \"When we use supervised learning to solve Natural Language Processing (NLP) problems, we typically train an individual model for each task with task-specific labeled data. However, our target task may be intrinsically linked to other tasks. Multi-task Learning (MTL) is one of the most effective solutions for knowledge transfer across tasks. In the context of neural network architectures, we usually perform MTL by sharing parameters across models. Previous studies have proven that MTL is an effective approach to boost the performance of related tasks such as MT and parsing.\",\n  \n  \"Challenges in Low-resource Language Settings\": \"Most NLP tasks, including some well-studied ones such as POS tagging, still suffer from the lack of training data for many low-resource languages. It is an unattainable goal to annotate data in all languages, especially for tasks with complicated annotation requirements. Furthermore, some special applications require rapid development of NLP systems for extremely low-resource languages. Therefore, in this paper, we concentrate on enhancing supervised models in low-resource settings by borrowing knowledge learned from related high-resource languages and tasks.\",\n  \n  \"Proposed Multi-lingual Multi-task Architecture\": \"To tackle the issue of data sparsity in low-resource languages, we propose a multi-lingual multi-task architecture which combines different transfer models within a unified architecture through two levels of parameter sharing. In the first level, we share character embeddings, character-level convolutional neural networks, and word-level long-short term memory layer across all models. In the second level, we adopt different sharing strategies for different transfer schemes. For example, we use the same output layer for all Name Tagging tasks to share task-specific knowledge.\",\n  \n  \"Model Architecture and Parameter Sharing\": \"We design our multi-task multilingual architecture based on the LSTM-CNNs model. The overall framework is illustrated in Figure 1. Each word is represented as a combination of word embedding and character feature vector. A bidirectional LSTM processes the sequence and encodes each word and its context into a fixed-size vector. A linear layer converts this to a score vector, and a CRFs layer generates the best tagging path. We emphasize that our actual implementation differs slightly from the LSTM-CNNs model by not using additional word-and character-level explicit symbolic features.\",\n  \n  \"Experiments and Data Sets\": \"For Name Tagging, we use data sets from the CoNLL 2002 and 2003 shared tasks, LDC2016E95, and TAC KBP 2017. For POS Tagging, we use data from the CoNLL 2017 shared task. We train word embeddings using the word2vec package and use the unsupervised model in the MUSE library to learn a mapping between monolingual word embeddings. We optimize parameters using Stochastic Gradient Descent with momentum, gradient clipping, and exponential learning rate decay.\",\n  \n  \"Comparison of Different Models\": \"In Figures 3, 4, and 5, we compare our model with the mono-lingual single-task LSTM-CNNs model, cross-task transfer model, and cross-lingual transfer model in low-resource settings. Our model substantially outperforms the mono-lingual single-task baseline model and obtains visible gains over single transfer models. When trained with less than 50 main tasks training sentences, cross-lingual transfer consistently surpasses cross-task transfer.\",\n  \n  \"Qualitative Analysis and Ablation Studies\": \"In Table 4, we compare Name Tagging results from the baseline model and our model. Shared character-level networks can transfer different levels of morphological and semantic information. Ablation studies on Dutch Name Tagging show that adding each component usually enhances performance, while the impact also depends on the size of the target task data.\",\n  \n  \"Effect of Auxiliary Task Data and Related Work\": \"To evaluate our model's sensitivity to the amount of auxiliary task data, we fix the size of main task data and downsample all auxiliary task data. The performance goes up when we raise the sample rate from 1% to 20%. Multi-task Learning has been applied in different NLP areas, such as machine translation, text classification, dependency parsing, and sequence labeling. Our model combines cross-task transfer and cross-lingual transfer within a unified architecture to transfer different types of knowledge from multiple auxiliary tasks simultaneously.\"\n}\n```",
    "```json\n{\n  \"Introduction to Event Detection and Challenges\": \"Event Detection (ED) is a crucial subtask of event extraction, focusing on identifying event triggers within sentences and determining their types. A significant challenge in ED is the context-dependent nature of event triggers, which can represent different event types in varying contexts. Existing methods primarily utilize sentence-level contextual information, but document-level information is also vital as it provides thematic coherence across sentences. For example, the trigger 'leave' can indicate different events depending on the surrounding context, highlighting the need for comprehensive contextual analysis.\",\n  \"Existing Methods and Their Limitations\": \"Current ED methods are categorized into feature-based and representation-based approaches. Feature-based methods rely on manually designed features, which can lead to error propagation and are time-consuming. Representation-based methods use distributed representations but often fail to capture event-specific information due to unsupervised training processes. Document-level information is underutilized, and existing methods struggle with capturing inter-event information effectively, often requiring complex rule-based systems.\",\n  \"Proposed DEEB-RNN Model\": \"The DEEB-RNN model is a novel approach for sentence-level ED, leveraging document embeddings through a hierarchical and supervised attention-based bidirectional RNN. This model focuses on word-level attention for event triggers and sentence-level attention for event-containing sentences. The learning process integrates attention-based loss functions, enhancing the model's ability to identify event triggers and types. This is the first study to apply a hierarchical and supervised attention mechanism for learning ED-oriented document embeddings.\",\n  \"ED Oriented Document Embedding Learning (EDODEL) Module\": \"The EDODEL module learns document embeddings using a hierarchical attention network with word-level and sentence-level Bi-GRU encoders. Word-level embeddings are obtained by concatenating word and entity type embeddings, followed by attention-weighted summation. Sentence-level embeddings are derived similarly, focusing on sentences with events. The module constructs gold attention signals to supervise learning, ensuring more attention is given to event-related words and sentences.\",\n  \"Document-level Enhanced Event Detector (DEED) Module\": \"The DEED module employs a Bi-GRU encoder and a softmax output layer to model the ED task, handling multi-word event triggers. It uses document embeddings as input to classify each word in a sentence to an event type. The module's loss function is based on cross-entropy error, ensuring accurate event type classification by leveraging document-level context.\",\n  \"Joint Training of DEEB-RNN Model\": \"The DEEB-RNN model's modules are jointly trained using a combined loss function that balances attention-based and classification losses. This joint training approach optimizes the model's parameters, enhancing its ability to detect events by integrating document-level and sentence-level information effectively.\",\n  \"Experimental Setup and Baseline Models\": \"The DEEB-RNN model is evaluated on the ACE-2005 dataset, with a training set of 529 documents, a validation set of 30, and a test set of 40 documents. Baseline models include both feature-based and representation-based approaches, such as Sentence-level, Joint Local, JRNN, Skip-CNN, ANN-S2, Cross-event, PSL, and DLRNN. These models serve as benchmarks to validate the effectiveness of the DEEB-RNN model.\",\n  \"Performance Comparison and Analysis\": \"The DEEB-RNN model outperforms state-of-the-art methods in recall and F1-measure, demonstrating the benefits of document-level embeddings and supervised attention mechanisms. Different attention strategies show varying impacts on precision and recall, with DEEB-RNN3 achieving the best performance by using gold attention signals at both word and sentence levels. The model's ability to automatically capture event types without manual rules is a significant advantage over existing methods.\"\n}\n```",
    "```json\n{\n  \"Introduction to SEQ2SEQ Models and Their Limitations\": \"Neural sequence-to-sequence models (SEQ2SEQ) have shown impressive performance in various natural language processing tasks, such as machine translation and syntactic constituency parsing. However, these models require large parallel training datasets, which can be a significant limitation. There has been extensive research on utilizing non-parallel corpora to enhance SEQ2SEQ models' performance, particularly in neural machine translation. Despite these efforts, current unsupervised SEQ2SEQ models for tasks like sentence compression often struggle to surpass simple baselines and are non-differentiable, relying on reinforcement learning, which is unstable and inefficient.\",\n  \"Proposed SEQ3 Model for Unsupervised Abstractive Sentence Compression\": \"The SEQ3 model is introduced as a sequence-to-sequence-to-sequence autoencoder that can be trained end-to-end via gradient-based optimization. It employs differentiable approximations for sampling from categorical distributions, which have been shown to outperform reinforcement learning. SEQ3 is a generic framework that can be extended to other tasks, such as machine translation and semantic parsing, through task-specific losses. In this work, SEQ3 is applied to unsupervised abstractive sentence compression, demonstrating state-of-the-art performance in this area.\",\n  \"Technical Details of the SEQ3 Model Architecture\": \"SEQ3 consists of two attentional encoder-decoder pairs: a compressor and a reconstructor. The compressor receives an input text and generates a summary, which acts as a latent variable. The reconstructor attempts to reproduce the input from the summary, minimizing a reconstruction loss. A pretrained language model acts as a prior, introducing an additional loss that encourages human-readable summaries. The model also includes a topic loss that rewards summaries with similar topic-indicating words as the input text.\",\n  \"Differentiable Word Sampling Techniques\": \"To generate the summary, SEQ3 uses differentiable word sampling techniques. Instead of sampling words directly, which is non-differentiable, the model uses a soft-argmax function during training to pass a weighted sum of all vocabulary word embeddings. The Gumbel-Softmax reparametrization trick is employed as a low variance approximation of sampling from categorical distributions. The Straight-Through estimator is used to discretize embeddings during the forward pass while computing gradients using the Gumbel-Softmax in the backward pass.\",\n  \"Loss Functions and Their Roles in SEQ3\": \"SEQ3 utilizes several loss functions to guide its training. The reconstruction loss ensures that the reconstructor can accurately reproduce the input text. The LM prior loss, computed as the Kullback-Leibler divergence between the compressor's and a pretrained language model's probability distributions, ensures readability of the summaries. The topic loss encourages the preservation of topic-indicating words in the summary. Additionally, a length penalty helps the compressor predict the end-of-sequence token at the target summary length.\",\n  \"Experimental Setup and Datasets Used\": \"SEQ3 was trained on the Gigaword sentence compression dataset, which consists of pairs of news article sentences and their headlines. The model was also tested on the DUC-2003 and DUC-2004 shared tasks without retraining. SEQ3 was evaluated against other unsupervised sentence compression models and baselines, such as LEAD-8 and PREFIX, using ROUGE scores to measure performance.\",\n  \"Comparison with Other Models and Baselines\": \"SEQ3 outperforms the unsupervised Pretrained Generator model across all metrics by a significant margin and surpasses the LEAD-8 baseline. The model's performance drops when the LM prior is removed, highlighting its importance. SEQ3 also performs well on the DUC-2003 and DUC-2004 datasets, although it does not surpass the PREFIX baseline in DUC-2003. The results demonstrate the effectiveness of SEQ3's loss functions and its ability to generate high-quality summaries.\",\n  \"Limitations and Future Work\": \"One limitation of SEQ3 is its tendency to copy the first words of the input sentence in the compressed text. This is hypothesized to be due to the autoregressive nature of the reconstructor, where early errors have cascading effects. Future work may explore modifying SEQ3 to address this issue and extend its application to other tasks, such as unsupervised machine translation and caption generation.\"\n}\n```",
    "```json\n{\n  \"Introduction to Word Embeddings and OOV Challenges\": \"One of the key advantages of word embeddings for natural language processing is that they enable generalization to words that are unseen in labeled training data, by embedding lexical features from large unlabeled datasets into a relatively low-dimensional Euclidean space. These low-dimensional embeddings are typically trained to capture distributional similarity, so that information can be shared among words that tend to appear in similar contexts. However, it is not possible to enumerate the entire vocabulary of any language, and even large unlabeled datasets will miss terms that appear in later applications. The issue of how to handle these out-of-vocabulary (OOV) words poses challenges for embedding-based methods. These challenges are particularly acute when working with low-resource languages, where even unlabeled data may be difficult to obtain at scale.\",\n  \n  \"MIMICK-RNN: A Quasi-Generative Approach\": \"We approach this challenge from a quasi-generative perspective. Knowing nothing of a word except for its embedding and its written form, we attempt to learn the former from the latter. We train a recurrent neural network (RNN) on the character level with the embedding as the target, and use it later to predict vectors for OOV words in any downstream task. We call this model the MIMICK-RNN, for its ability to read a word's spelling and mimick its distributional embedding. Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features.\",\n  \n  \"Character-Based Embeddings and Nearest-Neighbor Analysis\": \"Through nearest-neighbor analysis, we show that vectors learned via this method capture both word-shape features and lexical features. As a result, we obtain reasonable near-neighbors for OOV abbreviations, names, novel compounds, and orthographic errors. Nearest-neighbor examination was conducted as a preliminary sanity check for the validity of our protocol, examining samples in languages such as English, Hebrew, Tamil, and Spanish.\",\n  \n  \"Evaluation on Stanford RareWord Dataset and Universal Dependencies\": \"Quantitative evaluation on the Stanford RareWord dataset provides more evidence that these character-based embeddings capture word similarity for rare and unseen words. As an extrinsic evaluation, we conduct experiments on joint prediction of part-of-speech tags and morphosyntactic attributes for a diverse set of 23 languages, as provided in the Universal Dependencies dataset. Our model shows significant improvement across the board against a single UNK-embedding backoff method.\",\n  \n  \"Comparison with Related Work and Supervised Subword Models\": \"Compositional models for embedding rare and unseen words make use of morphological or orthographic information when training word embeddings, enabling the prediction of embeddings for unseen words based on their internal structure. Supervised subword models learn task-specific character-based word embeddings within end-to-end supervised systems. These methods struggle in several settings: low-resource languages, morphologically rich languages, and in Chinese, where the number of characters is orders of magnitude larger than in non-logographic scripts.\",\n  \n  \"MIMICK Word Embeddings: Methodology and Implementation\": \"We approach the problem of out-of-vocabulary (OOV) embeddings as a generation problem: regardless of how the original embeddings were created, we assume there is a generative wordform-based protocol for creating these embeddings. Our predictive function of choice is a Word Type Character Bi-LSTM. The model is trained to find a function that approximates the assignments of pre-trained word embeddings. The training objective is to match the predicted embeddings to the pre-trained word embeddings by minimizing the squared Euclidean distance.\",\n  \n  \"Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes\": \"The Universal Dependencies scheme features a minimal set of 17 POS tags and supports tagging further language-specific features using attribute-specific inventories. Our approach for tagging morphosyntactic attributes is similar to the part-of-speech tagging model, extending it by duplicating the projection layer for each attribute type. The input to our multilayer perceptron projection network is the hidden state produced for each token in the sentence by an underlying LSTM.\",\n  \n  \"Experimental Settings and Results Across Languages\": \"The morphological complexity and compositionality of words varies greatly across languages. We select languages that comprise a sample of multiple morphological patterns. Our approach is built on the Polyglot word embeddings, and we measure part-of-speech tagging by overall token-level accuracy. We report performance in both low-resource and full-resource settings, with results showing significant improvements in languages such as Slavic languages and Chinese.\"\n}\n```",
    "```json\n{\n  \"Introduction to Multilingual Corpora and Comparability\": \"Multilingual corpora with varying levels of comparability are essential for numerous natural language processing (NLP) tasks. Initially, comparable corpora were utilized for extracting parallel lexicons and later for feeding statistical machine translation (SMT) systems. SMT systems rely on bilingual texts to estimate statistical models, making domain-specific corpora crucial for high coverage. However, large collections of parallel texts for all domains and language pairs are scarce. Wikipedia, with its multilingual editions, serves as a valuable source for extracting comparable and parallel corpora, despite its complex and chaotic taxonomy.\",\n  \"Challenges in Extracting Comparable Corpora from Wikipedia\": \"Extracting comparable corpora from Wikipedia is challenging due to its community-generated taxonomy, which is dense and chaotic. Categories are linked arbitrarily, often without clear relationships, leading to over-categorization and cycles. This complexity makes it difficult to define domains accurately. Heuristic strategies are necessary to mitigate these issues, such as using characteristic vocabulary to query a search engine or exploring the category graph from a root category. Despite these challenges, Wikipedia's interlanguage links and multilinguality offer significant potential for extracting comparable corpora.\",\n  \"Domain-Specific Comparable Corpora Extraction Methodology\": \"The proposed methodology for extracting domain-specific comparable corpora from Wikipedia involves starting from a top category of interest and exploring the category graph. The domain vocabulary is defined by collecting articles from the top category and applying preprocessing steps. Graph exploration is controlled by constraints to avoid loops and over-exploration. Categories are scored based on their likelihood of belonging to the domain, using vocabulary terms as indicators. The methodology was tested on English and Spanish Wikipedia editions across domains like Computer Science, Science, and Sports, resulting in domain vocabularies of 100-400 lemmas.\",\n  \"Parallel Sentence Extraction Techniques\": \"Parallel sentence extraction from comparable corpora involves estimating similarity between cross-language sentence pairs using various text similarity measures. These include character n-grams, pseudo-cognates, and word 1-grams translated into a common language. A threshold is set for each measure to extract parallel sentences, resulting in a noisy parallel corpus. The quality of similarity measures affects the corpus purity, but the SMT system's word alignment process can handle some noise. Experiments showed that character 3-grams and mono-lingual translations performed well, with length factor measures enhancing F1 scores.\",\n  \"Evaluation of Extracted Corpora in Statistical Machine Translation\": \"The extracted corpora were evaluated by their impact on statistical machine translation (SMT). Systems trained with Wikipedia-derived corpora showed significant improvements in BLEU scores compared to those trained with Europarl, especially for domain-specific translations. The union of different similarity measures resulted in the best performance, doubling BLEU scores in some cases. Even with a noisy corpus, the extracted data proved useful for SMT, outperforming general-purpose corpora like Europarl in domain-specific tasks. The evaluation confirmed the utility of Wikipedia-derived corpora for enhancing translation quality.\",\n  \"Conclusions and Future Work\": \"The paper presents a model for automatically extracting in-domain comparable corpora from Wikipedia, enabling the generation of monolingual and comparable article collections. The extracted parallel sentences significantly improved machine translation quality for domain-specific tasks. The model is applicable to other language pairs and domains, given the existence of a corresponding Wikipedia edition and basic processing tools. Future work aims to develop a more robust model for parallel sentence identification and design indirect evaluation schemes to validate performance. The prototype is operational in multiple languages, demonstrating its versatility.\"\n}\n```",
    "```json\n{\n  \"Introduction to Abstractive Sentence Summarization\": \"The exponentially growing online information has necessitated the development of effective automatic summarization systems. Abstractive sentence summarization generates a shorter version of a given sentence while attempting to preserve its original meaning. This task has gained attention due to its potential applications, such as designing or refining appealing headlines. The attentional sequence-to-sequence (seq2seq) framework has achieved state-of-the-art performance in this area, although it faces challenges with longer text generation.\",\n  \n  \"Challenges with Seq2Seq Models\": \"Seq2Seq models, while effective, face significant challenges as the length of the generated text increases. Studies have shown that these models can 'lose control,' resulting in summaries that are either too short or repetitive, which reduces informativeness and readability. Additionally, seq2seq models often focus on copying source words in order, rather than truly summarizing the content, indicating that relying solely on the source sentence is insufficient.\",\n  \n  \"Template-Based Summarization Approaches\": \"Template-based summarization is a traditional approach where templates are incomplete sentences filled with input text using manually defined rules. These templates produce fluent and informative summaries but are time-consuming to construct and require extensive domain knowledge. The approach is limited by the impossibility of developing templates for all domains. Soft templates, inspired by retrieve-based conversation systems, offer a more flexible alternative by using existing summaries as reference points without needing explicit rules.\",\n  \n  \"Re3Sum: A Hybrid Summarization System\": \"Re3Sum is a hybrid summarization system that combines seq2seq and template-based approaches. It consists of three modules: Retrieve, Rerank, and Rewrite. The system uses an Information Retrieval platform to find candidate soft templates from a training corpus, then extends the seq2seq model to jointly learn template saliency measurement and final summary generation. This approach significantly improves informativeness and readability compared to state-of-the-art seq2seq models.\",\n  \n  \"Retrieve, Rerank, and Rewrite Modules\": \"The Retrieve module identifies candidate templates from a large corpus, assuming similar sentences share summary patterns. The Rerank module measures the informativeness of these templates using a BiRNN encoder, selecting the most relevant one. The Rewrite module generates summaries by combining the hidden states of the input sentence and the selected template, leveraging the seq2seq model's rewriting ability to ensure faithfulness and informativeness.\",\n  \n  \"Experimental Setup and Evaluation Metrics\": \"Experiments were conducted on the Annotated English Gigaword corpus, using ROUGE as the evaluation metric. ROUGE measures summary quality by computing overlapping lexical units between candidate and actual summaries. Additional metrics include length difference, copy ratio, and the presence of new named entities, which assess readability, informativeness, and faithfulness. The seq2seq framework Open-NMT was used as a baseline for comparison.\",\n  \n  \"Results and Analysis\": \"Re3Sum outperformed state-of-the-art seq2seq models in terms of informativeness and readability. The system demonstrated stability and reduced the copy ratio, indicating a shift from mere compression to abstraction. The Rerank module effectively improved template selection, and the Rewrite module ensured summaries were faithful to the input. The model also showed potential for generating diverse summaries, a valuable trait for practical applications.\",\n  \n  \"Future Work and Conclusion\": \"The paper concludes that introducing soft templates as additional input can guide seq2seq summarization effectively. Future work includes improving the Retrieve module and testing the system on other tasks like document-level summarization. The model's ability to generate diverse and informative summaries suggests promising applications in various domains.\"\n}\n```",
    "```json\n{\n  \"Introduction to Automatic Evaluation in Machine Translation\": \"To determine system performance in machine translation (MT), it is often more practical to use an automatic evaluation, rather than a manual one. Manual/human evaluation can be costly and time-consuming, and so an automatic evaluation metric, given that it sufficiently correlates with manual evaluation, can be useful in developmental cycles. In studies involving hyperparameter tuning or architecture search, automatic metrics are necessary as the amount of human effort implicated in manual evaluation is generally prohibitively large. As objective, reproducible quantities, metrics can also facilitate cross-paper comparisons. The WMT Metrics Shared Task annually serves as a venue to validate the use of existing metrics (including baselines such as BLEU), and to develop new ones.\",\n  \"Task Setup and Language Pairs\": \"This year, we provided task participants with one test set for each examined language pair, i.e., a set of source texts, corresponding MT outputs, and a reference translation. In the system-level, metrics aim to correlate with a system's score which is an average over many human judgments of segment translation quality produced by the given system. Participants were free to choose which language pairs and tracks they wanted to take part in. The source, reference texts, and MT system outputs for the Metrics task come from the News Translation Task. The texts involve translations of English to/from Czech, German, Finnish, Gujarati, Kazakh, Lithuanian, Russian, and Chinese, but excluding cs-en (15 language pairs).\",\n  \"Manual Quality Assessment and Golden Truth\": \"Direct Assessment (DA) was employed as the source of the 'golden truth' to evaluate metrics again this year. The basis of DA is to collect a large number of quality assessments for the outputs of all MT systems. These scores are then standardized per annotator. This year, the official WMT19 scores are reference-based for some language pairs and reference-free for others. System-level manual scores are different than those of the segment-level. For segment-level evaluation, collecting enough DA judgments for each segment is infeasible, so we resort to converting DA judgments to relative rankings.\",\n  \"Baseline Metrics and Their Implementation\": \"In addition to validating popular metrics, including baselines metrics serves as comparison and prevents 'loss of knowledge'. Moses scorer is one of the MT evaluation tools that aggregated several useful metrics over time. The baselines we report are: BLEU and NIST, computed using mteval-v13a.pl, and TER, WER, PER, and CDER, produced by the Moses scorer. The baselines serve in system and segment-level evaluations as customary: BLEU, TER, WER, PER, CDER, sacreBLEU-BLEU, and sacreBLEU-chrF for system-level only; sentBLEU for segment-level only and chrF for both.\",\n  \"Submitted Metrics and Their Characteristics\": \"The rest of this section provides a brief summary of all the metrics that participated. BEER is a trained evaluation metric with a linear model that combines sub-word feature indicators and global word order features. BERTr uses contextual word embeddings to compare the MT output with the reference translation. CharacTER is a character-level metric inspired by the commonly applied translation edit rate (TER). EED is a character-based metric, which builds upon CDER. ESIM is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. hLEPOR_baseline is a metric based on the factor combination of length penalty, precision, recall, and position difference penalty.\",\n  \"System-Level Evaluation and Results\": \"As in previous years, we employ the Pearson correlation as the main evaluation measure for system-level metrics. The Pearson correlation is calculated between human assessment scores and the corresponding scores as predicted by a given metric. System-level results show that the series of YiSi metrics achieve the highest correlations in several language pairs and are not significantly outperformed by any other metrics for almost all language pairs. The new metric ESIM performs best on 5 language languages and obtains 11 'wins' out of 16 language pairs in which ESIM participated.\",\n  \"Segment-Level Evaluation and Results\": \"Segment-level evaluation relies on the manual judgments collected in the News Translation Task evaluation. We measure the quality of metrics' segment-level scores against the daRR golden truth using a Kendall's Tau-like formulation. Results of the segment-level human evaluation for translations sampled from the News Translation Task show that YiSi-1_srl stands out as the 'winner' for all language pairs in which it participated. ESIM obtains 6 'winners' out of all 18 languages pairs. Both YiSi and ESIM are based on neural networks, confirming a trend observed last year.\",\n  \"Discussion on Metric Performance and Stability\": \"This year, human data was collected from reference-based evaluations and reference-free evaluations. The observed performance of metrics depends on the underlying texts and systems that participate in the News Translation Task. For the strongest MT systems, distinguishing which system outputs are better is hard, even for human assessors. If the systems are spread across a wide performance range, it will be easier for metrics to correlate with human judgments. Our analysis reveals weak correlation in comparing top state-of-the-art systems in several language pairs. Metrics and QE systems such as BERTr, ESIM, YiSi that perform well at judging our best systems often use more semantic features compared to our n-gram/char-gram based baselines.\"\n}\n```",
    "```json\n{\n  \"Introduction to Neural Machine Translation and Coverage Issues\": \"Neural machine translation (NMT) has emerged as a successful paradigm in recent years, offering more fluent translations compared to previous statistical systems. However, adequacy remains a concern, with common issues such as dropping source words and repeating words in the generated translation. Various approaches have been proposed to address these issues, including coverage and length penalties during beam search, coverage vectors to track attention history, gating architectures, adaptive attention, and reconstruction loss. Despite these efforts, challenges persist, prompting the exploration of new solutions.\",\n  \"Proposed Solution: Constrained Sparsemax Transformation\": \"In this paper, we propose a novel solution that modifies the attention transformation without altering the overall architecture. We introduce the constrained sparsemax transformation, which combines the benefits of promoting attention sparsity and bounding the attention a word can receive. This transformation is end-to-end differentiable, allowing for training with gradient backpropagation. Our contributions include formulating constrained sparsemax, deriving efficient algorithms for forward and backward propagation, and providing a detailed empirical comparison of various attention transformations.\",\n  \"Sparse and Constrained Attention Mechanisms\": \"We explore alternatives to the traditional softmax attention mechanism, which can be wasteful and lead to repeated attention on the same source words. The sparsemax transformation projects scores onto the probability simplex, resulting in sparse distributions. The constrained softmax transformation bounds attention probabilities using fertility values, leading to sparsity over time steps. We propose the constrained sparsemax transformation, which combines these properties to provide both sparse and bounded probabilities, enhancing the interpretability and efficiency of attention mechanisms.\",\n  \"Fertility Bounds and Exhaustion Strategies\": \"We experiment with different strategies for setting fertility bounds, including CONSTANT, GUIDED, and PREDICTED methods. The GUIDED strategy uses a word aligner to set fertilities based on observed values, while the PREDICTED strategy uses model predictions. To prevent missing source words, we implement an exhaustion strategy that encourages attention to words with larger remaining credit. This involves adjusting pre-attention scores to favor words that have not exhausted their fertility, ensuring comprehensive coverage of source words.\",\n  \"Experimental Setup and Evaluation Metrics\": \"Our experiments focus on small datasets, which are more prone to coverage mistakes. We evaluate our attention transformations on language pairs using datasets like IWSLT 2014 for DE-EN, KFTT for JA-EN, and WMT 2016 for RO-EN. We use tokenized BLEU and METEOR as evaluation metrics, along with new metrics like REP-score and DROP-score to assess over and under-translation. Our implementation is based on the OpenNMT-py toolkit, and we compare our methods against baseline models using softmax attention and coverage models.\",\n  \"Results and Analysis of Attention Transformations\": \"The results indicate that sparse models, including csparsemax and sparsemax combined with coverage models, achieve higher scores on BLEU and METEOR compared to csoftmax and softmax. These models also perform better on REP and DROP scores, suggesting that sparse attention helps alleviate coverage issues. The analysis shows that csparsemax effectively reduces repetitions by providing a sparse set of alignments, as demonstrated in attention maps and translation examples.\",\n  \"Comparison of Fertility Strategies\": \"In our experiments on the DE-EN language pair, the PREDICTED fertility strategy outperforms CONSTANT and GUIDED strategies in terms of BLEU and METEOR scores. This suggests that predicting fertilities based on model insights leads to more accurate translations. The comparison highlights the importance of selecting appropriate fertility strategies to optimize translation quality and coverage.\",\n  \"Conclusions and Future Directions\": \"We propose a new approach to address the coverage problem in NMT by replacing the softmax attentional transformation with sparse and constrained alternatives. Our newly proposed constrained sparsemax transformation, along with efficient algorithms for its implementation, demonstrates improved translation quality by avoiding repeated words and providing sparse alignments. Future directions include further exploration of fertility prediction models and extending the approach to larger datasets and diverse language pairs.\"\n}\n```",
    "```json\n{\n  \"Introduction to the Split-and-Rephrase Task\": \"The Split-and-Rephrase task involves breaking down complex sentences into simpler ones while retaining the original meaning. Narayan et al. (2017) introduced a dataset and evaluation method for this task, achieving a BLEU score of 48.9 for text-to-text systems and 78.7 for RDF-aware systems. The task is challenging as it requires models to split and rephrase sentences without losing information.\",\n  \"Challenges in Processing Complex Sentences\": \"Processing long, complex sentences is difficult for both humans and NLP systems. This complexity affects tasks like parsing and machine translation. An automatic system that can split complex sentences into simpler ones is desirable, but current models often memorize facts rather than truly understanding and rephrasing the content.\",\n  \"Evaluation of SEQ2SEQ Models\": \"Vanilla SEQ2SEQ models with attention have been used to tackle the Split-and-Rephrase task, achieving a BLEU score of 77.5, outperforming previous text-to-text baselines. However, these models often produce unsupported facts, repeat information, or omit details, indicating a tendency to memorize rather than learn to split and rephrase.\",\n  \"Issues with Current Models and Data Splits\": \"Current models, particularly SEQ2SEQ, tend to memorize entity-fact pairs due to the overlap in training and test data. This results in high BLEU scores but poor generalization. The original data split allows for 'cheating' by memorization, as 99% of simple sentences in validation and test sets appear in the training set.\",\n  \"Proposed New Data Split for Better Generalization\": \"A new data split is proposed to improve generalization by ensuring that RDF triplets are unique to each split. This approach prevents models from memorizing facts and encourages learning to identify and rephrase relations. The new split maintains the original 80%/10%/10% division but reduces simple sentence overlap.\",\n  \"Copy-Augmented SEQ2SEQ Models\": \"To address the limitations of vanilla SEQ2SEQ models, a copy mechanism is introduced. This mechanism helps models better handle the Split-and-Rephrase task by allowing them to copy relevant parts of the input sentence. The copy-augmented models show improved performance, particularly in reducing unsupported and repeated facts.\",\n  \"Experimental Results and Analysis\": \"Experiments reveal that copy-augmented models outperform vanilla SEQ2SEQ models, especially on the new data split. The analysis shows that these models distribute attention more evenly across input symbols, reducing the number of unsupported and repeated facts. However, challenges remain, as models still struggle with the new split.\",\n  \"Conclusions and Future Directions\": \"The study demonstrates that while SEQ2SEQ models can achieve high scores, they often fail to truly learn the Split-and-Rephrase task. A new data split and copy-augmented models provide a more robust baseline, but the task remains unsolved. Future research should focus on the new data split or the updated dataset version 1.0 to address these challenges.\"\n}\n```",
    "```json\n{\n  \"Introduction to Retraction Citations\": \"Studies on retracted articles show that the amount of retracted articles has increased in relative measure to the overall increase in scientific publications. Although retracting articles helps purge the scientific literature of erroneous or unethical research, citations to such research present a real challenge. There are two types of retraction citations; citations that a retracted article received prior to its retraction and citations that are received post retraction and despite retraction notices. Both types of citations put the scientific process in jeopardy, especially when they are cited as legitimate references to previous work.\",\n  \n  \"Data Collection and Methodology\": \"ScienceDirect, Elsevier's full text database was accessed in October 2014. The database was queried for the term 'RETRACTED' in the article title and its retraction notice. A total of 1,203 results were retrieved from which 988 were retracted articles. For this study, we selected the five top articles that were cited most (more than 20 times) since 2015. Each citing document was inspected to identify the precise mention of the retracted article within the text. Each mention was categorized as positive, negative, or neutral.\",\n  \n  \"Case Study: Donmez et al. Article\": \"This article was published in 2010 in Cell and retracted in 2014 due to irregularities in graphs and data misrepresentation in the images. Although the graphs and images did not have any bearing on the validity of the results, the editors stated that the level of care in figure preparation fell below the standard expected. Our results show that the citations are mostly positive. One negative mention was found in a letter to the editor of Journal of Korean Medical Science, highlighting how altered graphics are causing bias in the biomedical field.\",\n  \n  \"Case Study: S\u00e9ralini Affair\": \"This article, published in 2012, was the subject of a debate surrounding the validity of the findings, use of animals, and accusations of fraud. The study described a 2-year study of rats fed genetically modified crops, showing increased tumors. Despite accusations, the article was retracted due to the low number of animals used. Post-retraction citations are divided, with more negative citations pointing to the media frenzy around the results. The study was republished in 2014, receiving mostly positive mentions.\",\n  \n  \"Case Study: Mukherjee Article\": \"The leading author, Dipak Das, and his lab were investigated for scientific misconduct and data manipulation. This article was retracted in 2014 due to serious data manipulation and falsification. Despite a detailed retraction notice, the article continued to be cited positively, with findings used to support other studies. The popularity of the subject 'authentic leadership' in management studies could explain the positive citations.\",\n  \n  \"Case Study: Walumbwa Article\": \"This article, published in 1999, was retracted due to an identical version published 2 years earlier. The authors re-used data they already published. Despite being retracted, the article continues to be cited positively. A content analysis of recent citations shows it is mostly referred to in a positive context or as a legitimate piece in the literature.\",\n  \n  \"Discussion on Continuous Citations\": \"Retracted articles continue to be cited years after retraction and despite retraction notices. In some cases, continuous citation rates could be due to public or media interest. For example, the S\u00e9ralini article evoked a public debate on GM foods. The Mukherjee article's continuing citations could be due to public interest in resveratrol. The Walumbwa article's topic of 'authentic leadership' is popular in management studies. In other cases, the reason for retraction does not deter citations, such as the Donmez article, where the results were not refuted.\",\n  \n  \"Recommendations for Publishers\": \"We recommend that publishers use reference checks to detect citations of retracted articles and remove them or request an explanation from authors. This explanation should appear in the paper. Additionally, retracted articles should be deleted from publishers' websites. While leaving a retraction notice, the article itself should not appear on platforms such as ScienceDirect. This would make it difficult for authors to download, read, and cite retracted articles.\"\n}\n```",
    "```json\n{\n  \"Introduction to Speech-to-Text Translation for Low-Resource Languages\": \"Speech-to-text Translation (ST) has numerous potential applications for low-resource languages, such as language documentation and crisis relief. Traditional ST systems require transcribed source audio and parallel text, which are often unavailable for low-resource languages. End-to-end ST is appealing in these scenarios, as it can leverage source language audio paired with target language text translations. Recent studies have shown that end-to-end ST can be effective, but it still requires substantial resources. Leveraging data from high-resource languages could improve low-resource ST performance.\",\n  \n  \"Challenges in Traditional Speech-to-Text Systems\": \"Traditional ST systems rely on a pipeline of automatic speech recognition (ASR) and machine translation (MT), necessitating transcribed source audio and parallel text. These resources are often unavailable for low-resource languages, making it challenging to train effective ST systems. Even with end-to-end ST, substantial amounts of translated audio are required for training, which is a significant hurdle in low-resource settings.\",\n  \n  \"Leveraging High-Resource Language Data for Low-Resource ST\": \"To improve low-resource ST, leveraging data from high-resource languages is a promising approach. For ASR, training a single model on multiple languages can be effective. For MT, transfer learning has been successful, where a model pre-trained on a high-resource language pair is fine-tuned for a low-resource pair. This study shows that pre-training a model on high-resource ASR tasks and fine-tuning it for low-resource ST can yield significant improvements.\",\n  \n  \"Methodology: Encoder-Decoder Model with Attention\": \"The study employs an encoder-decoder model with attention, adapted from previous works. The model architecture is consistent across ASR and ST tasks, facilitating parameter transfer. Pre-trained ASR models are used to initialize ST models, with all model parameters updated during ST training. The decoder shares vocabulary across tasks when the target text language is high-resource.\",\n  \n  \"Experimental Setup and Data Sets\": \"The study uses various datasets, including the Switchboard Telephone speech corpus for English ASR, the GlobalPhone collection for French ASR, and the Fisher Spanish speech corpus for Spanish-English ST. To simulate low-resource conditions, smaller training corpora are constructed. The Mboshi-French parallel corpus is used for Mboshi-French ST. Speech preprocessing involves converting raw input to MFCCs, and text is modeled using byte pair encoding.\",\n  \n  \"Evaluation Metrics and Baselines\": \"BLEU scores are reported for all models, though they can be difficult to interpret in low-resource settings. Word-level unigram precision and recall are also reported using METEOR, which considers stem, synonym, and paraphrase matches. Naive baselines predict the most frequent words as a bag of words for each test utterance, providing an empirical lower bound on precision and recall.\",\n  \n  \"Results and Analysis of Transfer Learning\": \"Transfer learning significantly improves ST performance, especially in very low-resource settings. For example, fine-tuning an English ASR model for Spanish-English ST improves BLEU scores and precision/recall. Transfer learning allows models trained on minimal ST data to achieve results comparable to those trained on much larger datasets. The main benefit comes from transferring encoder parameters, which capture language-independent acoustic features.\",\n  \n  \"Conclusion and Future Directions\": \"The study demonstrates that pre-training on high-resource ASR data can substantially improve low-resource ST performance. The pre-trained models train faster and achieve higher BLEU scores than baselines. Future work could explore multilingual training, pre-trained multilingual features, and alternative language modeling strategies to further enhance low-resource ST systems.\"\n}\n```",
    "```json\n{\n  \"Introduction to Translation and Its Historical Context\": \"Translation has played a major role in human civilization since the rise of law, religion, and trade in multilingual societies. Evidence of scribe translations goes as far back as four millennia ago, to the time of Hammurabi; this practice is also mentioned in the Bible. For thousands of years, translators have tried to remain invisible, setting a standard according to which the act of translation should be seamless, and its product should look as if it were written originally in the target language. Cicero commented on his translation ethics, 'I did not hold it necessary to render word for word, but I preserved the general style and force of the language.' These words were echoed by St. Jerome, who wrote, 'I render, not word for word, but sense for sense.'\",\n  \"Translation Universals and Interference Phenomena\": \"Despite the continuous efforts of translators, translations are known to feature unique characteristics that set them apart from non-translated texts. This is not the result of poor translation, but rather a statistical phenomenon: various features distribute differently in originals than in translations. Several factors may account for the differences between originals and translations; many are classified as universal features of translation. Such universals include simplification, standardization, and explicitation. In contrast, interference reflects the 'fingerprints' of the source language on the translation product, defined as phenomena pertaining to the make-up of the source text that tend to be transferred to the target text.\",\n  \"Hypotheses on Language Distance and Phylogenetic Trees\": \"Our main hypothesis is that, due to interference, languages with shared isomorphic structures are likely to share more features in the target language of a translation. Consequently, the distance between two languages, when assessed using such features, can be retained to some extent in translations from these two languages to a third one. Furthermore, we hypothesize that by extracting structures from translated texts, we can generate a phylogenetic tree that reflects the 'true' distances among the source languages. Finally, we conjecture that the quality of such trees will improve when constructed using features that better correspond to interference phenomena.\",\n  \"Methodology and Dataset Description\": \"This corpus-based study uses Europarl, the proceedings of the European Parliament and their translations into all the official European Union languages. We use a version of Europarl spanning the years 1999 through 2011, in which the direction of translation has been established through a comprehensive cross-lingual validation of the speakers' original language. All parliament speeches were translated from the original language into all other EU languages using English as an intermediate, pivot language. We focus on 17 source languages, grouped into 3 language families: Germanic, Romance, and Balto-Slavic.\",\n  \"Feature Sets for Translation Analysis\": \"Following standard practice, we represented both original and translated texts as feature vectors, where the choice of features determines the extent to which we expect source-language interference to be present in the translation product. We use the following feature sets: the top-1,000 most frequent POS trigrams, function words, and cohesive markers. The first two feature sets are strongly associated with interference, whereas the third is assumed to be universal and an instance of explicitation.\",\n  \"Evaluation of Phylogenetic Trees\": \"To evaluate the quality of the reconstructed trees, we define a metric to accurately assess their distance from the 'true' tree. We reconstructed phylogenetic language trees by performing agglomerative clustering of feature vectors extracted separately from English and French translations. We performed clustering using the variance minimization algorithm with Euclidean distance. Evaluation results reveal that POS-trigrams induce trees closest to the gold standard among distinct feature sets, corroborating our hypothesis that this feature set carries over interference of the source language to a considerable extent.\",\n  \"Analysis of Linguistic Phenomena in Translations\": \"We demonstrated that source-language traces are dominant in translation products to an extent that facilitates reconstruction of the history of the source languages. For example, languages vary greatly in their use of articles, possessive constructions, verb-particle constructions, and tense and aspect. We computed the frequencies of patterns that reflect these phenomena in texts translated to English from each individual language, and averaged the measures over each language family. The results show significant overuse of 'the' in translations from Balto-Slavic languages, and some overuse in translations from Romance languages.\",\n  \"Conclusions and Implications for Language Typology\": \"Translations may be considered distortions of the original text, but this distortion is far from random. It depicts a very clear picture, reflecting language typology to the extent that disregarding the sources altogether, a phylogenetic tree can be reconstructed from a monolingual corpus consisting of multiple translations. This holds for the product of highly professional translators, who conform to a common standard, and whose products are edited by native speakers. Remarkably, translations to contemporary English and French capture part of the millennium-old history of the source languages from which the translations were made.\"\n}\n```",
    "```json\n{\n  \"Introduction to Text Coherence\": \"Coherence plays an important role in a text because it enables a text to convey its communication purpose and meaning to its readers. Coherence also decreases reading time as a more coherent text is easier to read with less reader's cognitive load. A text is coherent if it can convey its communication purpose and meaning to its readers, is integrated as a whole, and every sentence has relation(s) to each other. Text coherence is greatly influenced by the presence of a certain organisation in the text, which helps readers anticipate upcoming information.\",\n  \"Existing Coherence Models and Their Limitations\": \"There are studies on computational modelling of text coherence based on the supervised learning approach, such as the Entity Grid model. However, these models have a few drawbacks, including a bias towards the original ordering of text and poor performance on ill-organised texts. The supervised model often suffers from data sparsity, domain dependence, and computational cost for training. To alleviate these problems, Guinaudeau and Strube proposed an unsupervised coherence model known as the Entity Graph model, which assumes coherence is achieved by local cohesion.\",\n  \"Proposed Unsupervised Text Coherence Model\": \"To tackle the limitations of past research, we present an unsupervised text coherence model that captures the contribution of related-yet-not-identical entities. Our proposed method employs an unsupervised learning approach, which suffers less from data sparsity, domain dependence, and computational cost for training. We encode a text into a graph where vertices represent sentences and edges represent semantic relations between sentences. This approach allows us to measure text coherence from a semantic similarity perspective.\",\n  \"Semantic Similarity Graph Construction\": \"We propose three methods for constructing a graph from a text based on semantic similarity between sentence pairs: Preceding Adjacent Vertex (PAV), Single Similar Vertex (SSV), and Multiple Similar Vertex (MSV). The PAV method mimics the reading process by establishing edges based on similarity with preceding sentences. The SSV method establishes edges based on semantic similarity without constraints on direction. The MSV method allows multiple outgoing edges based on a similarity threshold. These methods aim to capture semantic dependencies among sentences.\",\n  \"Evaluation Methodologies\": \"We evaluate the proposed methods on two experimental tasks: the document discrimination task and the insertion task. In the document discrimination task, sentences in a text are randomly permutated to generate another text, and the task is to identify the original text. The insertion task evaluates the coherence measure based on estimating the original sentence position in a text. The performance of the proposed methods is compared with reimplementations of the Entity Grid and Entity Graph models.\",\n  \"Document Discrimination Task Results\": \"In the document discrimination task, the Entity Grid without saliency performed the best, followed by Entity Grid with saliency, PAV, MSV, Entity Graph, and SSV. The PAV and MSV methods performed better than the Entity Graph, suggesting that semantic similarity between sentences should be considered. The PAV-MSV pair shared the largest number of the same judgements, indicating that local coherence is sufficient for this task.\",\n  \"Insertion Task Results\": \"In the insertion task, the PAV-based graph construction method outperformed both baselines: Entity Grid and Entity Graph. The PAV method obtained the best performance at \u03b1 = 0.0, while MSV performed best at \u03b8 = 0.8. However, the difference in accuracy between methods was not statistically significant, likely due to the limited size of the insertion data. The analysis suggests that the SSV and MSV methods could capture distant sentence relations better than the PAV method.\",\n  \"Conclusion and Future Directions\": \"This paper presented three novel unsupervised text coherence scoring methods, with text coherence regarded as cohesion of sentences represented in a graph structure. The PAV method outperformed the unsupervised baseline in the document discrimination task but not the supervised baseline. In the insertion task, the PAV method outperformed both baselines, but the difference was not statistically significant. Further experiments with larger data sizes are necessary. The error analysis suggests improving sentence vector representation and similarity calculation to capture distant relations between sentences.\"\n}\n```",
    "```json\n{\n  \"Introduction to BioASQ Challenge and Task B\": \"BioASQ is a semantic indexing, question answering (QA), and information extraction challenge. We participated in Task B, which focuses on biomedical QA, specifically in Phase B. This phase involves returning a list of answer candidates given a question and gold-standard snippets. The fifth BioASQ challenge consists of five batches of 100 questions each, released every two weeks. Our work concentrates on answering factoid and list questions, evaluated using mean reciprocal rank (MRR) for factoid questions and F1 score for list questions.\",\n  \n  \"Traditional vs. Neural Network QA Systems\": \"Most existing biomedical QA systems use a traditional QA pipeline with discrete steps like named-entity recognition and question classification, requiring extensive resources and feature engineering. In contrast, our system is based on a neural network QA architecture trained end-to-end on the target task. We build upon FastQA, an extractive factoid QA system, and train it on the SQuAD dataset before fine-tuning on the BioASQ training set. This approach restricts responses to substrings in the provided snippets and generalizes the output layer to handle list questions.\",\n  \n  \"System Architecture and Modifications\": \"Our system is a neural network that takes a question and context as input and outputs start and end pointers to tokens in the context. We use FastQA with modified input vectors and an output layer supporting list answers. The input layer maps context and question tokens to high-dimensional word vectors, including GloVe embeddings, character embeddings, and biomedical Word2Vec embeddings. We concatenate a one-hot encoding of the question type to these vectors. The output layer computes start, end, and span probabilities, enabling the model to output multiple spans for list questions.\",\n  \n  \"Training Methodology and Dataset Preparation\": \"We train the network in two steps: pre-training on SQuAD and fine-tuning on BioASQ. The Adam optimizer with an exponentially decaying learning rate is used, starting with learning rates of 10^-3 and 10^-4 for pre-training and fine-tuning, respectively. During fine-tuning, we extract answer spans from BioASQ training data by identifying occurrences of the gold standard answer in snippets. This approach can produce false positives and negatives, so we process all snippets independently and aggregate answer spans based on their probability.\",\n  \n  \"Decoding and Answer Selection Process\": \"During inference, we retrieve the top 20 answer spans via beam search with a beam size of 20. Duplicate strings are removed from this sorted list. For factoid questions, we output the top five answer strings as a ranked list. For list questions, we use a probability cutoff threshold optimized for the list F1 score on the development set. This process ensures that the most probable answer spans are selected for each question type.\",\n  \n  \"Ensemble Model and Performance Optimization\": \"To enhance performance, we built a model ensemble by training five single models using 5-fold cross-validation. These models are combined by averaging their start and end scores before computing span probabilities. We submit the best single model and the ensemble model to the challenge. The ensemble model generally performs slightly better, especially on factoid questions, where it won three out of five batches.\",\n  \n  \"Results and Discussion on BioASQ 5B\": \"We report results for all five test batches of BioASQ 5 (Task 5b, Phase B). The single and ensemble systems perform similarly, with ranks often next to each other. The ensemble model performs slightly better on average. Our system excels in factoid questions, winning three out of five batches, but performance on list questions varies. The pre-training dataset (SQuAD) lacks list questions, affecting performance. We also submitted yes/no question responses as a baseline, but these results are not discussed due to the method's simplicity.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Graphs and Relation Prediction\": \"Semantic graphs, such as WordNet, encode the structural qualities of language as a representation of human knowledge. They describe connections between specific semantic concepts through individual edges representing relations like hypernymy or meronymy. While local properties have been extensively studied for relation prediction using distributional methods, global properties have rarely been used in prediction settings. This paper shows how global semantic graph features can facilitate local tasks such as relation prediction, hypothesizing that features extracted from the entire graph can help constrain local predictions to structurally sound ones.\",\n  \n  \"Global Graph Features in Local Relation Prediction\": \"Global semantic graph features can facilitate local tasks such as relation prediction. Features are often manifested as aggregate counts of small subgraph structures, known as motifs. For example, the number of nodes with two or more outgoing edges or the number of cycles of length 3 can affect predictions. The paper examines the effect of global graph properties on the link structure via the WordNet relation prediction task, showing that features from the entire graph can help constrain local predictions to structurally sound ones.\",\n  \n  \"Exponential Random Graph Models and Their Limitations\": \"The Exponential Random Graph Model (ERGM) is a log-linear model over networks utilizing global graph features. In ERGMs, the likelihood of a graph is computed by exponentiating a weighted sum of the features and then normalizing over all possible graphs. This normalization term grows exponentially in the number of nodes, making it difficult to fit ERGMs on large graphs like WordNet. Approximations are necessary, and the largest known ERGMs scale only to thousands of nodes, which is insufficient for WordNet.\",\n  \n  \"Max-Margin Markov Graph Model (M3GM) Framework\": \"The Max-Margin Markov Graph Model (M3GM) extends the ERGM framework by replacing the maximum likelihood objective with a margin-based objective. This model compares the observed network against alternative networks, using a local relational model as a proposal distribution. The complexity of each epoch of estimation is linear in the number of edges, allowing it to scale up to the 10^5 nodes in WordNet. M3GM incorporates a combinatorial set of labeled motifs and links graph-level relational features with distributional information.\",\n  \n  \"Graph Motifs as Features in Semantic Graphs\": \"Graph motifs are small subgraph structures used as features in semantic graphs. Common features include total edge count, number of cycles of length k, number of nodes with exactly k outgoing edges, and transitivity. Semantic graphs are multigraphs with multiple relationships overlaid atop a common set of nodes. For each relation, features are extracted for all possible combinations of relation types existing in the graph, scaling the number of features significantly.\",\n  \n  \"Parameter Estimation and Proposal Distribution\": \"Parameter estimation in ERGMs involves approximating the normalization term that sums over all possible graphs. The Monte Carlo Maximum Likelihood Estimation (MCMLE) approach is used, where a large number of graphs is randomly generated. The proposal distribution for sampling negative edges is defined to be proportional to the local association scores of edges not present in the training graph. This helps push the M3GM parameters away from likely false positives.\",\n  \n  \"Relation Prediction and Evaluation on WN18RR Dataset\": \"The WN18RR dataset is a popular relation prediction dataset for WordNet, containing 18 relations for about 41,000 synsets. It addresses leakage issues in the original WN18 dataset by removing certain relations. The evaluation metrics include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at k (H@k). The M3GM model is applied as a re-ranker, showing improvements in performance on all commonly-used metrics.\",\n  \n  \"Analysis of M3GM's Impact on Relation Prediction\": \"The M3GM model shows a clear advantage in re-ranking top local candidates using the score signal from the model. The results are further improved when the graph score is weighted against the association component per relation. The model captures interesting inter-relational properties, such as the preference for a broad scattering of targets for certain relations and the undesirability of multiple unique hypernyms. The analysis suggests that adding graph motifs which qualify their adjacent nodes in terms of syntactic function or semantic category may prove useful.\"\n}\n```",
    "```json\n{\n  \"Introduction to Complaints in Natural Language\": \"Complaining is a basic speech act used to express a negative mismatch between reality and expectations towards a state of affairs, product, organization, or event. Understanding the expression of complaints in natural language and automatically identifying them is crucial for linguists, psychologists, organizations, and the development of NLP applications. Complaints are distinctly different from expressing negative sentiment, focusing on the breach of expectations. This paper presents the first extensive analysis of complaints in computational linguistics, including a publicly available dataset of complaints from Twitter, a quantitative analysis of linguistic features, and predictive models achieving high performance.\",\n  \"Related Work and Linguistic Background\": \"Complaints have been studied in linguistics and marketing, defined as a breach of expectations. They are considered distinct speech acts and can threaten both positive and negative face. Previous research has examined complaints across cultures and socio-demographic traits, and in various forms of communication. The only previous computational work on complaints defined them differently, focusing on negative connotation rather than breached expectations.\",\n  \"Data Collection and Annotation Process\": \"A new dataset of complaints was created using Twitter as the data source, focusing on tweets directed to customer service accounts. A sampling method was used to increase the hit rate of complaints, and tweets were manually annotated to identify complaints. The annotation achieved a substantial agreement with a Cohen's Kappa of 0.731. The dataset includes 1,232 complaints and 739 non-complaints, with tweets anonymized and preprocessed for analysis.\",\n  \"Linguistic Feature Analysis of Complaints\": \"A quantitative analysis of linguistic features distinctive of complaints was performed using univariate Pearson correlation. Complaints often use negations, possessive pronouns, and temporal references, while avoiding personal pronouns and exclamation marks. Distinctive patterns include possessive pronouns followed by nouns and verbs in past participle. Complaints focus on describing issues rather than expressing sentiment, with question marks being common.\",\n  \"Predictive Modeling of Complaints\": \"Predictive models were built using various features and machine learning methods, achieving high performance. Logistic Regression with Elastic Net regularization and neural methods like MLP and LSTM were used. The best performance was obtained using bag-of-word features, reaching an F1 of up to 77.5. Combining all features boosted accuracy to 78 F1. Neural network approaches were comparable but did not outperform feature-based models.\",\n  \"Distant Supervision and Domain Adaptation\": \"Distant supervision was used to identify extra complaint data, boosting predictive performance. Tweets with complaint-related hashtags were collected and combined with annotated data using domain adaptation. The EasyAdapt algorithm improved F1 by 1 point to 79 and ROC AUC by 0.012. Simply pooling the data hurt performance, highlighting the importance of domain adaptation.\",\n  \"Domain-Specific and Cross-Domain Experiments\": \"Models were tested using in-domain, out-of-domain, and domain adaptation approaches. Adding out-of-domain data generally improved performance, with domain adaptation being beneficial in most cases. Cross-domain experiments showed consistent performance across domains, with some exceptions. The apparel domain showed different features due to the nature of complaints.\",\n  \"Conclusions and Future Work\": \"The study presented a computational approach to modeling complaints, introducing a dataset of annotated Twitter posts. Predictive models achieved up to 79% Macro F1, with domain adaptation improving accuracy. Future work includes identifying complaint targets, using additional context, and studying complaints in personal and political domains. The data and code are available for further research.\"\n}\n```",
    "```json\n{\n  \"Introduction to Semantic Parsing and Challenges\": \"Semantic parsing aims to map language utterances to executable programs. Early work on statistical learning of semantic parsers relied on supervised learning, which required expert annotators to pair language utterances with programs. This approach faced scalability issues due to the need for expert knowledge. Consequently, weakly-supervised semantic parsing emerged, where training examples are utterance-denotation pairs, simplifying data collection but introducing challenges such as search complexity and spuriousness. Spuriousness, where incorrect programs yield correct denotations, is particularly problematic and has been less explored.\",\n  \n  \"Weakly-Supervised Semantic Parsing and Spuriousness\": \"Weakly-supervised semantic parsing involves training with utterance-denotation pairs, which simplifies data collection but complicates training due to the vast search space of programs and the issue of spuriousness. Spuriousness occurs when incorrect programs produce correct denotations, misleading the learning process. This problem has been less addressed in the literature, but the CNLVR dataset provides an opportunity to explore it further. The dataset's binary nature exacerbates spuriousness, as it is easier to generate programs that yield correct denotations by chance.\",\n  \n  \"Cornell Natural Language for Visual Reasoning (CNLVR) Dataset\": \"The CNLVR dataset consists of images with objects of various shapes, colors, and sizes, paired with complex natural language statements. The task is to determine the truthfulness of these statements. The dataset offers two input types: images and knowledge bases (KBs) from which images are synthesized. This setup allows CNLVR to be framed as a semantic parsing problem, where the goal is to translate language utterances into programs executed against the KB to verify correctness. The binary nature of the task makes spuriousness a significant challenge.\",\n  \n  \"Proposed Semantic Parser for CNLVR\": \"We present the first semantic parser for the CNLVR dataset, which involves a lexical task of mapping words to program constants and a structural task of mapping language composition to program operators. Our approach leverages closed-world scenarios with clear semantic types to construct a small lexicon, reducing the lexical problem. By using abstract representations, we can share information across examples, improving search and reducing spuriousness. Our parser achieves an accuracy of 82.5%, a 14.7% improvement over the state-of-the-art.\",\n  \n  \"Abstract Representations and Lexicon Construction\": \"In closed, well-typed domains like visual reasoning, the main challenge is language compositionality. We alleviate the lexical mapping problem by using a compact KB schema and typing system, constructing a small lexicon that maps lexical items to typed program constants. Abstract representations allow us to generalize across examples, reducing search and spuriousness challenges. We define seven abstract clusters corresponding to main semantic types, with 25 mappings used to define abstract representations, improving training and search accuracy.\",\n  \n  \"Model Architecture and Search Techniques\": \"Our model is based on an encoder-decoder architecture with a bi-directional LSTM for encoding utterances and a feed-forward network with attention for decoding. We address the search challenge by using beam search and semantic typing to construct only syntactically valid programs, significantly pruning the search space. A globally-normalized re-ranker scores programs in the final beam, addressing the label bias problem. This approach ensures that only promising program prefixes are considered, improving search efficiency and accuracy.\",\n  \n  \"Training from Weak Supervision and Data Augmentation\": \"Training from weak supervision involves treating programs as latent variables and approximately marginalizing them. We redefine training examples to include multiple KBs, reducing spuriousness by requiring correct denotations across all KBs. Abstract examples are used to generate new training data, improving model coverage and accuracy. Data augmentation involves generating non-abstract utterance-program pairs from abstract examples, allowing the model to generalize better. This approach, combined with caching abstract examples, combats search and spuriousness challenges.\",\n  \n  \"Experimental Evaluation and Results\": \"Our weakly-supervised semantic parser with re-ranking achieves 84.0% accuracy and 65.0% consistency on the public test set, and 82.5% accuracy on the hidden test set, a 14.7-point improvement over the state-of-the-art. The rule-based parser provides a reasonable baseline, while the supervised parser trained on augmented data shows better performance. Our approach significantly improves over the supervised parser, demonstrating the effectiveness of abstract examples and data augmentation. Error analysis reveals challenges with complex utterances and highlights the model's strengths in handling spatial relations.\"\n}\n```",
    "```json\n{\n  \"Introduction and Objectives of the BioASQ Challenge 2017\": \"The aim of this paper is twofold. First, we aim to give an overview of the data issued during the BioASQ challenge in 2017. In addition, we aim to present the systems that participated in the challenge and evaluate their performance. To achieve these goals, we begin by giving a brief overview of the tasks, which took place from February to May 2017, and the challenge's data.\",\n  \"Overview of the Tasks in the BioASQ Challenge\": \"The challenge comprised three tasks: (1) a large-scale semantic indexing task (Task 5a), (2) a question answering task (Task 5b), and (3) a funding information extraction task (Task 5c), described in more detail in the following sections.\",\n  \"Large-scale Semantic Indexing (Task 5a)\": \"In Task 5a, the goal is to classify documents from the PubMed digital library into concepts of the MeSH hierarchy. New PubMed articles that are not yet annotated by MEDLINE indexers are collected and used as test sets for the evaluation of the participating systems. Articles from all journals were included in the test data sets of task 5a. The performance of each system is calculated using standard flat information retrieval measures, as well as hierarchical ones. An on-line and large-scale scenario was provided, dividing the task into three independent batches of 5 weekly test sets each.\",\n  \"Biomedical Semantic Question Answering (Task 5b)\": \"The goal of Task 5b was to provide a large-scale question answering challenge where the systems had to cope with all the stages of a question answering task for four types of biomedical questions: yes/no, factoid, list, and summary questions. The task comprised two phases: In phase A, BioASQ released 100 questions and participants were asked to respond with relevant elements from specific resources. In phase B, the released questions were enhanced with relevant articles and snippets selected manually, and the participants had to respond with exact answers, as well as with summaries in natural language.\",\n  \"Funding Information Extraction (Task 5c)\": \"Task 5c was introduced for the first time this year and the challenge was to extract grant information from Biomedical articles. Funding information can be very useful to estimate the impact of an agency's funding in the biomedical scientific literature or to identify agencies actively supporting specific research directions. MEDLINE citations are annotated with information about funding from specified agencies. The systems were asked to extract grant information mentioned in the full text, but author-provided information is not necessarily mentioned in the article.\",\n  \"System Approaches and Methodologies\": \"An overview of the systems and their approaches can be seen in Table 4. Systems for which no description was available at the time of writing are omitted. The 'Search system' and its variants were developed as a UIMA-based text and data mining workflow, where different search strategies were adopted to automatically annotate documents with MeSH terms. The 'MZ' systems applied Binary Relevance (BR) classification, using TF-IDF features, and Latent Dirichlet allocation (LDA) models with label frequencies per journal as prior frequencies, using regression for threshold prediction.\",\n  \"Evaluation Metrics and Results\": \"Task 5a: Each of the three batches of task 5a was evaluated independently. The classification performance of the systems was measured using flat and hierarchical evaluation measures. The micro F-measure (MiF) and the Lowest Common Ancestor F-measure (LCA-F) were used to choose the winners for each batch. Task 5b: For phase A, systems were ranked according to the Mean Average Precision (MAP) measure. For phase B, systems were ranked according to accuracy for yes/no questions, mean reciprocal rank (MRR) for factoids, and mean F-measure for list questions. Task 5c: Micro-recall was the evaluation measure used for all three sub-tasks.\",\n  \"Conclusions and Future Directions\": \"In this paper, an overview of the fifth BioASQ challenge is presented. The challenge consisted of three tasks: semantic indexing, question answering, and funding information extraction. Overall, the best systems were able to outperform the strong baselines provided by the organizers. This suggests that advances over the state of the art were achieved through the BioASQ challenge. In future editions of the challenge, we aim to provide even more benchmark data derived from a community-driven acquisition process and design a multi-batch scenario for Task 5c similar to the other tasks.\"\n}\n```",
    "```json\n{\n  \"Introduction to Incremental Domain Adaptation\": \"Incremental domain adaptation, also known as online adaptation, significantly enhances statistical and neural machine translation systems. A typical use case is in computer-aided translation (CAT), where user translations are immediately used as new training examples to adapt the machine translation system to specific documents. This approach is particularly beneficial in collaborative translation scenarios, where the system needs to quickly adapt to in-domain vocabulary. The paper highlights the importance of measuring the speed of lexical acquisition for in-domain vocabulary, proposing metrics to evaluate the responsiveness of adaptation.\",\n  \"Challenges in Evaluating Machine Translation Adaptation\": \"Evaluating machine translation adaptation poses challenges, especially in capturing user experience in collaborative scenarios. Traditional metrics like BLEU and TER focus on string similarity but may not reflect the system's ability to handle domain-specific vocabulary. The paper emphasizes the need for specialized metrics to evaluate vocabulary acquisition, as NMT models often struggle with content words compared to phrase-based MT. The focus is on translation performance concerning content words, setting aside word order and other aspects.\",\n  \"Metrics for Measuring Lexical Acquisition\": \"The paper proposes three metrics to measure vocabulary acquisition: one-shot vocabulary acquisition, zero-shot vocabulary acquisition, and a combined metric. These metrics focus on the recall of target-language content words, allowing automatic computation by comparing translation hypotheses to reference translations. The metrics operate on a set-level, without clipping or alignment, and are designed to evaluate the system's ability to translate content words correctly after observing them once or not at all.\",\n  \"Impact of Fine-Tuning on Vocabulary Recall\": \"Fine-tuning a neural model improves one-shot vocabulary recall but degrades zero-shot vocabulary recall. The paper evaluates several adaptation techniques, finding that fine-tuning all parameters maximizes one-shot acquisition but harms zero-shot recall. By contrast, fine-tuning with group lasso regularization achieves a balance between zero-shot and one-shot vocabulary acquisition, maintaining high corpus-level translation quality. This trade-off highlights the impact of fine-tuning on vocabulary recall in machine translation systems.\",\n  \"Adaptation Techniques and Their Trade-offs\": \"The paper explores various adaptation techniques, including full parameter adaptation, bias adaptation, and group lasso regularization. Each method has trade-offs in terms of vocabulary acquisition and translation quality. Full adaptation is effective but can be infeasible due to the large number of parameters. Bias adaptation introduces a second bias term, while group lasso regularization allows for a fixed number of parameters to be selected after each update. The lasso technique provides a good balance between zero-shot and one-shot recall, making it a promising approach for immediate adaptation.\",\n  \"Experimental Evaluation and Surprising Results\": \"The experimental evaluation reveals surprising results, showing that methods with similar performance on corpus quality metrics like BLEU can differ significantly in zero-shot and one-shot vocabulary acquisition. The lasso technique outperforms others in zero-shot recall, while full adaptation shows high one-shot recall but degrades zero-shot recall. These findings suggest that traditional metrics may not fully capture the effectiveness of adaptation techniques, highlighting the need for specialized metrics to evaluate immediate adaptation performance.\",\n  \"Analysis of Novel Word Adaptation\": \"The analysis focuses on the adaptation of novel words, confirming that the lasso method improves zero-shot recall by learning new combinations of subwords. The results indicate that the improvement in zero-shot recall for lasso comes from learning new subword combinations, while full adaptation shows high one-shot recall. This analysis provides insights into the adaptation behavior of different techniques, emphasizing the importance of evaluating novel word adaptation in machine translation systems.\",\n  \"Conclusions and Future Directions\": \"The paper concludes that while fine-tuning improves corpus-level translation quality, it can negatively impact zero-shot recall of content words, affecting user experience. Existing regularization methods mitigate this effect, but more effective techniques for immediate adaptation are needed. The proposed metrics R0, R1, and R0+1 are valuable for measuring immediate adaptation performance, crucial for adaptive CAT systems. Future work should focus on developing techniques that enhance zero-shot recall without compromising overall translation quality.\"\n}\n```"
]