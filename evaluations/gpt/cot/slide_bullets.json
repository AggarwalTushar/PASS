[
    "Slide Title: Introduction to Similarity Search\nBullet Points:\n- Similarity search, or nearest-neighbor search, finds documents similar to a query document.\n- Core to applications like plagiarism analysis, collaborative filtering, and multimedia retrieval.\n- Semantic hashing offers fast similarity search using binary hashing codes.\n- Existing methods often require two-stage training, leading to suboptimal results.\n\nSlide Title: Challenges in Existing Text Hashing Methods\nBullet Points:\n- Two-stage training procedures can lead to suboptimal local optima.\n- Binary constraints are often handled ad-hoc, losing information during binarization.\n- Separate training stages require additional hyperparameter tuning.\n\nSlide Title: Proposed Neural Architecture for Text Hashing\nBullet Points:\n- Introduces an end-to-end neural architecture for text hashing.\n- Utilizes neural variational inference (NVI) for learning binary latent codes.\n- Joint optimization of inference and generative networks.\n- Incorporates data-dependent noise for balancing rate and distortion.\n\nSlide Title: Related Work and Motivation\nBullet Points:\n- Discrete random variables in deep learning have gained attention.\n- Previous works focus on continuous representations; discrete at sentence/document level is less explored.\n- Our work focuses on learning binary representations for text documents.\n\nSlide Title: Hashing under the NVI Framework\nBullet Points:\n- Approach training of binary latent variables from a generative perspective.\n- Define generative model for encoding and decoding distributions.\n- Use bag-of-words representation and multivariate Bernoulli prior for binary latent variables.\n\nSlide Title: Training with Binary Latent Variables\nBullet Points:\n- Maximize a variational lower bound for parameter estimation.\n- Use straight-through (ST) estimator for gradient estimation.\n- Softmax decoding function for reconstructing input from binary codes.\n\nSlide Title: Injecting Data-dependent Noise\nBullet Points:\n- Use stochastic decoders with Gaussian noise for better regularization.\n- Rate-distortion theory applied to explore trade-offs between rate and distortion.\n- Data-dependent noise provides flexibility for different training observations.\n\nSlide Title: Supervised Hashing and Experimental Setup\nBullet Points:\n- Extend NASH to supervised hashing with a mapping from latent variables to labels.\n- Use datasets like Reuters21578, 20Newsgroups, and TMC for evaluation.\n- Compare with baselines like LSH, S-RBM, and VDSH.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- NASH variants outperform baselines in unsupervised semantic hashing.\n- Noise injection improves robustness and retrieval performance.\n- Supervised NASH-DN-S model shows superior retrieval precision.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- First step towards end-to-end semantic hashing with effective gradient estimation.\n- Neural variational framework with data-dependent noise injection.\n- Demonstrated effectiveness through extensive experiments.",
    "Slide Title: Understanding Human Emotions\nBullet Points:\n- Emotions are complex and challenging to define\n- Ekman's six basic emotions: anger, disgust, fear, guilt, joy, sadness\n- Importance of emotional phrase identification in NLP\n- Extraction of seven emotional statements from a psychological corpus\n\nSlide Title: Related Work in Emotion Analysis\nBullet Points:\n- Development of WORDNET-AFFECT for affective labels\n- Opinion lexicon and semantic orientation models\n- SentiWord-Net for synset classification\n- SenticNet and EmotiNet for emotion detection\n\nSlide Title: Data Preprocessing Framework\nBullet Points:\n- Emotional statements collected from ISEAR database\n- Tokenization into sentences and words\n- Formation of trigrams as Context Windows\n- Use of WordNet Affect for affect word identification\n\nSlide Title: Context Vector Formation\nBullet Points:\n- Mapping Context Windows into vector space\n- Calculation of Affinity Scores using distance metrics\n- Importance of semantic relation in emotion classification\n\nSlide Title: Feature Selection and Analysis\nBullet Points:\n- POS tagging and extraction of Context Windows\n- Calculation of TF and TF-IDF scores\n- Analysis of variance and ranking of Context Windows\n\nSlide Title: Classification Methods and Results\nBullet Points:\n- Use of classifiers: BayesNet, J48, NaiveBayesSimple, DecisionTree\n- Comparison of accuracy using different distance metrics\n- BayesNet and DecisionTree show high accuracy\n\nSlide Title: Affinity Scores and Distance Metrics\nBullet Points:\n- Calculation of affinity scores using Chebyshev, Euclidean, and Hamming distances\n- Impact of distance metrics on classification accuracy\n- DecisionTree performs best with Chebyshev distance\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Successful vector formation and classification of Context Windows\n- Future plans to incorporate more lexicons and associative learning\n- Aim to improve classification rules and accuracy",
    "Slide Title: Introduction to the Scientific Publication Lifecycle\nBullet Points:\n- Key activities: literature review, research work, dissemination of results\n- Challenges faced by inexperienced researchers\n- Assistance from supervisors, experts, and librarians\n- Academic assistive systems to bridge expertise gaps\n\nSlide Title: Assistive Systems in Research\nBullet Points:\n- Search systems with faceted interfaces\n- Bibliometric tools for visualizing citation networks\n- Scientific paper recommender systems\n- Manuscript writing and publication support\n\nSlide Title: Rec4LRW System Overview\nBullet Points:\n- Aimed at literature review and manuscript preparation\n- Tasks: building reading lists, finding similar papers, shortlisting for manuscripts\n- Utilizes ACM Digital Library dataset\n- Beneficial for novice researchers\n\nSlide Title: Unique Features of Rec4LRW\nBullet Points:\n- Displays author-specified keywords, references count, and paper summaries\n- Information cue labels: Popular, Recent, High Reach, Survey/Review\n- Option to view papers in parent clusters\n\nSlide Title: Shortlisting Technique\nBullet Points:\n- Identifies important and unique papers for citation\n- Uses Girvan-Newman algorithm for clustering\n- Considers article-type preferences for shortlisting\n- Varies shortlisted papers by article-type\n\nSlide Title: User Evaluation Study\nBullet Points:\n- Conducted with 116 participants: students and staff\n- Evaluated usefulness and effectiveness of tasks\n- High agreement on importance and shortlisting feature\n- Feedback on preferred and critical aspects\n\nSlide Title: Results and Discussion\nBullet Points:\n- Students showed higher agreement percentages\n- Importance and shortlisting feature highly rated\n- Some reluctance in citing papers due to context\n- Suggestions for improvement in dataset and algorithm\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Rec4LRW addresses gap in manuscript preparation support\n- Validates aim of identifying important papers for citation\n- Future work: expand article-type variations, improve user control\n- Integrate with citation context recommendation tasks",
    "Slide Title: Introduction to NMT Improvements\nBullet Points:\n- Recent advancements in Neural Machine Translation (NMT)\n- Three categories: string-to-string, tree-to-string, string-to-tree systems\n- Tree-based systems offer syntactic information and prior knowledge\n- Focus on linearization methods for tree-based NMT\n\nSlide Title: Challenges in Tree-Based NMT\nBullet Points:\n- Major drawback: reliance on 1-best parse tree\n- Potential translation errors due to parsing mistakes\n- Forest-based methods in SMT address this issue\n- Complexity in linearizing a forest for NMT\n\nSlide Title: Proposed Forest-Based NMT Approach\nBullet Points:\n- Inspired by tree-based NMT linearization methods\n- Efficient forest-based NMT approach introduced\n- Novel weighted linearization method for packed forests\n- Decoding under sequence-to-sequence framework\n\nSlide Title: Preliminaries and Sequence-to-Sequence Model\nBullet Points:\n- Overview of sequence-to-sequence model in NMT\n- Encoding source sequence into context vector\n- Attention mechanism for handling long sequences\n- Adaptation to string-to-string, tree-to-string, string-to-tree systems\n\nSlide Title: Linear-Structured Tree-Based NMT Systems\nBullet Points:\n- Linearization methods for tree-to-string and string-to-tree NMT\n- Utilization of syntax information in encoding and decoding\n- Current systems use only one tree for encoding/decoding\n- Proposal to utilize multiple trees (forest) for better representation\n\nSlide Title: Packed Forest and Linearization\nBullet Points:\n- Packed forest represents exponentially many parsing trees\n- Linearization challenges due to lack of fixed traversal order\n- Proposed algorithm for linearizing packed forests\n- Preservation of word sequential and parent-child information\n\nSlide Title: Encoding the Linearized Forest\nBullet Points:\n- Input of encoder: symbol sequence and score sequence\n- New frameworks: Score-on-Embedding (SoE) and Score-on-Attention (SoA)\n- Incorporation of scores in embedding and attention layers\n- Connection with conventional sequence-to-sequence NMT framework\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Evaluation on English-to-Chinese and English-to-Japanese tasks\n- Forest-based NMT systems outperform string-to-string models\n- Importance of syntactic information and score sequences\n- Packed forests yield better results than 1-best trees\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Comparison with other syntax-based NMT systems\n- Advantages of using linearized packed forests\n- Proposed framework's effectiveness demonstrated\n- Future work: design more elaborate structures for score layer integration",
    "Slide Title: Introduction to Adversarial Attacks in Machine Learning\nBullet Points:\n- Adversarial perturbations alter model inputs to achieve adversarial goals like misclassification.\n- Highlight vulnerabilities and brittleness of models to out-of-distribution samples.\n- Security concerns for safety-critical applications, e.g., self-driving cars.\n- Challenges in applying adversarial attacks to discrete spaces like NLP.\n\nSlide Title: Adversarial Perturbations in NLP\nBullet Points:\n- Adversarial examples in NLP should preserve source meaning but destroy target meaning.\n- Importance of meaning-preserving perturbations for seq2seq models.\n- Evaluation of adversarial attacks using human and automatic metrics.\n\nSlide Title: Framework for Evaluating Adversarial Attacks\nBullet Points:\n- Evaluation includes target similarity measure and source similarity score.\n- Importance of quantifying discrepancies sensitive to meaning.\n- Reporting both source similarity and target relative score decrease.\n\nSlide Title: Similarity Metrics for Evaluation\nBullet Points:\n- Human judgment as the gold standard for semantic similarity.\n- Automatic metrics: BLEU, METEOR, and chrF.\n- chrF shows better correlation with human judgment than BLEU and METEOR.\n\nSlide Title: Gradient-Based Adversarial Attacks\nBullet Points:\n- Gradient-based attacks replace words to maximize adversarial loss.\n- Constraints like kNN and CharSwap to preserve meaning.\n- CharSwap involves character swaps to maintain semantic similarity.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Experiments on IWSLT2016 dataset with LSTM and Transformer models.\n- chrF metric correlates better with human judgment for adversarial attacks.\n- Constrained attacks (kNN, CharSwap) preserve source meaning better.\n\nSlide Title: Adversarial Training with Meaning-Preserving Attacks\nBullet Points:\n- Adversarial training augments data with adversarial examples.\n- CharSwap-adv improves robustness without impacting test performance.\n- Unconstrained-adv may decrease performance on non-adversarial input.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Importance of meaning-preserving adversarial perturbations in NLP.\n- Proposed evaluation framework and comparison of automatic metrics.\n- Encouragement for future research to focus on meaning conservation.",
    "Slide Title: Introduction to Discourse Phenomena in Translation\nBullet Points:\n- Importance of handling discourse phenomena in translation\n- Traditional systems translate sentences in isolation\n- Context-aware NMT models show performance improvements\n- Aim to enhance understanding of discourse phenomena in NMT\n\nSlide Title: Neural Machine Translation (NMT)\nBullet Points:\n- NMT models predict target sentence words sequentially\n- Encoder-decoder structure maps input to distributed representations\n- Attention mechanism focuses on parts of the source sentence\n\nSlide Title: Transformer Architecture\nBullet Points:\n- Proposed by Vaswani et al. (2017)\n- Uses stacked self-attention and fully connected layers\n- More parallelizable and faster to train than recurrent models\n- Strong baseline for NMT experiments\n\nSlide Title: Context-Aware Model Architecture\nBullet Points:\n- Based on Transformer architecture\n- Incorporates context information on the encoder side\n- Uses shared parameters for source and context encoders\n- Special token added to context sentences\n\nSlide Title: Experiments and Data\nBullet Points:\n- Used OpenSubtitles2018 corpus for English and Russian\n- Data cleaning and selection of 2 million training instances\n- Byte-pair encoding for sentence encoding\n- BLEU metric used for performance evaluation\n\nSlide Title: Results and Analysis\nBullet Points:\n- Context-aware model shows genuine improvements\n- Best model uses previous sentence as context\n- Performance drops with shuffled context sentences\n- Model relies on context for translation quality\n\nSlide Title: Analysis of Contextual Information\nBullet Points:\n- Model attends heavily to contextual history for pronoun translation\n- High attention for ambiguous pronouns like \"it\", \"you\", \"I\"\n- Context especially helpful at the beginning of sentences\n- Context-aware model learns latent anaphora resolution\n\nSlide Title: Pronoun Translation and Anaphora Resolution\nBullet Points:\n- Context-aware model improves translation of ambiguous pronouns\n- BLEU scores higher for sentences with co-referential pronouns\n- Model's attention mechanism performs latent anaphora resolution\n- Human evaluation confirms model's effectiveness\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Context-aware NMT system outperforms baselines\n- Improvements prominent for sentences with ambiguous pronouns\n- Model induces anaphora relations through attention mechanism\n- Future work to explore other discourse phenomena in NMT",
    "Slide Title: Introduction to Machine Translation\nBullet Points:\n- Goal: Convert source language utterances into fluent target language utterances with the same meaning.\n- Recent approaches use statistical techniques on parallel data.\n- Unified semantic representations like Abstract Meaning Representation (AMR) are gaining interest.\n- Deeper models allow for more parsimonious translation models.\n\nSlide Title: Recent Advances in Semantic Representation\nBullet Points:\n- AMR is a novel representation of sentential semantics.\n- Deep analyses normalize morphology, syntax, and semantics.\n- Language models using deep representations can model plausible interpretations.\n- Discriminative method for modeling target language likelihood using source language semantics.\n\nSlide Title: Related Work in Machine Translation\nBullet Points:\n- Deep language representation has been utilized in NLP and MT since the 1950s.\n- Early systems used syntactic and semantic parsing for translation.\n- AMR used for abstractive summarization and semantic graph exploitation.\n- Discriminative models like Global lexicon model and phrase-sense disambiguation have been used.\n\nSlide Title: Semantic Representation Techniques\nBullet Points:\n- Based on Logical Form (LF) with labeled directed graphs.\n- Nodes correspond to content words; edges describe semantic relations.\n- Graphs may have cycles and missing obligatory subjects.\n- Conversion to AMR specification is possible but not fully used in this work.\n\nSlide Title: Graph-to-String Translation Models\nBullet Points:\n- Discriminative translation models using semantic-graph-to-string approach.\n- Translation viewed as generating target-side sentence from source-side semantic graph.\n- Generative process involves transitioning through graph nodes to generate target words.\n- Alignment of target words to source semantic graph nodes is assumed.\n\nSlide Title: Alignment Techniques for Semantic Graphs\nBullet Points:\n- Techniques include Gibbs sampling and Direct GIZA++.\n- Composed alignments involve two stages: semantic graph to source words and source to target words.\n- Composition of gold alignment and GIZA++ found to produce superior results.\n\nSlide Title: Discriminative Model and Feature Set\nBullet Points:\n- Alignment assumed to be given; uses maximum entropy model for translation distribution.\n- Rich feature set includes lemma, part of speech, and semantic graph path features.\n- Features capture agreement phenomena and semantic context.\n- Online learning with stochastic gradient descent and feature hashing used.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Tested model in n-best re-ranking experiment for English\u2192French translation.\n- Added six scores from the model to each translation in n-best lists.\n- Modest but consistent improvement in BLEU scores observed.\n- Gains expected to increase with direct decoder integration.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Initial attempt at including semantic features in statistical MT system.\n- Gains are modest; further feature engineering and decoder integration needed.\n- Impact of morphological, syntactic, and semantic features to be evaluated separately.\n- Potential for preventing \"word salad\" outputs by modeling semantic plausibility.",
    "Slide Title: Introduction to Deep Neural Networks in NLP\nBullet Points:\n- DNNs excel in supervised NLP tasks like dependency parsing, named entity recognition, and semantic role labeling.\n- Challenge: Learning from unlabeled data, especially under domain shift.\n- Many domain adaptation approaches are task-specific and don't generalize well.\n- Re-evaluation of bootstrapping algorithms in the context of DNNs.\n\nSlide Title: Contributions of the Study\nBullet Points:\n- Proposal of a novel multi-task tri-training method.\n- Tri-training as a robust semi-supervised learning baseline for NLP models.\n- Extensive evaluation of bootstrapping algorithms on benchmark datasets.\n- Insights into task and data characteristics for optimal model performance.\n\nSlide Title: Overview of Neural Bootstrapping Methods\nBullet Points:\n- Introduction to self-training, tri-training, and tri-training with disagreement.\n- Novel multi-task tri-training method introduced.\n- Self-training uses model's predictions on unlabeled data for training.\n\nSlide Title: Self-Training Method\nBullet Points:\n- Leverages model's confident predictions on unlabeled data.\n- Calibration issues with neural networks' output probabilities.\n- Online learning setup for DNNs enhances self-training performance.\n\nSlide Title: Tri-Training and Tri-Training with Disagreement\nBullet Points:\n- Tri-training reduces bias using agreement of three models.\n- Tri-training with disagreement focuses on strengthening model weaknesses.\n- Sampling and confidence thresholding improve efficiency and performance.\n\nSlide Title: Multi-Task Tri-Training (MT-Tri)\nBullet Points:\n- Reduces time and space complexity of tri-training.\n- Shares parameters across models using multi-task learning.\n- Orthogonality constraint ensures diverse model predictions.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Evaluation on unsupervised domain adaptation datasets for POS tagging and sentiment analysis.\n- Tri-training outperforms state-of-the-art methods in sentiment analysis.\n- MT-Tri shows efficiency and effectiveness in domain adaptation scenarios.\n\nSlide Title: Analysis and Related Work\nBullet Points:\n- POS tagging accuracy analyzed with respect to word frequency and unseen word-tag combinations.\n- High OOV rates and unknown tags challenge models.\n- Related work on domain adaptation and neural network ensembling.\n\nSlide Title: Conclusions\nBullet Points:\n- Classic tri-training excels in semi-supervised learning under domain shift.\n- MT-Tri offers a more efficient alternative, especially for sentiment analysis.\n- Importance of comparing neural approaches to strong baselines across multiple runs.",
    "Slide Title: Introduction to Conversational Interfaces\nBullet Points:\n- Advances in speech recognition and language understanding have popularized conversational interfaces like Alexa, Cortana, and Siri.\n- Two main use cases: task completion (e.g., playing music) and Q&A via knowledge graphs or databases.\n- Current systems use separate architectures for each use case: SLU for tasks and semantic parsing for Q&A.\n\nSlide Title: Unified Framework for SLU and Q&A\nBullet Points:\n- Proposal to use executable semantic parsing as a common framework for both SLU and Q&A.\n- SLU framed as parsing user intent and slots; Q&A as parsing questions into logical forms.\n- Benefits include handling complex linguistic phenomena and coordinated intents.\n\nSlide Title: Challenges in Semantic Parsing\nBullet Points:\n- Annotated logical forms for training parsers are expensive to obtain.\n- Solutions include distant supervision and transfer learning from related domains.\n- Transfer learning involves pre-training on large datasets and fine-tuning on target domains.\n\nSlide Title: Transition-based Parsing\nBullet Points:\n- Transition-based parsers are used for dependency and semantic parsing.\n- System defined by states, actions, initial state, and end states.\n- Neural transition-based parser uses Bi-LSTM and Stack-LSTM for encoding.\n\nSlide Title: Enhancements in Neural Parsing\nBullet Points:\n- Incorporation of character-level embeddings and copy mechanism.\n- Copy mechanism allows handling of rare or unknown words by copying from input.\n- Attention mechanism used to focus on relevant words during parsing.\n\nSlide Title: Data Sets for Evaluation\nBullet Points:\n- Q&A datasets: Overnight and NLmaps, with varying domain sizes and vocabularies.\n- SLU datasets: Five domains with a mix of high and low-resource data.\n- Evaluation based on exact match accuracy and handling of data sparsity.\n\nSlide Title: Transfer Learning and Multi-task Learning\nBullet Points:\n- Transfer learning across domains improves parsing accuracy, especially for low-resource domains.\n- Multi-task learning shares architecture across tasks, improving generalization.\n- Experiments show benefits of both pre-training and MTL in different scenarios.\n\nSlide Title: Handling Data Sparsity\nBullet Points:\n- Comparison of delexicalization and copy mechanism for rare words.\n- Copy mechanism generally outperforms delexicalization across datasets.\n- Effective for SLU due to nature of data; mixed results for Overnight dataset.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Unified framework for SLU and Q&A using executable semantic parsing is effective.\n- Transfer learning and MTL enhance parsing in low-resource domains.\n- Future work includes exploring transfer learning across heterogeneous data sets.",
    "Slide Title: Introduction to Abstract Meaning Representation (AMR)\nBullet Points:\n- AMR is a semantic graph representation abstracting syntactic realization.\n- Nodes represent concepts; edges represent semantic relations.\n- AMR graphs can have reentrancies due to co-references and control structures.\n- AMR-to-text generation uses encoder/decoder architectures.\n\nSlide Title: AMR Graph Encoding Approaches\nBullet Points:\n- Sequential encoders reduce AMR graphs to sequences.\n- Tree encoders ignore reentrancies.\n- Graph encoders maintain reentrancies and encode them explicitly.\n\nSlide Title: Contributions of the Study\nBullet Points:\n- Introduced structural encoders for AMR-to-text generation.\n- Demonstrated benefits of graph encoders over sequential and tree encoders.\n- Achieved state-of-the-art results on LDC2015E86 and LDC2017T10 datasets.\n\nSlide Title: Input Representations for AMR\nBullet Points:\n- Graph-structured AMRs are rooted and directed graphs.\n- Tree-structured AMRs discard reentrancies.\n- Sequential AMRs linearize and anonymize AMR graphs.\n\nSlide Title: Encoder Architectures\nBullet Points:\n- Recurrent Neural Network Encoders use BiLSTM for sequential input.\n- TreeLSTM Encoders process tree-structured input with bottom-up and top-down passes.\n- Graph Convolutional Network (GCN) Encoders learn node embeddings from graph structure.\n\nSlide Title: Stacking Encoders\nBullet Points:\n- Structure on top of sequence: Structural encoders applied after BiLSTM.\n- Sequence on top of structure: BiLSTM applied after structural encoders.\n- Proposed approach found more effective for AMR-to-text generation.\n\nSlide Title: Experimental Results\nBullet Points:\n- Evaluated models using BLEU and Meteor metrics.\n- GCNSEQ model achieved best performance as both tree and graph encoder.\n- Graph encoder outperformed sequential and tree encoders on test datasets.\n\nSlide Title: Handling Reentrancies and Long-range Dependencies\nBullet Points:\n- Graph encoders explicitly model reentrancies, improving performance.\n- Graph models handle long-range dependencies better than sequential models.\n- Benefits more evident in examples with multiple reentrancies and long dependencies.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Graph encoders improve AMR-to-text generation by encoding reentrancies.\n- Larger benefits observed with more reentrant structures and longer dependencies.\n- Future work to explore further improvements in graph encoder performance.",
    "Slide Title: Introduction to Abstractive Sentence Summarization\nBullet Points:\n- Growing need for effective automatic summarization systems due to online information explosion\n- Focus on abstractive sentence summarization to create shorter versions of sentences\n- Attentional seq2seq framework has achieved state-of-the-art performance\n- Challenges with seq2seq models: loss of control and lack of true summarization\n\nSlide Title: Limitations of Seq2Seq Models\nBullet Points:\n- Seq2seq models often depend solely on source text\n- Performance deteriorates with longer generation lengths\n- Tend to copy source words without actual summarization\n- Need for additional input beyond the source sentence\n\nSlide Title: Template-Based Summarization\nBullet Points:\n- Traditional approach using manually defined templates\n- Templates are fluent and informative but time-consuming to create\n- Difficulty in developing templates for various domains\n\nSlide Title: Introduction of Soft Templates\nBullet Points:\n- Inspired by retrieve-based conversation systems\n- Use existing summaries as soft templates without manual rules\n- Combine seq2seq and template-based approaches in Re3Sum system\n\nSlide Title: Re3Sum System Overview\nBullet Points:\n- Consists of three modules: Retrieve, Rerank, and Rewrite\n- Uses Information Retrieval (IR) to find candidate soft templates\n- Jointly learns template saliency measurement and summary generation\n\nSlide Title: Methodology of Re3Sum\nBullet Points:\n- Retrieve: Find candidate templates from training corpus using IR system\n- Rerank: Measure informativeness of templates using hidden state relevance\n- Rewrite: Generate summary using hidden states of sentence and template\n\nSlide Title: Evaluation and Experiments\nBullet Points:\n- Conducted on Annotated English Gigaword corpus\n- Evaluation metrics: ROUGE scores, LEN DIF, LESS 3, COPY, NEW NE\n- Re3Sum outperforms state-of-the-art seq2seq models in informativeness\n\nSlide Title: Linguistic Quality and Diversity\nBullet Points:\n- Re3Sum generates longer, more fluent summaries\n- Demonstrates ability to produce diverse summaries\n- Soft templates guide generation, reducing copy ratio and improving faithfulness\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Soft templates improve readability and stability of summaries\n- Promising prospects for generation diversity\n- Future work: Improve Retrieve module and test on other tasks like document-level summarization",
    "Slide Title: Introduction to Diverse Paraphrasing\n\nBullet Points:\n- Importance of diversity in paraphrasing for NLP applications\n- Traditional focus on semantic similarity, neglecting diversity\n- Aim to generate diverse paraphrases without compromising quality\n- Applications in text simplification, document summarization, QA systems, and more\n\nSlide Title: Challenges in Current Paraphrasing Models\n\nBullet Points:\n- Reliance on top-k beam search sequences leading to structural similarity\n- Existing methods like DPP and SSR are restrictive in maintaining fidelity\n- Need for a balance between diversity and fidelity in paraphrasing\n\nSlide Title: Proposed Solution: Diverse Paraphraser using Submodularity (DiPS)\n\nBullet Points:\n- Introduction of DiPS to maximize a novel submodular objective function\n- Focus on generating structurally diverse paraphrases\n- Demonstrated effectiveness in data augmentation for tasks like intent classification\n\nSlide Title: Submodular Function Maximization\n\nBullet Points:\n- Explanation of monotone submodular functions and their properties\n- Use of a greedy algorithm for approximate solutions\n- Benefits of submodular functions: flexibility, scalability, and efficiency\n\nSlide Title: Methodology Overview\n\nBullet Points:\n- SEQ2SEQ framework with a diversity-inducing decoder\n- Use of submodular objectives for high-quality paraphrase generation\n- Balance between fidelity and diversity through parameterized submodular functions\n\nSlide Title: Fidelity and Diversity Objectives\n\nBullet Points:\n- Fidelity measured through syntactic and semantic similarity\n- Diversity encouraged by rewarding distinct n-grams and structural variations\n- Use of edit distance for structural diversity\n\nSlide Title: Experiments and Evaluation\n\nBullet Points:\n- Evaluation on datasets for paraphrase recognition and classification tasks\n- Comparison with baseline models and diversity-inducing schemes\n- DiPS outperforms in fidelity metrics and diversity scores\n\nSlide Title: Data Augmentation Results\n\nBullet Points:\n- Use of DiPS-generated paraphrases in intent and question classification\n- Significant accuracy gains over baseline augmentation schemes\n- Importance of high-quality paraphrases for data augmentation\n\nSlide Title: Conclusion and Future Work\n\nBullet Points:\n- DiPS generates high-quality, diverse paraphrases with a submodular approach\n- Effective in data augmentation and other NLP tasks\n- Potential applications in conversational agents and text summarization",
    "Slide Title: Introduction to Machine Question Answering\nBullet Points:\n- Long-term goal of NLP: teaching machines to answer arbitrary user-generated questions\n- Existing methods locate documents with potential answers, but extracting answers remains challenging\n- Neural models show promise in answering questions from related paragraphs\n- Two main approaches: pipelined and confidence-based methods\n\nSlide Title: Pipelined Method Overview\nBullet Points:\n- Pipelined QA system selects a single paragraph for answer extraction\n- Paragraph selection based on TF-IDF cosine distance with the question\n- Linear classifier used for multiple documents, trained on distantly supervised data\n- Improves answer selection accuracy from 83.1% to 85.1% on TriviaQA web\n\nSlide Title: Handling Noisy Labels\nBullet Points:\n- Distant supervision labels all matching text spans as correct, causing noise\n- Summed objective function optimizes negative log-likelihood of correct spans\n- Focuses model on relevant spans, ignoring irrelevant ones\n\nSlide Title: Model Architecture\nBullet Points:\n- Embedding layer with pretrained word vectors and character-derived embeddings\n- Bi-directional GRU for processing question and passage embeddings\n- Attention mechanism builds query-aware context representation\n- Self-attention layer enhances passage understanding\n- Prediction layer computes answer start and end probabilities\n\nSlide Title: Confidence Method for Multi-Paragraph Setting\nBullet Points:\n- Uses un-normalized scores as confidence measure for answer spans\n- Trained on paragraphs with and without answers for better calibration\n- Shared-normalization objective ensures comparable scores across paragraphs\n\nSlide Title: Experimental Setup and Datasets\nBullet Points:\n- Evaluated on TriviaQA unfiltered, wiki, web, and SQuAD datasets\n- Preprocessing includes merging small paragraphs and marking answer spans\n- Sampling strategy for training includes paragraphs with and without answers\n\nSlide Title: Results and Performance\nBullet Points:\n- Shared-norm approach outperforms other methods, especially on SQuAD and TriviaQA unfiltered\n- Model achieves significant improvements over prior work on TriviaQA datasets\n- Document-level evaluation shows strong adaptation to multi-paragraph tasks\n\nSlide Title: Error Analysis and Discussion\nBullet Points:\n- Common errors include insufficient evidence, complex syntax, and coreference issues\n- Shared-norm approach minimizes distraction from irrelevant text\n- Future improvements: enhance sentence/paragraph comprehension and handle coreferences\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Advances in neural reading comprehension driven by large datasets\n- Open QA systems use pipelined methods for retrieval and answer selection\n- Our approach improves paragraph-level QA with shared-norm training\n- Direct application to deep-learning-powered open QA systems demonstrated",
    "Slide Title: Introduction to Crisis Event Tweet Classification\nBullet Points:\n- Focus on real-time classification of tweets during crisis events\n- Importance of timely information for humanitarian response\n- Challenge: Lack of labeled data for new events\n- Solution: Leverage past event data for domain adaptation and semi-supervised learning\n\nSlide Title: Proposed Model Overview\nBullet Points:\n- Novel model combining domain adaptation and semi-supervised learning\n- Utilizes deep neural networks for feature learning\n- Incorporates three components: supervised, semi-supervised, and domain adversarial\n\nSlide Title: Model Architecture\nBullet Points:\n- Convolutional Neural Network (CNN) for feature extraction\n- Shared representation used across three branches\n- Supervised branch for classification using labeled data\n- Semi-supervised branch for graph-based learning\n- Domain adversarial branch for domain invariance\n\nSlide Title: Semi-supervised Learning Component\nBullet Points:\n- Graph-based approach to induce structural similarity\n- Uses labeled and unlabeled data for learning\n- Constructs a similarity graph using k-nearest neighbors\n- Demonstrates significant improvements over supervised methods\n\nSlide Title: Domain Adversarial Training\nBullet Points:\n- Domain discriminator to distinguish between source and target domains\n- Adversarial training to induce domain-invariant features\n- Effective in improving classification across different events\n\nSlide Title: Experimental Setup\nBullet Points:\n- Datasets: Nepal Earthquake and Queensland Floods\n- Evaluation metrics: Precision, Recall, F-measure, AUC\n- Comparison with baselines: Supervised and self-training methods\n\nSlide Title: Results and Discussion\nBullet Points:\n- Graph-based semi-supervised approach outperforms baselines\n- Domain adversarial training shows improvements in F1 scores\n- Combined approach yields further gains in performance\n- Effective use of unlabeled data from target domain\n\nSlide Title: Related Work\nBullet Points:\n- Overview of semi-supervised learning and domain adaptation techniques\n- Comparison with existing models like DANN and graph convolution networks\n- Unique contributions of the proposed model\n\nSlide Title: Conclusions\nBullet Points:\n- Successful integration of domain adaptation and semi-supervised learning\n- Significant improvements in crisis tweet classification\n- Potential for future enhancements with labeled target domain data\n- Contribution to real-time crisis response efforts through improved data analysis",
    "Slide Title: Introduction to Multilingual Corpora\n\nBullet Points:\n- Multilingual corpora are essential for various NLP tasks.\n- Comparable corpora help in extracting parallel lexicons and feeding SMT systems.\n- Wikipedia is a valuable source for multilingual data due to its encyclopedic nature.\n- Challenges include chaotic taxonomy and lack of clear domain boundaries.\n\nSlide Title: Understanding Comparability in Corpora\n\nBullet Points:\n- Comparability in corpora lacks a unified definition.\n- Four-class classification: Parallel, Strongly Comparable, Weakly Comparable, Non-comparable.\n- Wikipedia's interlanguage links facilitate multilingual text extraction.\n\nSlide Title: Challenges in Extracting Comparable Corpora from Wikipedia\n\nBullet Points:\n- Wikipedia's category graph is complex and densely connected.\n- Categories are manually crafted, leading to arbitrary and overlapping classifications.\n- Heuristic strategies are needed to manage the chaotic taxonomy.\n\nSlide Title: Domain-Specific Comparable Corpora Extraction\n\nBullet Points:\n- Proposal to extract domain-specific corpora using Wikipedia's category graph.\n- Vocabulary is defined from Wikipedia articles within a domain.\n- Graph exploration uses breadth-first search with constraints to identify domain-relevant categories.\n\nSlide Title: Parallel Sentence Extraction Technique\n\nBullet Points:\n- Extracts parallel sentences from comparable corpora using text similarity measures.\n- Similarity measures include cosine similarity, character n-grams, and length factor.\n- Non-supervised method generates a noisy parallel corpus, useful for SMT systems.\n\nSlide Title: Evaluation of Extracted Corpora in SMT\n\nBullet Points:\n- Quality of extracted corpora validated through SMT task.\n- Comparison with Europarl corpus for domain-specific translation.\n- Wikipedia-derived corpora significantly improve translation quality.\n\nSlide Title: Combining Wikipedia and Europarl for Translation\n\nBullet Points:\n- Enriching general translators with Wikipedia data enhances performance.\n- Domain-specific corpora outperform general corpora in translation tasks.\n- General Wikipedia corpus necessary for building a versatile translator.\n\nSlide Title: Conclusions and Future Work\n\nBullet Points:\n- Model enables automatic extraction of in-domain comparable corpora from Wikipedia.\n- Significant improvements in machine translation quality for specific domains.\n- Future work includes robust parallel sentence identification and broader language application.",
    "Slide Title: Introduction to Temporal and Causal Relations\nBullet Points:\nUnderstanding events is crucial for natural language understanding\nFocus on joint extraction of temporal and causal relations\nTemporal and causal relations often interact and influence each other\nProposed a joint framework for Temporal and Causal Reasoning (TCR)\n\nSlide Title: Importance of Temporal and Causal Relations\nBullet Points:\nSupport applications like story completion, summarization, and timeline construction\nTemporal relations can dictate causal relations and vice versa\nExamples illustrate the interaction between temporal and causal relations\n\nSlide Title: Joint Framework for Temporal and Causal Reasoning\nBullet Points:\nCombines temporal and causal extraction systems using a constrained conditional model (CCM)\nUses integer linear programming (ILP) to enforce constraints\nConstraints include causality preceding effect, symmetry, and transitivity\n\nSlide Title: Development of Jointly Annotated Dataset\nBullet Points:\nCreated a dataset with dense temporal annotations\nAugmented the Event-Causality dataset for joint analysis\nShowed statistically significant improvements using the joint framework\n\nSlide Title: Related Work and Global Reasoning\nBullet Points:\nTemporal and causal relations represented by directed acyclic graphs\nExisting methods often break symmetric and transitive constraints\nAdopted ILP approach for global reasoning and consistency\n\nSlide Title: Temporal and Causal Reasoning Framework\nBullet Points:\nIntroduced temporal component and transitivity constraints\nIncorporated causal relations into the TCR framework\nConverted joint problem into ILP for optimization\n\nSlide Title: Experiments and Results\nBullet Points:\nImproved temporal relation identification on TimeBank-Dense dataset\nJoint framework improved both temporal and causal performance\nNew dataset demonstrated the effectiveness of joint reasoning\n\nSlide Title: Discussion and Conclusion\nBullet Points:\nObserved differences in precision and recall across datasets\nJoint framework synthesizes temporal and causal signals effectively\nFuture work includes better event definitions and handling complex causality\nTCR framework fosters interest in studying multiple event aspects in natural language",
    "Slide Title: Introduction to Machine Translation Evaluation\nBullet Points:\n- Automatic evaluation metrics are preferred over manual evaluation due to cost and time efficiency.\n- Metrics facilitate cross-paper comparisons and are essential in hyperparameter tuning and architecture search.\n- WMT Metrics Shared Task validates existing metrics and develops new ones.\n\nSlide Title: Task Setup and Language Pairs\nBullet Points:\n- Participants provided with test sets for each language pair, including source texts, MT outputs, and reference translations.\n- Metrics evaluated at system-level and segment-level, with options for reference-based or reference-free tracks.\n- 18 language pairs with 10 target languages, including English and non-English pairs.\n\nSlide Title: Manual Quality Assessment\nBullet Points:\n- Direct Assessment (DA) used as the \"golden truth\" for evaluating metrics.\n- DA scores standardized per annotator and averaged for system-level evaluation.\n- Segment-level evaluation uses daRR, converting DA scores into relative rankings.\n\nSlide Title: Baseline Metrics\nBullet Points:\n- Popular metrics like BLEU, NIST, TER, WER, PER, and CDER used as baselines.\n- Baselines serve as a comparison to prevent \"loss of knowledge.\"\n- Metrics computed using tools like Moses scorer and sacreBLEU.\n\nSlide Title: Submitted Metrics Overview\nBullet Points:\n- Metrics like BEER, BERTr, CharacTER, EED, and ESIM participated in the task.\n- Metrics use various approaches, including character-level edits, contextual embeddings, and semantic vector spaces.\n- YiSi metrics focus on semantic similarity using BERT embeddings.\n\nSlide Title: System-Level Evaluation Results\nBullet Points:\n- Pearson correlation used to evaluate system-level metrics.\n- YiSi metrics achieved high correlations across several language pairs.\n- Metrics like ESIM and EED performed well in specific language pairs.\n\nSlide Title: Segment-Level Evaluation Results\nBullet Points:\n- Segment-level evaluation uses Kendall's Tau-like formulation.\n- YiSi-1_srl and ESIM stood out as top performers in segment-level evaluation.\n- Metrics based on embeddings showed high performance.\n\nSlide Title: Discussion and Observations\nBullet Points:\n- Stability of metrics depends on the underlying MT systems.\n- Metrics like BERTr and QE systems showed better performance for top systems.\n- Semantic features in metrics may contribute to performance gains.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- WMT19 Metrics Task evaluated metrics' correlation with human judgment.\n- Best metrics achieved high system-level correlations, but results vary with system quality.\n- Future metrics may explore semantic features and their impact on performance.",
    "Slide Title: Introduction to Zero-Shot Learning (ZSL)\n\nBullet Points:\n- Automatic classification is a fundamental problem in machine learning.\n- Traditional models struggle in dynamic environments with emerging new classes.\n- Zero-shot learning (ZSL) aims to classify documents of unseen classes.\n- Humans transfer knowledge from known to unknown domains using semantic representations.\n- ZSL frameworks exploit semantic knowledge to infer features of unseen classes.\n\nSlide Title: Types of Semantic Knowledge in ZSL\n\nBullet Points:\n- Semantic attributes: Visual concepts and semantic properties.\n- Concept ontology: Class hierarchy and knowledge graphs.\n- Semantic word embeddings: Capture implicit relationships between words.\n- Few studies exploit these knowledge types in text classification.\n\nSlide Title: Proposed Two-Phase Framework\n\nBullet Points:\n- Novel deep learning-based framework for zero-shot text classification.\n- Coarse-grained classification: Determines if a document is from seen or unseen classes.\n- Fine-grained classification: Specifies the class of the document.\n- Framework uses labelled data of seen classes and augmented text data.\n\nSlide Title: Data and Feature Augmentation Techniques\n\nBullet Points:\n- Data augmentation: Topic translation introduces unseen classes to classifiers.\n- Feature augmentation: Uses integrated semantic knowledge for knowledge transfer.\n- Augmented features relate documents to unseen classes for zero-shot reasoning.\n\nSlide Title: Methodology Overview\n\nBullet Points:\n- Problem formulation: Disjoint sets of seen and unseen classes.\n- Coarse-grained classification: Binary classification using CNN classifiers.\n- Fine-grained classification: Uses traditional or zero-shot classifiers based on Phase 1 prediction.\n- Relationship vectors quantify relatedness between words and classes.\n\nSlide Title: Experiments and Datasets\n\nBullet Points:\n- Datasets: DBpedia ontology and 20newsgroups.\n- Experiments with different rates of unseen classes (50% and 25%).\n- Evaluation metrics: Accuracy for binary and multi-class classification.\n- Results show improved accuracy with data and feature augmentation.\n\nSlide Title: Results and Discussion\n\nBullet Points:\n- Phase 1 outperformed DOC in detecting documents from unseen classes.\n- Augmented data improved accuracy in detecting unseen classes.\n- Zero-shot classifier with integrated inputs achieved highest accuracy.\n- Framework showed competitive prediction accuracy on unseen classes.\n\nSlide Title: Related Work and Conclusion\n\nBullet Points:\n- Previous works focused on semantic relationships via embeddings.\n- Proposed framework leverages multiple types of semantic knowledge.\n- Data augmentation in NLP is less common but effective in this framework.\n- Future work: Extend framework to multi-label classification with larger datasets.",
    "Slide Title: Introduction to Multi-task Learning in NLP\nBullet Points:\n- Supervised learning in NLP often involves training individual models for each task.\n- Tasks like POS tagging and Name Tagging are related and can benefit from shared knowledge.\n- Multi-task Learning (MTL) is effective for knowledge transfer across tasks.\n- MTL helps improve performance in low-resource languages by leveraging high-resource language data.\n\nSlide Title: Challenges in Low-resource NLP\nBullet Points:\n- Many languages lack sufficient labeled data for NLP tasks.\n- Annotating data for all 7,099 living languages is impractical.\n- Rapid development is needed for low-resource languages in special applications.\n- Data sparsity affects both target and related tasks in low-resource settings.\n\nSlide Title: Proposed Multi-lingual Multi-task Architecture\nBullet Points:\n- Combines different transfer models within a unified architecture.\n- Two levels of parameter sharing: character embeddings and task-specific knowledge.\n- Uses LSTM-CNNs model for sequence labeling tasks.\n- Achieves significant F-score gains with minimal auxiliary data.\n\nSlide Title: Model Basic Architecture\nBullet Points:\n- Sequence labeling assigns categorical labels to tokens in a sentence.\n- LSTM-CRFs-based models are suitable for multi-lingual multi-task learning.\n- Character embeddings transfer morphological and semantic information.\n- Highway networks enhance performance by transforming character feature vectors.\n\nSlide Title: Multi-task Multi-lingual Architecture Details\nBullet Points:\n- Parameter sharing across tasks and languages improves performance.\n- Character embeddings and CharCNN shared among similar scripts.\n- Bidirectional LSTM layer extracts character, word, and contextual information.\n- Cross-task and cross-lingual transfer strategies enhance word representations.\n\nSlide Title: Experiment Setup and Results\nBullet Points:\n- Used datasets from CoNLL shared tasks and TAC KBP evaluations.\n- Pre-trained word embeddings and character embeddings used.\n- Cross-lingual embeddings boost performance in low-resource settings.\n- Model outperforms baseline and single transfer models in low-resource tasks.\n\nSlide Title: Qualitative Analysis and Ablation Studies\nBullet Points:\n- Shared character-level networks transfer morphological and semantic information.\n- Cross-lingual transfer improves tag prediction accuracy.\n- Ablation studies show contributions of individual components.\n- Model does not heavily rely on the amount of auxiliary task data.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- MTL applied in various NLP areas like machine translation and text classification.\n- Previous models focused on different transfer schemes separately.\n- Proposed model combines cross-task and cross-lingual transfer.\n- Future work includes applying architecture to other tasks and integrating incremental learning.",
    "Slide Title: Introduction to Natural Language Inference (NLI)\nBullet Points:\n- NLI is a key task in natural language understanding.\n- Involves determining the relationship between a premise and a hypothesis.\n- Labels: entailment, neutral, contradiction.\n- Recent advancements due to large-scale datasets like SNLI and MultiNLI.\n\nSlide Title: Role of Discourse Markers in NLI\nBullet Points:\n- Discourse markers like \"but\" and \"so\" indicate sentence relationships.\n- Few NLI models utilize discourse markers effectively.\n- Proposed method: Discourse Marker Augmented Network (DMAN).\n\nSlide Title: Discourse Marker Augmented Network (DMAN)\nBullet Points:\n- Transfers knowledge from Discourse Marker Prediction (DMP) to NLI.\n- Uses a sentence encoder model from DMP.\n- Incorporates reinforcement learning to optimize label confidence.\n\nSlide Title: Task Descriptions: NLI and DMP\nBullet Points:\n- NLI: Determine logical relationship between premise and hypothesis.\n- DMP: Predict discourse markers linking sentence pairs.\n- Sentence encoder model trained on BookCorpus for DMP.\n\nSlide Title: Encoding and Interaction Layers in DMAN\nBullet Points:\n- Encoding Layer: Uses word, character, POS, and NER embeddings.\n- Interaction Layer: Employs attention mechanism for premise-hypothesis linking.\n- Bi-directional LSTMs capture sentence interactions.\n\nSlide Title: Training and Objective Function\nBullet Points:\n- Combines log probabilities and reinforcement learning rewards.\n- Objective function balances cross-entropy loss and expected reward.\n- Uses REINFORCE algorithm to simulate human-like label confidence.\n\nSlide Title: Experimental Results and Datasets\nBullet Points:\n- Datasets: BookCorpus, SNLI, MultiNLI.\n- DMAN achieves state-of-the-art results on SNLI and MultiNLI.\n- Ablation analysis shows importance of discourse markers and features.\n\nSlide Title: Semantic and Visualization Analysis\nBullet Points:\n- Performance varies with different discourse markers.\n- \"But\", \"because\", \"although\" crucial for contradiction and entailment.\n- Visualization shows improved handling of contradictory meanings with DMAN.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Inspired by DisSent model and discourse prediction tasks.\n- Previous works focus on interaction architectures and transfer learning.\n- DMAN augments semantic representation and optimizes using annotator views.\n- Future work: explore more discourse markers and transfer learning sources.",
    "Slide Title: Introduction to Instruction Execution Challenges\nBullet Points:\n- Agents must ground language to their environment and reason about discourse dependencies.\n- Example: Moving and mixing chemicals in beakers.\n- Model maps instructions to actions, considering previous utterances and world state.\n- Focus on directly generating action sequences without explicit reference modeling.\n\nSlide Title: Contextual Cues in Instruction Execution\nBullet Points:\n- Instructions often refer to previously mentioned objects or actions.\n- World state determines available actions and object references.\n- Contexts change with new instructions and agent actions.\n- Proposed model uses attention to process instructions and world states.\n\nSlide Title: Task and Notation in Instruction Execution\nBullet Points:\n- Define world states (S), instructions (X), and actions (A).\n- Example: ALCHEMY domain with beakers and actions like POP and PUSH.\n- Goal: Generate action sequences from instructions starting from a given state.\n\nSlide Title: Model Architecture for Instruction Execution\nBullet Points:\n- Neural network policy predicts actions based on current and previous instructions.\n- Uses attention and RNN decoder to generate actions.\n- Model switches instructions upon predicting STOP action.\n\nSlide Title: Learning Approach for Instruction Execution\nBullet Points:\n- Training with instruction sequences paired with start and goal states.\n- Maximizes expected immediate reward using potential-based reward shaping.\n- Addresses exploration problem by observing rewards for all possible actions.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Evaluated on SCONE domains: ALCHEMY, SCENE, and TANGRAMS.\n- Achieved up to 89.1% accuracy for single instructions, 62.7% for complete sequences.\n- Compared with baseline systems and previous approaches.\n\nSlide Title: Related Work and Contextual Understanding\nBullet Points:\n- Previous studies on navigation and manipulation using logical representations.\n- Focus on sequences of environment manipulation instructions.\n- SCONE corpus designed for discourse context-dependence phenomena.\n\nSlide Title: Discussion and Future Directions\nBullet Points:\n- Model reasons about context-dependent instructional language.\n- SESTRA learning algorithm overcomes biases in on-policy learning.\n- Future work: Use intermediate world states and predict rewards for all actions.",
    "Slide Title: Introduction to Semantic Parsers\nBullet Points:\n- Semantic parsers predict predicate-argument structures of sentences.\n- Traditionally trained from complex datasets.\n- Recent research explores non-expert training for semantic supervision.\n- Crowdsourcing enables large-scale dataset creation for training parsers.\n\nSlide Title: QA-SRL Annotation Scheme\nBullet Points:\n- QA-SRL uses question-answer pairs for predicate-argument relationships.\n- Intuitive for non-experts and aligns with traditional annotation schemes.\n- High precision but challenging to achieve high recall.\n- New scalable crowdsourcing approach introduced for QA-SRL.\n\nSlide Title: QA-SRL Bank 2.0 Dataset\nBullet Points:\n- Created using a streamlined web interface with autosuggest and quality control.\n- Contains 133,479 verbs from 64,018 sentences across 3 domains.\n- Total of 265,140 question-answer pairs produced in 9 days.\n- High precision with good recall, though not exhaustive.\n\nSlide Title: Crowdsourcing Pipeline for Data Annotation\nBullet Points:\n- Consists of generation and validation steps.\n- Autocomplete feature aids in question writing.\n- Validation ensures high precision by requiring answerable questions.\n- Cost-effective and scalable compared to previous methods.\n\nSlide Title: Models for QA-SRL Parsing\nBullet Points:\n- Pipeline approach: span detection and question generation.\n- Span-based representation and custom LSTM for question decoding.\n- Models trained directly from crowdsourced span labels.\n- Achieved high accuracy in span detection and question generation.\n\nSlide Title: Data Expansion and Model Improvement\nBullet Points:\n- Data bootstrapping approach to expand dataset.\n- Over-generation of questions validated through annotation pipeline.\n- Expanded dataset improved model performance.\n- Final parser achieved high question accuracy and span-level precision.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Automatic and human evaluations conducted.\n- Span detection and question generation metrics used.\n- Expanded data led to modest improvements in model performance.\n- Final model demonstrated high accuracy in human evaluation.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Semantic resources often require expert annotation.\n- Crowdsourcing offers scalable solutions for semantic annotation.\n- First high-quality parser for QA-SRL developed.\n- QA-SRL Bank 2.0 enables large-scale predicate-argument structure labeling.",
    "Slide Title: Introduction to SIGHAN 2015 Bake-off\n\nBullet Points:\n- Chinese spelling checkers are challenging due to lack of word delimiters and numerous characters.\n- Errors often arise from phonologically and visually similar words.\n- SIGHAN 2013 and 2014 provided benchmarks for spelling checkers.\n- SIGHAN 2015 focuses on essays by learners of Chinese as a Foreign Language (CFL).\n\nSlide Title: Task Description\n\nBullet Points:\n- Evaluate the capability of Chinese spelling checkers.\n- Input: Passage with/without spelling errors.\n- Output: Locations of incorrect characters and suggested corrections.\n- Format: \"pid [, location, correction]+\" for errors, \"pid, 0\" for no errors.\n\nSlide Title: Data Preparation\n\nBullet Points:\n- Learner corpus from TOCFL essays in Taiwan.\n- Errors manually annotated by native speakers.\n- Data split into Training, Dryrun, and Test Sets.\n- Focus on spelling error correction, not grammar.\n\nSlide Title: Training, Dryrun, and Test Sets\n\nBullet Points:\n- Training Set: 970 essays, 3,143 spelling errors.\n- Dryrun Set: 39 passages for familiarization.\n- Test Set: 1,100 passages, half with errors.\n- Participants can use additional resources for error detection.\n\nSlide Title: Performance Metrics\n\nBullet Points:\n- Evaluation at detection and correction levels.\n- Metrics: True Positive, False Positive, True Negative, False Negative.\n- Importance of reducing false positives.\n- F1 score used to balance precision and recall.\n\nSlide Title: Evaluation Results\n\nBullet Points:\n- CAS team achieved highest detection and correction accuracy.\n- CAS used HMM-based approach and two-stage filter process.\n- NCTU&NTUT used word vector/conditional random field method.\n- No team excelled in all metrics, but CAS and NCTU&NTUT performed best overall.\n\nSlide Title: Approaches and Techniques\n\nBullet Points:\n- CAS: Unified framework, HMM-based segmentation, candidate re-ranking.\n- NCTU&NTUT: Error detection guides language model rescoring.\n- Different approaches used by 6 participant teams.\n- Focus on improving error detection and correction.\n\nSlide Title: Conclusions and Future Work\n\nBullet Points:\n- Overview of SIGHAN 2015 Bake-off and participant approaches.\n- Contributions to Chinese spell checker development.\n- Data sets and evaluation tools made publicly available.\n- Future focus on Chinese grammatical error correction and new language resources.",
    "Slide Title: Introduction to Language Identification\n\nBullet Points:\n- Language identification is crucial in natural language processing applications.\n- Improves accuracy in letter-to-phoneme conversion and machine transliteration.\n- Traditionally approached using character-level n-gram language models.\n- Proposal to use support vector machines (SVMs) for identifying languages in short texts.\n\nSlide Title: Previous Work on Language Identification\n\nBullet Points:\n- N-gram models popular for language identification, achieving high accuracy with longer texts.\n- Accuracy drops with shorter texts; SVMs show better performance.\n- Specific focus on proper nouns shows improved results with tailored models.\n\nSlide Title: Language Identification with SVMs\n\nBullet Points:\n- SVMs use character n-gram occurrences as features.\n- Include word length and space characters in feature set.\n- Tested various kernels; linear, sigmoid, and RBF performed best.\n- Optimal n-gram lengths determined through cross-validation.\n\nSlide Title: Intrinsic Evaluation: Transfermarkt Corpus\n\nBullet Points:\n- European soccer player names annotated with 13 national languages.\n- SVMs outperform language models, especially on last names.\n- Data is noisy; player nationality does not always match name language.\n\nSlide Title: Intrinsic Evaluation: CEJ Corpus\n\nBullet Points:\n- Corpus of Chinese, English, and Japanese names.\n- Linear kernel used due to large corpus size.\n- SVMs reduce error rate by over 50% compared to language models.\n\nSlide Title: Application to Machine Transliteration\n\nBullet Points:\n- Language identification can influence word pronunciation in transliteration.\n- Tested on English-Hindi corpus of names.\n- SVMs used to classify names by origin before transliteration.\n\nSlide Title: Experimental Setup and Results\n\nBullet Points:\n- Language identification model splits data into disjoint classes.\n- Separate transliteration models trained on each class.\n- No significant improvement in accuracy; data size reduction a factor.\n\nSlide Title: Conclusion and Future Work\n\nBullet Points:\n- SVMs with n-gram counts outperform traditional language models.\n- Language identification as preprocessing shows no significant accuracy change.\n- Future exploration of language identification probabilities in transliteration systems.",
    "Slide Title: Introduction to Retraction Citations\nBullet Points:\n- Increase in retracted articles correlates with rise in scientific publications\n- Retractions help eliminate erroneous research, but citations of retracted work pose challenges\n- Two types of retraction citations: pre-retraction and post-retraction\n- High pre-retraction citations often lead to continued post-retraction citations\n- Example: Scott S. Reuben's fabricated studies continued to be cited post-retraction\n\nSlide Title: Study Objective and Sample\nBullet Points:\n- Aim: Analyze context of post-retraction citations\n- Focus: Determine if citations are negative, positive, or neutral\n- Sample: Five retracted articles with post-retraction citations tracked in 2015 and 2016\n\nSlide Title: Data Collection Methodology\nBullet Points:\n- Accessed ScienceDirect in October 2014 for \"RETRACTED\" articles\n- 1,203 results retrieved; 988 were retracted articles\n- Selected top five articles cited over 20 times since 2015\n- Analyzed 109 citing documents from 2015 and 2016\n\nSlide Title: Citation Context Analysis\nBullet Points:\n- Positive: Cited as legitimate prior work\n- Negative: Mentioned as inappropriate\n- Neutral: Mentioned without judgment on validity\n- Example: Donmez et al. article retracted for data misrepresentation but results deemed valid\n\nSlide Title: Case Study: S\u00e9ralini Affair\nBullet Points:\n- Study on GM crops and tumors in rats retracted due to small sample size\n- Media attention led to ongoing public debate on GM food safety\n- Post-retraction citations divided: negative, positive, and neutral\n- Republished in 2014, stirring further controversy\n\nSlide Title: Case Study: Dipak Das and Authentic Leadership\nBullet Points:\n- Das' papers retracted for data manipulation; continued positive citations\n- \"Authentic leadership\" popular in management studies, explaining positive citations\n- Walumbwa article cited despite retraction for duplication\n\nSlide Title: Discussion on Continuous Citations\nBullet Points:\n- Retractions often ignored due to public/media interest or perceived validity\n- Examples: Donmez, Mukherjee, and Li articles\n- Invalid research continues to appear as valid, raising concerns\n\nSlide Title: Recommendations for Publishers\nBullet Points:\n- Implement reference checks to detect retracted article citations\n- Require explanations for citing retracted papers\n- Delete retracted articles from publisher websites, leaving only retraction notices\n- Restrict access to retracted articles to prevent further citations\n\nSlide Title: Conclusion\nBullet Points:\n- Retraction citations pose a significant challenge to scientific integrity\n- Continuous citations often driven by media interest or perceived validity\n- Urgent need for improved publisher practices to address this issue",
    "Slide Title: Introduction to Semantic Parsing and Surface Realization\nBullet Points:\nIncreased interest in semantic parsing and surface realization using graph-structured semantic representations\nFocus on Abstract Meaning Representation (AMR), Elementary Dependency Structure (EDS), and Dependency-based Minimal Recursion Semantics (DMRS)\nNeed for a formal framework for graph manipulation similar to automata and transducers for strings and trees\n\nSlide Title: Graph Grammars and DAG Automata\nBullet Points:\nIntroduction of graph grammar and DAG automata for NLP\nGraph grammar example: Hyperedge Replacement Grammar (HRG)\nDAG automata extended by Chiang et al. (2018) for NLP applications\n\nSlide Title: Challenges in DAG Transducers for NLG\nBullet Points:\nExisting DAG-to-tree transducers unsuitable for type-logical semantic graphs\nDifficulties in transforming type-logical semantic graphs into strings\nNeed for a novel DAG transducer design for graph-to-program transformation\n\nSlide Title: A New DAG Transducer Design\nBullet Points:\nIntroduction of a novel DAG transducer for structure transformation\nOutput is a program licensed by a declarative programming language\nTarget structures constructed by executing programs\n\nSlide Title: Declarative Programming for DAG Transduction\nBullet Points:\nUse of a simple declarative programming language for string concatenation\nSyntax and semantics of the language\nExample program illustrating string calculation\n\nSlide Title: DAG Transduction-based NLG Strategy\nBullet Points:\nTwo-step strategy for meaning-to-text transformation\nPhase 1: Translate semantic graph into sequential lemmas\nPhase 2: Use neural sequence-to-sequence model for final surface strings\n\nSlide Title: Inducing Transduction Rules\nBullet Points:\nEmpirical study on DAG transduction-based NLG using EDS representations\nConstraints for generating reasonable strings\nFine-to-coarse strategy for robustness in NLG systems\n\nSlide Title: Evaluation and Analysis\nBullet Points:\nUse of DeepBank and wikiwoods data sets for training and testing\nBLEU-4 scores for generated lemma sequences and sentences\nComparison with neural baseline and efficiency of the system\n\nSlide Title: Conclusion\nBullet Points:\nExtension of DAG automata work to build flexible DAG transducer\nLeveraging declarative programming to minimize computation burden\nSystem is accurate, efficient, and robust for semantic-graph-to-string tasks",
    "Slide Title: Introduction to Knowledge Base Question Answering\nBullet Points:\n- Large-scale knowledge bases like DBPedia and Freebase support open-domain question answering (QA).\n- Semantic parsing maps questions to formal meaning representations for QA.\n- Traditional semantic parsing faces challenges with ontology matching and predicate selection.\n- Proposed framework leverages knowledge base tightly for semantic parsing.\n\nSlide Title: Semantic Parsing Framework\nBullet Points:\n- Framework reduces semantic parsing to query graph generation.\n- Query graph mapped to logical form in \u03bb-calculus.\n- Staged actions: locate topic entity, find main relationship, expand with constraints.\n- Efficient search by grounding utterance to entities and predicates in KB.\n\nSlide Title: Knowledge Base and Query Graph Design\nBullet Points:\n- Knowledge base consists of subject-predicate-object triples.\n- Query graph captures logical form using nodes and directed edges.\n- Types of nodes: grounded entity, existential variable, lambda variable, aggregation function.\n- Example: Query graph for \"Who first voiced Meg on Family Guy?\"\n\nSlide Title: Staged Query Graph Generation\nBullet Points:\n- Tree graph with topic entity as root and lambda variable as answer node.\n- Core inferential chain describes main relationship between answer and topic entity.\n- Additional constraints added to refine answer set.\n- Search problem with staged states and actions.\n\nSlide Title: Linking Topic Entity and Identifying Core Inferential Chain\nBullet Points:\n- Entity linking system identifies topic entities in questions.\n- Core inferential chain maps natural utterance to predicate sequence.\n- Use of neural networks for semantic similarity measurement.\n- Example: \"Who first voiced Meg on Family Guy?\"\n\nSlide Title: Deep Convolutional Neural Networks for Semantic Parsing\nBullet Points:\n- Siamese neural networks identify core inferential chain.\n- Continuous-space representation for semantic parsing.\n- CNN architecture: word hashing, convolutional layer, max pooling, feed-forward layers.\n- Training with positive pairs from semantic parses.\n\nSlide Title: Augmenting Constraints and Aggregations\nBullet Points:\n- Core inferential chain forms simplest query graph.\n- Constraints and aggregation nodes refine answer set.\n- Example: Attaching MegGriffin to y with predicate character.\n- Use of simple rules to retain legitimate constraints.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Evaluation on WEBQUESTIONS dataset.\n- System outperforms previous methods with 52.5% F1 score.\n- Contributions of each component analyzed.\n- Error analysis reveals areas for improvement.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Semantic parsing framework for QA using knowledge base.\n- Query graph as meaning representation mapped to logical form.\n- Future work: extend query graph for complex questions, explore more features and models.\n- Investigate structured-output prediction methods for graph generation.",
    "Slide Title: Introduction to Social Media and Occupational Class\n\nBullet Points:\n- Growth of online social networks allows analysis of user text in broader contexts.\n- Language use influenced by user attributes like location, gender, and age.\n- Hypothesis: Language use on social media indicates a user's occupational class.\n- Study focuses on predicting occupational class using Twitter data.\n\nSlide Title: Standard Occupational Classification (SOC)\n\nBullet Points:\n- SOC is a UK government system for classifying occupations.\n- Jobs categorized hierarchically based on skill requirements and content.\n- SOC includes 9 major groups, 25 sub-major groups, 90 minor groups, and 369 unit groups.\n- Used for mapping Twitter users to occupations in the study.\n\nSlide Title: Data Collection and Set Creation\n\nBullet Points:\n- Created a new data set of Twitter users mapped to their occupation.\n- Used user description field to identify self-disclosed job titles.\n- Retrieved accounts using Twitter's Search API and aggregated into minor categories.\n- Final data set includes 5,191 users from 55 minor groups across 9 major SOC groups.\n\nSlide Title: Features for Occupational Class Prediction\n\nBullet Points:\n- Features divided into user level and textual features.\n- User level features include number of followers, friends, and tweet statistics.\n- Textual features derived from user's tweets using word embeddings and clusters.\n- SVD and neural embeddings used for feature representation.\n\nSlide Title: Classification with Gaussian Processes\n\nBullet Points:\n- Gaussian Processes (GP) used for classification in a Bayesian non-parametric framework.\n- GP offers feature interpretability via Automatic Relevance Determination.\n- Conducted one-vs-all classification for each occupational class.\n- GP outperforms other methods like logistic regression and SVM.\n\nSlide Title: Experimental Results and Predictive Accuracy\n\nBullet Points:\n- Compared accuracy of classification methods using different feature sets.\n- Textual features improve performance over user level features.\n- Clusters show better accuracy than embeddings.\n- GP classifier provides strong predictive performance and interpretability.\n\nSlide Title: Qualitative Analysis of Language Use\n\nBullet Points:\n- Word clusters used to extract qualitative insights from predictive task.\n- Topics like 'Corporate', 'Software Engineering', and 'Arts' identified.\n- Topic usage varies across occupational classes, indicating thematic divergence.\n- Higher skilled classes discuss topics like 'Politics' more frequently.\n\nSlide Title: Related Work and Conclusions\n\nBullet Points:\n- Previous studies in psychology and economics explored occupational prediction.\n- Our study is the first large-scale investigation of language use for occupational inference.\n- Introduced a labeled data set from Twitter for classification task.\n- Results highlight the influence of occupation on text use and potential for future applications.",
    "Slide Title: Introduction to the ACL Anthology\nBullet Points:\n- Open access service by the Association for Computational Linguistics (ACL)\n- Hosts over 43,000 computational linguistics and NLP papers\n- Supports text and faceted search, author-specific pages, and third-party metadata\n- Maintained by volunteers, with code available on GitHub\n\nSlide Title: Current State of the Anthology\nBullet Points:\n- Launched in 2002, with a second version in 2012\n- Provides access to papers in PDF and metadata in multiple formats\n- Hosted at Saarland University, with future hosting under ACL\n- Daily administration by volunteers, with paid assistance for larger issues\n\nSlide Title: Technology and Workflow\nBullet Points:\n- Tech stack includes Ruby on Rails, Solr, PostgreSQL, Nginx/Jetty, Debian GNU-Linux\n- Semi-automated process for importing and indexing new papers\n- XML format for bibliographic metadata, integrated with conference management software\n\nSlide Title: Community-Driven Project\nBullet Points:\n- Volunteer recruitment at ACL conferences\n- New features developed by researchers using the Anthology\n- Importance of preserving operational workflow knowledge\n- Consideration of commercial support for publishing\n\nSlide Title: Future Proofing the Anthology\nBullet Points:\n- Documentation and setup instructions on GitHub\n- Use of Docker for virtualized environment and dependency management\n- Testing and staging on low-cost cloud servers\n- Security concerns with outdated software dependencies\n\nSlide Title: Challenges and Opportunities\nBullet Points:\n- Maintaining community support and extending capabilities\n- Addressing the popularity of pre-prints with anonymous options\n- Leveraging Anthology data for reviewer matching\n- Encouraging community involvement in development and operations\n\nSlide Title: Anonymous Pre-prints and Reviewer Matching\nBullet Points:\n- Challenges with non-anonymous pre-prints\n- Potential collaboration with existing pre-print services or hosting within Anthology\n- Using Anthology data to support reviewer assignment\n- Importance of integrating solutions directly into the Anthology\n\nSlide Title: Conclusion and Call to Action\nBullet Points:\n- ACL Anthology as a key resource for NLP researchers\n- Importance of volunteer recruitment and training\n- Invitation to community members to experiment with Anthology images\n- Open invitation for involvement in system administration, development, and more",
    "Slide Title: Introduction to Machine Translation Systems\nBullet Points:\n- Statistical Machine Translation (SMT) is widely used but costly for large parallel corpora.\n- Rule-Based Machine Translation (RBMT) developed using monolingual corpora.\n- Advantages of Statistical Post Editing (SPE) in domain adaptation.\n- Participation in WAT2015 for various language tasks.\n\nSlide Title: Overview of Toshiba System\nBullet Points:\n- RBMT system is transfer-based with morphological and syntactic analysis.\n- High translation performance through extensive rule sets and dictionaries.\n- SPE adapts translation output to target domains using phrase-based SMT.\n\nSlide Title: Statistical Post Editing (SPE)\nBullet Points:\n- SPE uses phrase-based SMT for domain adaptation.\n- Training involved translating source sentences with RBMT.\n- Language models trained using KenLM for various language pairs.\n\nSlide Title: System Combination using RNNLM\nBullet Points:\n- SPE and SMT generate different translation candidates.\n- System combination achieved through n-best reranking with RNNLM.\n- RNNLMs trained with 500 hidden layers and 50 classes.\n\nSlide Title: Pre/Post Processing Techniques\nBullet Points:\n- Technical term dictionaries selected for each task.\n- English word correction applied for misspelled words.\n- Japanese KATAKANA normalization improved BLEU scores.\n\nSlide Title: Experimental Results\nBullet Points:\n- SPE improves RBMT results on BLEU and RIBES scores.\n- Reranking improves BLEU score by 0.3-0.5 in most tasks.\n- System combination (COMB) shows better performance than other systems.\n\nSlide Title: Official Results and Human Evaluation\nBullet Points:\n- Toshiba systems ranked top three in human evaluation for ja-en, ja-zh, and JPOzh-ja.\n- System combination improves human evaluation for Chinese translations.\n- Discrepancy between automatic and human evaluation noted.\n\nSlide Title: Discussion and Conclusion\nBullet Points:\n- Analysis of differences between Toshiba system and Online A.\n- Importance of faithful translation in human evaluation.\n- Need for multi-system combination for practical use.",
    "Slide Title: Introduction to Phrase-based Statistical Machine Translation (PB-SMT)\nBullet Points:\n- PB-SMT is a data-oriented approach used for over 10 years.\n- Moses toolkit developed by the University of Edinburgh.\n- Training is crucial for creating core knowledge in machine translation.\n- Word/phrase alignment affects translation quality and is time-consuming.\n- IBM models and HMM alignment models are widely used.\n\nSlide Title: Alignment Models in PB-SMT\nBullet Points:\n- GIZA++ implements IBM Models for word alignment.\n- MGIZA is a parallel implementation of IBM models.\n- Associative approaches use statistical measures like Dice coefficient and mutual information.\n- Anymalign and Cutnalign are examples of associative approaches.\n\nSlide Title: Anymalign and Cutnalign Methods\nBullet Points:\n- Anymalign is a multilingual associative aligner using sub-corpora sampling.\n- Cutnalign uses recursive binary segmentation for bilingual alignment.\n- Both methods aim to improve alignment efficiency and quality.\n\nSlide Title: Data Used in Experiments\nBullet Points:\n- Chinese-Japanese JPO Patent Corpus used for experiments.\n- Contains 1 million parallel sentences in Chemistry, Electricity, Mechanical engineering, and Physics.\n- Sentences of 40 words or less used for translation models.\n\nSlide Title: Bilingual Hierarchical Sub-sentential Alignment\nBullet Points:\n- Cutnalign measures translation link strength between word pairs.\n- Uses optimal joint clustering of bipartite graphs for alignment.\n- Recursive segmentation process to align sentence pairs.\n\nSlide Title: Improvements in Alignment Methods\nBullet Points:\n- Multi-processing introduced to accelerate alignment process.\n- Core of Cutnalign re-implemented in C for speed.\n- Approximations in N cut computation reduce search space.\n\nSlide Title: Experiment Settings and Evaluation\nBullet Points:\n- Experiments conducted with GIZA++, MGIZA, and Moses toolkit.\n- Evaluated using BLEU and RIBES metrics.\n- Anymalign and Cutnalign used to build phrase tables.\n- Different timeouts set for Anymalign to test efficiency.\n\nSlide Title: Results and Conclusion\nBullet Points:\n- No significant differences in translation quality with different methods.\n- Fastest training time achieved with Moses 2.1.1 and C version of Cutnalign.\n- Training time reduced to 57 minutes, one fifth of GIZA++/MGIZA.\n- Translation quality remains steady with reduced training time.",
    "Slide Title: Introduction to Neural Machine Translation (NMT)\nBullet Points:\n- NMT has emerged as a successful paradigm in recent years.\n- It offers more fluent translations than previous statistical systems.\n- Adequacy issues remain, such as dropping and repeating words.\n- Various solutions have been proposed, including coverage vectors and gating architectures.\n\nSlide Title: Proposed Solution: Constrained Sparsemax\nBullet Points:\n- Introduces a novel attention transformation: constrained sparsemax.\n- Combines attention sparsity and bounded attention weights.\n- Provides a compact and interpretable set of alignments.\n- End-to-end differentiable, suitable for gradient backpropagation.\n\nSlide Title: Model Architecture and Attention Mechanism\nBullet Points:\n- Utilizes a standard attentional encoder-decoder model.\n- Source words represented by a Bi-LSTM encoder.\n- Attention computed using a probability distribution over source words.\n- Traditional softmax replaced by alternative transformations.\n\nSlide Title: Sparse and Constrained Attention Transformations\nBullet Points:\n- Sparsemax: Projects scores onto a probability simplex, yielding sparse distributions.\n- Constrained Softmax: Bounded attention probabilities using fertility values.\n- Constrained Sparsemax: Combines properties of sparse and bounded probabilities.\n\nSlide Title: Fertility Bounds and Strategies\nBullet Points:\n- Fertility bounds limit attention each word can receive.\n- Three strategies: CONSTANT, GUIDED, and PREDICTED.\n- Sink token added to handle target sentence length issues.\n- Exhaustion strategies encourage attention to words with larger credit.\n\nSlide Title: Experimental Evaluation\nBullet Points:\n- Evaluated on DE-EN, JA-EN, and RO-EN language pairs.\n- Focused on small datasets to address coverage mistakes.\n- Compared against softmax and coverage models.\n- Sparse models showed higher BLEU and METEOR scores.\n\nSlide Title: New Evaluation Metrics: REP-score and DROP-score\nBullet Points:\n- REP-score measures repetition in translations.\n- DROP-score accounts for potentially dropped words.\n- Sparse models generally achieved better REP and DROP scores.\n- Indicates improved coverage and reduced repetition.\n\nSlide Title: Conclusions and Contributions\nBullet Points:\n- Proposed sparse and constrained attention transformations for NMT.\n- Efficient algorithms for forward and backward propagation.\n- Fertility prediction model led to sparse alignments.\n- Improved translation quality by addressing coverage problems.",
    "Slide Title: Introduction to Cross-Lingual Transfer Learning\nBullet Points:\n- Supervised learning dominates NLP but requires large annotated corpora.\n- Many languages lack annotated data, limiting NLP progress.\n- Cross-lingual transfer learning transfers knowledge from high-resource to low-resource languages.\n- Traditional methods use single source languages, limiting transferability.\n- Novel method proposed for zero-shot multilingual transfer inspired by truth inference in crowd-sourcing.\n\nSlide Title: Proposed Method for Multilingual Transfer\nBullet Points:\n- Adapts truth inference ideas to learn quality of transfer and language-specific errors.\n- Uses Bayesian graphical model to infer best labeling in target language.\n- Unsupervised approach identifies reliable models without explicit supervision.\n- Supervised setting uses small annotated corpus for model fine-tuning.\n\nSlide Title: Approach to Multilingual Transfer\nBullet Points:\n- Problem framed as using H models trained in high-resource settings.\n- Simple approaches include single model selection or ensemble voting.\n- Proposed BEA uns method for zero-shot setting without labeled data.\n- RaRe method for few-shot setting with limited annotations.\n\nSlide Title: Zero-Shot Transfer Methodology\nBullet Points:\n- BEA uns model learns non-uniform weighting of component models.\n- Uses probabilistic graphical model to infer model reliability.\n- Differentiates reliable models based on confusion matrices.\n- Entity-level aggregation resolves conflicts in predictions.\n\nSlide Title: Few-Shot Transfer Techniques\nBullet Points:\n- BEA sup uses labeled data to find posterior parameters of Bayesian model.\n- RaRe ranks systems and retrains model on target language.\n- RaRe exploits regularities in text and fine-tunes with supervised data.\n\nSlide Title: Experimental Setup and Data\nBullet Points:\n- Evaluation on Wikiann NER corpus with 41 languages.\n- Comparison with CoNLL 2002 and 2003 datasets.\n- Use of fastText embeddings mapped to English space.\n- BiLSTM-CRF model used for sequential tagging.\n\nSlide Title: Results and Analysis\nBullet Points:\n- Single source direct transfer often underperforms.\n- Multilingual methods outperform majority voting.\n- RaRe and BEA sup show substantial gains with limited supervision.\n- Choice of source languages affects transfer performance.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Cross-lingual transfer includes representation and annotation projection.\n- Multi-source transfer improves NER, POS tagging, and parsing.\n- Proposed models outperform state-of-the-art unsupervised methods.\n- Light supervision and model retraining enhance performance.",
    "Slide Title: Introduction to Neural Machine Translation (NMT)\n\nBullet Points:\n- Evaluation of NMT models for English-Japanese translation\n- Comparison with Statistical Machine Translation (SMT) baselines\n- Exploration of network architectures and recurrent units\n- Examination of pre-reordered data impact on training\n\nSlide Title: Recurrent Neural Networks (RNNs) in NMT\n\nBullet Points:\n- RNNs model temporal data with neural networks\n- Elman Network as a foundational RNN model\n- Backpropagation Through Time (BPTT) for training\n- Challenges in capturing long-term dependencies\n\nSlide Title: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\n\nBullet Points:\n- LSTM units address long-term dependency issues in RNNs\n- Incorporation of gates to control information flow\n- GRU as a simplified alternative to LSTM\n- Comparison of computational complexity between LSTM and GRU\n\nSlide Title: Encoder-Decoder Architecture in NMT\n\nBullet Points:\n- Encodes input sequence into vector representation\n- Generates output sequence using stacked multilayer recurrent units\n- Use of trainable word embeddings and softmax function\n- Challenges with long input sentences and vocabulary size\n\nSlide Title: Soft-Attention Mechanism in NMT\n\nBullet Points:\n- Allows focus on specific parts of input at each step\n- Differentiable computational graph for training\n- Application in RNNsearch model for improved performance\n- Mitigates performance degradation in long sentences\n\nSlide Title: Solutions for Unknown Words in NMT\n\nBullet Points:\n- Fixed vocabulary size challenge in output layer\n- Model-specific approaches like noise-contrastive training\n- Translation-specific approaches using token alignment\n- Back-off system for recovering unknown words\n\nSlide Title: Experiment Setup and Evaluation\n\nBullet Points:\n- Use of ASPEC-JE corpus for English-Japanese translation\n- Evaluation of models based on perplexity\n- Comparison of soft-attention models with LSTM and GRU\n- Impact of pre-reordered data on model performance\n\nSlide Title: Translation Performance and Human Evaluation\n\nBullet Points:\n- Evaluation results from the 2nd Workshop on Asian Translation\n- Ensemble models for improved BLEU scores\n- Human evaluation insights on translation quality\n- Observed error patterns and areas for improvement\n\nSlide Title: Conclusion and Future Directions\n\nBullet Points:\n- Soft-attention NMT models outperform SMT baselines\n- NMT models effective without pre-reordering steps\n- Simple workaround for unknown words with back-off system\n- Need for sophisticated solutions for unknown words in NMT",
    "Slide Title: Introduction to Event Detection (ED)\nBullet Points:\n- ED is a subtask of event extraction, identifying event triggers and types.\n- Challenges include context-dependent event triggers.\n- Existing methods: feature-based and representation-based.\n- Importance of document-level information for ED.\n\nSlide Title: Limitations of Existing ED Methods\nBullet Points:\n- Feature-based methods require manual feature design and may propagate errors.\n- Representation-based methods struggle with capturing event-specific information.\n- Document-level information is often underutilized.\n\nSlide Title: Proposed DEEB-RNN Model\nBullet Points:\n- DEEB-RNN: Document Embedding Enhanced Bi-RNN for sentence-level ED.\n- Utilizes hierarchical and supervised attention mechanisms.\n- Focuses on word-level and sentence-level attention for event detection.\n\nSlide Title: ED Oriented Document Embedding Learning (EDODEL)\nBullet Points:\n- Learns document embeddings using word-level and sentence-level Bi-GRU encoders.\n- Attention mechanism highlights event triggers and relevant sentences.\n- Embeddings enhance event detection in individual sentences.\n\nSlide Title: Document-level Enhanced Event Detector (DEED)\nBullet Points:\n- Uses Bi-GRU encoder and softmax layer for event trigger classification.\n- Integrates document embeddings with word and entity type embeddings.\n- Joint training with EDODEL for improved event detection.\n\nSlide Title: Experimental Setup and Baseline Models\nBullet Points:\n- Evaluated on ACE-2005 dataset with 30 validation, 40 test, and 529 training documents.\n- Compared against feature-based and representation-based baseline models.\n- Hyper-parameters tuned for optimal performance.\n\nSlide Title: Impact of Attention Strategies\nBullet Points:\n- Document embeddings significantly improve ED performance.\n- DEEB-RNN variants with gold attention signals outperform basic Bi-GRU.\n- DEEB-RNN3, using both word and sentence-level attention, shows best results.\n\nSlide Title: Performance Comparison\nBullet Points:\n- DEEB-RNN outperforms state-of-the-art methods in recall and F1-measure.\n- Exploits document-level information effectively.\n- Automatically captures event types without manual rules.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- DEEB-RNN enhances ED with hierarchical and supervised attention.\n- Achieves superior performance on ACE-2005 dataset.\n- Future work: automate weight determination for sentence and document embeddings.",
    "Slide Title: Introduction to Semantic Role Labeling (SRL)\nBullet Points:\n- SRL captures predicate-argument relations, e.g., \"who did what to whom.\"\n- Recent models are BIO-taggers, labeling argument spans for a single predicate.\n- Proposed end-to-end approach predicts all predicates and argument spans in one pass.\n- Utilizes learned, contextualized span representations for SRL graphs.\n- Overcomes limitations of semi-markov and BIO-based models.\n\nSlide Title: Model Overview\nBullet Points:\n- Considers all tokens as possible predicates and continuous spans as arguments.\n- Predicts relations between predicate-argument pairs, including no relation.\n- Uses random variables for each predicate-argument pair, conditionally independent.\n- Scoring function decomposed into unary scores and label-specific scores.\n\nSlide Title: Learning and Beam Pruning\nBullet Points:\n- Minimizes negative log likelihood of the gold structure.\n- Beam pruning addresses computational impracticality of O(n^3|L|) relations.\n- Defines beams for candidate arguments and predicates, ranked by unary scores.\n- Limits maximum span width to reduce computed unary factors.\n\nSlide Title: Neural Architecture\nBullet Points:\n- Builds contextualized representations using BiLSTM outputs.\n- Word-level contexts include pre-trained embeddings and character-based representations.\n- Argument representation includes BiLSTM endpoints, soft head word, and span width features.\n- Predicate representation is the BiLSTM output at the predicate's position.\n\nSlide Title: Scoring Functions\nBullet Points:\n- Implemented with feed-forward networks based on argument and predicate representations.\n- Unary scores for arguments and predicates, and label-specific relational scores.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Tested on CoNLL 2005 and CoNLL 2012 benchmarks with end-to-end and gold predicates setups.\n- Evaluated on micro-averaged F1 for predicting (predicate, argument span, label) tuples.\n- Joint model outperforms previous best pipeline system by F1 difference of 1.3 to 6.0.\n- ELMo embeddings added to improve performance.\n\nSlide Title: Analysis and Effectiveness\nBullet Points:\n- Beam pruning efficiently prunes unlikely argument spans and predicates.\n- Model better at predicting long-distance arguments compared to BIO taggers.\n- Achieves comparable syntactic agreement to previous syntax-based systems.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- New SRL model jointly predicts predicates and argument spans.\n- Better at long-range dependencies and syntactic boundaries, weaker at global consistency.\n- Future work: incorporate higher-order inference methods and self-attention layers.",
    "Slide Title: Introduction to Presuppositions in Pragmatics\nBullet Points:\n- Presuppositions are assumptions or beliefs shared by discourse participants.\n- They are crucial for smooth communication in both spoken and written contexts.\n- Presupposition triggers include definite descriptions, factive verbs, and adverbs.\n- Adverbial triggers like \"again,\" \"also,\" and \"still\" indicate event recurrence or continuation.\n\nSlide Title: Importance of Adverbial Presupposition Triggers\nBullet Points:\n- Adverbial triggers are common in English journalistic texts.\n- They are second only to existential triggers in frequency.\n- Understanding these triggers is vital for language technology systems.\n- They enhance readability and coherence in language generation applications.\n\nSlide Title: Research Objectives and Datasets\nBullet Points:\n- Investigate detection of contexts for adverbial presupposition triggers.\n- Create datasets from Penn Treebank and English Gigaword corpora.\n- Focus on adverbs: too, again, also, still, and yet.\n- Control for confounding factors to ensure model accuracy.\n\nSlide Title: Attention-Based Deep Learning Models\nBullet Points:\n- Explore potential of attention-based models for detecting adverbial triggers.\n- Attention allows models to weigh information from multiple context points.\n- Introduce a new weighted pooling attention mechanism for RNNs.\n- Achieve 82.42% accuracy on predicting \"also\" in the Gigaword dataset.\n\nSlide Title: Related Work and Theoretical Background\nBullet Points:\n- Presupposition theory traces back to Frege and Strawson.\n- Previous studies focus on adverbs as pragmatic presupposition triggers.\n- Attention-based models have shown success in predicting noun phrase definiteness.\n\nSlide Title: Data Extraction and Sample Creation\nBullet Points:\n- Extract datasets from Penn Treebank and English Gigaword corpora.\n- Define samples as 3-tuples: label, tokens, and POS tags.\n- Control for confounding factors and balance positive/negative samples.\n- Use governors of adverbs to extract positive and negative contexts.\n\nSlide Title: Learning Model and Architecture\nBullet Points:\n- Extend bidirectional LSTM with a weighted-pooling attention mechanism.\n- Model all possible word-pair interactions within a sequence.\n- Use attention-over-attention method for aggregating attention scores.\n- No new parameters introduced, minimizing computational impact.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Compare WP model against baselines: MFC, LogReg, LSTM, and CNN.\n- WP model outperforms baselines in predicting presupposition triggers.\n- Analyze model predictions and misclassifications.\n- Highlight challenges in detecting long-range dependencies and coreferences.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- WP model shows promise in classification tasks involving presupposition triggers.\n- Future work to focus on models optimized for data imbalance.\n- Explore applications in language generation and additional attention-based architectures.",
    "Slide Title: Introduction to Extractive Summarization\nBullet Points:\n- Focus on extractive summarization by extracting salient word sequences from source documents.\n- Challenges with using human abstracts for annotations: majority of source words tagged as 0s, mislabeling issues.\n- Neural abstractive summarization offers solutions but faces challenges in maintaining factual accuracy.\n- Study focuses on neural extractive summarization using Cloze-style comprehension questions.\n\nSlide Title: Research Contributions\nBullet Points:\n- Investigate alternative training scheme for extractive summarization.\n- Encourage summaries to be semantically close to human abstracts.\n- Compare methods for converting human abstracts to Cloze-style questions.\n- Achieve superior results on a standard summarization dataset.\n\nSlide Title: Related Work\nBullet Points:\n- Study focuses on generic summarization, differing from query-based summarization.\n- Utilizes Cloze questions for reading comprehension and language understanding.\n- Leverages reinforcement learning to explore extractive summaries.\n- Encourages summaries to preserve salient content and share common words with abstracts.\n\nSlide Title: Our Approach\nBullet Points:\n- System generates summaries by identifying word sequences from source documents.\n- Introduces question-oriented reward to encourage content useful for answering key questions.\n- Uses reinforcement learning to explore extractive summaries.\n- Attention mechanism locates relevant summary segments for questions.\n\nSlide Title: Question-Focused Reward\nBullet Points:\n- Reward summaries that can answer important questions.\n- Encode questions and summaries using Bi-LSTM.\n- Use attention mechanism to predict answers.\n- Optimize system to generate summaries preserving salient content.\n\nSlide Title: Extracting Question-Answer Pairs\nBullet Points:\n- Convert human abstracts to question-answer pairs.\n- Extract entities and keywords as answer tokens.\n- Ensure summaries contain content to answer questions.\n- Encourage system to extract at least one QA pair per sentence.\n\nSlide Title: Reinforcement Learning for Summarization\nBullet Points:\n- Optimize policy for generating extractive summaries.\n- Use Bi-LSTM to encode source documents.\n- Accommodate previous sampling decisions for fluency.\n- Pretrain using bigram-based method for stability.\n\nSlide Title: Experimental Results\nBullet Points:\n- Conduct experiments using CNN dataset.\n- Compare methods with state-of-the-art systems.\n- EntityQ and KeywordQ variants outperform baselines.\n- Vary number of QA pairs to assess impact on performance.\n\nSlide Title: Future Work and Conclusion\nBullet Points:\n- Explore automatic grouping of summary segments into clusters.\n- Conduct usability studies to evaluate summary highlights.\n- New training paradigm for extractive summarization using QA pairs.\n- Approach surpasses state-of-the-art systems in effectiveness.",
    "Slide Title: Introduction to Neural Network Mappings\nBullet Points:\n- Neural network mappings bridge modalities in cross-modal retrieval and zero-shot learning.\n- Typically trained to predict vectors of one modality from another.\n- Success depends on making predicted vectors similar to target vectors.\n- Recent findings show input modality information often propagates into predicted modality.\n\nSlide Title: Key Findings and Objectives\nBullet Points:\n- Introduced a measure to quantify similarity between neighborhood structures of vector sets.\n- Experiments reveal mapped vectors resemble input vectors more than target vectors.\n- Aim to encourage better architectures and semantic-based evaluation criteria.\n\nSlide Title: Related Work and Motivation\nBullet Points:\n- Neural networks and linear mappings popular in cross-modal retrieval.\n- Zero-shot image classification uses image-to-text neural network mapping.\n- Recent work shows predicted vectors retain input vector information.\n- Mapped vectors often resemble input vectors more than target vectors.\n\nSlide Title: Theoretical Background\nBullet Points:\n- Networks with one hidden layer can approximate any function.\n- Continuity ensures topological properties of input are preserved.\n- Real-world unpredictability and noise affect perfect prediction.\n\nSlide Title: Proposed Approach\nBullet Points:\n- Two cross-modal mappings: linear and feed-forward neural network.\n- Explored deeper networks with 3 and 5 hidden layers.\n- Primary loss function: Mean Squared Error (MSE).\n- Focus on cross-space mapping problem independently of class labels.\n\nSlide Title: Experiment 1: Neighborhood Structure of Mapped Vectors\nBullet Points:\n- Introduced mean nearest neighbor overlap measure (mNNO).\n- Measures similarity between neighborhood structures of paired vectors.\n- mNNO quantifies semantic similarity between vector sets.\n\nSlide Title: Experiment 2: Mapping with Untrained Networks\nBullet Points:\n- Explored concept similarity tasks with untrained networks.\n- Compared input vectors with their projections through random mappings.\n- Investigated preservation of semantic structure in input vectors.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Diverse datasets: ImageNet, IAPR TC-12, Wiki.\n- Used various image and text features for robustness.\n- Results show mapped vectors resemble input vectors more than target vectors.\n- Untrained mappings preserve semantic structure of input.\n\nSlide Title: Conclusions\nBullet Points:\n- Neural net mappings often produce vectors akin to input vectors.\n- Proposed measure quantifies similarity between neighborhood structures.\n- Semantic structure is crucial in cross-modal applications.\n- Encourages development of better architectures and evaluation criteria.",
    "Slide Title: Introduction to Complaints in Natural Language\nBullet Points:\n- Complaining is a speech act expressing a negative mismatch between reality and expectations.\n- Importance for linguists, psychologists, organizations, and NLP applications.\n- Complaints differ from negative sentiment; focus on breached expectations.\n\nSlide Title: Contributions of the Study\nBullet Points:\n- First publicly available dataset of complaints from Twitter.\n- Quantitative analysis of linguistic features of complaints.\n- Development of predictive models with high performance.\n- Use of distant supervision and domain adaptation to enhance predictions.\n\nSlide Title: Related Work on Complaints\nBullet Points:\n- Complaints studied in linguistics and marketing as distinct speech acts.\n- Addressed directly or indirectly to responsible parties.\n- Complaints co-occur with other speech acts like warnings and suggestions.\n- Previous computational work on complaints is limited.\n\nSlide Title: Data Collection and Annotation\nBullet Points:\n- New dataset created from Twitter, focusing on customer service interactions.\n- Manual annotation of tweets to identify complaints.\n- High inter-annotator agreement achieved.\n- Dataset includes diverse domains for analysis.\n\nSlide Title: Linguistic Features of Complaints\nBullet Points:\n- Use of unigrams, LIWC, and Word2Vec clusters for feature extraction.\n- Complaints characterized by negations, issues, and pronoun usage.\n- Temporal references and specific verbs are distinctive.\n- Complaints often lack positive sentiment expressions.\n\nSlide Title: Predictive Models for Complaints\nBullet Points:\n- Logistic Regression and neural methods used for prediction.\n- Feature-based models outperform sentiment analysis models.\n- Best performance with bag-of-word features and domain adaptation.\n- Distant supervision improves predictive accuracy.\n\nSlide Title: Domain and Cross-Domain Experiments\nBullet Points:\n- Domain adaptation enhances performance across most domains.\n- Apparel domain shows unique complaint features.\n- Cross-domain models maintain consistent performance.\n- Transport domain presents challenges in prediction.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- First computational approach to modeling complaints in linguistics.\n- Dataset and code available for further research.\n- Potential applications in customer service and opinion analysis.\n- Future work includes identifying complaint targets and sociolinguistic factors.",
    "Slide Title: Introduction to Word Embeddings\nBullet Points:\n- Word embeddings are crucial in NLP applications.\n- Analysis of embedding spaces helps in understanding their utility.\n- Neighborhood-based analysis is limited by unreliability.\n- Geometric analysis offers insights into embedding properties.\n- Aim: Bridge semantic and geometric analysis of embeddings.\n\nSlide Title: Related Work on Word Embeddings\nBullet Points:\n- Evaluating word embedding quality is challenging.\n- Intrinsic evaluations like word similarity have limitations.\n- Nearest neighbors used for semantic information are unstable.\n- Geometric analysis provides quantitative insights.\n\nSlide Title: Geometric Properties of Word Embeddings\nBullet Points:\n- Investigate geometric properties affecting model performance.\n- Consider attributes: position, distribution, global and local distances.\n- Transform embeddings to expose subsets of these attributes.\n\nSlide Title: Affine Transformations\nBullet Points:\n- Used for post-processing word embeddings.\n- Types: translations, reflections, rotations, homotheties.\n- Affect positions of points relative to the origin.\n\nSlide Title: Cosine Distance Encoding (CDE)\nBullet Points:\n- Obfuscates feature distribution using pairwise distance matrix.\n- Avoids non-interpretability of embedding features.\n- Uses autoencoder for mapping back to original space.\n\nSlide Title: Nearest Neighbor Encoding (NNE)\nBullet Points:\n- Retains information about nearest neighborhoods.\n- Sparse vector output forms a directed nearest neighbor graph.\n- Variants: thresholded, weighted, unweighted.\n\nSlide Title: Evaluation of Transformed Embeddings\nBullet Points:\n- Intrinsic evaluations: word similarity, relatedness.\n- Extrinsic evaluations: relation classification, sentiment analysis.\n- CDE transformation affects performance due to semantic noise.\n\nSlide Title: Analysis and Discussion\nBullet Points:\n- Models robust to simple linear transformations.\n- Local geometric structure crucial for downstream tasks.\n- Methods should focus on preserving local similarity.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Quantitative analysis of geometric attributes in embeddings.\n- Intrinsic evaluations sensitive to absolute position.\n- Future work: focus on local geometric structure in embeddings.",
    "Slide Title: Introduction to Social Media-Based Influenza Detection\nBullet Points:\n- Social media platforms, especially Twitter, are valuable for personal information sharing.\n- Twitter's role in event detection systems like earthquakes and disease outbreaks.\n- Focus on detecting seasonal influenza epidemics using Twitter data.\n- Over 30 Twitter-based influenza detection systems developed globally.\n- Challenge: Distinguishing actual flu cases from noise in tweets.\n\nSlide Title: Importance of Forecasting Words\nBullet Points:\n- \"Fever\" and \"headache\" as indicators of upcoming influenza outbreaks.\n- Time lag between tweet frequency and actual patient numbers.\n- Example: 16-day lag for \"fever\" and 55-day lag for \"injection.\"\n- Need to estimate optimal time lag for each forecasting word.\n\nSlide Title: Methodology Overview\nBullet Points:\n- Use of cross-correlation to determine time lag for forecasting words.\n- Construction of a word frequency matrix with time-shifted data.\n- Development of nowcasting and forecasting models using Lasso and Elastic Net.\n\nSlide Title: Dataset and Data Processing\nBullet Points:\n- Collection of 7.7 million influenza-related tweets from 2012 to 2016.\n- Filtering of noise by removing retweets and links.\n- Use of Japanese morphological parser to extract word stems.\n- Analysis of word frequency per day to build a word matrix.\n\nSlide Title: Nowcasting Model Development\nBullet Points:\n- Linear model to estimate current influenza patient numbers.\n- Parameter estimation using Lasso and Elastic Net regularization.\n- Time-shifted models show improved accuracy over baseline models.\n\nSlide Title: Forecasting Model and Performance\nBullet Points:\n- Extension of nowcasting model to predict future patient numbers.\n- Evaluation of forecasting accuracy up to 3 weeks into the future.\n- Superior performance compared to baseline models for short-term predictions.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Nowcasting model achieved a correlation ratio of 0.93.\n- Forecasting model showed a correlation ratio of 0.91 for 1-week prediction.\n- Challenges with overestimation due to news-related tweets.\n- Importance of preprocessing to handle outlier words.\n\nSlide Title: Discussion and Future Directions\nBullet Points:\n- Time shift operation effectively fits IDSC reports.\n- Challenges with inconsistent time shifts across different seasons.\n- Need for techniques to distinguish actual forecasting words from noise.\n- Potential for application to other event predictions.\n\nSlide Title: Conclusion\nBullet Points:\n- Novel social media-based influenza surveillance using forecasting words.\n- Successful prediction of future epidemics beyond practical baseline.\n- First model in literature enabling future epidemic prediction.\n- Potential for broader application to other events.",
    "Slide Title: Importance of Coherence in Texts\nBullet Points:\n- Coherence enables a text to convey its purpose and meaning effectively.\n- Reduces reading time by decreasing cognitive load.\n- Involves integration of sentences around a central theme.\n- Requires logical, spatial, and temporal arrangement of sentences.\n\nSlide Title: Aspects of Text Coherence\nBullet Points:\n- Sentences must relate to each other, exhibiting discourse relations.\n- Coherence is influenced by text organization and structural formation.\n- Lexical and semantic continuity are essential for coherence.\n- Coherent texts are easier to read and revise.\n\nSlide Title: Computational Models of Text Coherence\nBullet Points:\n- Entity Grid model evaluates local cohesion using Centering theory.\n- Role Matrix model extends Entity Grid by incorporating discourse roles.\n- Entity Graph model measures global coherence using graph structures.\n- Limitations include bias towards original text order and data sparsity.\n\nSlide Title: Unsupervised Coherence Models\nBullet Points:\n- Entity Graph model proposed to address supervised model limitations.\n- Focuses on global coherence by relating nonadjacent sentences.\n- Uses bipartite graphs to represent sentences and entities.\n- Measures coherence through average OutDegree of vertices.\n\nSlide Title: Semantic Similarity Graphs\nBullet Points:\n- Approach coherence from a cohesion perspective.\n- Encode text into graphs using semantic similarity between sentences.\n- Utilize GloVe word vectors for sentence representation.\n- Construct graphs using Preceding Adjacent Vertex (PAV), Single Similar Vertex (SSV), and Multiple Similar Vertex (MSV) methods.\n\nSlide Title: Evaluation of Coherence Models\nBullet Points:\n- Document discrimination task: Identify original text from permutated versions.\n- Insertion task: Determine correct position for a removed sentence.\n- PAV method outperformed unsupervised baseline in both tasks.\n- Performance varied based on dataset characteristics and parameters.\n\nSlide Title: Results and Discussion\nBullet Points:\n- PAV method showed strong local coherence performance.\n- Semantic similarity between sentences is crucial for coherence.\n- Distant sentence relations contribute to text coherence.\n- Further refinement needed for capturing long-distance relations.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Proposed unsupervised methods focus on sentence cohesion.\n- PAV method outperformed Entity Graph but not Entity Grid in some tasks.\n- Need for larger datasets in insertion tasks for better evaluation.\n- Future work to refine sentence representation and similarity calculations.",
    "Slide Title: Introduction to BioASQ Challenge\nBullet Points:\nBioASQ is a semantic indexing, question answering (QA), and information extraction challenge.\nFocus on Task B, Phase B: Biomedical QA with factoid and list questions.\nEvaluation metrics: Mean Reciprocal Rank (MRR) for factoid questions, F1 score for list questions.\n\nSlide Title: Traditional vs. Neural QA Systems\nBullet Points:\nTraditional systems use a QA pipeline with named-entity recognition and question classification.\nRequire extensive resources and feature engineering specific to the biomedical domain.\nOur system uses a neural network QA architecture, trained end-to-end.\n\nSlide Title: System Overview\nBullet Points:\nBased on FastQA, an extractive factoid QA system.\nTrained on SQuAD dataset, fine-tuned on BioASQ training set.\nFocus on factoid and list questions, not designed for yes/no questions.\n\nSlide Title: Network Architecture\nBullet Points:\nInput layer maps context and question tokens to high-dimensional word vectors.\nWord vectors include GloVe embeddings, character embeddings, and biomedical Word2Vec embeddings.\nOne-hot encoding of question type (list or factoid) is concatenated to vectors.\n\nSlide Title: Training and Optimization\nBullet Points:\nTwo-step training: Pre-training on SQuAD, fine-tuning on BioASQ.\nUse of Adam optimizer with exponentially decaying learning rate.\nBioASQ dataset preparation involves extracting answer spans from snippets.\n\nSlide Title: Decoding and Ensemble Approach\nBullet Points:\nInference phase retrieves top 20 answer spans via beam search.\nFor factoid questions, output top five answer strings.\nEnsemble model combines five single models using 5-fold cross-validation.\n\nSlide Title: Results and Discussion\nBullet Points:\nReported results for five test batches of BioASQ 5.\nEnsemble model performed slightly better on average.\nSuccessful on factoid questions, varied performance on list questions.\n\nSlide Title: Conclusion\nBullet Points:\nSummarized system design for BioASQ 5B submission.\nNeural architecture trained end-to-end on QA task.\nAchieved state-of-the-art results on factoid questions, competitive on list questions.",
    "Slide Title: Introduction to Stance Classification\nBullet Points:\n- Stance classification identifies users' positions on specific targets from text.\n- Traditionally requires a target-specific classifier built from scratch.\n- Cross-target classification adapts classifiers from related targets.\n- Example: Using climate change stance to predict mining project stance.\n\nSlide Title: Challenges in Cross-Target Classification\nBullet Points:\n- Language models may not be compatible between different targets.\n- Generalization possible through domain-related aspects.\n- Example: \"Reef destruction\" relates to both climate change and mining projects.\n\nSlide Title: Introducing CrossNet Model\nBullet Points:\n- CrossNet is a novel neural model for cross-target stance classification.\n- Utilizes self-attention mechanism to generalize between domain-related targets.\n- Focuses on learning domain-specific aspects for stance prediction.\n\nSlide Title: CrossNet Model Architecture\nBullet Points:\n- Consists of four layers: Embedding, Context Encoding, Aspect Attention, and Prediction.\n- Embedding Layer uses word embeddings for input representation.\n- Context Encoding Layer captures contextual information using BiLSTM.\n\nSlide Title: Aspect Attention Layer\nBullet Points:\n- Discovers domain-specific aspects for stance inference.\n- Uses self-attention to identify core parts of stance-bearing sentences.\n- Captures recurring domain aspects across the corpus.\n\nSlide Title: Prediction and Model Training\nBullet Points:\n- Prediction Layer uses domain aspect encoding for stance label prediction.\n- Model trained using multi-class cross-entropy loss.\n- Utilizes pre-trained GloVe word vectors and ADAM optimizer.\n\nSlide Title: Experimental Results\nBullet Points:\n- Evaluated on domains: Women's Rights, American Politics, and Environments.\n- CrossNet outperforms baseline models in cross-target classification.\n- Shows significant improvement in cross-target tasks compared to in-target tasks.\n\nSlide Title: Visualization and Learned Aspects\nBullet Points:\n- Self-attention highlights domain-relevant sentence parts.\n- Examples: \"feminist\", \"rights\" for women's rights; \"reef\", \"destroy\" for environmental issues.\n- Model captures both domain-specific and sentiment-carrying words.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- CrossNet effectively generalizes stance classification across related targets.\n- Future work: Explore target selection, consider multiple source targets, and evaluate multilingual scenarios.\n- Aim to enhance model's generalization and multilingual capabilities.",
    "Slide Title: Introduction to Sentence Acceptability\n\nBullet Points:\n- Sentence acceptability: well-formedness or naturalness to native speakers\n- Encompasses semantic, syntactic, pragmatic plausibility, and non-linguistic factors\n- Grammaticality: syntactic well-formedness, a theoretical concept\n- Research focus: predicting acceptability judgments\n\nSlide Title: Research Motivation and Hypotheses\n\nBullet Points:\n- Investigate the impact of context on acceptability judgments\n- Test sophisticated language models incorporating document context\n- Hypotheses:\n  - H1: Document context boosts sentence acceptability judgments\n  - H2: Context helps language models model acceptability\n  - H3: Models predict acceptability more accurately with context\n\nSlide Title: Methodology: Data Collection and Annotation\n\nBullet Points:\n- Dataset: Sentences from 100 English Wikipedia articles\n- Round-trip machine translation to generate varying acceptability\n- Crowdsourced acceptability judgments with and without context\n- 20 judgments per sentence, totaling 20,000 annotations\n\nSlide Title: Influence of Context on Acceptability Ratings\n\nBullet Points:\n- Strong correlation between ratings with and without context (Pearson's r = 0.80)\n- Context improves acceptability for ill-formed sentences\n- Context diminishes acceptability for well-formed sentences\n- Context compresses rating distribution, affecting correlation\n\nSlide Title: Modelling Sentence Acceptability\n\nBullet Points:\n- Experiment with neural models: LSTM and topically-driven language model (TDLM)\n- Models trained on 100K English Wikipedia articles\n- Context incorporated during training and testing\n- SLOR measure outperforms others in predicting acceptability\n\nSlide Title: Model Performance and Observations\n\nBullet Points:\n- Context improves model performance across all models and ratings\n- TDLM outperforms LSTM in predicting acceptability\n- Lower correlation when predicting context-judged acceptability\n- Human ratings with context are less predictable\n\nSlide Title: Related Work in Acceptability Prediction\n\nBullet Points:\n- Limited work on acceptability prediction compared to grammatical error detection\n- Previous models: linear regression, unsupervised learners, RNNs\n- Contextual factors often ignored in past research\n\nSlide Title: Conclusions and Future Work\n\nBullet Points:\n- Context positively influences acceptability for ill-formed sentences\n- Incorporating context improves model performance\n- Prediction performance declines with context-judged ratings\n- Future work: Investigate context's reverse effect on well-formed sentences, explore more models",
    "Slide Title: Introduction to Translation and Its Historical Role\nBullet Points:\n- Translation has been crucial in human civilization for law, religion, and trade.\n- Historical evidence of translations dates back four millennia.\n- Translators aim for invisibility, making translations appear as originals.\n- Translation quality has been standardized (ISO 17100).\n\nSlide Title: Universal Features and Interference in Translation\nBullet Points:\n- Translations exhibit unique characteristics compared to originals.\n- Universal features include simplification, standardization, and explicitation.\n- Interference reflects source language \"fingerprints\" on translations.\n- Interference is language-pair specific and affects translation outcomes.\n\nSlide Title: Hypotheses and Main Contributions\nBullet Points:\n- Hypothesis: Shared isomorphic structures lead to shared features in translations.\n- Phylogenetic trees can reflect true distances among source languages.\n- Interference phenomena can cluster source languages into families.\n- Analysis of linguistic phenomena supports language typology reconstruction.\n\nSlide Title: Related Work and Methodology\nBullet Points:\n- Historical linguistics uses bioinformatics methods for phylogenetic trees.\n- Translation studies distinguish between originals and translations using machine learning.\n- Europarl dataset used for analysis, focusing on 17 source languages.\n- Translations into English and French analyzed for interference markers.\n\nSlide Title: Feature Sets for Translation Analysis\nBullet Points:\n- Feature sets include POS trigrams, function words, and cohesive markers.\n- POS trigrams and function words are associated with interference.\n- Cohesive markers reflect universal explicitation effects.\n- Trees based on POS trigrams closely match the gold standard.\n\nSlide Title: Evaluation of Phylogenetic Trees\nBullet Points:\n- Trees evaluated using unweighted and weighted metrics.\n- POS trigrams produce trees closest to the gold standard.\n- Function words also capture grammatical constructs of source languages.\n- Cohesive markers are less effective for source-language identification.\n\nSlide Title: Identification of Source Language\nBullet Points:\n- Machines outperform humans in identifying source languages of translations.\n- Classification accuracy varies with language family proximity.\n- Intra-family linguistic ties influence translation resemblance.\n- Pivot language in translation affects classification accuracy.\n\nSlide Title: Reconstruction of Phylogenetic Language Trees\nBullet Points:\n- Phylogenetic trees generated from English and French translations.\n- POS trigrams effectively capture source-language interference.\n- Trees group languages into Germanic, Romance, and Balto-Slavic families.\n- English translations outperform French in reconstructing language ties.\n\nSlide Title: Analysis of Linguistic Phenomena\nBullet Points:\n- Overuse of definite articles in translations from Balto-Slavic languages.\n- Possessive constructions vary across language families.\n- Verb-particle constructions more common in Germanic translations.\n- Tense and aspect usage reflects source language typology.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Translations reflect language typology and historical connections.\n- Interference is a dominant force in translation products.\n- Future research to explore translations' impact on language evolution.\n- Translations may play a role in language change through interference.",
    "Slide Title: Introduction to Word Embeddings\nBullet Points:\n- Word embeddings map words into low-dimensional vector spaces.\n- Beneficial for NLP tasks like Machine Translation and Question Answering.\n- Limitation: Difficulty in distinguishing different meanings of the same word.\n- Previous solutions lack interpretability and integration with lexical resources.\n\nSlide Title: Addressing Word Sense Disambiguation\nBullet Points:\n- Traditional methods rely on statistics from text corpora.\n- Recent approaches use semantic networks but still face meaning conflation issues.\n- SW2V proposed to learn embeddings for both words and senses using text and semantic networks.\n\nSlide Title: Related Work on Word and Sense Embeddings\nBullet Points:\n- Word2vec and GloVe are popular for word embeddings.\n- Limited research on using lexical resources for disambiguation.\n- Previous models require multiple training steps and small resources like WordNet.\n\nSlide Title: SW2V Model Overview\nBullet Points:\n- SW2V learns embeddings for words and senses in the same vector space.\n- Flexible and scalable for large semantic networks and text corpora.\n- Integrates explicit knowledge into word embedding models.\n\nSlide Title: Shallow Word-Sense Connectivity Algorithm\nBullet Points:\n- Connects words to senses using a semantic network.\n- Linear complexity, efficient for large corpora.\n- Allows words to have multiple associated senses.\n\nSlide Title: Joint Training of Words and Senses\nBullet Points:\n- Extends CBOW architecture to include word senses.\n- Predicts both words and their associated senses.\n- Different configurations for input and output layers.\n\nSlide Title: Evaluation of SW2V\nBullet Points:\n- Evaluated on word similarity and sense clustering tasks.\n- Outperforms state-of-the-art models in capturing word sense semantics.\n- Demonstrates strong interconnectivity between word and sense embeddings.\n\nSlide Title: Applications and Future Work\nBullet Points:\n- Potential for integration into WSD and Entity Linking systems.\n- Plans to apply SW2V to multilingual and cross-lingual applications.\n- Aims to enhance downstream NLP applications with improved sense embeddings.",
    "Slide Title: Introduction to Machine Translation Evaluation\n\nBullet Points:\n- Criticism of automatic MT evaluation metrics\n- BLEU scores often correlate with human judgments\n- Examples of BLEU score failures in translation quality\n- Real-world example of high BLEU but poor manual evaluation\n\nSlide Title: Understanding BLEU Scores\n\nBullet Points:\n- BLEU is a precision-based metric\n- Incorporates brevity penalty to emulate recall\n- High BLEU score indicates closeness to reference translation\n- Traditionally shows high correlation with human judgments\n\nSlide Title: Limitations of BLEU\n\nBullet Points:\n- BLEU fails to account for reordering\n- Example of BLEU's inadequacy in translation quality\n- Human evaluators often prefer translations with lower BLEU scores\n\nSlide Title: Introduction to RIBES\n\nBullet Points:\n- RIBES addresses BLEU's reordering limitations\n- Uses rank correlation coefficient for evaluation\n- Example of RIBES effectiveness in penalizing wrong order\n\nSlide Title: Other MT Evaluation Metrics\n\nBullet Points:\n- Metrics like METEOR and ROUGE-S offer alternatives\n- Focus on linguistic features and semantic adequacy\n- Use of paraphrases and lexical similarity measures\n\nSlide Title: Experimental Setup for WAT 2015\n\nBullet Points:\n- System submission for Korean to Japanese patent translation\n- Use of Moses toolkit for phrase-based SMT\n- Language modeling with KenLM and MERT tuning\n\nSlide Title: Human Evaluation Process\n\nBullet Points:\n- Conducted via Lancers crowdsourcing platform\n- Pairwise comparisons between system and baseline translations\n- Human judgment score calculation\n\nSlide Title: Results and Analysis\n\nBullet Points:\n- High BLEU scores did not correlate with human judgments\n- Segment level meta-evaluation showed disparities\n- Factors contributing to poor correlation identified\n\nSlide Title: Conclusion\n\nBullet Points:\n- High BLEU and RIBES scores may not indicate better translation\n- Analysis of factors affecting correlation with human judgments\n- Aim to improve automatic translation evaluation metrics",
    "Slide Title: Introduction to BioASQ Challenge 2017\nBullet Points:\n- Overview of BioASQ challenge and its objectives\n- Focus on data issued and systems' performance evaluation\n- Tasks conducted from February to May 2017\n- Evaluation using state-of-the-art measures and manual assessment\n\nSlide Title: Overview of BioASQ Tasks\nBullet Points:\n- Three main tasks: semantic indexing, question answering, funding information extraction\n- Task 5a: Large-scale semantic indexing\n- Task 5b: Biomedical semantic question answering\n- Task 5c: Funding information extraction\n\nSlide Title: Task 5a: Large-scale Semantic Indexing\nBullet Points:\n- Classification of PubMed documents into MeSH hierarchy concepts\n- Inclusion of articles from all journals in test data sets\n- Evaluation using flat and hierarchical information retrieval measures\n- Online and large-scale scenario with three independent batches\n\nSlide Title: Task 5b: Biomedical Semantic Question Answering\nBullet Points:\n- Challenge for four types of biomedical questions: yes/no, factoid, list, summary\n- Two phases: retrieval of relevant elements and submission of exact/ideal answers\n- Evaluation based on Mean Average Precision and manual assessment\n- Participation of 51 systems from 17 teams\n\nSlide Title: Task 5c: Funding Information Extraction\nBullet Points:\n- Extraction of grant information from biomedical articles\n- Use of MEDLINE citations as golden data for training and evaluation\n- Evaluation based on micro-recall for grant IDs, agencies, and full-grants\n- Participation of 10 teams with 31 different systems\n\nSlide Title: Systems and Approaches for Task 5a\nBullet Points:\n- Various systems using UIMA-based workflows, Binary Relevance classification, and sequence-to-sequence prediction\n- Use of document to vector (d2v) and tf-idf feature embeddings\n- Integration of multiple evidence using learning to rank (LTR)\n- Baselines: Medical Text Indexer (MTI) and its extensions\n\nSlide Title: Systems and Approaches for Task 5b\nBullet Points:\n- Modular approach with question analysis, candidate answer generation, and answer ranking\n- Use of MetaMap, BM25 model, and semantic role labeling\n- Focus on ideal answer generation using extractive summarization and neural networks\n- Baselines: OAQA system with UIMA-based framework\n\nSlide Title: Systems and Approaches for Task 5c\nBullet Points:\n- Use of SVM, regular expressions, and ensemble methods for grant information extraction\n- Different methodologies including sequential learning models and heuristic rules\n- Baseline: BioASQ Filtering using Naive Bayes and SVM\n\nSlide Title: Results and Evaluation\nBullet Points:\n- Task 5a: Evaluation using micro F-measure and Lowest Common Ancestor F-measure\n- Task 5b: Evaluation based on MAP, accuracy, and manual assessment\n- Task 5c: Evaluation using micro-recall for grant IDs and agencies\n- Systems outperforming strong baselines, indicating methodological advancements\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Overview of the fifth BioASQ challenge and its impact\n- Successful advancement over state-of-the-art systems\n- Plans for more benchmark data and multi-batch scenarios in future editions\n- Growth of the BioASQ community and its significance in biomedical information systems",
    "Slide Title: Introduction to Simultaneous Interpretation\nBullet Points:\n- Simultaneous Interpretation (SI) is a challenging task with cognitive and attentional demands.\n- Interpreters use strategies like generalization and summarization for efficient message delivery.\n- Errors increase with time due to memory and stamina limits.\n- Estimating interpreter performance can enhance Computer-Assisted Interpretation (CAI).\n\nSlide Title: Computer-Assisted Interpretation (CAI)\nBullet Points:\n- CAI systems provide live terminology support to interpreters.\n- Ideal support level depends on interpreter performance.\n- Quality Estimation (QE) systems can adjust information flow to minimize cognitive load.\n\nSlide Title: Quality Estimation for Machine Translation\nBullet Points:\n- QE measures MT output quality without reference translations.\n- QuEst++ is a popular QE pipeline with feature extraction and learning modules.\n- Supports word-level, sentence-level, and document-level QE.\n\nSlide Title: Adapting QE for Interpretation\nBullet Points:\n- SI output differs from MT: includes pauses, hesitations, and paraphrases.\n- New features added to detect struggling interpreters: pauses, hesitations, incomplete words.\n- Features like 'quasi-'cognates and word ratios help in quality estimation.\n\nSlide Title: Evaluation Metrics for Interpretation\nBullet Points:\n- METEOR metric used for evaluating interpreter output.\n- Considers paraphrases and content-function word distinctions.\n- Better suited for SI than traditional MT metrics like BLEU.\n\nSlide Title: Interpretation Corpora and Data\nBullet Points:\n- EN-JA data from NAIST TED SI corpus with different proficiency levels.\n- EN-FR and EN-IT data from EPTIC corpus with European Parliament speeches.\n- Google Translate used for generating reliable METEOR scores.\n\nSlide Title: Interpreter Quality Experiments\nBullet Points:\n- Pearson's r correlation used to evaluate QE system accuracy.\n- Baseline, trimmed, and proposed feature sets compared.\n- Proposed model outperforms in all language settings, especially EN-FR.\n\nSlide Title: Results Analysis and Conclusion\nBullet Points:\n- Ablation studies show importance of specific features like cognates and pauses.\n- Proposed model captures interpretation-specific qualities better.\n- QE application can help CAI systems offer targeted assistance to interpreters.",
    "Slide Title: Introduction to Word Embeddings\n\nBullet Points:\n- Word embeddings enable generalization to unseen words by embedding lexical features into a low-dimensional space.\n- Challenges arise with out-of-vocabulary (OOV) words, especially in low-resource languages.\n- MIMICK-RNN model predicts vectors for OOV words using character-level RNNs.\n- Demonstrates effectiveness in capturing word similarity and improving tagging tasks.\n\nSlide Title: Related Work on Word Embeddings\n\nBullet Points:\n- Compositional models use morphological or orthographic information for unseen words.\n- Character-based approaches avoid issues with names and foreign words.\n- MIMICK is a post-processing step applicable to any word embeddings set.\n\nSlide Title: Supervised Subword Models\n\nBullet Points:\n- Task-specific character-based embeddings are learned within supervised systems.\n- Struggles in low-resource and morphologically rich languages.\n- MIMICK can be combined with supervised models for additive improvements.\n\nSlide Title: Morphosyntactic Attribute Tagging\n\nBullet Points:\n- Evaluated on tagging word tokens for morphosyntactic attributes.\n- Uses a neural sequence labeling approach inspired by POS tagging models.\n- MIMICK shows improvements in tagging across multiple languages.\n\nSlide Title: MIMICK Word Embeddings\n\nBullet Points:\n- Approaches OOV embeddings as a generation problem using a Word Type Character Bi-LSTM.\n- Predicts embeddings for unseen words by training on existing vocabulary.\n- Minimizes squared Euclidean distance to match predicted and pre-trained embeddings.\n\nSlide Title: MIMICK Polyglot Embeddings\n\nBullet Points:\n- Uses Polyglot multilingual word embeddings for experiments.\n- High OOV rates in many languages, addressed by MIMICK prediction models.\n- Nearest-neighbor examination validates the model's effectiveness.\n\nSlide Title: Joint Tagging of POS and Morphosyntactic Attributes\n\nBullet Points:\n- Extends POS tagging models to include morphosyntactic attributes.\n- Uses a multilayer perceptron projection network for attribute-specific tagging.\n- MIMICK vectors improve tagging accuracy, especially in low-resource settings.\n\nSlide Title: Experimental Settings and Results\n\nBullet Points:\n- Evaluated on 23 languages with varying morphological complexity.\n- MIMICK outperforms other models in low-resource and full-resource settings.\n- Significant improvements in Slavic and agglutinative languages.\n\nSlide Title: Conclusion\n\nBullet Points:\n- MIMICK infers OOV word embeddings from limited-vocabulary models.\n- Effective for low-resource languages and task-agnostic applications.\n- Future work may explore other subword units for training.",
    "Slide Title: Introduction to Span-Based Neural Constituency Parsing  \nBullet Points:  \n- High accuracy and simplicity of span-based parsing  \n- Comparison with other neural constituency parsers  \n- Challenges with existing span-based parsers: speed vs. accuracy  \n- Proposal: Combine greedy and chart-based approaches for efficiency  \n\nSlide Title: Proposed Linear-Time Span-Based Neural Parser  \nBullet Points:  \n- Combines merits of greedy and chart-based approaches  \n- Uses left-to-right dynamic programming  \n- Achieves linear-time complexity with beam search  \n- Introduces cube pruning for improved runtime  \n\nSlide Title: Key Contributions  \nBullet Points:  \n- First neural parser with linear time and large search space  \n- Application of cube pruning to incremental parsing  \n- Novel loss function for penalizing wrong spans  \n- Faster parsing for long sentences and discourse parsing  \n\nSlide Title: Span-Based Shift-Reduce Parsing  \nBullet Points:  \n- Maintains a stack of spans for parsing  \n- Shift and reduce actions to form new spans  \n- Labeling spans as constituents or null  \n- Efficient querying with bi-directional LSTM features  \n\nSlide Title: Dynamic Programming and Score Decomposition  \nBullet Points:  \n- Decomposition of tree scores into span scores  \n- Independent label choice for spans  \n- Efficient representation of parsing stack  \n\nSlide Title: Action-Synchronous Beam Search and Cube Pruning  \nBullet Points:  \n- Maintains top b parsing states at each step  \n- Cube pruning to manage beam size and runtime  \n- Efficient handling of predecessor states  \n\nSlide Title: Training with Structured SVM  \nBullet Points:  \n- Loss-augmented decoding for training  \n- Cross-span loss function for better accuracy  \n- Max violation updates for improved training  \n\nSlide Title: Experiments and Results  \nBullet Points:  \n- Experiments on Penn Treebank and PTB-RST discourse treebank  \n- Use of dropout and Adam optimizer for training  \n- State-of-the-art results with beam size optimization  \n- Tractability of parsing longer sequences  \n\nSlide Title: Conclusions  \nBullet Points:  \n- Development of a new efficient neural parser  \n- Linear time complexity with cube pruning  \n- Introduction of a new loss function  \n- Achieved state-of-the-art results in parsing tasks",
    "Slide Title: Introduction to Query Auto-Completion (QAC)\nBullet Points:\n- QAC provides suggested queries as users type, reducing time and cognitive load.\n- Most approaches extend the Most Popular Completion (MPC) algorithm.\n- Personalization in QAC considers user-specific query likelihoods.\n\nSlide Title: Advancements in QAC\nBullet Points:\n- Park and Chiba's approach uses a character LSTM language model.\n- This method handles unseen prefixes and saves memory.\n- Combines personalization with language model effectiveness.\n\nSlide Title: Personalized QAC with FactorCell Model\nBullet Points:\n- FactorCell model adapts language model weights for personalization.\n- Uses user embeddings to transform recurrent layer weights.\n- Demonstrates improved adaptation over traditional methods.\n\nSlide Title: Learning User Embeddings\nBullet Points:\n- Embeddings capture latent demographic factors for each user.\n- Users with fewer queries are grouped together.\n- Online updating of embeddings for unseen users during evaluation.\n\nSlide Title: Recurrent Layer Adaptation Techniques\nBullet Points:\n- Unadapted LM, ConcatCell, and FactorCell models compared.\n- FactorCell allows stronger adaptation with low-rank matrix.\n- Personalized weight matrix for each user enhances performance.\n\nSlide Title: Experimental Data and Implementation\nBullet Points:\n- Experiments use AOL Query data from 2006.\n- Models trained with Adam optimizer; Adadelta for evaluation.\n- Small and large model configurations tested.\n\nSlide Title: Results and Performance Evaluation\nBullet Points:\n- Neural models outperform MPC baseline.\n- FactorCell model shows best performance, especially for seen prefixes.\n- Personalized models improve with more user queries.\n\nSlide Title: Case Studies and Semantic Coherence\nBullet Points:\n- FactorCell model adapts to repetitive search behaviors.\n- Case studies show semantically coherent query adaptations.\n- FactorCell focuses on user-specific search patterns.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Personalization improves LSTM model performance.\n- FactorCell model offers computational efficiency and adaptability.\n- Future work to explore combining language model with MPC.",
    "Slide Title: Introduction to Neural Vector Representations\nBullet Points:\n- Neural vector representations are prevalent in natural language processing.\n- Word vectors have been extensively studied for their properties.\n- Sentence embeddings aim to encode sentences into fixed-length vectors.\n- Evaluated based on cosine similarities reflecting semantic relatedness.\n- Limited understanding of specific semantic properties in sentence embeddings.\n\nSlide Title: Analysis of Sentence Embeddings\nBullet Points:\n- Minor sentence alterations can lead to significant meaning shifts.\n- Sentence triplets used to explore syntactic and semantic changes.\n- High cosine similarity expected between semantically related sentences.\n- Sentence triplets help in insightful automated analyses.\n\nSlide Title: Sentence Modification Schemes\nBullet Points:\n- Not-Negation: Inserting \"not\" before the first verb.\n- Quantifier-Negation: Prepending \"there is no\" to sentences.\n- Synonym Substitution: Replacing verbs with synonyms.\n- Embedded Clause Extraction: Extracting clauses from sentences.\n- Passivization: Changing active voice to passive voice.\n- Argument Reordering: Swapping subject and object.\n- Fixed Point Inversion: Inverting word order around a pivot.\n\nSlide Title: Sentence Triplet Generation\nBullet Points:\n- Five evaluation datasets created using sentence triplets.\n- Negation Detection: Original, Synonym Substitution, Not-Negation.\n- Negation Variants: Quantifier-Negation, Not-Negation, Original.\n- Clause Relatedness: Original, Embedded Clause, Not-Negation.\n- Argument Sensitivity: Original, Passivization, Argument Reordering.\n- Fixed Point Reorder: Original, Semantically equivalent, Fixed Point Inversion.\n\nSlide Title: Experimental Evaluation\nBullet Points:\n- Evaluation datasets created from SICK, Penn Treebank WSJ, MSR Paraphrase corpus.\n- Human annotators ensure grammaticality and semantics.\n- Comparison of three sentence embedding methods: GloVe Averaging, P-Means, Sent2Vec.\n\nSlide Title: Results and Discussion\nBullet Points:\n- Negation Detection: InferSent and SkipThought distinguish negation effectively.\n- Negation Variants: InferSent understands negation quantifiers better.\n- Clause Relatedness: Sent2Vec fares best in distinguishing embedded clauses.\n- Argument Sensitivity: None of the methods distinguish semantic from structural information well.\n- Fixed Point Reorder: SkipThought and InferSent perform well with similar structures.\n\nSlide Title: Conclusion\nBullet Points:\n- Proposed method inspects semantic properties of sentence embeddings.\n- SkipThought and InferSent distinguish negation from synonymy.\n- InferSent identifies semantic equivalence regardless of word order.\n- SkipThought suitable for tasks where semantics align with structure.\n- Neural network-based embeddings outperform simple word averaging.",
    "Slide Title: Introduction to Weblio Pre-reordering SMT System\nBullet Points:\n- Overview of Weblio Pre-reordering Statistical Machine Translation (SMT) System\n- Application of pre-reordering method for English-Japanese translation\n- Integration of statistical language model in pre-reordering\n- Use of N-best reordering results and parse trees for improved translation accuracy\n\nSlide Title: Head-restructured CFG Parse Tree\nBullet Points:\n- Importance of reordering SVO to SOV order in translation\n- Use of dependency trees to capture reordering patterns\n- Introduction of Head-restructured CFG (HRCFG) parse tree\n- Benefits of HRCFG tree in capturing long-distance reordering patterns\n\nSlide Title: Reordering Model with Language Model Integration\nBullet Points:\n- Integration of language model to determine best node order\n- Use of bilingual training data and word alignments\n- Process of reordering node layers in HRCFG tree\n- Example of reordering process using language model\n\nSlide Title: N-best Reordering Approach\nBullet Points:\n- Collection of N-best reordering results for improved translation\n- Example of multiple plausible reordering results\n- Ranking of reordering results by language model score\n\nSlide Title: Experimental Settings and Methodology\nBullet Points:\n- Use of 1-best parse trees for baseline system\n- Tools used: Stanford tokenizer, Berkeley parser, Kytea\n- Training on 1.5M bilingual text for English-to-Japanese translation\n- Implementation of Phrase-based model using Moses toolkit\n\nSlide Title: Experiment Results and Evaluation\nBullet Points:\n- Combination of N-best parse trees and reordering results\n- Improvements in BLEU and RIBES metrics\n- Human evaluation results and comparison with baseline\n- Growth of evaluation scores with increased candidate numbers\n\nSlide Title: Evaluation of Pre-reordering Performance\nBullet Points:\n- Estimation of Kendall's \u03c4 from word alignments\n- Comparison of natural and adjusted order in ASPEC corpus\n- Increase in identical word order after pre-reordering\n\nSlide Title: Error Analysis and Context-aware Translation\nBullet Points:\n- Issues with reordering model and rare/unknown words\n- Efforts to utilize context information in translation\n- Phrase selection problem in English-Japanese translation\n- Use of interpolated language models for domain-specific translation\n\nSlide Title: Conclusion and Future Research\nBullet Points:\n- Summary of reordering model and N-best results\n- Need for further improvement in reordering model stability\n- Potential of interpolating general and in-domain language models\n- Future exploration of pre-reordering models across language pairs",
    "Slide Title: Introduction to Cross-Language Plagiarism\n\nBullet Points:\n- Plagiarism is a significant issue in higher education.\n- Cross-language plagiarism involves translation without attribution.\n- Detecting cross-language plagiarism is challenging due to language differences.\n- Word embeddings can aid in detecting cross-lingual similarities.\n\nSlide Title: Main Contributions\n\nBullet Points:\n- Augmented state-of-the-art methods with word embeddings.\n- Introduced syntax weighting in sentence representations.\n- Achieved high F1 scores for English-French similarity detection.\n\nSlide Title: Evaluation Dataset\n\nBullet Points:\n- Multilingual dataset with French, English, and Spanish texts.\n- Contains cross-language alignment at document, sentence, and chunk levels.\n- Includes both human and machine-translated texts.\n- Designed for rigorous evaluation of cross-language similarity detection.\n\nSlide Title: Overview of State-of-the-Art Methods\n\nBullet Points:\n- CL-CnG: Uses character n-grams for similarity detection.\n- CL-CTS: Measures semantic similarity using conceptual thesaurus.\n- CL-ASA: Uses bilingual unigram dictionary for translation detection.\n- CL-ESA: Represents document meaning using Wikipedia-derived concepts.\n- T+MA: Translates units for monolingual comparison.\n\nSlide Title: Proposed Methods Using Word Embeddings\n\nBullet Points:\n- Word embeddings capture context and semantic similarity.\n- MultiVec toolkit used for computing text representations.\n- CL-CTS-WE: Replaces lexical resources with word embeddings.\n- CL-WES: Direct comparison using cosine similarity of sentence embeddings.\n\nSlide Title: Improving Textual Similarity with Syntax\n\nBullet Points:\n- CL-WESS: Enhances CL-WES with syntax-based weighting.\n- Uses part-of-speech tagging and optimized weights.\n- Achieves better performance than previous methods.\n\nSlide Title: Combining Multiple Methods\n\nBullet Points:\n- Weighted fusion combines similarity scores from different methods.\n- Decision tree fusion uses C4.5 algorithm for optimal classification.\n- Fusion methods outperform individual methods significantly.\n\nSlide Title: Results and Discussion\n\nBullet Points:\n- Word embeddings improve performance over lexical resources.\n- CL-WESS shows significant gains over other methods.\n- Decision tree fusion achieves highest F1 scores at chunk and sentence levels.\n- Methods complement each other for better detection.\n\nSlide Title: Conclusion and Future Work\n\nBullet Points:\n- Word embeddings enhance cross-language similarity detection.\n- CL-WESS outperforms previous state-of-the-art methods.\n- Fusion of methods significantly boosts performance.\n- Future work includes improving CL-WESS and analyzing syntactic weights.",
    "Slide Title: Introduction to Copy & Paste in NLP\n\nBullet Points:\n- Copy & paste is essential for document creation.\n- Archives enable the study of reuse and plagiarism in NLP.\n- Focus on written and spoken materials in NLP.\n\nSlide Title: Context and Background\n\nBullet Points:\n- Originated from ACL's 50th anniversary workshop in 2012.\n- Studies on reuse and plagiarism in NLP papers.\n- PAN international plagiarism detection competition.\n\nSlide Title: Objectives of the Study\n\nBullet Points:\n- Focus on extrinsic detection of near-matches in texts.\n- Compare verbatim raw strings vs. linguistic parsing.\n- Study practices in the NLP field.\n\nSlide Title: The NLP4NLP Corpus\n\nBullet Points:\n- Large corpus of NLP research from 1965 to 2015.\n- Contains 65,003 documents from conferences and journals.\n- Preprocessing for common format and metadata transformation.\n\nSlide Title: Definitions and Terminology\n\nBullet Points:\n- Self-reuse vs. self-plagiarism: citation presence.\n- Reuse vs. plagiarism: citation presence and authorship.\n- Clarification of \"source papers\" and study directions.\n\nSlide Title: Algorithm and Evaluation\n\nBullet Points:\n- Comparison of word sequences for detection.\n- Use of sliding windows and Jaccard distance.\n- Linguistic parsing for higher-level comparison.\n\nSlide Title: Results and Findings\n\nBullet Points:\n- High prevalence of self-reuse and self-plagiarism.\n- Low incidence of reuse and plagiarism.\n- Time delay between publication and reuse.\n\nSlide Title: Discussion and Ethical Considerations\n\nBullet Points:\n- Self-reuse is more common than reusing others' content.\n- Legal and ethical aspects of plagiarism.\n- Justifications for self-reuse and self-plagiarism.\n\nSlide Title: Further Developments and Conclusions\n\nBullet Points:\n- Limitations in detecting altered copy & paste.\n- Future directions: rogeting and table isolation.\n- First study on copy & paste in large NLP corpora.",
    "Slide Title: Introduction to Split-and-Rephrase Task\nBullet Points:\n- Processing long, complex sentences is challenging for humans and NLP tasks.\n- Split-and-Rephrase task aims to break complex sentences into simple ones.\n- Narayan et al. (2017) introduced a dataset and evaluation method for this task.\n- Two system setups: text-to-text and semantics-augmented.\n- Focus on text-to-text setup with SEQ2SEQ models achieving 77.5 BLEU score.\n\nSlide Title: Challenges in SEQ2SEQ Models\nBullet Points:\n- Manual inspection reveals unwanted behaviors in SEQ2SEQ outputs.\n- Issues include unsupported facts, repeated facts, and missing facts.\n- Model memorizes entity-fact pairs instead of learning to split and rephrase.\n- High BLEU score due to memorization and small number of distinct sentences.\n\nSlide Title: Proposed Improvements and New Data Split\nBullet Points:\n- Propose a more challenging data split to aid further research.\n- Establish a stronger baseline with SEQ2SEQ and copy mechanism.\n- New split ensures better generalization and reduces fact memorization.\n- Encourages use of new data split or v1.0 split for future research.\n\nSlide Title: Preliminary Experiments and Task Definition\nBullet Points:\n- Task: Split a complex sentence into simple sentences conveying the same information.\n- Evaluation using averaged individual multi-reference BLEU score.\n- Focus on text-to-text models without RDF triples.\n- Comparison with baselines proposed by Narayan et al. (2017).\n\nSlide Title: Results and Analysis\nBullet Points:\n- SEQ2SEQ models outperform baseline with up to 28.35 BLEU improvement.\n- Single-layer models achieve better results than 3-layer models.\n- Analysis reveals common mistakes: unsupported facts, repetitions, missing facts.\n- Attention weights show model's focus on entities rather than cues.\n\nSlide Title: Copy-Augmented Model\nBullet Points:\n- Augment SEQ2SEQ models with a copy mechanism for better performance.\n- Copy mechanism helps in tasks like abstractive summarization and language modeling.\n- Biasing model towards copying improves performance on split-and-rephrase task.\n- Copy mechanism uses a \"copy switch\" probability for distribution over tokens.\n\nSlide Title: Experiments and Results on New Data Split\nBullet Points:\n- Experiments with copy-enhanced models of varying LSTM widths.\n- Models trained on original, proposed, and v1.0 data splits.\n- Copy-augmented models reduce issues of unsupported and repeated facts.\n- New data split poses a significant challenge for SEQ2SEQ models.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- SEQ2SEQ models achieve high scores without learning to split-and-rephrase.\n- New data split remedies fact memorization and improves task evaluation.\n- Copy-mechanism establishes a new competitive baseline.\n- Encouragement for future research on proposed split or v1.0 dataset.",
    "Slide Title: Introduction to Bilingual Lexicon Induction\n\nBullet Points:\n- New methods for bilingual lexicon induction for low-resource language pairs\n- Importance of reducing reliance on expensive parallel data\n- Multilingual word embeddings as a prevalent strategy\n- Recent advances in unsupervised bilingual lexicon induction\n- Lack of linguistic feature utilization in modern methods\n\nSlide Title: Background and Methodology\n\nBullet Points:\n- Based on Artetxe et al. (2017) framework\n- Word embedding matrices for source and target languages\n- Objective: Find mapping matrix W for aligned embeddings\n- Iterative self-learning framework to reduce seed dictionary need\n\nSlide Title: Orthographic Extension of Word Embeddings\n\nBullet Points:\n- Augment embeddings with orthographic information\n- Append letter count vector to each word's embedding\n- Normalize augmented vectors for self-learning framework\n- Use of combined alphabets for orthographic extension\n\nSlide Title: Orthographic Similarity Adjustment\n\nBullet Points:\n- Modify similarity score with orthographic similarity\n- Use normalized string edit distance for similarity measure\n- Calculate similarity for a subset of word pairs\n- Symmetric Delete spelling correction algorithm for pair selection\n\nSlide Title: Experiments and Datasets\n\nBullet Points:\n- Datasets: English-Italian, English-German, English-Finnish\n- Use of numeral identity translations for dictionary\n- Tunable hyperparameters for method optimization\n- Development data for hyperparameter selection\n\nSlide Title: Results and Discussion\n\nBullet Points:\n- Orthographic extension and similarity adjustment results\n- Significant computation savings with heuristic\n- Comparison with Artetxe et al. (2017) system\n- Identity translation component for improved accuracy\n\nSlide Title: Performance Analysis\n\nBullet Points:\n- Best results with combined methods for most language pairs\n- Different performance trends for English-Finnish\n- Mapping between source and target letters as a factor\n- Correct translations missed by baseline system\n\nSlide Title: Conclusion and Future Work\n\nBullet Points:\n- Improved accuracy with orthographic information\n- Applicability to low-resource language pairs\n- Future work: Extend to languages with different alphabets\n- Potential for other unsupervised bilingual lexicon systems",
    "Slide Title: Introduction to Semantic Graphs\nBullet Points:\n- Semantic graphs like WordNet represent human knowledge through language structure.\n- Local properties focus on specific semantic concept connections.\n- Global properties encode emergent regularities in relation graphs.\n- Global features can aid local tasks like relation prediction.\n\nSlide Title: Motivation and Hypothesis\nBullet Points:\n- Local models may misinterpret relations due to lack of global context.\n- Global graph features can constrain local predictions to be structurally sound.\n- Features include aggregate counts of subgraph structures, known as motifs.\n\nSlide Title: Exponential Random Graph Model (ERGM)\nBullet Points:\n- ERGM is a log-linear model over networks using global graph features.\n- Likelihood of a graph is computed by exponentiating a weighted sum of features.\n- Challenges include scaling ERGMs to large graphs like WordNet.\n\nSlide Title: Max-Margin Markov Graph Model (M3GM)\nBullet Points:\n- M3GM extends ERGM with a margin-based objective for better scalability.\n- Incorporates labeled motifs and combines graph-level features with word sense embeddings.\n- Trained as a re-ranker, improving performance on the WN18RR dataset.\n\nSlide Title: Related Work in Relation Prediction\nBullet Points:\n- Embedding-based methods model relations but often lack structural information.\n- Recent works explore composing single edges into motifs and using Graph Convolutional Networks.\n- M3GM introduces interaction features between relations for prediction.\n\nSlide Title: Graph Motifs as Features\nBullet Points:\n- Features include edge count, cycles, paths, and transitivity.\n- Semantic graphs are multigraphs with multiple relation types.\n- Combinatory motifs extracted for all relation type combinations.\n\nSlide Title: Parameter Estimation and Proposal Distribution\nBullet Points:\n- MCMLE used for approximating ERGM probability.\n- Margin loss objective ensures negative samples score lower than observed network.\n- Proposal distribution samples negative edges based on local association scores.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- M3GM evaluated on relation graph edge prediction task using WN18RR dataset.\n- Metrics include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at k (H@k).\n- M3GM shows clear advantage in re-ranking top local candidates.\n\nSlide Title: Synset Embeddings and Initialization\nBullet Points:\n- Synset embeddings created using pre-trained word embeddings.\n- Methods include averaging and AutoExtend retrofitting with Mimick.\n- Distributional signal crucial for semantic setup.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- M3GM improves link prediction by combining distributional and structural coherence.\n- Potential for scoring bundles of new edges for semantic graph extension.\n- Future exploration includes multilingual applications and knowledge graph expansion.",
    "Slide Title: Introduction to Geolocation Prediction in Social Media\nBullet Points:\n- Social media as a source for analyzing public opinions\n- Importance of geolocation prediction for various analyses\n- Twitter's suitability for geolocation prediction\n- Proposal of a neural network model for Twitter geolocation prediction\n\nSlide Title: Key Contributions of the Study\nBullet Points:\n- Introduction of a neural network model with attention mechanism\n- Model outperforms previous ensemble approaches\n- Analysis of model components for insight into unification processes\n- Potential extension to other social media analyses\n\nSlide Title: Related Works: Text-based Approaches\nBullet Points:\n- Use of word distributions for geolocation prediction\n- Success of maximum likelihood and language modeling approaches\n- Exploration of topic modeling and supervised machine learning methods\n\nSlide Title: Related Works: User-network and Metadata-based Approaches\nBullet Points:\n- User interactions as network links for geolocation prediction\n- Use of friendship and follower information in models\n- Metadata fields as effective clues for geolocation prediction\n\nSlide Title: Proposed Neural Network Model\nBullet Points:\n- Integration of text, metadata, and user network information\n- Use of attention mechanism for unified representations\n- Model components: Text, Metadata, and User Network\n\nSlide Title: Evaluation and Performance Metrics\nBullet Points:\n- Evaluation on TwitterUS and W-NUT datasets\n- Metrics: accuracy, accuracy@161, median and mean error distances\n- Significant improvements in accuracy and accuracy@161\n\nSlide Title: Discussion and Analysis\nBullet Points:\n- Analysis of attention probabilities and unification strategies\n- Identification of typical attention patterns through clustering\n- Limitations in handling location changes and travel states\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Successful unification of text, metadata, and user network information\n- Plans to expand model for multiple locations and temporal states\n- Application to other social media analyses like gender and age analysis",
    "Slide Title: Introduction to NLP Learning Methods\nBullet Points:\n- NLP learning methods are increasingly dominated by end-to-end differentiable functions.\n- Traditional NLP used modular processing pipelines (e.g., tokenization, tagging, parsing).\n- Pipelines make discrete decisions, seemingly incompatible with neural learning.\n- Recent findings suggest benefits from linguistic preprocessing.\n- Proposed solution: Treat pipelines as layers in neural architectures.\n\nSlide Title: Challenges with Current Methods\nBullet Points:\n- Reinforcement learning and structured attention replace argmax with sampling/marginalization.\n- Downsides: Inefficiency and difficulty in inspecting intermediate outputs.\n- Straight-through estimator (STE) treats discrete decisions as differentiable, ignoring constraints.\n- Quality of intermediate representations degrades under STE.\n\nSlide Title: Introduction to SPIGOT\nBullet Points:\n- SPIGOT: Structured Projection of Intermediate Gradients Optimization Technique.\n- Defines a proxy for the gradient of a loss function with respect to argmax input.\n- Aims to respect constraints in the argmax problem.\n- Applicable to any intermediate layer expressible as a constrained maximization problem.\n\nSlide Title: Methodology of SPIGOT\nBullet Points:\n- Allows structured argmax layer to be treated like a differentiable function.\n- Introduces a gradient proxy for backpropagation through argmax layers.\n- Uses projection of gradients to respect constraints.\n- Applicable to structured intermediate tasks followed by end tasks.\n\nSlide Title: Relaxed Decoding and SPIGOT\nBullet Points:\n- Decoding problems decomposed into parts, e.g., arcs in a dependency tree.\n- Relaxation transforms discrete problems into linear objectives over convex polytopes.\n- SPIGOT introduces a projection step to keep updates within feasible sets.\n- Ensures parameter updates respect constraints.\n\nSlide Title: Backpropagation through Pipelines\nBullet Points:\n- SPIGOT enables backpropagation through NLP pipelines.\n- Intermediate task outputs used as features for end tasks.\n- Joint learning by minimizing end-task and intermediate-task loss functions.\n- Flexible for various training scenarios.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Evaluated on syntactic-then-semantic parsing and sentiment classification.\n- SPIGOT outperforms state-of-the-art semantic dependency parsers.\n- Demonstrates improvement over pipelines, structured attention, and STE.\n- Semantic parsing improves sentiment classification accuracy.\n\nSlide Title: Analysis and Impact\nBullet Points:\n- End-task training signals can influence intermediate predictions.\n- Joint training with SPIGOT improves semantic parsing performance.\n- Changes in syntactic parsing accuracy can benefit semantic parsing.\n- SPIGOT's flexibility allows for better model understanding and improvement.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- SPIGOT provides a novel approach to backpropagating through discrete decisions.\n- Empirical results show stronger performance than baselines.\n- Open-source implementation available for further research.\n- Future work: Explore more syntactic information for improved performance.",
    "Slide Title: Introduction to Story Understanding\n\nBullet Points:\n- Understanding stories involves reasoning about causal links and mental states of characters.\n- Commonsense story cloze task highlights the difficulty for machine readers.\n- Language models struggle with abstracting complex implied dynamics.\n- New annotation formalism labels mental states in commonsense stories.\n\nSlide Title: Dataset Properties\n\nBullet Points:\n- Provides a fully-specified chain of motivations and emotional reactions.\n- Captures implied effects not directly mentioned in the story.\n- Includes formal labels from psychology theories and open text descriptions.\n- Corpus spans 15k stories with 300k annotations for 150k character-line pairs.\n\nSlide Title: Mental State Representations\n\nBullet Points:\n- Draws from philosophy and psychology to annotate causal dynamics.\n- Uses Maslow's hierarchy of needs and Reiss's basic motives for motivation categories.\n- Plutchik's wheel of emotions used for emotion categorization.\n- Open text descriptions complement theory categories for mental state explanations.\n\nSlide Title: Annotation Framework and Challenges\n\nBullet Points:\n- Annotates simple commonsense stories with rich character interactions.\n- Unique challenges include interpreting discourse and understanding implicit effects.\n- New annotation framework uses crowdsourced workers for scalability.\n\nSlide Title: Annotation Pipeline\n\nBullet Points:\n- Entity Resolution identifies characters and their mentions in sentences.\n- Action Resolution determines if a character's action has a motivation.\n- Affect Resolution identifies emotional reactions, even if characters aren't mentioned.\n- Motivation and Emotional Reaction tasks annotate motives and emotions.\n\nSlide Title: Dataset Statistics and Insights\n\nBullet Points:\n- Theory category assignments are costly, so only a third of stories are fully annotated.\n- Dataset includes 300k annotations across 15,000 stories.\n- Quality control measures ensure high-quality annotations.\n- Agreement analysis shows thematic similarities in annotator disagreements.\n\nSlide Title: Tasks and Baseline Models\n\nBullet Points:\n- New tasks include state classification, annotation classification, and explanation generation.\n- Baseline models use various encoders like TF-IDF, GloVe, LSTM, REN, and NPN.\n- Models predict psychological states and generate explanations for character states.\n\nSlide Title: Experimental Results and Conclusion\n\nBullet Points:\n- Models outperform random baselines in state classification tasks.\n- Pretraining on free-response data boosts performance.\n- Dataset serves as a resource for training and evaluating mental state tracking.\n- Future testbed for models learning common sense about emotional reactions and motivations.",
    "Slide Title: Introduction to Satire Detection\nBullet Points:\n- Satire is an art form used to criticize entertainingly using humor, irony, sarcasm, and exaggeration.\n- Satirical news mimics regular news but doesn't aim to deceive readers.\n- Satire detection is crucial for distinguishing satirical from regular news.\n- Current models often misrepresent satire by focusing on publication characteristics.\n\nSlide Title: Challenges in Satire Detection\nBullet Points:\n- Human readers sometimes struggle to recognize satire.\n- Automatic systems often rely on publication sources for labeling.\n- Models may misinterpret satire by focusing on publisher traits.\n\nSlide Title: Proposed Model for Satire Detection\nBullet Points:\n- Introduces adversarial training to control for publication source bias.\n- Maintains satire detection performance while focusing on satire characteristics.\n- Publishes a large German dataset for satire detection.\n\nSlide Title: Previous Work in Satire Detection\nBullet Points:\n- Early models used handcrafted features like entity context and story coherence.\n- Recent models use word embeddings and neural networks.\n- Adversarial training is used to improve model robustness.\n\nSlide Title: Methods for Satire Classification\nBullet Points:\n- Limitations of previous methods include reliance on publication sources.\n- New model uses shared feature extractor for satire detection and publication identification.\n- Adversarial training helps focus on satire rather than publication traits.\n\nSlide Title: Model Architecture and Training\nBullet Points:\n- Uses word embeddings and a bidirectional LSTM with self-attention.\n- Satire detector performs binary classification at the document level.\n- Publication identifier performs multi-class classification.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Evaluates using precision, recall, and F1 score for satire detection.\n- Adversarial training reduces publication identification performance.\n- Satire detection remains effective despite reduced publication focus.\n\nSlide Title: Analysis and Interpretation\nBullet Points:\n- Adversarial training shifts focus from publication-specific words to satire cues.\n- Attention weights show model's ability to identify linguistic satire characteristics.\n- Future work could explore other confounding variables like time and region.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Adversarial training helps models focus on satire characteristics.\n- Future research could investigate other confounding factors in satire detection.\n- More qualitative and quantitative analysis needed for model behavior understanding.",
    "Slide Title: Introduction to Deep Learning Sentence Representations\nBullet Points:\n- Deep learning enables automatic learning of continuous sentence representations.\n- Representations can be task-specific or aim for \"universal\" performance across tasks.\n- Learning can be unsupervised or supervised.\n- Novel encoder-decoder architecture proposed for static sentence representations.\n\nSlide Title: Advances in Neural Machine Translation (NMT)\nBullet Points:\n- NMT has improved translation quality with the introduction of \"attention.\"\n- Attention allows dynamic focus on important parts of the source sentence.\n- Proposed architecture combines attention with static sentence representations.\n\nSlide Title: Related Work in Sentence Representations\nBullet Points:\n- Continuous sentence representations have been explored in NMT.\n- RNN sequence-to-sequence models visualize semantic and syntactic structures.\n- Evaluations include sentence classification and semantic similarity tasks.\n\nSlide Title: Proposed Model Architectures\nBullet Points:\n- Variants differ in encoder state processing and decoder usage.\n- Main architecture: compound attention with multiple attention heads.\n- Experimentation with RNN-based and Transformer models.\n\nSlide Title: Compound Attention Mechanism\nBullet Points:\n- Incorporates attention in both encoder and decoder.\n- Uses inner attention to compute weighted averages of encoder states.\n- Structured sentence embedding used in decoder attention.\n\nSlide Title: Evaluation of Sentence Representations\nBullet Points:\n- Evaluated using SentEval classification and similarity tasks.\n- Paraphrase evaluation with COCO and HyTER datasets.\n- Metrics include cluster classification accuracy and nearest-neighbor retrieval.\n\nSlide Title: Experimental Results and Translation Quality\nBullet Points:\n- English-to-German and English-to-Czech NMT models trained.\n- Translation quality assessed using BLEU scores.\n- Relation between representation quality and translation performance analyzed.\n\nSlide Title: Discussion and Conclusion\nBullet Points:\n- Negative correlation between BLEU scores and representation quality.\n- Multi-headed sentence representation challenges in paraphrase tasks.\n- Compound attention provides translation quality but struggles with meaning representation.\n- Importance of considering meaning in machine translation systems.",
    "Slide Title: Introduction to Neural Sequence-to-Sequence Models\nBullet Points:\n- SEQ2SEQ models excel in NLP tasks like machine translation and parsing.\n- Require large parallel datasets; non-parallel corpora can enhance performance.\n- Unsupervised SEQ2SEQ models proposed for summarization tasks.\n- SEQ3 autoencoder introduced for end-to-end training via gradient optimization.\n\nSlide Title: SEQ3 Model Overview\nBullet Points:\n- SEQ3 is a sequence-to-sequence-to-sequence autoencoder.\n- Comprises two attentional encoder-decoder pairs: Compressor (C) and Reconstructor (R).\n- C generates a summary from input text; R reconstructs input from the summary.\n- Employs differentiable approximations for sampling from categorical distributions.\n\nSlide Title: Compressor Functionality\nBullet Points:\n- Embedding layer projects source sequence to word embeddings.\n- Bidirectional RNN encodes embeddings, producing concatenated states.\n- Attentional RNN decoder generates summary using global attention and input feeding.\n- Summary length sampled from a uniform distribution for varying compression ratios.\n\nSlide Title: Differentiable Word Sampling Techniques\nBullet Points:\n- Soft-Argmax used for training by passing weighted sum of word embeddings.\n- Gumbel-Softmax trick adds stochasticity and facilitates exploration.\n- Straight-Through estimator used to align training and test time embeddings.\n\nSlide Title: Reconstructor and Decoder Initialization\nBullet Points:\n- Reconstructor operates on embeddings of summary words.\n- Decoder initialized using transformation of last hidden states and length vector.\n- Similar initialization for both compressor and reconstructor decoders.\n\nSlide Title: Loss Functions in SEQ3\nBullet Points:\n- Reconstruction Loss: Negative log-likelihood of reconstructed words.\n- LM Prior Loss: Ensures readability using pretrained language model.\n- Topic Loss: Encourages preservation of topic-indicating words.\n- Length Penalty: Helps predict end-of-sequence token.\n\nSlide Title: Experimental Results and Comparisons\nBullet Points:\n- SEQ3 trained on Gigaword dataset; tested on DUC-2003 and DUC-2004.\n- Outperforms unsupervised Pretrained Generator and LEAD-8 baseline.\n- LM prior and topic loss crucial for performance.\n- Comparable to supervised methods in some metrics.\n\nSlide Title: Limitations and Future Work\nBullet Points:\n- Tendency to copy first words of input in compressed text.\n- Autoregressive nature of reconstructor may cause cascading errors.\n- Future exploration in tasks like unsupervised machine translation and caption generation.\n\nSlide Title: Implementation and Hyper-parameters\nBullet Points:\n- Implemented in PyTorch with LSTMs for RNNs.\n- Shared encoder for compressor and reconstructor; separate decoders.\n- GloVe embeddings used for initialization; layer normalization applied.\n- Training involves multiple loss functions with specific hyper-parameters.",
    "Slide Title: Introduction to Taylor's Law in Text Analysis\nBullet Points:\n- Taylor's law describes variance growth with respect to the mean in event clustering.\n- Applied across various domains: ecology, physics, finance, etc.\n- Limited application in natural language analysis.\n- Study applies Taylor's law to lexical fluctuation in texts.\n- Taylor exponent found to be a universal characteristic of texts.\n\nSlide Title: Related Work and Fluctuation Analysis\nBullet Points:\n- Quantifying text complexity through entropy rate and scaling behaviors.\n- Power laws like Zipf's and Heaps' are known in linguistic data.\n- Fluctuation analysis quantifies event clustering in sequences.\n- Long-range correlation analysis applied to text intervals.\n\nSlide Title: Measuring the Taylor Exponent\nBullet Points:\n- Taylor's law holds when mean and standard deviation are correlated by a power law.\n- Taylor exponent (\u03b1) ranges from 0.5 to 1.0 in various domains.\n- Method involves segmenting text and calculating mean and standard deviation for words.\n- Least-squares method used to estimate exponent in log-log coordinates.\n\nSlide Title: Taylor Exponents for Real Data\nBullet Points:\n- Taylor exponent indicates consistent word co-occurrence.\n- Exponent for natural language texts around 0.58.\n- Exponent varies slightly with text size.\n- Different data types show distinct Taylor exponents.\n\nSlide Title: Analysis of Different Data Types\nBullet Points:\n- Natural language texts vs. other data: programming, music, child-directed speech.\n- Exponents for non-text data are significantly larger.\n- Taylor exponent can distinguish data types.\n- Potential for application in diverse data sources.\n\nSlide Title: Evaluating Machine-Generated Text\nBullet Points:\n- Machine-generated text often has a Taylor exponent of 0.50.\n- Indicates limitations in capturing word clustering behavior.\n- Neural models evaluated by perplexity and Taylor exponent.\n- Machine translation retains original text's clustering behavior.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Taylor's law quantifies word co-occurrence in texts.\n- Written texts have higher complexity than programming or music.\n- Taylor exponent useful for evaluating machine-generated text.\n- Future work: Twitter data, adult speech, grammatical complexity analysis.\n\nSlide Title: Implications and Applications\nBullet Points:\n- Taylor exponent as a tool for computational linguistics.\n- Potential to assess language models and text complexity.\n- Application in statistical tests for structural complexity.\n- Broader implications for understanding language and data patterns.",
    "Slide Title: Introduction to Deep Learning in Question Answering\nBullet Points:\n- Deep learning applied to various question answering tasks: images, tabular data, text\n- Importance of evaluating model accuracy on representative test sets\n- Proposal of techniques to analyze model sensitivity to question words\n\nSlide Title: Key Contributions\nBullet Points:\n- Analysis workflow for understanding question answering models\n- Use of Integrated Gradients (IG) for attributing predictions to question words\n- Identification of model weaknesses and crafting adversarial questions\n- Introduction of an overstability test for question answering networks\n\nSlide Title: Visual Question Answering (VQA) Analysis\nBullet Points:\n- Task: Answer questions about images using deep networks\n- Findings: Network relies heavily on images, ignoring many question words\n- Example: Model retains accuracy even when most question words are removed\n\nSlide Title: QA on Tables Analysis\nBullet Points:\n- Task: Answer questions on tabular data using Neural Programmer (NP)\n- Findings: NP influenced by content-free words more than important ones\n- Example: Dropping content-free words reduces accuracy significantly\n\nSlide Title: Reading Comprehension Analysis\nBullet Points:\n- Task: Answer questions based on text passages\n- Findings: Network often ignores important words\n- Example: Adversarial attacks more successful when key question words are included\n\nSlide Title: Integrated Gradients (IG) Methodology\nBullet Points:\n- Attribution technique to isolate influential question words\n- IG aggregates gradients to attribute prediction changes to input variables\n- Empirical validation through question perturbations\n\nSlide Title: Overstability and Attacks\nBullet Points:\n- Overstability: Networks overly stable to changes in question semantics\n- Attacks: Exploit reliance on generic words to craft adversarial examples\n- Example: Subject ablation and prefix attacks significantly reduce model accuracy\n\nSlide Title: Related Work and Methodology\nBullet Points:\n- Comparison with Jia and Liang (2017) on adversarial attacks\n- Use of IG for model-specific analysis and targeted attacks\n- Discussion of task-specific related work and alternative attribution methods\n\nSlide Title: Conclusion and Implications\nBullet Points:\n- Attribution techniques reveal model weaknesses\n- Importance of addressing under-reliance on key question terms\n- Potential for improving QA models and datasets using attribution insights\n\nSlide Title: Reproducibility and Future Work\nBullet Points:\n- Code available for generating attributions and reproducing results\n- Encouragement for further research on attribution-based model analysis\n- Exploration of additional QA models for shared weaknesses",
    "Slide Title: Introduction to Constituency Parsing\nBullet Points:\n- Importance of fast and accurate constituency parsing in NLP\n- Applications in relation extraction, paraphrase detection, and more\n- Recent success of neural network-based approaches\n\nSlide Title: Parsing Approaches\nBullet Points:\n- Transition-based models: sequence of local decisions\n- Chart-based models: structured loss functions and exact inference\n- Challenges: compounding errors and computational cost\n\nSlide Title: Proposed Model: Syntactic Distance\nBullet Points:\n- Novel fully-parallel model based on \"syntactic distance\"\n- Top-down approach to construct parse trees\n- Parallelization benefits with modern computing architectures\n\nSlide Title: Syntactic Distances of a Parse Tree\nBullet Points:\n- Definition and role of syntactic distances\n- Conversion of parse tree to compact representation\n- Algorithms for mapping representation back to parse tree\n\nSlide Title: Model Architecture\nBullet Points:\n- Use of neural networks to estimate syntactic distances\n- Combination of LSTM and convolutional layers\n- Prediction of constituent labels and syntactic distances\n\nSlide Title: Objective and Loss Functions\nBullet Points:\n- Training objective: prediction losses of syntactic distances and labels\n- Use of softmax classifier and cross-entropy loss for labels\n- Pair-wise learning-to-rank loss for syntactic distances\n\nSlide Title: Experimental Results\nBullet Points:\n- Evaluation on Penn Treebank and Chinese Treebank datasets\n- Achieved strong performance with labeled F1 scores\n- Comparison with other models and state-of-the-art results\n\nSlide Title: Parsing Speed and Efficiency\nBullet Points:\n- Efficient O(n log n) algorithm for distance to tree conversion\n- Parallel execution reduces running time\n- Comparison of parsing speed with other neural parsers\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Overview of neural network models in parsing\n- Transition-based and chart-based parsing methods\n- Conclusion: Novel scheme with strong performance and efficiency",
    "Slide Title: Introduction to Sentiment-to-Sentiment Translation\nBullet Points:\n- Sentiment-to-sentiment translation changes the sentiment of a sentence while preserving its semantic content.\n- Important in NLP for applications like review sentiment transformation and news rewriting.\n- Lack of parallel training data is a major challenge.\n- Existing methods often fail to preserve semantic content.\n\nSlide Title: Proposed Cycled Reinforcement Learning Approach\nBullet Points:\n- Introduces a neutralization module and an emotionalization module.\n- Neutralization module extracts non-emotional semantic content.\n- Emotionalization module adds sentiment to the neutralized content.\n- Uses cycled reinforcement learning to train with unpaired data.\n\nSlide Title: Neutralization and Emotionalization Modules\nBullet Points:\n- Neutralization module filters out emotional words using LSTM.\n- Emotionalization module uses a bi-decoder framework to add sentiment.\n- Pre-training with a self-attention based sentiment classifier.\n\nSlide Title: Cycled Reinforcement Learning Mechanism\nBullet Points:\n- Neutralization module neutralizes input to semantic content.\n- Emotionalization module reconstructs the original sentence with sentiment.\n- Uses policy gradient to train the neutralization module.\n- Rewards based on sentiment confidence and content preservation.\n\nSlide Title: Experimental Setup and Datasets\nBullet Points:\n- Experiments conducted on Yelp and Amazon review datasets.\n- Reviews categorized as positive or negative based on ratings.\n- Pre-processing includes filtering and sentiment classification.\n\nSlide Title: Evaluation Metrics and Baselines\nBullet Points:\n- Automatic evaluation using sentiment accuracy and BLEU score.\n- Human evaluation for sentiment and semantic similarity.\n- Baselines include Cross-Alignment Auto-Encoder and Multi-Decoder with Adversarial Learning.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Proposed method outperforms baselines in content preservation.\n- Improved BLEU scores indicate better semantic preservation.\n- Human evaluation shows agreement with automatic metrics.\n\nSlide Title: Error Analysis and Future Work\nBullet Points:\n- Challenges with sentiment-conflicted and neutral sentences.\n- Need for better sentiment-aware decoders.\n- Future work to explore fine-grained sentiment translation.",
    "Slide Title: Introduction to Domain-Specific Sentiment Analysis\nBullet Points:\n- Words used to express opinions vary by domain\n- Polarity of words can change across domains (e.g., \"unpredictable\")\n- Supervised algorithms struggle with cross-domain generalization\n- Domain adaptation techniques transfer knowledge across domains\n\nSlide Title: Challenges in Cross-Domain Sentiment Analysis\nBullet Points:\n- Supervised learning requires re-training for each new domain\n- Manual annotation is costly and time-consuming\n- Transfer learning can negatively impact performance with polarity-changing words\n\nSlide Title: Proposed Solution: Significant Consistent Polarity (SCP) Words\nBullet Points:\n- SCP words have consistent polarity across domains\n- \u03c7\u00b2 test and cosine-similarity used to identify SCP words\n- SCP words represent usable information for cross-domain sentiment analysis\n\nSlide Title: Ensemble-Based Adaptation Algorithm\nBullet Points:\n- Classifier trained on SCP words in source domain\n- Initiates classifier on target-specific features\n- Weighted ensemble combines classifiers for improved performance\n\nSlide Title: Related Work in Cross-Domain Text Classification\nBullet Points:\n- Structured Correspondence Learning (SCL) and Structured Feature Alignment (SFA)\n- Co-occurrence and context vector similarity used to overcome data sparsity\n- Deep learning approaches for feature mapping and representation\n\nSlide Title: Extracting SCP Features\nBullet Points:\n- \u03c7\u00b2 test identifies significant words in labeled source domain\n- Context vector and cosine-similarity assign polarity in unlabeled target domain\n- Pivot selection based on word frequency for accurate polarity estimation\n\nSlide Title: Cross-Domain Adaptation Algorithm\nBullet Points:\n- Identify SCP features from source and target domains\n- Train SVM classifier on SCP words and pseudo-labeled target instances\n- Weighted Sum Model (WSM) combines classifiers for enhanced performance\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- SCP-based approach outperforms SCL and common unigrams\n- Strong positive correlation between SCP words and classification accuracy\n- Iterative process improves accuracy in target domain\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- SCP words provide accurate transferable information for sentiment classification\n- Ensemble of classifiers reduces errors and improves accuracy\n- Future work: explore more domains and refine SCP identification techniques",
    "Slide Title: Introduction to Search-Based Structured Prediction\nBullet Points:\n- Models natural language structure generation as a search problem\n- Competitive in accuracy and running time\n- Challenges: reference policy ambiguities and training-testing discrepancies\n- Previous solutions: ensemble techniques and exploration during training\n\nSlide Title: Proposed Method: Integrated Knowledge Distillation\nBullet Points:\n- Distill a single model from an ensemble of baselines\n- Match ensemble's output distribution on reference and exploration states\n- Improves model performance in structured prediction tasks\n\nSlide Title: Background: Search-Based Structured Prediction\nBullet Points:\n- Maps input to structural output with internal dependencies\n- Formalized as a search problem with states, actions, and policies\n- Commonly used in dependency parsing and neural machine translation\n\nSlide Title: Knowledge Distillation in Structured Prediction\nBullet Points:\n- Transfers generalization ability from a cumbersome model to a smaller one\n- Uses teacher model's output as \"soft target\" for student model\n- Combines distillation loss with negative log-likelihood loss\n\nSlide Title: Distillation Techniques: Reference and Exploration\nBullet Points:\n- Distillation from Reference: Learns from states produced by reference policy\n- Distillation from Exploration: Learns from arbitrary states sampled by exploration policy\n- Combining both methods enhances model performance\n\nSlide Title: Experiments: Transition-Based Dependency Parsing\nBullet Points:\n- Conducted on Penn Treebank dataset\n- Distillation model outperforms greedy models and strong baselines\n- Achieves significant improvement in labeled attachment score (LAS)\n\nSlide Title: Experiments: Neural Machine Translation\nBullet Points:\n- Conducted on German-to-English IWSLT 2014 dataset\n- Distillation model outperforms baseline and reinforcement learning models\n- Achieves significant improvement in BLEU score\n\nSlide Title: Analysis and Insights\nBullet Points:\n- Ensemble models generalize better on ambiguous and non-optimal states\n- Distillation loss provides stable learning and less sensitivity to initialization\n- Fully learning from distillation loss is feasible and beneficial\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Knowledge distillation applied to various NLP problems\n- Distillation from both reference and exploration states proposed\n- Significant performance improvements in structured prediction tasks",
    "Slide Title: Introduction to Labeled Datasets\nBullet Points:\n- Traditional method: Human annotators label each example.\n- Challenge: Limited information per example.\n- Goal: Extract more information using natural language explanations.\n\nSlide Title: Natural Language Explanations\nBullet Points:\n- Framework: Annotators provide explanations for each label.\n- Explanations parsed into labeling functions (LFs).\n- LFs map examples to labels, creating a large training set.\n\nSlide Title: BabbleLabble Framework\nBullet Points:\n- Converts explanations and unlabeled data into a labeled training set.\n- Key components: Semantic parser, filter bank, label aggregator.\n- Semantic parser: Converts explanations into logical forms.\n\nSlide Title: Semantic Parser and Filter Bank\nBullet Points:\n- Semantic parser: Rule-based, domain-independent.\n- Filter bank: Discards incorrect LFs using semantic and pragmatic filters.\n- Filters remove 86% of incorrect parses.\n\nSlide Title: Label Aggregator and Discriminative Model\nBullet Points:\n- Label aggregator: Combines labels from LFs into probabilistic labels.\n- Discriminative model: Trained on noisily-labeled data.\n- Outperforms direct use of label aggregator by 4.3 F1 points.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Evaluated on three tasks: Spouse, Disease, Protein.\n- BabbleLabble achieves high F1 scores with fewer inputs.\n- Data programming approach outperforms feature-based approach.\n\nSlide Title: Utility of Incorrect Parses\nBullet Points:\n- Filter bank removes 95% of incorrect LFs.\n- Incorrect LFs can still provide useful signals.\n- Simple rule-based parser performs comparably to a perfect parser.\n\nSlide Title: Related Work and Discussion\nBullet Points:\n- Focus on natural language explanations and weak supervision.\n- Comparison with other methods using natural language feedback.\n- Potential for high-bandwidth communication in machine learning.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- BabbleLabble: Efficiently creates labeled datasets from explanations.\n- Promising results in relation extraction tasks.\n- Potential to extend framework to other tasks and interactive settings.",
    "Slide Title: Introduction to Human-Computer Conversation\nBullet Points:\n- Human-computer conversation is a critical task in AI and NLP.\n- Two major research streams: task-oriented dialog and general-purpose dialog (chit-chat).\n- General-purpose dialog aims for natural conversations across various topics.\n- Generative conversational models learn from large-scale social data.\n- Challenges include avoiding trivial, general responses.\n\nSlide Title: Challenges in General-Purpose Dialog\nBullet Points:\n- General responses like \"I don't know\" are common but uninformative.\n- Previous models favor high-frequency, general responses.\n- Efforts to address this include Maximum Mutual Information (MMI) and topic guidance.\n- Need for a model that controls response specificity.\n\nSlide Title: Specificity Controlled Seq2Seq Model (SC-Seq2Seq)\nBullet Points:\n- Inspired by human conversation control of response specificity.\n- Introduces a specificity control variable in the Seq2Seq framework.\n- Uses a Gaussian Kernel layer to guide response generation.\n- Does not rely on external corpora; trained end-to-end on conversation data.\n\nSlide Title: Distant Supervision for Specificity Control\nBullet Points:\n- Specificity control variable is unknown in raw data.\n- Two methods to acquire distant labels: Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF).\n- NIRF based on response frequency; NIWF based on word frequency.\n\nSlide Title: Experiment and Dataset\nBullet Points:\n- Experiments conducted on the Short Text Conversation (STC) dataset.\n- STC contains 3.8 million post-comment pairs from Sina Weibo.\n- Model compared against state-of-the-art baselines.\n\nSlide Title: Evaluation Methodologies\nBullet Points:\n- Automatic evaluations: distinct-1 & distinct-2, BLEU, Average & Extrema.\n- Human evaluation: semantic relevance, grammaticality, informativeness.\n- Inter-rater consistency measured with Fleiss' kappa.\n\nSlide Title: Results and Analysis\nBullet Points:\n- SC-Seq2Seq NIWF model achieves varying specificity levels.\n- Outperforms baselines in specificity and fitting ground truth data.\n- Human evaluations confirm effectiveness of controlled generation.\n\nSlide Title: Case Study and Usage Representation Analysis\nBullet Points:\n- SC-Seq2Seq NIWF generates specific responses with high control variable values.\n- Usage representations differ from semantic representations.\n- Visualization shows distinct clustering of specific and general words.\n\nSlide Title: Conclusion\nBullet Points:\n- Novel controlled response generation mechanism proposed.\n- Specificity control variable enables generation of diverse responses.\n- Model significantly outperforms existing methods in generating specific responses.",
    "Slide Title: Introduction to Metaphor Processing\nBullet Points:\n- Metaphor enriches language, crucial for communication and cognition.\n- One-third of sentences in corpora contain metaphor expressions.\n- Computational metaphor processing aids NLP tasks like Machine Translation and Sentiment Analysis.\n- Challenges in translating metaphors due to literal interpretations.\n\nSlide Title: Current Approaches and Limitations\nBullet Points:\n- Metaphor identification using word embeddings is popular.\n- Existing models parse sentences into phrases, missing word-level metaphors.\n- Contextual information is often discarded, leading to misidentification.\n\nSlide Title: Proposed Unsupervised Metaphor Processing Model\nBullet Points:\n- Identifies and interprets metaphors at the word level.\n- Built on word embedding methods and WordNet for lexical relations.\n- Operates at the sentence level without preprocessing.\n- Captures metaphor in an unsupervised, data-driven manner.\n\nSlide Title: Methodology Overview\nBullet Points:\n- Hypothesis: Metaphorical words come from different domains than their literal senses.\n- Word embeddings trained on Wikipedia for input and output vectors.\n- Candidate word set constructed using WordNet synonyms and hypernyms.\n- Best fit word identified by cosine similarity with context.\n\nSlide Title: Experimental Design and Evaluation\nBullet Points:\n- Evaluated against strong baselines on metaphor identification.\n- Extensive experimentation on a publicly available dataset.\n- Model outperforms unsupervised baselines and matches state-of-the-art deep learning.\n\nSlide Title: Metaphor Processing for Machine Translation\nBullet Points:\n- Evaluated metaphor processing's impact on English-Chinese MT.\n- Paraphrasing metaphors improves translation accuracy.\n- Significant improvements observed in Google and Bing Translators.\n\nSlide Title: Experimental Results\nBullet Points:\n- SIM-CBOW I+O model shows highest F1 score in sentence-level identification.\n- Skip-gram performs better in phrase-level evaluation.\n- Overall translation performance improved by 11% for Google and 9% for Bing.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Framework identifies and interprets metaphors with unsupervised learning.\n- Enhances translation systems by paraphrasing metaphors.\n- Future work: Introduce weighted CBOW and Skip-gram for positional information.",
    "Slide Title: Introduction to Controversial Content Detection\nBullet Points:\n- Controversial content can stimulate healthy debate and community engagement.\n- Detecting controversy is beneficial for users and moderators.\n- Controversiality is context-dependent and varies across communities.\n- Early detection approach proposed using initial post and comment content.\n\nSlide Title: Understanding Controversiality in Communities\nBullet Points:\n- Controversial topics differ across subreddits (e.g., \"break up\" in relationships vs. AskWomen).\n- Community moderators may struggle to predict controversial posts.\n- Early opinions can influence subsequent discussions more than content.\n\nSlide Title: Research Approach and Methodology\nBullet Points:\n- Analyzed over 15,000 discussion trees across six subreddits.\n- Incorporated structural and textual features of comment trees.\n- Early detection improves predictive performance quickly.\n- Studied feature transferability across different communities.\n\nSlide Title: Datasets and Data Collection\nBullet Points:\n- Data sourced from Reddit, covering posts and comments from 2007 to 2014.\n- Focused on six text-based subreddits: AskMen, AskWomen, Fitness, LifeProTips, personalfinance, and relationships.\n- Posts with fewer than 30 comments were discarded for analysis.\n\nSlide Title: Assigning Controversy Labels\nBullet Points:\n- Binary labels assigned: controversial vs. non-controversial.\n- Process involved sorting posts by upvote percentage.\n- Top quartile labeled non-controversial, bottom quartile labeled controversial.\n- Validation against Reddit's sort-by-controversy function.\n\nSlide Title: Early Discussion Features and Prediction\nBullet Points:\n- Analyzed comments posted within one hour of original submission.\n- Structural features of conversation trees are predictive of controversy.\n- Textual content of comments provides significant predictive gain.\n- Early prediction models outperform post-time baselines.\n\nSlide Title: Comparing Text Models for Controversy Prediction\nBullet Points:\n- Various text models tested, including BERT and LSTM.\n- BERT features generally provided the best performance.\n- Textual features are community-specific and less transferable.\n\nSlide Title: Domain Transfer and Feature Transferability\nBullet Points:\n- Experiments conducted to test model transferability across subreddits.\n- Text features strong in-domain but brittle across communities.\n- Rate and tree features transfer better between communities.\n- Future work could focus on improving textual feature transferability.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Early discussion features are effective in predicting controversiality.\n- Expressive feature sets enhance early prediction capabilities.\n- Future work could explore higher-quality textual representations.\n- Aim to develop more robust algorithms for cross-community prediction.",
    "Slide Title: Introduction to Multimodal Social Media\n\nBullet Points:\n- Social platforms like Snapchat, Twitter, Instagram, and Pinterest enhance communication.\n- Platforms are increasingly multimodal, combining images, videos, audios, and texts.\n- Name tagging is crucial for language understanding in social media.\n- Challenges include short text, linguistic variations, and multimodality.\n\nSlide Title: Challenges in Name Tagging\n\nBullet Points:\n- Textual components in social media are often short and context-limited.\n- Common linguistic variations include slangs, typos, and colloquial language.\n- Multimodal nature requires combining textual and visual information for understanding.\n\nSlide Title: Proposed Model for Name Tagging\n\nBullet Points:\n- Extends state-of-the-art models to incorporate visual information using Attention mechanism.\n- Uses images to provide context for name tagging in text.\n- Attention-based model extracts relevant visual features from images.\n\nSlide Title: Model Architecture\n\nBullet Points:\n- BLSTM-CRF sequence labeling model predicts labels for words.\n- Visual Attention Model extracts regional visual features.\n- Modulation Gate combines visual and textual features for tag prediction.\n\nSlide Title: Visual Feature Representation\n\nBullet Points:\n- Uses CNNs, specifically ResNet, to obtain image representations.\n- Visual attention mechanism focuses on image regions related to text.\n- Visual context vector aids in initializing the BLSTM sequence labeling model.\n\nSlide Title: Datasets and Evaluation\n\nBullet Points:\n- Two datasets from Twitter and Snapchat used for evaluation.\n- Datasets contain named entities: Location, Person, Organization, Miscellaneous.\n- Proposed model outperforms baseline models on both datasets.\n\nSlide Title: Experiment and Results\n\nBullet Points:\n- Tokenization and labeling schema used for training.\n- Fine-tuning and optimization techniques applied.\n- Final model shows significant F1 score improvement.\n\nSlide Title: Attention Visualization and Error Analysis\n\nBullet Points:\n- Attention visualization helps understand model decisions.\n- Successful cases show alignment between visual and textual information.\n- Challenges include poor alignment, blur images, and incorrect attention.\n\nSlide Title: Related Work and Future Directions\n\nBullet Points:\n- Overview of previous work on name tagging and visual attention.\n- Proposed model introduces new visual attention component and modulation gate.\n- Future work includes expanding to fine-grained name tagging and entity linking.\n\nSlide Title: Conclusion\n\nBullet Points:\n- Gated Visual Attention model improves name tagging in multimodal social media.\n- Encourages further research on multimodal social media.\n- Plans to expand model for more fine-grained name tagging tasks.",
    "Slide Title: Introduction to Incremental Domain Adaptation\n\nBullet Points:\n- Incremental domain adaptation enhances machine translation, especially NMT.\n- Commonly used in computer-aided translation (CAT) scenarios.\n- User translations serve as new training examples for adaptation.\n- Focus on measuring speed of lexical acquisition for in-domain vocabulary.\n\nSlide Title: Measuring Immediate Adaptation\n\nBullet Points:\n- Adaptation performance is crucial for user acceptance in MT systems.\n- Lexical choice errors for domain-specific items are particularly salient.\n- Traditional metrics like BLEU may not capture specific adaptation needs.\n- Focus on content words for evaluating translation performance.\n\nSlide Title: Proposed Metrics for Adaptation\n\nBullet Points:\n- Three metrics: one-shot, zero-shot, and combined vocabulary acquisition.\n- Metrics measure recall of target-language content words.\n- Content words defined as nouns, main verbs, adjectives, or adverbs.\n- Metrics calculated automatically by comparing translation hypotheses to references.\n\nSlide Title: Online Adaptation Techniques\n\nBullet Points:\n- NMT systems adapted by fine-tuning with cross-entropy loss.\n- Full adaptation updates all parameters, but can be infeasible.\n- Bias adaptation introduces a second bias term in the output layer.\n- Group lasso regularization allows selection of a fixed number of parameters.\n\nSlide Title: Experimental Setup\n\nBullet Points:\n- English\u2192German NMT system based on Transformer architecture.\n- Adaptation uses a learning rate of 10^-2, no dropout, and no label-smoothing.\n- Various adaptation schemes: full, fixed, top, and lasso.\n- Metrics operate on words, outputs converted to full-form words.\n\nSlide Title: Results and Analysis\n\nBullet Points:\n- Lasso outperforms other methods in zero-shot recall (R0).\n- Full adaptation shows high one-shot recall but degrades zero-shot recall.\n- Lasso improves R0 by learning new combinations of subwords.\n- Adaptation behavior varies over time, affecting user experience.\n\nSlide Title: Conclusions and Implications\n\nBullet Points:\n- Fine-tuning NMT models can degrade zero-shot recall of content words.\n- Regularization methods like lasso mitigate this effect.\n- Proposed metrics R0, R1, and R0+1 are crucial for measuring adaptation performance.\n- Lasso provides the best tradeoff in performance across metrics.\n\nSlide Title: Additional Results and Observations\n\nBullet Points:\n- Similar patterns observed across various datasets.\n- Lasso consistently outperforms in zero-shot recall.\n- Adaptation with bias method often aligns closely with baseline.\n- Traditional MT metrics show predominantly positive results for adaptation.",
    "Slide Title: Introduction to Speech-to-Text Translation for Low-Resource Languages\nBullet Points:\n- Potential applications in language documentation and crisis relief\n- Traditional ST requires transcribed audio and parallel text, often unavailable for low-resource languages\n- End-to-end ST is appealing but requires substantial resources\n- Leveraging high-resource language data can improve low-resource ST\n\nSlide Title: Leveraging High-Resource Language Data\nBullet Points:\n- Pre-training on high-resource ASR tasks can improve low-resource ST\n- Example: Spanish-English ST improved by pre-training on English ASR\n- Encoder parameter transfer is key to improvement\n- Pre-training on unrelated languages like French also shows benefits\n\nSlide Title: Methodology\nBullet Points:\n- Encoder-decoder model with attention used for both ASR and ST\n- Pre-trained ASR models used to initialize ST models\n- Shared vocabulary across ASR and ST tasks in high-resource settings\n- Transfer learning by sharing encoder parameters\n\nSlide Title: Experimental Setup and Data Sets\nBullet Points:\n- English ASR: 300 hours from Switchboard corpus\n- French ASR: 20 hours from GlobalPhone collection\n- Spanish-English ST: Fisher Spanish corpus with varying training data sizes\n- Mboshi-French ST: 4 hours of Mboshi speech\n\nSlide Title: Preprocessing and Model Architecture\nBullet Points:\n- Speech input converted to MFCCs, text segmented using byte pair encoding\n- Speech encoder: CNN layers followed by LSTM\n- Text decoder: Softmax layer with attention mechanism\n- Training with dropout and Gaussian noise for regularization\n\nSlide Title: Evaluation Metrics and Baselines\nBullet Points:\n- BLEU scores reported for model evaluation\n- METEOR used for word-level precision and recall\n- Naive baseline models predict frequent words for comparison\n- Transfer learning shows substantial improvements over baselines\n\nSlide Title: Results and Analysis\nBullet Points:\n- Transfer learning improves BLEU scores significantly\n- Gains observed even with very low-resource data (e.g., 2.5 hours)\n- Encoder parameter transfer provides most benefits\n- Code-switching in data sets can influence results\n\nSlide Title: Impact of ASR Data and Code-Switching\nBullet Points:\n- More ASR data leads to better ST performance\n- Code-switching in data sets can enhance transfer learning benefits\n- French ASR pre-training also improves Spanish-English ST\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Pre-training with high-resource ASR data boosts low-resource ST\n- Encoder parameter transfer is crucial for improvements\n- Future work: explore multilingual training and pre-trained language models\n- Potential for further gains with improved training strategies",
    "Slide Title: Introduction to Semantic Parsing\nBullet Points:\n- Semantic parsing maps language utterances to executable programs.\n- Early work relied on supervised learning with language-program pairs.\n- Weakly-supervised parsing uses utterance-denotation pairs, simplifying data collection.\n- Challenges include search complexity and spuriousness in program generation.\n- CNLVR dataset aids in studying spuriousness in visual reasoning tasks.\n\nSlide Title: CNLVR Dataset and Semantic Parsing\nBullet Points:\n- CNLVR involves images with objects and natural language statements.\n- Task: Determine statement truthfulness using image or knowledge-base (KB).\n- Semantic parsing translates language to programs executed against KB.\n- Spuriousness is a significant challenge due to binary denotation outcomes.\n\nSlide Title: Semantic Parsing Approach\nBullet Points:\n- Divided into lexical and structural tasks.\n- Use of a small lexicon to cluster language tokens and program constants.\n- Abstract representations help tackle search and spuriousness challenges.\n- Sharing information across examples improves parser accuracy.\n\nSlide Title: Programming Language for Visual Reasoning\nBullet Points:\n- Inspired by CLEVR dataset, adapted for spatial reasoning.\n- Programs are sequences of function applications in prefix notation.\n- Functions check object attributes like size, shape, and color.\n- Set-theoretic reasoning is crucial for CNLVR.\n\nSlide Title: Model Architecture\nBullet Points:\n- Based on encoder-decoder architecture with bi-directional LSTM.\n- Beam search used to navigate large program space.\n- Semantic typing system prunes search space for valid programs.\n- Discriminative re-ranking addresses label bias problem.\n\nSlide Title: Training from Weak Supervision\nBullet Points:\n- Programs treated as latent variables, approximately marginalized.\n- Spuriousness reduced by using multiple KBs per utterance.\n- Abstract examples improve search and spuriousness handling.\n- Data augmentation generates new training examples.\n\nSlide Title: Learning from Abstract Examples\nBullet Points:\n- Abstract representations cover many dataset questions.\n- Rule-based parser uses manually annotated abstract examples.\n- Data augmentation generates non-abstract pairs for training.\n- Caching mechanism tracks successful abstract programs.\n\nSlide Title: Experimental Evaluation\nBullet Points:\n- Evaluated on CNLVR public and hidden test sets.\n- Weakly-supervised parser with re-ranking achieves state-of-the-art accuracy.\n- Abstract examples and data augmentation significantly improve performance.\n- Error analysis highlights challenges with complex utterances.\n\nSlide Title: Related Work and Discussion\nBullet Points:\n- Semantic parsers trained from denotations have evolved over the decade.\n- Visual reasoning datasets like VQA and CLEVR provide context.\n- Spuriousness remains a central challenge in semantic parsing.\n- Future work aims to automate lexicon learning for broader domains."
]