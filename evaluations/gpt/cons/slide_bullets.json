[
    "Slide Title: Introduction to Similarity Search\nBullet Points:\n- Similarity search identifies documents similar to a query in a large corpus.\n- Key applications include plagiarism analysis, collaborative filtering, and multimedia retrieval.\n- Semantic hashing offers fast similarity search using binary hashing codes.\n- Hamming distance measures similarity between document codes efficiently.\n- Existing methods often use two-stage training, leading to suboptimal results.\n\nSlide Title: Challenges in Existing Text Hashing Methods\nBullet Points:\n- Two-stage training can lead to suboptimal local optima in text hashing.\n- Binary constraints are often handled ad-hoc, losing information.\n- Separate training stages require additional hyperparameter tuning.\n- Continuous representations lose information during binarization.\n- Existing methods lack end-to-end learning for binary codes.\n\nSlide Title: Proposed Neural Architecture for Text Hashing\nBullet Points:\n- Introduces an end-to-end neural architecture for text hashing.\n- Utilizes neural variational inference for learning binary codes.\n- Joint optimization of encoder and decoder networks.\n- Incorporates data-dependent noise for rate-distortion tradeoff.\n- Establishes connection with rate-distortion theory for effectiveness.\n\nSlide Title: Related Work in Discrete Neural Representations\nBullet Points:\n- Discrete random variables are gaining attention in deep learning.\n- Discrete representations efficient for storage, comparable to continuous embeddings.\n- End-to-end learning of discrete representations at document level is rare.\n- Semantic hashing evaluates quality of learned binary codes.\n- Focus on learning discrete representations for text documents.\n\nSlide Title: Hashing under the NVI Framework\nBullet Points:\n- Uses variational autoencoders for training binary latent variables.\n- Models both encoding and decoding distributions for semantic hashing.\n- Employs bag-of-words representation for input documents.\n- Uses Bernoulli prior for binary latent variables.\n- Stochastic and deterministic binarization strategies explored.\n\nSlide Title: Training with Binary Latent Variables\nBullet Points:\n- Variational lower bound maximized for parameter estimation.\n- Straight-through estimator used for gradient estimation.\n- Softmax decoding function reconstructs input from binary codes.\n- Data-dependent noise injected for improved retrieval performance.\n- Rate-distortion theory guides noise injection strategy.\n\nSlide Title: Experimental Setup and Evaluation\nBullet Points:\n- Experiments conducted on Reuters, 20Newsgroups, and TMC datasets.\n- TFIDF features used for document representation.\n- Comparison with unsupervised and supervised baselines.\n- Precision measured by retrieving nearest documents based on Hamming distance.\n- NASH variants outperform baselines in unsupervised semantic hashing.\n\nSlide Title: Results and Observations\nBullet Points:\n- NASH variants show superior performance in unsupervised hashing tasks.\n- Noise injection improves robustness of binary representations.\n- Longer hash codes may lead to overfitting and reduced retrieval precision.\n- NASH-DN-S outperforms supervised baselines in retrieval precision.\n- Stochastic sampling enhances retrieval results across datasets.\n\nSlide Title: Qualitative Analysis and Conclusions\nBullet Points:\n- NASH effectively groups semantically-similar words in learned vector space.\n- Binary hashing codes efficiently compress documents with shared topics.\n- End-to-end semantic hashing achieved with effective gradient estimator.\n- Neural variational framework introduced for training.\n- Data-dependent noise injection enhances retrieval performance.",
    "Slide Title: Understanding Human Emotions\nBullet Points:\n- Human emotions are complex and challenging to define and understand.\n- Ekman identified six basic emotions: anger, disgust, fear, guilt, joy, and sadness.\n- Emotions are intertwined with psychology, yet distinct in their nature.\n- Emotional phrase identification is crucial in Natural Language Processing (NLP).\n- Seven emotional statements extracted from a psychological corpus for analysis.\n\nSlide Title: Emotional Phrase Classification\nBullet Points:\n- Emotional statements tokenized and grouped into trigrams as Context Vectors.\n- Context Vectors are POS tagged with TF and TF-IDF scores calculated.\n- Affinity Scores calculated using Chebyshev, Euclidean, and Hamming distance metrics.\n- Classification methods applied: NaiveBayes, J48, Decision Tree, and BayesNet.\n- Results compared to determine the most effective classification method.\n\nSlide Title: Related Work in Emotion Analysis\nBullet Points:\n- WORDNET-AFFECT assigns affective labels to WORDNET synsets.\n- Opinion lexicons developed from annotated corpora for emotion analysis.\n- SentiWord-Net classifies synsets using ternary classifiers for polarity.\n- SenticNet enriched with affective information for emotion detection.\n- EmotiNet extends emotion classification with external sources for better recall.\n\nSlide Title: Data Preprocessing Framework\nBullet Points:\n- Emotional statements collected from ISEAR database with 3000 respondents.\n- Statements tokenized into words and grouped into trigrams for analysis.\n- Context Windows (CW) formed to acquire emotional phrases.\n- WordNet Affect list updated with 2,958 affect words for analysis.\n- Trigrams analyzed for patterns and stop word occurrences.\n\nSlide Title: Context Vector Formation and Analysis\nBullet Points:\n- Context Windows mapped into vector space for emotion classification.\n- Affinity Score calculated for Context Vectors using distance metrics.\n- Vector formation based on non-affect and affect word occurrences.\n- Semantic relations between vectors analyzed for classification accuracy.\n- Affinity Scores reflect similarity between Context Vectors.\n\nSlide Title: Feature Selection and Analysis\nBullet Points:\n- Feature selection crucial for building effective pattern classifiers.\n- POS tagged Context Windows (PTCW) and POS Tagged Windows (PTW) extracted.\n- Term Frequencies (TF) and Inverse Document Frequencies (IDF) calculated.\n- Guilt emotion shows highest Max_TF and variance in analysis.\n- Ranking score calculated for Context Windows using SentiWordnet lexicon.\n\nSlide Title: Classification Results and Analysis\nBullet Points:\n- Classifiers tested with user-defined data and 10-fold cross validation.\n- BayesNet Classifier achieves highest accuracy with Euclidean distance.\n- DecisionTree classifier performs best with Hamming and Chebyshev distances.\n- Comparative results show BayesNet and DecisionTree as top performers.\n- Classifier accuracy varies based on distance metrics used.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Vector formation and affinity score calculation inspired by Newton's law.\n- Classifiers used: BayesNet, J48, NaiveBayesSimple, and DecisionTable.\n- Future work includes incorporating more lexicons for emotion classification.\n- Plan to include associative learning for identifying classification rules.\n- Aim to enhance emotion classification accuracy and effectiveness.",
    "Slide Title: Introduction to Scientific Publication Lifecycle\nBullet Points:\n- Scientific Publication Lifecycle involves literature review, research work, and result dissemination.\n- Researchers face challenges in research activities, often needing expert assistance.\n- Academic assistive systems help bridge expertise gaps in research execution.\n- Rec4LRW system aids in literature review and manuscript preparation.\n- Study conducted with 116 participants to evaluate Rec4LRW system's effectiveness.\n\nSlide Title: Related Work in Manuscript Writing Assistance\nBullet Points:\n- Various models proposed to assist researchers in manuscript writing.\n- Citation context recommendations help find candidate citations for manuscripts.\n- Content-oriented techniques used due to lack of user ratings for collaborative filtering.\n- ActiveCite and Docear provide citation suggestions and reference management.\n- Article-type recommendations discussed but not practically implemented.\n\nSlide Title: Overview of Rec4LRW System\nBullet Points:\n- Rec4LRW assists in literature review and manuscript preparation tasks.\n- Tasks include building reading lists, finding similar papers, and shortlisting papers.\n- System uses ACM Digital Library dataset for paper recommendations.\n- Unique UI features include author-specified keywords and information cue labels.\n- Shortlisting feature helps identify important papers for manuscript citation.\n\nSlide Title: Shortlisting Papers Technique\nBullet Points:\n- Girvan-Newman algorithm used for clustering in citation networks.\n- Clusters help identify important and unique papers for citation.\n- Article-type choice influences the number of shortlisted papers.\n- Historical data used to determine average references for article-types.\n- Procedure involves sorting and selecting top-cited papers from clusters.\n\nSlide Title: User Evaluation Study Methodology\nBullet Points:\n- User evaluation study conducted with 116 participants experienced in research writing.\n- Participants evaluated three tasks and overall system effectiveness.\n- Tasks included selecting research topics, finding similar papers, and shortlisting.\n- Participants provided feedback through survey questions and subjective comments.\n- Study aimed to assess usefulness and effectiveness of Rec4LRW system.\n\nSlide Title: Results and Discussion of User Evaluation\nBullet Points:\n- Agreement percentages indicate high usefulness and importance of shortlisting feature.\n- Students showed higher agreement percentages compared to staff participants.\n- Participants acknowledged task's effectiveness in identifying important papers.\n- Some reluctance observed in confidently citing shortlisted papers.\n- Feedback highlights need for improvements in dataset and algorithm.\n\nSlide Title: Qualitative Feedback from Participants\nBullet Points:\n- Participants appreciated shortlisting feature for efficiency and quality.\n- Information cue labels aided in relevance judgment of shortlisted papers.\n- Viewing papers in parent clusters helped discover additional relevant papers.\n- Some participants found study procedure lengthy and dataset limited.\n- Suggestions for improvement include better ranking and UI enhancements.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Rec4LRW addresses gap in manuscript preparation assistance for researchers.\n- Shortlisting task validated by user evaluation study with positive feedback.\n- Future work includes expanding article-type variations and improving user control.\n- Aim to integrate with citation context recommendation for comprehensive support.\n- Continued research to enhance system's effectiveness and user experience.",
    "Slide Title: Introduction to NMT Improvements\nBullet Points:\n- NMT has seen promising improvements with different input-output systems.\n- Tree-based systems offer advantages over string-to-string systems.\n- Tree-based methods focus on syntactic information and prior knowledge.\n- Linearization methods preferred for computational efficiency.\n- Tree-based NMT systems face challenges with 1-best parse tree limitations.\n\nSlide Title: Preliminaries of NMT Systems\nBullet Points:\n- Sequence-to-sequence model is a common framework in NMT.\n- Uses RNNs to encode source sequence into a context vector.\n- Attention mechanism addresses long sequence issues.\n- Framework adaptable to string-to-string, tree-to-string, and string-to-tree systems.\n- Packed forest represents multiple parse trees compactly.\n\nSlide Title: Linear-Structured Tree-Based NMT Systems\nBullet Points:\n- Linearization methods encode syntax information in tree-to-string NMT.\n- Different approaches for source and target side linearization.\n- Current systems use only one tree for encoding or decoding.\n- Aim to utilize multiple trees (a forest) for better representation.\n- Packed forest offers compact representation of multiple trees.\n\nSlide Title: Forest-Based NMT Approach\nBullet Points:\n- Proposes a novel linearization method for packed forests.\n- Preserves word sequential and parent-child information.\n- Uses operators to combine different parts of constituent trees.\n- Linearized sequence is weighted, incorporating parser confidence.\n- Aims to improve NMT performance by encoding syntax information.\n\nSlide Title: Encoding the Linearized Forest\nBullet Points:\n- Linearized packed forest forms input for the encoder.\n- Input sequence includes symbol and score sequences.\n- Symbols consist of words and constituent labels combined by operators.\n- Two frameworks proposed: Score-on-Embedding (SoE) and Score-on-Attention (SoA).\n- Frameworks connect with conventional sequence-to-sequence NMT.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Evaluated on English-to-Chinese and English-to-Japanese translation tasks.\n- Compared forest-based NMT with string-to-string and tree-based models.\n- Forest-based configurations yield better results than s2s baseline.\n- SoA framework performs best, highlighting importance of score sequences.\n- Packed forests improve accuracy by providing more syntactic information.\n\nSlide Title: Qualitative Analysis of Translation Results\nBullet Points:\n- S2S model lacks syntactic information, leading to ungrammatical output.\n- 1-best tree model ignores some phrases due to parsing errors.\n- Packed forest model translates phrases correctly, improving attention.\n- Syntax information helps decoder focus on correct structures.\n- Forest model generates more accurate and grammatical translations.\n\nSlide Title: Related Work in Syntax-Based NMT\nBullet Points:\n- Tree-based systems outperform string-to-string systems using syntactic trees.\n- Previous methods used 1-best parsing tree, sensitive to errors.\n- Some studies use latent graphs instead of trees for syntax representation.\n- Our framework encodes exponentially many trees efficiently.\n- Demonstrated advantages of using linearized packed forests in NMT.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Proposed NMT framework encodes packed forest using linear networks.\n- Utilizes many linearized parsing trees without efficiency loss.\n- First attempt at using forest in string-to-string NMT framework.\n- Future work includes designing better structures for score layer.\n- Plan to apply linearization method to other tasks for further improvement.",
    "Slide Title: Introduction to Adversarial Attacks in Machine Learning\nBullet Points:\n- Adversarial attacks modify inputs to mislead machine learning models, causing misclassification or mistranslation.\n- These attacks reveal vulnerabilities in models, posing security risks for critical applications like self-driving cars.\n- Initial focus was on computer vision; challenges arise in discrete spaces like natural language processing (NLP).\n- No standard distance metric for text; semantic similarity evaluation is complex.\n- Adversarial examples in NLP should preserve source meaning but destroy target meaning.\n\nSlide Title: Evaluating Adversarial Attacks on Seq2Seq Models\nBullet Points:\n- A framework is proposed to evaluate adversarial attacks on seq2seq models using source and target similarity scores.\n- The goal is to measure the model's failure cases by comparing original and adversarial inputs.\n- Source similarity score (s_src) and target relative score decrease (d_tgt) are key metrics.\n- Reporting both s_src and d_tgt provides a comprehensive view of attack effectiveness.\n- The framework can be adapted for various tasks beyond meaning-preserving attacks.\n\nSlide Title: Semantic Similarity Metrics\nBullet Points:\n- Human judgment is the gold standard for evaluating semantic similarity between sentences.\n- A 6-level evaluation scheme rates similarity from completely different to well-formed and equal meaning.\n- Automatic metrics like BLEU, METEOR, and chrF offer alternatives to human evaluation.\n- chrF, based on character n-gram F-score, shows higher correlation with human judgment.\n- Automatic metrics are crucial for low-resource languages where human evaluation is challenging.\n\nSlide Title: Gradient-Based Adversarial Attacks\nBullet Points:\n- Gradient-based attacks replace words to maximize adversarial loss, affecting model predictions.\n- The approach involves optimizing word substitutions to create adversarial inputs.\n- Constraints like kNN and CharSwap ensure meaning preservation in adversarial examples.\n- kNN replaces words with nearest neighbors in embedding space, maintaining semantic relation.\n- CharSwap involves character swaps, minimally affecting human readability and preserving meaning.\n\nSlide Title: Experiments and Evaluation Framework\nBullet Points:\n- Experiments evaluate adversarial attacks using the IWSLT2016 dataset for multiple language pairs.\n- Two neural machine translation models, LSTM and Transformer, are tested for robustness.\n- chrF metric is used to assess source meaning preservation and target meaning destruction.\n- Constrained attacks like kNN and CharSwap show better meaning preservation than unconstrained attacks.\n- Results highlight differences in model robustness and effectiveness of adversarial constraints.\n\nSlide Title: Adversarial Training with Meaning-Preserving Attacks\nBullet Points:\n- Adversarial training augments data with adversarial examples to improve model robustness.\n- Training with meaning-preserving attacks like CharSwap maintains test performance on non-adversarial input.\n- Unconstrained adversarial training can degrade test performance due to lack of constraints.\n- CharSwap-adv training enhances robustness against CharSwap attacks without performance loss.\n- Adversarial training with constraints offers a balanced approach to improving model resilience.\n\nSlide Title: Related Work in Adversarial Attacks\nBullet Points:\n- Gradient-based attacks and adversarial training were introduced by Goodfellow et al. (2014).\n- Various attack and defense mechanisms have been proposed for different NLP tasks.\n- Previous work often overlooked meaning preservation, focusing on target-side evaluation.\n- Recent efforts explore meaning-preserving attacks through paraphrase generation and rule-based methods.\n- Proposed attacks are often language-specific, highlighting the need for broader applicability.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Meaning-preserving adversarial perturbations are crucial for evaluating NLP models.\n- A general evaluation framework and automatic metrics serve as proxies for human judgment.\n- Naive attacks often fail to preserve meaning; alternatives like CharSwap offer improvements.\n- Adversarial training with constraints enhances model robustness without compromising performance.\n- Future research should focus on consistent evaluation of meaning conservation in adversarial attacks.",
    "Slide Title: Introduction to Discourse Phenomena in Translation\nBullet Points:\n- Handling discourse phenomena is crucial for coherent and accurate translations.\n- Traditional machine translation often ignores discourse, translating sentences in isolation.\n- Context-aware NMT models show performance improvements by using extended context.\n- Understanding discourse modeling in NMT can guide future model development.\n- Our work enhances understanding of discourse phenomena in NMT using a simple model.\n\nSlide Title: Neural Machine Translation (NMT) Overview\nBullet Points:\n- NMT models predict target sentence words from a given source sentence.\n- Encoder-decoder structure maps input symbols to distributed representations.\n- Attention mechanism focuses on source sentence parts during translation.\n- Transformer architecture avoids recurrence, enhancing parallelizability and training speed.\n- Transformer uses multi-head attention and feed-forward networks for encoding.\n\nSlide Title: Context-Aware Model Architecture\nBullet Points:\n- Our model is based on the Transformer architecture with context integration.\n- Context encoder shares parameters with the source encoder for efficiency.\n- Special token <bos> distinguishes context from source sentences.\n- Contextual information flows through a single attention layer.\n- Model achieves improvements over discourse-agnostic baselines.\n\nSlide Title: Experiments and Data Setting\nBullet Points:\n- Used OpenSubtitles2018 corpus for English-Russian translation experiments.\n- Data cleaning and byte-pair encoding applied to training instances.\n- BLEU metric used to estimate overall performance of discourse-aware model.\n- Context-aware model shows 0.7 BLEU improvement over baseline.\n- Previous sentence context provides more clues for discourse phenomena.\n\nSlide Title: Analysis of Contextual Information\nBullet Points:\n- Model relies on context for translating ambiguous pronouns like \"it\" and \"you.\"\n- Contextual attention is high for short sentences and sentence-initial words.\n- Context is especially helpful at the beginning of a sentence.\n- Model implicitly learns coreference phenomena without feature engineering.\n- Attention mechanism acts as latent anaphora resolution.\n\nSlide Title: Pronoun Translation and Contextual Attention\nBullet Points:\n- Context-aware model improves translation of ambiguous pronouns.\n- Pronouns like \"it\" have multiple translations based on antecedent gender.\n- Model shows significant BLEU improvement for sentences with coreferential pronouns.\n- Attention weights align with coreference system predictions.\n- Human evaluation confirms model's effective anaphora resolution.\n\nSlide Title: Related Work and Comparisons\nBullet Points:\n- Early work relied on external systems for coreference resolution in translation.\n- Context-aware NMT models implicitly learn coreference without feature engineering.\n- Our model demonstrates latent anaphora resolution through attention mechanism.\n- Contrasts with previous models that did not show increased pronoun-antecedent attention.\n- Our model's non-recurrent nature makes it suitable for analysis.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- Context-aware NMT system based on Transformer outperforms baselines.\n- Improvements are notable for sentences with ambiguous pronouns.\n- Model induces anaphora relations, enhancing translation quality.\n- Future work could explore other discourse phenomena like elliptical constructions.\n- Incorporating specialized features in attention model may further improve translation.",
    "Slide Title: Introduction to Machine Translation\nBullet Points:\n- Machine translation converts source language utterances into fluent target language with same meaning.\n- Recent approaches use statistical techniques on parallel data for learning transformations.\n- Unified semantic representations like AMR are gaining interest in machine translation.\n- Deeper models allow for more parsimonious translation by handling related words and concepts.\n- Initial discriminative method models likelihood of target language string from source semantics.\n\nSlide Title: Related Work in Machine Translation\nBullet Points:\n- Deep language representation in NLP and MT brings languages closer together.\n- Early systems used transfer methods, parsing source sentences into target representations.\n- AMR used for abstractive summarization and semantic graph exploitation in MT.\n- Discriminative models like global lexicon model and phrase-sense disambiguation are notable.\n- Our model uses deep representation features to capture semantic relations between languages.\n\nSlide Title: Semantic Representation in Translation\nBullet Points:\n- Sentence semantics based on Logical Form, represented as labeled directed graphs.\n- Nodes correspond to content words; edges describe semantic relations between nodes.\n- Additional linguistic information stored in graph nodes as bits.\n- Graphs may have cycles; logical form converted to AMR specification.\n- Semantic graphs used for discriminative modeling of translation.\n\nSlide Title: Graph-to-String Translation Models\nBullet Points:\n- Models developed for semantic-graph-to-string translation using discriminative methods.\n- Translation viewed as generating target sentence from source semantic graph.\n- Process involves transitioning to graph nodes and generating target-side words.\n- Likelihood of target string and alignment calculated using generative process.\n- Translation distribution uses feature-rich approach conditioned on source and target context.\n\nSlide Title: Alignment of Semantic Graph Nodes\nBullet Points:\n- Techniques for aligning source-side semantic graph nodes to target-side words explored.\n- Gibbs sampling aligns target words to source nodes using generative HMM-style model.\n- Direct GIZA++ aligns linearized semantic graph nodes to target-side sentences.\n- Composed alignments divide problem into two stages for superior results.\n- Alignment composition used for remaining experiments with gold alignment and GIZA++.\n\nSlide Title: Discriminative Model and Feature Set\nBullet Points:\n- Discriminative model assumes alignment given by parser and GIZA++ word alignment.\n- Maximum entropy model learns conditional probability for translation distribution.\n- Rich feature set includes lemma, part of speech, and semantic graph path features.\n- Features capture agreement phenomena and ground nodes in semantic context.\n- Target-side features include previous tokens as discriminative language model features.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Model tested in n-best re-ranking experiment for English\u2192French translation.\n- Basic phrase-based MT system trained on 1 million parallel sentence pairs.\n- Reranking added six scores from model to translations in n-best lists.\n- Modest but consistent improvement observed in BLEU scores.\n- Direct model use in decoder expected to increase gains.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Initial attempt at including semantic features in statistical machine translation.\n- Discriminative training captures morphological, syntactic, and semantic information.\n- Gains limited by semantic parser accuracy; future work on feature engineering needed.\n- Potential for integrating target semantic language model to prevent \"word salad\" outputs.\n- Exploring robust features and multiple parser outputs could enhance translation quality.",
    "Slide Title: Introduction to Deep Neural Networks in NLP\nBullet Points:\n- DNNs excel in supervised NLP tasks like dependency parsing and named entity recognition.\n- Learning from unlabeled data under domain shift remains challenging in real-world applications.\n- Many domain adaptation approaches lack generalization across different tasks.\n- Classic semi-supervised learning algorithms provide strong baselines for NLP tasks.\n- Re-evaluation of bootstrapping algorithms in DNNs context is explored.\n\nSlide Title: Contributions of the Study\nBullet Points:\n- Proposed a novel multi-task tri-training method for domain adaptation.\n- Tri-training serves as a robust semi-supervised learning baseline for NLP models.\n- Extensive evaluation of bootstrapping algorithms against state-of-the-art approaches.\n- Insights into task and data characteristics for optimal model performance.\n- Establishes new state-of-the-art in unsupervised domain adaptation for sentiment analysis.\n\nSlide Title: Overview of Neural Bootstrapping Methods\nBullet Points:\n- Introduction to self-training, tri-training, and tri-training with disagreement.\n- Self-training uses model's predictions on unlabeled data for additional training.\n- Tri-training reduces bias using agreement of three independently trained models.\n- Tri-training with disagreement focuses on strengthening model's weak points.\n- Sampling and confidence thresholding enhance bootstrapping methods' efficiency.\n\nSlide Title: Multi-task Tri-training Method\nBullet Points:\n- MT-Tri reduces time and space complexity of traditional tri-training.\n- Leverages multi-task learning to share knowledge across models.\n- Orthogonality constraint ensures diverse feature usage in predictions.\n- Joint training of models improves convergence and efficiency.\n- Majority voting of output layers enhances final predictions.\n\nSlide Title: Experiments and Evaluation\nBullet Points:\n- Evaluated on unsupervised domain adaptation datasets for sequence labeling and classification.\n- POS tagging uses SANCL 2012 dataset; sentiment analysis uses Amazon reviews dataset.\n- Compared against top results and various baseline models.\n- MT-Tri outperforms state-of-the-art in sentiment analysis across multiple domains.\n- Classic tri-training excels in POS tagging, especially on OOVs and low-frequency tokens.\n\nSlide Title: Results and Analysis\nBullet Points:\n- Tri-training achieves best average results on two target domains in sentiment analysis.\n- MT-Tri outperforms traditional tri-training in scenarios with strong domain shift.\n- Classic tri-training shows superior performance in POS tagging tasks.\n- Orthogonality constraint in MT-Tri aids in inducing diversity and efficiency.\n- Asymmetry observed in domain adaptation results due to domain characteristics.\n\nSlide Title: Related Work in Domain Adaptation\nBullet Points:\n- Domain adaptation includes bootstrapping, shared feature representations, and instance weighting.\n- Recent approaches involve adversarial learning and fine-tuning techniques.\n- Limited work on bootstrapping for neural NLP under domain shift.\n- Neural network ensembling and multi-task learning are key areas of research.\n- MT-Tri model aligns with shared-private models for source and target-specific encoders.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- Classic tri-training outperforms recent state-of-the-art methods in NLP tasks.\n- MT-Tri model offers efficient alternative with reduced complexity in sentiment analysis.\n- Importance of comparing neural approaches to strong baselines emphasized.\n- Future work could explore shared-private models for enhanced domain adaptation.\n- Reporting results across multiple runs ensures objective evaluation of approaches.",
    "Slide Title: Introduction to Conversational Interfaces\nBullet Points:\n- Advances in speech recognition and language understanding boost conversational interfaces like Alexa and Siri.\n- Two main use cases: task completion and question answering using knowledge graphs.\n- Spoken Language Understanding (SLU) systems predict user intent and tag utterance words.\n- Q&A systems require complex structures like trees and graphs for language understanding.\n- Proposal: Use executable semantic parsing to unify SLU and Q&A systems.\n\nSlide Title: Executable Semantic Parsing Framework\nBullet Points:\n- Semantic parsing generates logical forms for querying knowledge bases or databases.\n- Framework unifies SLU and Q&A by parsing input into machine-readable representations.\n- Parsing handles complex linguistic phenomena, improving traditional SLU systems.\n- Extension of neural transition-based parser enhances parsing capabilities.\n- Annotated logical forms for training parsers are expensive and scarce.\n\nSlide Title: Transfer Learning and Multi-Task Learning\nBullet Points:\n- Transfer learning helps parse low-resource domains using high-resource domain data.\n- Pre-training on large data sets followed by fine-tuning on target domains.\n- Multi-task learning (MTL) improves generalization by training on multiple tasks.\n- MTL used in NLP for tagging, parsing, translation, and semantic parsing.\n- Overnight data set used for domain adaptation with Lambda DCS annotations.\n\nSlide Title: Transition-Based Parsing and Neural Models\nBullet Points:\n- Transition-based parsers used for dependency and semantic parsing tasks.\n- Neural transition-based parser uses Bi-LSTM and Stack-LSTM for parsing.\n- Attention mechanism focuses on relevant words during parsing.\n- Character-level embeddings and copy mechanism handle rare words.\n- Comparison of delexicalization and copy mechanism for data sparsity.\n\nSlide Title: Data Sets and Experiments\nBullet Points:\n- Evaluation on Q&A data sets: Overnight and NLmaps, and SLU data sets.\n- Overnight data set has small vocabularies, challenging for data-hungry models.\n- NLmaps has larger vocabularies, beneficial for attention mechanisms.\n- SLU data set includes domains like search, recipes, cinema, and bookings.\n- Experiments compare attention, delexicalization, and copy mechanisms.\n\nSlide Title: Handling Data Sparsity and Attention Mechanisms\nBullet Points:\n- Attention mechanism beneficial for larger data sets, less so for smaller ones.\n- Copy mechanism outperforms delexicalization across most data sets.\n- SLU data benefits from copy mechanism due to slot tagging nature.\n- Attention and copy mechanisms improve parsing accuracy significantly.\n- Results confirm unification of Q&A and SLU under semantic parsing framework.\n\nSlide Title: Transfer Learning Experiments\nBullet Points:\n- Transfer learning across Overnight domains shows average 4% improvement.\n- Pre-training and MTL both effective, especially for smaller domains.\n- SLU domains benefit from transfer learning, improving parsing accuracy.\n- Preliminary results on cross-task transfer learning show promise.\n- Future work suggested for transfer learning across heterogeneous data sets.\n\nSlide Title: Related Work and Conclusions\nBullet Points:\n- Various logical forms exist in semantic parsing literature for different purposes.\n- Multi-task learning (MTL) used in NLP for improved generalization.\n- Traditional SLU systems use domain-specific parsers for intent and slot identification.\n- Proposed framework effective for both Q&A and SLU tasks.\n- Future work includes exploring transfer learning across diverse data sets.",
    "Slide Title: Introduction to Abstract Meaning Representation (AMR)\nBullet Points:\n- AMR is a semantic graph representation abstracting syntactic realization of sentences.\n- Nodes represent concepts; edges represent semantic relations in AMR graphs.\n- AMR graphs allow for reentrancies, nodes with multiple parents.\n- AMR-to-text generation converts AMR graphs into natural language.\n- Graph encoding explicitly encodes reentrant structures in AMR graphs.\n\nSlide Title: Types of AMR Encoders\nBullet Points:\n- Sequential encoders reduce AMR graphs to sequences for processing.\n- Tree encoders ignore reentrancies, transforming graphs into tree structures.\n- Graph encoders maintain reentrancies, encoding structural information.\n- Study compares sequential, tree, and graph encoders for AMR-to-text generation.\n- Focus on reentrancies and long-range dependencies in AMR graphs.\n\nSlide Title: Graph-Structured AMRs\nBullet Points:\n- AMRs are rooted, directed graphs with labeled edges.\n- Labeled edges are replaced with unlabeled edges for model simplicity.\n- Transformation results in equivalent Levi graph representation.\n- Tree-structured AMRs discard reentrancies for tree conversion.\n- Sequential AMRs linearize and anonymize graphs for sequence processing.\n\nSlide Title: Encoder Architectures\nBullet Points:\n- Recurrent Neural Network (RNN) encoders use BiLSTM for sequential input.\n- TreeLSTM encoders process hierarchical tree structures, removing reentrancies.\n- Graph Convolutional Networks (GCN) learn node embeddings from graph structure.\n- GCNs applied to both graph and tree encoders by removing reentrancies.\n- Stacking encoders combines structural and sequential information effectively.\n\nSlide Title: Stacking Encoder Approaches\nBullet Points:\n- Structure on top of sequence: Structural encoders applied after BiLSTM.\n- Sequence on top of structure: BiLSTM applied after structural encoder.\n- GCNSEQ and TREELSTMSEQ models use structure-first approach.\n- Structure-first approach provides better input embeddings for BiLSTM.\n- Stacking allows testing contributions of structural and sequential components.\n\nSlide Title: Experimental Results\nBullet Points:\n- Evaluation metrics: BLEU and Meteor scores on AMR datasets.\n- GCNSEQ model achieves state-of-the-art results on tested datasets.\n- Graph encoders outperform sequential and tree encoders.\n- Explicit encoding of reentrancies benefits graph encoders.\n- Graph encoders handle long-range dependencies better than other models.\n\nSlide Title: Reentrancies and Long-Range Dependencies\nBullet Points:\n- Graph encoders model reentrancies explicitly, improving performance.\n- Graph models outperform others in examples with many reentrancies.\n- Preserving reentrancies benefits handling of long-range dependencies.\n- Graph encoders excel in examples with longer dependency lengths.\n- Long-range dependencies complement reentrancy benefits in graph models.\n\nSlide Title: Manual Inspection and Contrastive Analysis\nBullet Points:\n- Manual inspection shows graph models handle pronouns and co-references better.\n- Contrastive pairs method evaluates model handling of pronouns.\n- Graph encoders outperform in pronoun number and gender replacements.\n- Sequential encoders perform well in contrastive analysis tasks.\n- Future work needed to analyze pronomial mentions in different models.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Graph encoders improve AMR-to-text generation by encoding reentrancies.\n- Benefits observed in examples with reentrant structures and long dependencies.\n- GCN wired to BiLSTM network achieves state-of-the-art results.\n- Further exploration of graph encoders could enhance performance.\n- Future work may focus on co-references and control structures in graph models.",
    "Slide Title: Introduction to Abstractive Sentence Summarization\nBullet Points:\n- Exponential growth of online information necessitates effective automatic summarization systems.\n- Abstractive sentence summarization generates shorter versions while preserving original meaning.\n- Seq2seq framework achieves state-of-the-art performance but struggles with longer generations.\n- Seq2seq models often copy source words without true summarization.\n- Template-based summarization offers fluent, informative summaries but is time-consuming.\n\nSlide Title: Challenges and Limitations of Seq2seq Models\nBullet Points:\n- Seq2seq models lose control with increased generation length, reducing informativeness.\n- 3% of summaries contain less than 3 words, affecting readability.\n- Seq2seq models focus on copying source words, lacking true summarization.\n- Free generation based on source sentence is insufficient for seq2seq models.\n- Need for improved methods to enhance seq2seq summarization performance.\n\nSlide Title: Introduction to Re3Sum System\nBullet Points:\n- Re3Sum combines seq2seq and template-based summarization approaches.\n- System consists of three modules: Retrieve, Rerank, and Rewrite.\n- Utilizes Information Retrieval (IR) platform to find candidate soft templates.\n- Extends seq2seq model for template saliency measurement and summary generation.\n- Recurrent Neural Network (RNN) encoder converts input sentence and templates into hidden states.\n\nSlide Title: Retrieve Module in Re3Sum\nBullet Points:\n- Retrieve module finds candidate templates from training corpus.\n- Assumes similar sentences hold similar summary patterns.\n- Uses Information Retrieve (IR) system Lucene for efficient indexing and searching.\n- Selects top 30 searching results as candidate templates for each input sentence.\n- Aims to provide reference points for input sentence summarization.\n\nSlide Title: Jointly Rerank and Rewrite in Re3Sum\nBullet Points:\n- Rerank and Rewrite conducted through shared encoder using BiRNN.\n- Measures saliency of template based on hidden state relevance to input sentence.\n- Rerank selects template with highest predicted informativeness as soft template.\n- Rewrite generates summary using hidden states of sentence and template.\n- Combines hidden states to generate decoding hidden state for summary.\n\nSlide Title: Evaluation and Experiments\nBullet Points:\n- Experiments conducted on Annotated English Gigaword corpus.\n- ROUGE metric used for automatic evaluation of summary quality.\n- Re3Sum outperforms state-of-the-art seq2seq models in informativeness.\n- High-quality external summaries improve stability and readability.\n- Re3Sum demonstrates promising prospects in generation diversity.\n\nSlide Title: Linguistic Quality and Diversity Evaluation\nBullet Points:\n- Re3Sum generates longer, more fluent summaries than OpenNMT.\n- Soft templates guide summary generation, reducing extreme short summaries.\n- Re3Sum diminishes copying from source sentences, enhancing abstraction.\n- New named entities in soft templates may affect faithfulness.\n- Re3Sum shows potential in generating diverse summaries with different templates.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Re3Sum introduces soft templates to guide seq2seq summarization.\n- Uses IR platform Lucene to retrieve candidate soft templates.\n- Jointly conducts template reranking and template-aware summary generation.\n- Generates informative, readable, and stable summaries with diversity potential.\n- Future work includes improving Retrieve module and testing on other tasks.",
    "Slide Title: Introduction to Diverse Paraphrasing\nBullet Points:\n- Paraphrasing diversity is crucial for NLP tasks like data augmentation and conversational agents.\n- Traditional methods focus on semantic similarity, often neglecting structural diversity.\n- Our approach targets diverse paraphrasing without compromising quality.\n- We introduce a novel submodular function maximization for diverse paraphrasing.\n- Source code is available to encourage further research.\n\nSlide Title: Challenges in Current Paraphrasing Models\nBullet Points:\n- Most models rely on top-k beam search, resulting in structurally similar paraphrases.\n- Existing methods like DPP and SSR are restrictive in maintaining source sentence fidelity.\n- Need for a balance between diversity and fidelity in paraphrasing.\n- Submodular function maximization offers flexibility in controlling diversity and fidelity.\n- Greedy algorithms provide near-optimal solutions efficiently.\n\nSlide Title: Diverse Paraphraser using Submodularity (DiPS)\nBullet Points:\n- DiPS maximizes a submodular objective function tailored for paraphrasing.\n- Extensive experiments show DiPS generates diverse paraphrases without losing fidelity.\n- DiPS is effective for data augmentation in tasks like intent classification.\n- Comparison with other diversity-inducing schemes highlights DiPS's advantages.\n- DiPS leverages SEQ2SEQ framework with a diversity-inducing decoder.\n\nSlide Title: Methodology Overview\nBullet Points:\n- Paraphrase generation formulated as a sequence-to-sequence learning problem.\n- Standard cross-entropy loss used for training, with a submodular objective for decoding.\n- Encoder processes source sentence; decoder generates diverse paraphrases.\n- Submodular objective optimizes fidelity and diversity trade-off.\n- Modified decoder selects diverse subsequences for high-quality paraphrases.\n\nSlide Title: Monotone Submodular Objectives\nBullet Points:\n- Submodular functions designed for paraphrase generation with fidelity and diversity.\n- Fidelity measured through syntactic and semantic similarity functions.\n- Diversity encouraged by rewarding sequences with distinct n-grams.\n- Edit distance used to ensure structural variation in paraphrases.\n- Trade-off coefficient \u03bb balances fidelity and diversity in generation.\n\nSlide Title: Experiments and Evaluation\nBullet Points:\n- Datasets used for evaluating DiPS include Quora-PR and Quora-Div.\n- Baseline models focus on dialog systems, not directly applicable to paraphrasing.\n- Intrinsic evaluation metrics include BLEU, METEOR, and TER-Plus for fidelity.\n- Diversity measured by distinct n-grams scaled by generated tokens.\n- DiPS outperforms baselines in both fidelity and diversity metrics.\n\nSlide Title: Data Augmentation with DiPS\nBullet Points:\n- High-quality paraphrases used for data augmentation in classification tasks.\n- Experiments conducted on intent and question classification models.\n- DiPS-generated paraphrases improve accuracy over other augmentation schemes.\n- Structural variations in DiPS paraphrases enhance model generalization.\n- Results indicate the importance of diverse paraphrases for data augmentation.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- DiPS generates high-quality paraphrases by maximizing a submodular objective.\n- Offers control over fidelity and diversity, unlike prior methods.\n- Effective in data augmentation for intent and question classification tasks.\n- Potential applications in conversational agents and text summarization.\n- Encourages further exploration of submodular functions in NLP tasks.",
    "Slide Title: Introduction to Machine Question Answering\nBullet Points:\n- Teaching machines to answer user-generated questions is a key goal in natural language processing.\n- Existing methods locate documents with answers but struggle to extract answers automatically.\n- Neural models show promise in answering questions from related paragraphs.\n- Two main approaches: pipelined and confidence-based methods for document-level input.\n- This paper proposes improved methods for training models to handle document-level data.\n\nSlide Title: Pipelined Method for QA Systems\nBullet Points:\n- Pipelined QA system selects a single paragraph for question answering.\n- Paragraph selection uses TF-IDF cosine distance to match questions.\n- Linear classifier trained to select paragraphs with answer spans.\n- Improves answer selection accuracy from 83.1% to 85.1% on TriviaQA web.\n- Handles noisy labels with a summed objective function for answer span selection.\n\nSlide Title: Model Architecture and Features\nBullet Points:\n- Model uses pretrained word vectors and character-derived embeddings.\n- Bi-directional GRU processes question and passage embeddings.\n- Attention mechanism builds query-aware context representation.\n- Self-attention layer enhances passage understanding.\n- Prediction layer computes answer start and end scores with dropout for regularization.\n\nSlide Title: Confidence Method for Multi-Paragraph QA\nBullet Points:\n- Confidence scores measure model's certainty in answer spans.\n- Shared-normalization objective normalizes scores across paragraphs.\n- Merge approach concatenates paragraphs with separator tokens.\n- No-answer option allows model to select \"no-answer\" for paragraphs.\n- Sigmoid loss objective evaluates scores independently for comparability.\n\nSlide Title: Experimental Setup and Datasets\nBullet Points:\n- Evaluated on TriviaQA unfiltered, wiki, web, and SQuAD datasets.\n- Preprocessing includes merging small paragraphs and marking answer spans.\n- Sampling strategy selects paragraphs with and without answers for training.\n- Model trained with Adadelta optimizer and GloVe word vectors.\n- Performance measured using exact match (EM) and F1 scores.\n\nSlide Title: Results and Performance Analysis\nBullet Points:\n- Ablation study shows effectiveness of proposed methods on TriviaQA web.\n- Confidence-based approaches improve performance with more paragraphs.\n- Shared-norm approach achieves highest scores on TriviaQA test sets.\n- Document-level SQuAD evaluation shows model adapts well to multi-paragraph tasks.\n- End-to-end QA system tested on TREC QA tasks with strong results.\n\nSlide Title: Error Analysis and Discussion\nBullet Points:\n- Errors often due to insufficient evidence or missing answer in key.\n- Complex syntax and paraphrasing challenge model's extraction ability.\n- Multi-sentence reading and coreference errors identified.\n- Shared-norm approach preserves per-paragraph performance.\n- No-answer and merge methods expose model to irrelevant text.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Neural models advanced by large datasets like SQuAD and TriviaQA.\n- Open QA systems use pipelined methods for text retrieval and answer selection.\n- Our work focuses on multi-paragraph application of QA models.\n- Shared-norm objective and paragraph selection improve state-of-the-art on TriviaQA.\n- Demonstration shows potential for deep-learning-powered open QA systems.",
    "Slide Title: Introduction to Crisis Event Tweet Classification\nBullet Points:\n- Focus on real-time classification of tweets during sudden-onset events like disasters.\n- Tweets provide crucial information: injuries, infrastructure damage, urgent needs.\n- Timely access to social media data aids humanitarian response and reduces losses.\n- Challenge: Lack of large labeled datasets for new events.\n- Need for models that adapt to new events using past event data.\n\nSlide Title: Proposed Model Overview\nBullet Points:\n- Novel model combines domain adaptation and semi-supervised learning.\n- Utilizes deep learning framework with convolutional neural networks.\n- Integrates three networks: task-solving, semi-supervised, and domain adaptation.\n- Learns high-level representations invariant across domains.\n- Trained using stochastic gradient descent (SGD) algorithm.\n\nSlide Title: Model Architecture and Components\nBullet Points:\n- Input: Tweets mapped to distributed representations using embedding matrix.\n- Convolution and pooling layers extract higher-level features.\n- Three branches: supervised classification, semi-supervised learning, domain adaptation.\n- Loss function combines classification, graph-based, and adversarial components.\n- Hyperparameters control the strength of each component.\n\nSlide Title: Semi-supervised Learning Approach\nBullet Points:\n- Graph-based semi-supervised learning induces structural similarity.\n- Constructs a similarity graph to encode relations between instances.\n- Learns internal representations by predicting graph context nodes.\n- Outperforms traditional self-training and supervised methods.\n- Significant improvements in F1 scores with unlabeled data.\n\nSlide Title: Domain Adaptation with Adversarial Training\nBullet Points:\n- Domain discriminator distinguishes between source and target domains.\n- Adversarial training induces domain-invariant features.\n- Improves classification performance across different events.\n- Uses only unlabeled data from the target domain.\n- Further gains achieved by combining with semi-supervised learning.\n\nSlide Title: Experimental Setup and Datasets\nBullet Points:\n- Experiments conducted on Nepal earthquake and Queensland floods datasets.\n- Datasets include millions of tweets with relevant and non-relevant labels.\n- Training involves labeled and unlabeled data from source and target events.\n- Evaluation metrics: precision, recall, F-measure, and AUC.\n- Results demonstrate effectiveness of proposed model over baselines.\n\nSlide Title: Results and Analysis\nBullet Points:\n- Graph-based semi-supervised approach shows 4% to 13% F1 score improvements.\n- Domain adversarial training yields 1.8% to 4.1% F1 score gains.\n- Combined approach achieves 5% to 7% further improvements.\n- Effective in leveraging unlabeled data for domain adaptation.\n- Future potential with small amounts of labeled target domain data.\n\nSlide Title: Related Work and Contributions\nBullet Points:\n- Combines semi-supervised learning and domain adaptation research.\n- Extends domain adversarial neural networks with graph embedding.\n- Demonstrates improvements over traditional semi-supervised and domain adaptation methods.\n- Highlights importance of domain-invariant feature learning.\n- Provides insights into labeled data requirements for crisis events.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Proposed framework effectively adapts to new crisis events using past data.\n- Combines domain adversarial training with semi-supervised learning.\n- Significant improvements over baseline models in tweet classification.\n- Future work: explore advanced tuning methods and labeled target data.\n- Potential applications in real-time crisis response and humanitarian efforts.",
    "Slide Title: Introduction to Multilingual Corpora\nBullet Points:\n- Multilingual corpora are essential for various natural language processing tasks.\n- Comparable corpora help extract parallel lexicons and feed statistical machine translation systems.\n- Large collections of parallel texts for all domains and languages are scarce.\n- Wikipedia serves as a valuable source for multilingual data extraction.\n- Challenges exist due to Wikipedia's chaotic taxonomy and category overlaps.\n\nSlide Title: Understanding Comparability in Multilingual Corpora\nBullet Points:\n- Comparability in multilingual corpora lacks a universally accepted definition.\n- Ideal comparable corpora have similar structure, function, and characteristics across languages.\n- Four-class classification: parallel, strongly comparable, weakly comparable, and non-comparable texts.\n- Wikipedia's interlanguage links facilitate multilingual text extraction.\n- Challenges arise from Wikipedia's arbitrary and overlapping category graph.\n\nSlide Title: Domain-Specific Comparable Corpora Extraction\nBullet Points:\n- Proposal to extract domain-specific comparable corpora from Wikipedia.\n- Domain vocabulary built from Wikipedia articles using pre-processing techniques.\n- Graph exploration identifies categories likely belonging to the domain.\n- Controlled exploration avoids loops and uses a stopping criterion.\n- Focus on English and Spanish Wikipedia editions for three domains.\n\nSlide Title: Vocabulary and Graph Exploration\nBullet Points:\n- Domain vocabulary represents terms characterizing the domain.\n- Vocabulary built from top terms ranked by frequency after pre-processing.\n- Graph exploration uses breadth-first search from a root category.\n- Categories scored based on domain vocabulary presence in titles.\n- Trade-off between precision and recall in corpus extraction.\n\nSlide Title: Parallel Sentence Extraction Technique\nBullet Points:\n- Simple technique for extracting parallel sentences from comparable corpora.\n- Similarity between cross-language sentences estimated using text similarity measures.\n- Non-supervised method generates a noisy parallel corpus.\n- Cosine similarity and length factor measures used for similarity computation.\n- Thresholds empirically set to maximize F1 score on development set.\n\nSlide Title: Evaluation of Extracted Corpora in SMT\nBullet Points:\n- Quality of extracted corpora validated through statistical machine translation.\n- Comparison with Europarl corpus for English-Spanish language pair.\n- Three settings: domain-specific translation, combined translation, and out-of-domain translation.\n- Specialised systems outperform general Europarl system in domain-specific translation.\n- Wikipedia corpus improves translation quality even with some noise.\n\nSlide Title: Impact of Wikipedia Data on Translation\nBullet Points:\n- Enriching general translator with Wikipedia data improves translation results.\n- Best performance achieved with pure in-domain system for independent test set.\n- General Wikipedia corpus necessary for building a general translator.\n- Combined systems benefit from inclusion of Europarl data.\n- Results suggest importance of domain-specific corpora for translation tasks.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Model enables automatic extraction of in-domain comparable corpora from Wikipedia.\n- Significant improvement in machine translation quality with domain-specific parallel sentences.\n- Prototype applicable to other language pairs and domains with basic processing tools.\n- Ongoing efforts focus on robust parallel sentence identification and indirect evaluation schemes.\n- Future work aims to enhance model performance and expand language applicability.",
    "Slide Title: Introduction to Event Relations\nBullet Points:\n- Understanding event relations is crucial for natural language understanding and applications like summarization.\n- Focus on joint extraction of temporal and causal relations between events.\n- Temporal and causal relations often interact, influencing each other's identification.\n- Example: Temporal relation between events can be dictated by causal relation.\n- Joint framework proposed for Temporal and Causal Reasoning (TCR).\n\nSlide Title: Joint Framework for Temporal and Causal Reasoning\nBullet Points:\n- TCR combines temporal and causal extraction systems using a constrained conditional model.\n- Integer linear programming (ILP) enforces constraints during inference.\n- Constraints include cause-effect temporal precedence and symmetry.\n- Transitivity constraints ensure temporal consistency across event pairs.\n- Novel approach to jointly extract temporal and causal relations.\n\nSlide Title: Development of Jointly Annotated Dataset\nBullet Points:\n- Lack of jointly annotated data is a key obstacle in studying temporal and causal relations.\n- Developed a dataset by augmenting Event-Causality dataset with dense temporal annotations.\n- Dataset shows significant improvements in both temporal and causal relation extraction.\n- Incorporates explicit time expressions and knowledge-based rules into ILP.\n- Results show improvements on benchmark dataset TimeBank-Dense.\n\nSlide Title: Related Work on Temporal and Causal Relations\nBullet Points:\n- Temporal and causal relations represented by directed acyclic graphs.\n- Early methods focused on local classification models for temporal relations.\n- Global reasoning methods enforce graph constraints like symmetry and transitivity.\n- Causal relation extraction in NLP often uses lexical features or discourse relations.\n- Joint framework captures both temporal and causal aspects for better event understanding.\n\nSlide Title: Temporal and Causal Reasoning Framework\nBullet Points:\n- Temporal component uses classifiers for event-event and event-timex pairs.\n- Transitivity constraints ensure consistent temporal relations across event pairs.\n- Linguistic rules incorporated as hard constraints for high-precision predictions.\n- Full TCR framework integrates causal relations with temporal component.\n- Joint inference formulated as ILP for optimization.\n\nSlide Title: Experiments and Results on Temporal Performance\nBullet Points:\n- Evaluated on TimeBank-Dense dataset with improved temporal relation identification.\n- Ablation study shows step-by-step improvement with added information sources.\n- Joint framework achieves best precision, recall, and F1 scores.\n- Comparison with state-of-the-art systems shows major improvement in recall.\n- Joint reasoning with causal relations enhances temporal performance.\n\nSlide Title: Joint Performance on New Dataset\nBullet Points:\n- New dataset with both temporal and causal annotations developed for evaluation.\n- Joint framework improves both temporal and causal relation extraction.\n- Temporal performance improved in precision, recall, and F1 score.\n- Causal accuracy increased significantly with joint framework.\n- Gold relations used to evaluate potential performance improvements.\n\nSlide Title: Discussion and Conclusion\nBullet Points:\n- Joint framework balances precision and recall better on new dataset.\n- Temporal improvements are small due to fewer causal links in newswires.\n- Causality encodes global knowledge, complementing local temporal information.\n- Some examples break temporal precedence assumption, needing better event definitions.\n- TCR framework shows notable improvement, encouraging joint study of event aspects.",
    "Slide Title: Introduction to Machine Translation Evaluation\nBullet Points:\n- Automatic evaluation metrics are practical for assessing machine translation (MT) performance.\n- Manual evaluation is costly and time-consuming, making automatic metrics essential.\n- Metrics facilitate cross-paper comparisons and developmental cycles in MT studies.\n- WMT Metrics Shared Task validates and develops new evaluation metrics annually.\n- Quality Estimation (QE) methods assess MT output without reference translations.\n\nSlide Title: Task Setup and Language Pairs\nBullet Points:\n- Participants received test sets for each language pair in the task.\n- System-level metrics correlate with average human judgments of segment quality.\n- Segment-level metrics correlate with human ranking of translations for a source segment.\n- Participants chose language pairs and tracks (system/segment, reference-based/free).\n- 18 language pairs with 10 target languages were evaluated in the task.\n\nSlide Title: Source and Reference Texts\nBullet Points:\n- Source and reference texts from newstest2019 were used in the task.\n- Approximately 2,000 sentences per translation direction, with exceptions.\n- German\u2192Czech and German\u2194French had unique translation directions.\n- Dataset previously had adverse artifacts in MT evaluation.\n- Reference translations matched MT system translation directions.\n\nSlide Title: System Outputs and Evaluation Challenges\nBullet Points:\n- MT systems' quality affects the difficulty of ranking them.\n- Unsupervised MT systems in German\u2192Czech were more creative.\n- French\u2194German translations focused on EU Election sub-domain.\n- Regular news tasks systems varied in training data constraints.\n- 233 systems across 18 language pairs participated in the evaluation.\n\nSlide Title: Manual Quality Assessment\nBullet Points:\n- Direct Assessment (DA) was used for human evaluation of MT systems.\n- DA scores standardized per annotator and averaged for system performance.\n- Segment-level evaluation used daRR judgements from DA scores.\n- Reference-based and reference-free evaluations were conducted.\n- System-level and segment-level evaluations had different golden truth methods.\n\nSlide Title: Baseline Metrics and Tools\nBullet Points:\n- Baseline metrics prevent \"loss of knowledge\" in MT evaluation.\n- BLEU, NIST, TER, WER, PER, and CDER were used as baselines.\n- Moses scorer aggregated several useful metrics over time.\n- chrF and sacreBLEU metrics were computed using original implementations.\n- Chinese word segmentation required special pre-processing.\n\nSlide Title: Submitted Metrics Overview\nBullet Points:\n- 24 metrics from 13 research groups participated in the task.\n- Metrics included BEER, BERTr, CharacTER, EED, and ESIM.\n- Metrics used various features like sub-word indicators and contextual embeddings.\n- hLEPOR and PReP metrics focused on length penalty and pseudo-references.\n- WMDO and YiSi metrics explored semantic vector space and lexical similarity.\n\nSlide Title: System-Level and Segment-Level Results\nBullet Points:\n- Pearson correlation used for system-level evaluation of metrics.\n- Segment-level evaluation relied on daRR judgements and Kendall's Tau-like formulation.\n- Metrics like YiSi and ESIM showed high performance in system-level evaluation.\n- Segment-level evaluation highlighted discerning metrics for language pairs.\n- QE systems showed instability in correlations across language pairs.\n\nSlide Title: Discussion and Future Directions\nBullet Points:\n- Stability of metrics depends on underlying texts and MT systems.\n- BLEU metric showed weak correlation for top state-of-the-art systems.\n- Semantic features in metrics like BERTr and ESIM showed promise.\n- Future metrics may explore semantic understanding for performance gains.\n- Some language pairs did not show strong degrading patterns with top-n systems.\n\nSlide Title: Conclusion and Key Takeaways\nBullet Points:\n- WMT19 Metrics Task evaluated metrics' correlation with human judgment.\n- System-level metrics achieved high Pearson correlation in several language pairs.\n- Segment-level evaluation confirmed the trend of using embeddings for high performance.\n- QE systems showed potential but were less stable than standard metrics.\n- Future research should focus on semantic features and understanding in MT evaluation.",
    "Slide Title: Introduction to Zero-Shot Learning\nBullet Points:\n- Zero-shot learning (ZSL) addresses classification without labeled data for unseen classes.\n- Humans transfer knowledge from known to unknown domains using semantic representations.\n- ZSL uses semantic knowledge like class descriptions and word embeddings.\n- Few studies combine different types of semantic knowledge in ZSL.\n- Proposed framework uses data and feature augmentation for effective learning.\n\nSlide Title: Proposed Two-Phase Framework\nBullet Points:\n- Novel deep learning framework with coarse-grained and fine-grained classification phases.\n- Coarse-grained classification identifies if a document is from seen or unseen classes.\n- Fine-grained classification determines the specific class of the document.\n- Framework trained using labeled data of seen classes only.\n- No semantic correspondence required between training and inference classes.\n\nSlide Title: Data Augmentation Technique\nBullet Points:\n- Topic translation strengthens framework's ability to detect unseen classes.\n- Translates documents from seen to unseen classes using word-level analogy.\n- Uses 3COSMUL method to solve analogy questions for word translation.\n- Augmented documents serve as negative examples for classifiers.\n- Helps classifiers learn to reject instances from unseen classes.\n\nSlide Title: Feature Augmentation with Semantic Knowledge\nBullet Points:\n- Relationship vectors show relatedness between words and classes using ConceptNet.\n- ConceptNet provides general knowledge of natural language words and phrases.\n- Class nodes, superclass nodes, and description nodes represent class in ConceptNet.\n- Relationship vectors constructed based on semantic closeness to class nodes.\n- Augmented features enable knowledge transfer from seen to unseen classes.\n\nSlide Title: Experiments and Datasets\nBullet Points:\n- Experiments conducted on DBpedia ontology and 20newsgroups datasets.\n- Vocabulary size limited to 20,000 most frequent words.\n- Two unseen class rates: 50% and 25% for evaluation.\n- Random selection of seen and unseen classes repeated ten times.\n- Results averaged across all groups for fair comparison.\n\nSlide Title: Evaluation of Coarse-Grained Classification\nBullet Points:\n- Phase 1 outperformed DOC in detecting documents from seen classes.\n- Augmented data improved accuracy for detecting unseen class documents.\n- Topic translation generated augmented data with target class tone.\n- Confidence scores distribution varied across different classes.\n- Traditional classifier necessary for accurate class prediction.\n\nSlide Title: Evaluation of Fine-Grained Classification\nBullet Points:\n- Traditional CNN classifier in Phase 2 showed high accuracy.\n- Zero-shot classifier tested with different input combinations.\n- Combination of word embeddings and relationship vectors improved accuracy.\n- Integration of semantic knowledge effective for knowledge transfer.\n- Framework achieved highest overall accuracy compared to baselines.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- Previous works focused on semantic relationships via embeddings.\n- Proposed framework leverages multiple types of semantic knowledge.\n- Data augmentation in NLP explored for machine translation and text classification.\n- Feature augmentation used for domain adaptation in various NLP tasks.\n- Future work includes multi-label classification and larger datasets.",
    "Slide Title: Introduction to Multi-task Learning in NLP\nBullet Points:\n- Supervised learning in NLP often involves training individual models for each task.\n- Tasks like POS tagging and Name Tagging can benefit from shared knowledge.\n- Multi-task Learning (MTL) enables knowledge transfer across related tasks.\n- MTL is effective for tasks with limited labeled data, especially in low-resource languages.\n- Enhancing models in low-resource settings by leveraging high-resource languages is crucial.\n\nSlide Title: Challenges in Low-resource NLP Tasks\nBullet Points:\n- Many NLP tasks lack sufficient training data, especially in low-resource languages.\n- Annotating data for all 7,099 living languages is impractical.\n- Rapid development is needed for special applications like disaster response.\n- Data sparsity affects performance in low-resource languages and related tasks.\n- A single transfer model offers limited improvement in data-sparse environments.\n\nSlide Title: Proposed Multi-lingual Multi-task Architecture\nBullet Points:\n- Combines different transfer models within a unified architecture.\n- Two levels of parameter sharing: character embeddings and task-specific layers.\n- Character embeddings and CharCNN shared across languages with similar scripts.\n- Bidirectional LSTM layer shared to extract contextual information.\n- Cross-task and cross-lingual transfer strategies enhance word representations.\n\nSlide Title: Model Basic Architecture\nBullet Points:\n- Sequence labeling assigns categorical labels to each token in a sentence.\n- LSTM-CRFs-based models are effective for multi-lingual multi-task learning.\n- Character embeddings transfer morphological and semantic information.\n- Highway networks enhance performance by transforming character feature vectors.\n- CRFs layer models correlations between tags for optimal tagging paths.\n\nSlide Title: Multi-task Multi-lingual Architecture Details\nBullet Points:\n- Parameter sharing provides universal word representation and feature extraction.\n- Word embeddings shared across tasks in the same language.\n- Linear and CRFs layers shared across languages for task-specific knowledge transfer.\n- Language-specific linear layer allows model adaptation to different languages.\n- Alternating training approach optimizes multiple tasks within one model.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Evaluated on Name Tagging and POS Tagging tasks with various language pairs.\n- Model outperforms mono-lingual single-task baseline and single transfer models.\n- Cross-lingual transfer surpasses cross-task transfer with limited main task data.\n- Data augmentation with related languages boosts performance in low-resource settings.\n- Model achieves good performance in both low-resource and high-resource settings.\n\nSlide Title: Qualitative Analysis and Ablation Studies\nBullet Points:\n- Shared character-level networks transfer morphological and semantic information.\n- Cross-lingual transfer model corrects tagging errors with shared CRFs layer.\n- Ablation studies show contributions of individual components to performance.\n- Model does not heavily rely on the amount of auxiliary task data.\n- Implicit data augmentation and inductive bias improve tag prediction.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- MTL applied in various NLP areas like machine translation and text classification.\n- Previous models focus on different transfer schemes and supervision levels.\n- Proposed architecture combines cross-task and cross-lingual transfer.\n- Future work includes applying architecture to tasks like Event Extraction.\n- Exploring incremental learning for rapid adaptation to new tasks.",
    "Slide Title: Introduction to Natural Language Inference (NLI)\nBullet Points:\n- NLI is a key task in natural language understanding, involving premise and hypothesis sentences.\n- Goal: Determine if the relationship is entailment, neutral, or contradiction.\n- High-quality datasets like SNLI and MultiNLI have boosted NLI performance.\n- Discourse markers like \"but\" and \"so\" relate to NLI intents.\n- Few NLI works utilize discourse marker information effectively.\n\nSlide Title: Discourse Marker Augmented Network\nBullet Points:\n- Proposes using discourse markers to enhance sentence representation in NLI.\n- Transfers knowledge from Discourse Marker Prediction (DMP) to NLI model.\n- Introduces a sentence encoder model from DMP task to NLI network.\n- Reinforcement learning optimizes label confidence in NLI datasets.\n- Achieves better performance than state-of-the-art NLI solutions.\n\nSlide Title: Task Descriptions: NLI and DMP\nBullet Points:\n- NLI: Judge logical relationship between premise and hypothesis sentences.\n- Labels: entailment, neutral, contradiction.\n- DMP: Predict discourse marker linking two sentence halves.\n- Uses BookCorpus dataset for training discourse marker prediction.\n- Glove embeddings and bi-directional LSTM used for sentence encoding.\n\nSlide Title: Sentence Encoder Model\nBullet Points:\n- Transforms sentences into vectors using Glove and bi-directional LSTM.\n- Max pooling extracts features from concatenated hidden states.\n- Combines last hidden state and max pooling for sentence representation.\n- Linear operations predict discourse marker between sentence pairs.\n- Softmax function normalizes probability distribution for prediction.\n\nSlide Title: Encoding and Interaction Layers\nBullet Points:\n- Encoding layer uses word, character, POS, and NER embeddings.\n- Exact match features indicate word presence in both sentences.\n- Bi-directional LSTM processes sequences for encoding.\n- Interaction layer uses attention mechanism for premise-hypothesis linking.\n- Similarity matrix and attention vectors model local inference.\n\nSlide Title: Output Layer and Training\nBullet Points:\n- Predicts logical relation: entailment, neutral, contradiction using softmax.\n- Combines sentence representations from DMP and NLI for prediction.\n- New objective function combines log probabilities and reinforcement learning.\n- REINFORCE algorithm minimizes negative expected reward.\n- Linear function integrates cross-entropy and reinforcement learning objectives.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Pre-trained sentence encoder model on BookCorpus for DMP task.\n- SNLI and MultiNLI datasets used for NLI task evaluation.\n- Achieves state-of-the-art results on SNLI and MultiNLI datasets.\n- Ensemble model improves performance with multiple initialized models.\n- Ablation analysis shows importance of discourse markers and features.\n\nSlide Title: Semantic Analysis and Visualization\nBullet Points:\n- Performance declines when removing key discourse markers like \"but\" and \"because\".\n- \"Contradiction\" label benefits most from pre-trained sentence encoder.\n- Visualization shows improved representation with discourse marker augmentation.\n- Synonyms and contradictory meanings better captured with discourse markers.\n- Discourse markers enhance semantic representation in NLI tasks.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Inspired by DisSent model and discourse prediction tasks.\n- Previous works used small datasets and single vector sentence representation.\n- Our method integrates sentence encoder into NLI model for better interaction.\n- Combines pre-trained encoder with NLI model for novel framework.\n- Achieves state-of-the-art results and considers annotator views with reinforcement learning.",
    "Slide Title: Introduction to Instruction Execution\nBullet Points:\n- Agents face challenges in grounding language to environment and reasoning about discourse dependencies.\n- Example: Moving chemicals between beakers requires resolving references and generating actions.\n- Model maps sequences of instructions to actions, considering previous utterances and world state.\n- Focus on directly generating action sequences without explicit reference modeling.\n- Contextual cues from interaction history and world state are crucial for execution.\n\nSlide Title: Technical Overview and Task Notation\nBullet Points:\n- Define world states (S), instructions (X), and actions (A) in the task.\n- ALCHEMY domain: Beakers as stacks, actions include POP and PUSH.\n- Goal: Generate action sequences from start state based on instructions.\n- Execution involves state-action pairs, terminating with STOP action.\n- Model predicts next action using current and previous instructions and states.\n\nSlide Title: Model Architecture\nBullet Points:\n- Neural network policy predicts actions using attention and RNN decoder.\n- Inputs: Current instruction, previous instructions, initial and current states.\n- Attention encodes inputs, RNN generates actions step-by-step.\n- Model switches instructions upon predicting STOP action.\n- Uses attention to process inputs and select next action.\n\nSlide Title: Learning Approach\nBullet Points:\n- Access to instruction sequences paired with start and goal states.\n- Learning maximizes expected immediate reward for given state.\n- Reward shaped by task completion and distance to goal.\n- Exploration problem addressed by observing rewards for all actions.\n- Learning algorithm maximizes immediate expected reward.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Evaluated on SCONE domains: ALCHEMY, SCENE, TANGRAMS.\n- Task completion accuracy: 89.1% for single instruction, 62.7% for sequences.\n- Approach outperforms baselines, addressing early learning biases.\n- High variance in performance due to model sensitivity.\n- Error analysis highlights reference resolution and instruction ambiguity.\n\nSlide Title: Related Work and Contextual Cues\nBullet Points:\n- Previous work focused on logical representations and navigation tasks.\n- SCONE corpus designed for discourse context-dependence phenomena.\n- Our approach directly generates actions modifying the environment.\n- Contextual cues from changing environment and instruction history are key.\n- Model learns procedures otherwise hardcoded in logic.\n\nSlide Title: SCONE Domains and Data\nBullet Points:\n- SCONE includes ALCHEMY, SCENE, and TANGRAMS domains.\n- ALCHEMY: Beakers with colored chemicals, actions are PUSH and POP.\n- SCENE: Positions with people, actions include ADD and REMOVE.\n- TANGRAMS: List of objects, actions are INSERT and REMOVE.\n- State encodings use RNNs for domain-specific representations.\n\nSlide Title: Discussion and Future Work\nBullet Points:\n- Model reasons about context-dependent instructional language.\n- SESTRA learning algorithm overcomes biases in on-policy learning.\n- Future work: Use intermediate world states for complex references.\n- Additional reward observations help recover from early learning biases.\n- Potential to use estimators for predicting rewards for all actions.",
    "Slide Title: Introduction to Semantic Parsers\nBullet Points:\n- Semantic parsers predict predicate-argument structures, a longstanding challenge in AI.\n- Traditional datasets for training are difficult to gather and require expert annotation.\n- Recent research explores non-expert training for semantic supervision.\n- Crowdsourcing offers a scalable solution for creating large datasets.\n- QA-SRL is an intuitive annotation scheme for non-experts.\n\nSlide Title: QA-SRL Annotation Scheme\nBullet Points:\n- QA-SRL labels predicate-argument relationships with question-answer pairs.\n- High precision QA-SRL annotations are achievable with limited training.\n- Crowdsourcing can produce large-scale QA-SRL datasets efficiently.\n- QA-SRL Bank 2.0 includes 133,479 verbs from 64,018 sentences.\n- The dataset was created in 9 days with high precision and good recall.\n\nSlide Title: Crowdsourcing Pipeline for QA-SRL\nBullet Points:\n- The pipeline includes generation and validation steps for QA-SRL annotations.\n- Autocomplete and auto-suggest features streamline question writing.\n- Validation ensures high precision by requiring answerable questions.\n- The pipeline is scalable and cost-effective compared to traditional methods.\n- High agreement rates among validators confirm data precision.\n\nSlide Title: Models for QA-SRL Parsing\nBullet Points:\n- The parser uses a pipeline approach: span detection and question generation.\n- Span-based representation and custom LSTM decode questions from spans.\n- Models do not require syntactic information, trained from crowdsourced labels.\n- Experiments show 82.2% span-detection F1 and 47.2% exact-match question accuracy.\n- Data bootstrapping expands the dataset, improving model performance.\n\nSlide Title: Span Detection and Question Generation\nBullet Points:\n- Span detection models: BIO sequence and span-based models.\n- BIO model uses sequence tagging with BIO encoding for span prediction.\n- Span-based model assigns probabilities to all possible spans.\n- Question generation models: local and sequence models.\n- Sequence model uses RNN to share information between question slots.\n\nSlide Title: Data Expansion and Model Retraining\nBullet Points:\n- Parser predictions validated to fill data gaps and improve recall.\n- Over-generation of QA pairs for already-annotated sentences.\n- Expansion step increased valid questions by 20% in train and dev sets.\n- Retrained model shows improvements in span detection and question generation.\n- Final evaluation shows high accuracy with expanded data.\n\nSlide Title: Final Evaluation and Results\nBullet Points:\n- Human evaluation confirms high accuracy of the final parser.\n- Sequence-based question generation models outperform local models.\n- Expanded data results in more questions and spans at the same threshold.\n- Final parser achieves 82.64% question accuracy and 77.61% span-level precision.\n- High accuracy maintained across different domains in the dataset.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- QA-SRL can be scaled to large datasets for predicate-argument structures.\n- Crowdsourcing approach enables rapid and cost-effective data collection.\n- QA-SRL Bank 2.0 provides over 250,000 question-answer pairs.\n- First high-quality parser for QA-SRL demonstrates data utility.\n- Future work may explore further improvements in recall and annotation exhaustiveness.",
    "Slide Title: Introduction to SIGHAN 2015 Bake-off\nBullet Points:\n- Chinese spelling checkers face challenges due to lack of word delimiters and numerous characters.\n- Over 13,000 Chinese characters complicate spelling checks compared to 26 English letters.\n- Errors often arise from phonologically similar but semantically distinct multi-character words.\n- SIGHAN 2013 was the first campaign to benchmark Chinese spelling checkers.\n- SIGHAN 2015 provides a platform for evaluating automatic Chinese spelling checkers.\n\nSlide Title: Task Description for Chinese Spelling Check\nBullet Points:\n- Evaluate the capability of a Chinese spelling checker with given passages.\n- Checker identifies incorrect characters and suggests corrections.\n- Input passages have unique passage numbers for identification.\n- Output format includes passage number, error location, and correction.\n- Example: Correct characters \"\u5e0c\" and \"\u671b\" replace wrong ones at specified locations.\n\nSlide Title: Data Preparation for SIGHAN 2015\nBullet Points:\n- Learner corpus collected from Test of Chinese as a Foreign Language essays.\n- Spelling errors manually annotated by trained native Chinese speakers.\n- Training set: 970 essays with 3,143 spelling errors for model training.\n- Dryrun set: 39 passages for participants to familiarize with testing process.\n- Test set: 1,100 passages, half with spelling errors, for open test evaluation.\n\nSlide Title: Performance Metrics and Evaluation\nBullet Points:\n- Correctness judged at detection and correction levels using a confusion matrix.\n- Detection level: Identify all incorrect character locations matching gold standard.\n- Correction level: Match locations and corrections with gold standard.\n- Reducing false positives is crucial for accurate performance evaluation.\n- Metrics include precision, recall, and F1 score for comprehensive assessment.\n\nSlide Title: Evaluation Results and Team Performances\nBullet Points:\n- NCTU&NTUT achieved lowest false positive rate at 0.0509.\n- CAS system excelled in error detection with F1 score of 0.6404.\n- CAS correction accuracy significantly outperformed others at 0.6918.\n- No system excelled in all metrics; CAS and NCTU&NTUT performed best overall.\n- CAS used HMM-based approach; NCTU&NTUT used word vector/CRF-based detector.\n\nSlide Title: Approaches and Techniques Used by Participants\nBullet Points:\n- CAS proposed a unified framework using HMM for sentence segmentation.\n- Two-stage filter process re-ranks correction candidates for best selection.\n- NCTU&NTUT used word vector/CRF-based detector for error detection.\n- Language model rescoring procedure speeds up error correction.\n- Participants contributed to knowledge in effective Chinese spell checking.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Overview of SIGHAN 2015 Bake-off, task design, data, and evaluation metrics.\n- Submissions contribute to knowledge in Chinese spell checker development.\n- Data sets and evaluation tools made publicly available for future research.\n- Future focus on Chinese grammatical error correction and language resources.\n- New data sets from CFL learners to enrich research in Chinese language processing.",
    "Slide Title: Introduction to Language Identification\nBullet Points:\n- Language identification enhances natural language processing applications like transliteration and phoneme conversion.\n- Traditional methods use character-level n-gram language models for language identification.\n- This paper proposes using support vector machines (SVMs) for identifying languages in short texts.\n- SVMs outperform language models in accuracy on datasets of personal names.\n- Language identification can improve transliteration by pre-processing source data.\n\nSlide Title: Previous Work on Language Identification\nBullet Points:\n- N-gram approaches are popular for general text categorization and language identification.\n- Cavnar and Trenkle achieved 99.8% accuracy with n-gram models on long texts.\n- Accuracy drops significantly for shorter texts using n-gram models.\n- SVMs achieve higher accuracy than n-gram models on short text language identification.\n- Specific training on smaller datasets improves n-gram model performance.\n\nSlide Title: SVMs for Language Identification\nBullet Points:\n- SVMs use character n-gram occurrences as features for language identification.\n- SVMs can handle a large number of features and weigh them appropriately.\n- Linear, sigmoid, and RBF kernels perform best in initial experiments.\n- Optimal n-gram lengths found: four for single names, five for full names.\n- SVMs outperform language models on the Transfermarkt and CEJ corpora.\n\nSlide Title: Intrinsic Evaluation with Transfermarkt Corpus\nBullet Points:\n- Transfermarkt corpus includes European soccer player names in 13 languages.\n- SVMs outperform language models, especially on last names.\n- Data is noisy; player nationality doesn't always match name language.\n- SVMs yield better accuracy on last names than language models on full names.\n- Kernel differences are not statistically significant.\n\nSlide Title: Intrinsic Evaluation with CEJ Corpus\nBullet Points:\n- CEJ corpus includes Chinese, English, and Japanese names for transliteration.\n- Linear kernel used due to large corpus size; reduces error rate by over 50%.\n- SVMs outperform language models in language identification accuracy.\n- RBF and sigmoid kernels are slower due to corpus size.\n- SVMs provide significant improvements over traditional language models.\n\nSlide Title: Application to Machine Transliteration\nBullet Points:\n- Language identification can enhance machine transliteration by determining word pronunciation.\n- English-Hindi corpus used to test language identification's impact on transliteration.\n- Language identification accuracy: 80% linear, 84% RBF, 83% sigmoid kernels.\n- Splitting data by language yields no significant accuracy improvement.\n- Data reduction impacts training of separate transliteration models.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Language identification model splits data into disjoint classes for transliteration.\n- DIRECTL system used for training transliteration models on separate classes.\n- Combined top-1 accuracy of 46.0% compared to 47.0% with full data model.\n- Data size reduction affects performance; language identification provides useful information.\n- Closer coupling of language identification and transliteration may improve accuracy.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- SVMs with n-gram counts outperform traditional language models for name identification.\n- Language identification as a preprocessing step shows no significant transliteration accuracy change.\n- Data size reduction impacts model training; language identification remains useful.\n- Future work: explore language identification probabilities as transliteration features.\n- Investigate non-disjoint data sets for separate transliteration models to leverage common information.",
    "Slide Title: Introduction to Retraction Citations  \nBullet Points:  \n- Retraction of articles has increased with the rise in scientific publications.  \n- Retracted articles pose challenges when cited, especially for unethical practices.  \n- Two types of retraction citations: pre-retraction and post-retraction.  \n- High pre-retraction citations often lead to more post-retraction citations.  \n- Self-citation by authors can falsely validate retracted work.\n\nSlide Title: Data Collection Methodology  \nBullet Points:  \n- ScienceDirect database queried for \"RETRACTED\" articles in October 2014.  \n- 1,203 results retrieved; 988 were retracted articles.  \n- Five top-cited articles post-2015 selected for analysis.  \n- 125 citing documents located; 109 analyzed for context.  \n- Citing documents categorized as positive, negative, or neutral mentions.\n\nSlide Title: Case Study: Donmez et al. Article  \nBullet Points:  \n- Retracted in 2014 due to data misrepresentation in images.  \n- 36 citations tracked in 2015-2016; mostly positive mentions.  \n- One negative mention highlighted altered graphics causing bias.  \n- Validity of results not questioned, leading to continued citations.  \n- Positive citations used to support current studies.\n\nSlide Title: Case Study: S\u00e9ralini Affair  \nBullet Points:  \n- Article on GM crops and tumors retracted due to small sample size.  \n- 23 post-retraction citations; mix of positive, negative, and neutral.  \n- Media frenzy contributed to ongoing public debate on GM foods.  \n- Republished in 2014, stirring further controversy.  \n- Criticism of peer-review practices and media influence noted.\n\nSlide Title: Case Study: Dipak Das Investigation  \nBullet Points:  \n- Retracted due to data manipulation and scientific misconduct.  \n- 24 post-retraction citations; all positive mentions.  \n- Popularity of \"authentic leadership\" in management studies.  \n- Continued citations despite detailed retraction notice.  \n- Ethical investigation led to retraction of all related papers.\n\nSlide Title: Case Study: Li Article  \nBullet Points:  \n- Retracted for duplicating significant parts of a previous paper.  \n- 18 out of 21 recent citations analyzed; mostly positive context.  \n- Data re-use violated originality rule, but data not refuted.  \n- Article continues to be cited as legitimate in literature.  \n- Positive citations despite retraction for data duplication.\n\nSlide Title: Discussion and Conclusions  \nBullet Points:  \n- Retracted articles continue to be cited due to public and media interest.  \n- Editors' approval of results can lead to ongoing citations.  \n- Invalid research appearing as valid is concerning.  \n- Publishers should implement reference checks for retracted citations.  \n- Recommend deletion of retracted articles from publishers' websites.\n\nSlide Title: Recommendations for Publishers  \nBullet Points:  \n- Implement reference checks to detect retracted article citations.  \n- Request explanations for citing retracted papers positively or neutrally.  \n- Delete retracted articles from publishers' websites, leaving retraction notices.  \n- Restrict access to retracted articles to prevent further citations.  \n- Encourage transparency and accountability in scientific publishing.",
    "Slide Title: Introduction to Semantic Parsing and Surface Realization\nBullet Points:\n- Increased interest in semantic parsing using graph-structured representations like AMR and EDS.\n- Need for formal frameworks to manipulate graphs, similar to automata for strings.\n- Study of DAG transducers for efficient natural language generation (NLG).\n- Type-logical semantic graphs offer a flat structure for modeling language semantics.\n- Existing DAG transducers have limitations in handling type-logical semantic graphs.\n\nSlide Title: Challenges in DAG Transducers for NLG\nBullet Points:\n- DAG automata are limited to recognition, not generation of sentences.\n- Existing DAG-to-Tree transducers struggle with multi-rooted semantic graphs.\n- Difficulty in reversing edge directions and handling unconnected nodes.\n- Need for a new approach to transform flat semantic graphs into strings.\n- Declarative programming offers a solution for flexible graph transduction.\n\nSlide Title: A New DAG Transducer Design\nBullet Points:\n- Introduces a transducer design for transforming structures into various data forms.\n- Utilizes declarative programming to generate target structures from DAGs.\n- Focus on natural language generation by transforming graphs into strings.\n- Simple programming language for string concatenation in transduction.\n- Constructs internal trees to solve programs and generate strings.\n\nSlide Title: DAG Transduction-based NLG Strategy\nBullet Points:\n- Different languages require different morphosyntactic and syntactico-semantic handling.\n- Two-step strategy: syntactico-semantic translation and morpho-syntactic modeling.\n- DAG transducer generates sequential lemmas with syntactic information.\n- Neural sequence-to-sequence model refines output to final surface strings.\n- Unified framework accommodates diverse linguistic properties.\n\nSlide Title: Inducing Transduction Rules\nBullet Points:\n- Empirical study on DAG transduction-based NLG using EDS representations.\n- Constraints ensure valid transduction and rooted tree formation.\n- Fine-to-coarse strategy enhances robustness and coverage.\n- Induced, extended, and dynamic rules cater to precision and robustness.\n- Rule extraction from EDS graphs and derivation trees.\n\nSlide Title: Evaluation and Analysis of DAG Transducer\nBullet Points:\n- Experiments conducted on DeepBank and wikiwoods datasets.\n- Induced rules achieve high accuracy but limited coverage.\n- Extended rules improve coverage with slight accuracy drop.\n- Dynamic rules ensure full coverage but reduce accuracy.\n- Comparison with neural baseline shows transducer's superior performance.\n\nSlide Title: Efficiency and Conclusion\nBullet Points:\n- Beam search decoder efficiently translates EDS graphs on CPU.\n- Dynamic rules slow down decoding but ensure robustness.\n- System generates over 5 sentences per second on average.\n- Proposed method leverages declarative programming for efficient graph transduction.\n- Empirical evaluation confirms effectiveness in resolving NLG tasks.",
    "Slide Title: Introduction to Knowledge Base Question Answering\nBullet Points:\n- Large-scale knowledge bases like DBPedia and Freebase support open-domain question answering.\n- Semantic parsing maps questions to formal meaning representations for knowledge base queries.\n- Traditional semantic parsing faces challenges with ontology matching and predicate selection.\n- Proposed framework leverages knowledge base tightly for efficient semantic parsing.\n- Staged design focuses on topic entity, main relationship, and additional constraints.\n\nSlide Title: Background and Knowledge Base Design\nBullet Points:\n- Semantic parser maps natural language questions to logical form queries.\n- Knowledge base consists of subject-predicate-object triples forming a knowledge graph.\n- Freebase used as a large database with millions of topics and facts.\n- Query graph captures logical form in a graph representation.\n- Nodes include grounded entities, variables, and aggregation functions.\n\nSlide Title: Query Graph and Semantic Parsing\nBullet Points:\n- Query graph maps to logical form for executing queries against the knowledge base.\n- Consists of grounded entities, existential variables, and lambda variables.\n- Aggregation functions operate on specific entities for numerical properties.\n- Query graph design inspired by \u03bb-DCS for graph databases.\n- Staged query graph generation focuses on topic entity and core inferential chain.\n\nSlide Title: Staged Query Graph Generation\nBullet Points:\n- Tree graph with topic entity as root and lambda variable as answer node.\n- Core inferential chain describes main relationship between answer and topic entity.\n- Additional constraints attached to variable nodes for answer requirements.\n- Search problem with staged states and actions for query graph generation.\n- Reward function estimates likelihood of correct query graph parsing.\n\nSlide Title: Linking Topic Entity and Core Inferential Chain\nBullet Points:\n- Entity linking system identifies topic entities in questions.\n- Core inferential chain maps natural utterance to correct predicate sequence.\n- Convolutional neural networks measure semantic similarity for inferential chains.\n- CNN model maps word sequences to low-dimensional semantic vectors.\n- Training involves positive pairs from semantic parses or hypothesized chains.\n\nSlide Title: Augmenting Constraints and Aggregations\nBullet Points:\n- Core inferential chain forms simplest query graph for retrieving answers.\n- Additional constraints restrict answer set based on question context.\n- Actions attach entity or aggregation nodes to variable nodes.\n- Constraints derived from neighboring nodes in knowledge base.\n- Simple rules retain legitimate constraints for query graph expansion.\n\nSlide Title: Learning the Reward Function\nBullet Points:\n- Reward function judges correctness of query graph semantic parse.\n- Features match graph portions to question, including entity linking scores.\n- CNN models measure quality of core inferential chain.\n- Constraints and aggregations checked against question words.\n- Model trained as ranker using F1 scores for query graph ranking.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- WEBQUESTIONS dataset used for evaluation with question/answer pairs.\n- System outperforms previous methods with 52.5% F1 score.\n- Entity linking component crucial for high-quality topic entity identification.\n- Core inferential chain performance strong, even without additional constraints.\n- Error analysis reveals label issues and challenges in inferential chains.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- Semantic parsing methods use domain-independent meaning representations.\n- Query graph design closely matches graph knowledge base for expressiveness.\n- Knowledge base helps prune search space in graph generation.\n- CNN models offer sophisticated matching for question and predicate sequences.\n- Future work to extend query graph for complex questions and explore new models.",
    "Slide Title: Introduction to Social Media and Occupational Class\nBullet Points:\n- Online social networks offer insights into user text and personal attributes.\n- Language use varies by user attributes like location, gender, and age.\n- User profiling aids sociolinguistic studies and targeted advertising.\n- Hypothesis: Social media language indicates a user's occupational class.\n- Study focuses on predicting occupational class using Twitter data.\n\nSlide Title: Standard Occupational Classification (SOC)\nBullet Points:\n- SOC is a UK system for classifying occupations hierarchically.\n- Jobs are categorized by skill requirements and content.\n- SOC includes 9 major groups, 25 sub-major groups, and 369 unit groups.\n- SOC is used for mapping Twitter users to occupations.\n- Offers a balanced hierarchy and variety of job titles.\n\nSlide Title: Data Collection and Set Creation\nBullet Points:\n- Created a new dataset of Twitter users mapped to occupations.\n- Used user descriptions to identify self-disclosed job titles.\n- Queried Twitter API for accounts matching occupation keywords.\n- Manual inspection removed ambiguous or irrelevant accounts.\n- Final dataset includes 5,191 users from 55 minor groups.\n\nSlide Title: User Level and Textual Features\nBullet Points:\n- User level features include followers, friends, and tweet statistics.\n- Textual features derived from aggregated user tweets.\n- SVD word embeddings and NPMI clusters used for feature representation.\n- Neural embeddings capture syntactic information in a dense vector space.\n- Clusters interpreted by presenting frequent or representative words.\n\nSlide Title: Classification with Gaussian Processes\nBullet Points:\n- Gaussian Processes (GP) used for Bayesian non-parametric classification.\n- GP defines a prior on functions with a covariance kernel.\n- Logistic function used for binary classification with GPs.\n- Expectation Propagation (EP) approximates non-Gaussian likelihood.\n- One-vs-all classification determines occupational class with highest likelihood.\n\nSlide Title: Predictive Accuracy and Error Analysis\nBullet Points:\n- Compared accuracy of classification methods on held-out data.\n- User level features not useful for predicting job class.\n- Textual features improve performance over baseline.\n- Clusters outperform embeddings in most cases.\n- Error analysis shows adjacent classes often misclassified.\n\nSlide Title: Qualitative Analysis of Topic Usage\nBullet Points:\n- Word clusters provide qualitative insights into occupational classes.\n- Topics like 'Corporate' and 'Higher Education' vary by class.\n- Higher skilled jobs discuss 'Politics' more frequently.\n- 'Arts' topic separates artists from other classes.\n- Lower skilled jobs tweet more about 'Beauty Care' and 'Elongated Words'.\n\nSlide Title: Related Work and Conclusions\nBullet Points:\n- Previous studies linked social status to language use.\n- Our study is the first large-scale analysis of social media language and occupation.\n- Introduced a labeled dataset from Twitter for occupational class prediction.\n- GP framework offers strong predictive performance and feature interpretability.\n- Results highlight occupation influences text use and capture class-based behaviors.",
    "Slide Title: Introduction to the ACL Anthology\nBullet Points:\n- ACL Anthology offers open access to computational linguistics conference proceedings and journal articles.\n- Hosts third-party literature from sister organizations, enhancing community goodwill and collaboration.\n- Provides text and faceted search, author-specific pages, and third-party metadata integration.\n- Over 43,000 papers hosted, serving 4,500 daily requests, with code available on GitHub.\n- Maintained by volunteers, facing challenges of continuous availability and frequent updates.\n\nSlide Title: Current State of the Anthology\nBullet Points:\n- Proposed by Steven Bird in 2001, launched in 2002, with a second version in 2012.\n- Provides access to papers in PDF and metadata in multiple formats like BIBTEX and Endnote.\n- Daily administration by volunteers, with paid assistance for larger problems.\n- Hosted at Saarland University, with future hosting planned under ACL's umbrella.\n- Key task: importing, indexing, and provisioning newly accepted papers efficiently.\n\nSlide Title: Running the Anthology as a Community Project\nBullet Points:\n- Contributors recruited through volunteer calls at main ACL conferences.\n- New features developed by researchers using the Anthology as a resource.\n- Knowledge preservation is crucial due to sporadic volunteer patterns.\n- Anthology editor ensures project continuity, facing heavy workload challenges.\n- Commercial support considered, but volunteer group development strongly advocated.\n\nSlide Title: Future Proofing the Anthology\nBullet Points:\n- All code, documentation, and feature requests hosted on GitHub for transparency.\n- Improved documentation reduces learning curve for new volunteers and eases recruitment.\n- Docker containers used for hosting mirrors, tackling dependency problems, and enhancing security.\n- Freezing software versions maintains stability, with plans for future upgrades.\n- Encourages researchers to use Docker images for reproducibility and experiment distribution.\n\nSlide Title: Challenges for the Anthology\nBullet Points:\n- Maintaining community buy-in by extending Anthology capabilities aligned with research efforts.\n- Anonymous pre-prints proposed to combat bias, with options for collaboration or direct hosting.\n- Reviewer matching using Anthology data to support conference organizers in peer review assignments.\n- Clean textual content extraction from PDFs needed for reviewer matching tool development.\n- Synergistic solutions for pre-prints and reviewer matching enhance Anthology's value.\n\nSlide Title: Conclusion and Call to Action\nBullet Points:\n- ACL Anthology is a key resource for NLP researchers, requiring ongoing software and maintenance work.\n- Improved documentation and server setup ease volunteer training and Anthology uptime.\n- Recruitment of new volunteers remains a challenge, with open invitations for community involvement.\n- Encourages experimentation with Anthology images for automated reviewer assignment and other use cases.\n- Current needs: system administration, software development, database management, and Docker integration.",
    "Slide Title: Introduction to Machine Translation Systems\nBullet Points:\n- Statistical machine translation (SMT) is widely used but requires costly parallel corpora.\n- Rule-based machine translation (RBMT) uses monolingual corpora for target word selection.\n- Word sense disambiguation applied to Japanese-Korean and Korean-Japanese translations.\n- Open Asian parallel corpora available for machine translation research.\n- Statistical post editing (SPE) enhances RBMT in domain adaptation.\n\nSlide Title: Overview of Toshiba System\nBullet Points:\n- RBMT system is transfer-based with morphological and syntactic analysis.\n- High translation performance achieved through extensive rule-based knowledge.\n- Japanese-Korean translation omits syntactic analysis due to grammatical similarity.\n- SPE adapts translation output to target domains using phrase-based SMT.\n- System combination with RNNLM improves translation results.\n\nSlide Title: Statistical Post Editing (SPE)\nBullet Points:\n- SPE uses phrase-based SMT to adapt translations to target domains.\n- Training data translated by RBMT, then modeled with Moses toolkit.\n- Language models trained using KenLM for various language pairs.\n- Distortion limits set based on grammatical similarity of language pairs.\n- SPE improves translation results over RBMT in BLEU and RIBES scores.\n\nSlide Title: System Combination using RNNLM\nBullet Points:\n- SPE and SMT generate different translation candidates due to RBMT features.\n- System combination achieved through n-best reranking with RNNLM.\n- RNNLM score added to basic features for improved translation results.\n- RNNLMs trained with 500 hidden layers and 50 classes.\n- System combination shows better performance than individual systems.\n\nSlide Title: Pre/Postprocessing Techniques\nBullet Points:\n- Technical term dictionaries selected for each task to improve translation.\n- English word correction applied for misspelled words using edit distance.\n- Japanese KATAKANA normalization improves translation accuracy.\n- RBMT applied to SMT results to reduce unknown words.\n- Postprocessing enhances translation quality in various language pairs.\n\nSlide Title: Experimental Results\nBullet Points:\n- SPE improves RBMT translation results in BLEU and RIBES scores.\n- Reranking improves BLEU score by 0.3-0.5 in most tasks.\n- COMB system shows superior performance over other systems.\n- SMT with reranking achieves highest scores in JPOko-ja task.\n- SPE accounts for majority of translations in ja-en, en-ja, and zh-ja.\n\nSlide Title: Official Results and Human Evaluation\nBullet Points:\n- Toshiba systems ranked top three in human evaluation for ja-en, ja-zh, JPOzh-ja.\n- System combination improves human evaluation in Chinese scientific documents.\n- BLEU scores of SPE and SMT nearly equal in ja-zh and zh-ja.\n- Human evaluation does not always correlate with automatic evaluation.\n- Complementary translation improves human evaluations in system combination.\n\nSlide Title: Discussion and Conclusion\nBullet Points:\n- Analysis of differences between Toshiba system and Online A in human evaluation.\n- Crowdsourcing results show discrepancies in number expression evaluations.\n- Toshiba system achieves higher BLEU scores with SPE and RNNLM reranking.\n- System combination improves BLEU score in Japanese-English and Japanese-Chinese pairs.\n- Need for multi-system combination for practical use, leveraging system strengths.",
    "Slide Title: Introduction to Phrase-based Statistical Machine Translation\nBullet Points:\n- Phrase-based Statistical Machine Translation (PB-SMT) is a data-oriented approach used for over 10 years.\n- Moses toolkit, developed by University of Edinburgh, is a key tool in PB-SMT.\n- Training is crucial in PB-SMT, creating core knowledge for machine translation.\n- Word or phrase alignment affects translation quality and is time-consuming.\n- IBM models and HMM alignment models are widely used for word alignment.\n\nSlide Title: Statistical and Associative Alignment Approaches\nBullet Points:\n- GIZA++ implements IBM Models, aligning words based on statistical models.\n- MGIZA is a parallel implementation of IBM models, with slight alignment variations.\n- Associative approaches use statistical measures like Dice coefficient and mutual information.\n- Anymalign and Cutnalign are examples of associative alignment methods.\n- Anymalign samples sub-corpora to determine word or phrase associations.\n\nSlide Title: Anymalign and Cutnalign Methods\nBullet Points:\n- Anymalign is an open-source multilingual associative aligner using sampling.\n- It uses occurrence distributions to determine word or phrase associations.\n- Cutnalign is a bilingual hierarchical sub-sentential alignment method.\n- It uses recursive binary segmentation of alignment matrices.\n- Anymalign and Cutnalign combined reduce training time significantly.\n\nSlide Title: Chinese and Japanese Data Utilization\nBullet Points:\n- Chinese-Japanese JPO Patent Corpus used for training in Chemistry, Electricity, and more.\n- Sentences with 40 words or less used for translation model training.\n- Stanford Segmenter and Juman used for Chinese and Japanese segmentation.\n- Development data used for tuning translation models.\n- Data statistics include tokenization, lowercase, and cleaning processes.\n\nSlide Title: Bilingual Hierarchical Sub-sentential Alignment\nBullet Points:\n- Cutnalign uses recursive binary segmentation for alignment matrix processing.\n- Measures translation link strength between source and target word pairs.\n- Computes optimal joint clustering of bipartite graphs for best alignment.\n- Segments and aligns sentence pairs using translation probabilities.\n- Recursive segmentation minimizes N cut for optimal alignment.\n\nSlide Title: Improvements in Alignment Methods\nBullet Points:\n- Multi-processing introduced to accelerate alignment process.\n- Core of Cutnalign re-implemented in C for speed improvements.\n- Approximations in N cut computation accelerate decision-making.\n- Search space reduction in hierarchical alignment for speed-ups.\n- Detailed improvements described in Yang and Lepage (2015).\n\nSlide Title: Experiment Settings and Evaluation\nBullet Points:\n- Experiments conducted with GIZA++, MGIZA, and Moses toolkit.\n- Phrase tables extracted using grow-diag-final-and heuristic.\n- Evaluations performed using BLEU and RIBES metrics.\n- Anymalign and Cutnalign used with different timeouts and implementations.\n- Fastest training time achieved with Moses 2.1.1 and C version of Cutnalign.\n\nSlide Title: Results and Conclusion\nBullet Points:\n- No significant differences in translation quality with different alignment methods.\n- Fastest training time was 57 minutes, one fifth of GIZA++/MGIZA time.\n- Translation quality remained steady despite reduced word association time.\n- Significant reduction in processing training time achieved.\n- Accelerated SMT system development following Lardilleux et al. and Yang and Lepage.",
    "Slide Title: Introduction to Neural Machine Translation (NMT)\nBullet Points:\n- NMT emerged as a successful paradigm, improving fluency over statistical systems.\n- Adequacy remains a concern with common issues like dropping and repeating words.\n- Previous solutions include coverage penalties and architectural changes.\n- Proposed solution modifies attention transformation without changing overall architecture.\n- Introduces constrained sparsemax for sparse and bounded attention weights.\n\nSlide Title: Preliminaries of NMT Model Architecture\nBullet Points:\n- Uses a standard attentional encoder-decoder model for translation tasks.\n- Bi-LSTM encoder represents source words as a matrix for processing.\n- Conditional probability of target sentence computed via softmax output layer.\n- Decoder state updated by auto-regressive LSTM with input context vector.\n- Attention over source words obtained through softmax transformation.\n\nSlide Title: Sparse and Constrained Attention Transformations\nBullet Points:\n- Softmax forces all source words to receive probability mass, causing inefficiencies.\n- Sparsemax transformation yields sparse probability distribution for attention.\n- Constrained softmax bounds attention probabilities using fertility values.\n- Constrained sparsemax combines sparse and bounded probabilities for efficiency.\n- Forward and backward propagation algorithms derived for constrained sparsemax.\n\nSlide Title: Fertility Bounds and Sink Token\nBullet Points:\n- Fertility bounds set using CONSTANT, GUIDED, and PREDICTED strategies.\n- Sink token added to source sentence to ensure target length flexibility.\n- Encourages more attention to words with larger credit to avoid missing words.\n- Pre-attention word scores adjusted to prioritize unexhausted fertility.\n- Strategies ensure feasibility and flexibility in translation length.\n\nSlide Title: Experiments and Evaluation Metrics\nBullet Points:\n- Evaluated attention transformations on small datasets for DE-EN, JA-EN, RO-EN.\n- Small datasets highlight adequacy issues, ideal for testing methods.\n- Tokenized BLEU, METEOR, REP-score, and DROP-score used as evaluation metrics.\n- Sparse models generally achieve higher BLEU and METEOR scores.\n- Sparse attention models alleviate coverage problems effectively.\n\nSlide Title: Results and Comparisons\nBullet Points:\n- Sparse models outperform softmax and csoftmax in BLEU and METEOR scores.\n- REP and DROP scores indicate improved coverage with sparse attention.\n- PREDICTED fertility strategy shows slight performance advantage.\n- Examples show csparsemax fixing repetitions with sparse alignments.\n- Sparse attention avoids repeated words, improving translation quality.\n\nSlide Title: Conclusions and Contributions\nBullet Points:\n- Proposed sparse and constrained attention transformations for NMT coverage issues.\n- Efficient algorithms for constrained sparsemax enable practical application.\n- Fertility prediction model leads to sparse alignments, reducing word repetition.\n- Sparse attention models improve translation adequacy and quality.\n- Future work may explore further optimizations and applications in NMT.",
    "Slide Title: Introduction to Cross-Lingual Transfer Learning\nBullet Points:\n- Supervised learning dominates NLP, requiring large annotated corpora for effective task performance.\n- Over 6,000 languages exist, many with limited or no annotated text resources.\n- Cross-lingual transfer learning transfers knowledge from high-resource to low-resource languages.\n- Traditional methods use annotation projection or transferable representations like phonetic transcriptions.\n- Most methods rely on a single source language, limiting transferable knowledge.\n\nSlide Title: Novel Zero-Shot Multilingual Transfer Method\nBullet Points:\n- Proposed method inspired by truth inference in crowd-sourcing to infer best labelling.\n- Adapts ideas to learn quality of transfer and language-specific errors.\n- Uses Bayesian graphical model to infer reliable models without explicit supervision.\n- Allows accurate inferences despite substantial noise in data.\n- Considers supervised setting with tiny annotated corpus for improved accuracy.\n\nSlide Title: Approach to Multilingual Transfer\nBullet Points:\n- Problem framed as using H models trained in high-resource settings for target language.\n- Simple approaches include single model selection or ensemble voting.\n- Proposed BEA uns method for zero-shot setting inspired by truth inference.\n- RaRe method for few-shot setting using limited annotations for model selection.\n- Focus on black-box setting without access to source models' data or implementation.\n\nSlide Title: Zero-Shot Transfer with BEA Model\nBullet Points:\n- BEA model selects subset of component models or learns non-uniform weighting.\n- Probabilistic graphical model developed for unsupervised inference.\n- Generative model assumes 'true' label corrupted by transfer models.\n- BEA model differentiates reliable from less reliable transfer models.\n- Mean-field variational Bayes used for inference and optimization.\n\nSlide Title: Few-Shot Transfer Techniques\nBullet Points:\n- BEA sup uses labelled data to find posterior for Bayesian model parameters.\n- RaRe ranks systems and retrains model on target language with limited annotations.\n- RaRe exploits regularities like common words or character patterns.\n- Fine-tuning corrects for phenomena not captured by model transfer.\n- Designed for low-resource settings with limited labelled data.\n\nSlide Title: Experiments and Data Evaluation\nBullet Points:\n- Evaluation over Wikiann NER corpus with 41 languages and CoNLL datasets.\n- Use of fastText 300 dimensional Wikipedia embeddings for language-independent features.\n- BiLSTM-CRF model used for sequential tagging in high and low-resource settings.\n- Comparison of supervised and unsupervised models with proposed transfer methods.\n- Results show multilingual transfer outperforms single source and majority voting.\n\nSlide Title: Multilingual Transfer Results\nBullet Points:\n- High supervision achieves best performance, limited supervision considerably lower.\n- Unsupervised zero-shot learning improves upon majority voting.\n- BEA ent uns outperforms BEA tok uns, showing effectiveness over entities.\n- RaRe outperforms BEA ent sup for isolated languages, exploiting character features.\n- Choice of source languages affects performance, with more inputs generally beneficial.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Cross-lingual transfer challenging with large source languages and distant targets.\n- Proposed RaRe and BEA models improve performance in multilingual transfer.\n- BEA uns provides fast, simple annotation in target language under noisy conditions.\n- Light supervision further enhances performance, with RaRe offering consistent improvements.\n- Future work could explore combining multiple source models for enhanced transfer.",
    "Slide Title: Introduction to Neural Machine Translation (NMT)\nBullet Points:\n- Evaluation of NMT models in English-Japanese translation task with various architectures.\n- Simple NMT models outperform all Statistical Machine Translation (SMT) baselines.\n- Unknown word recovery is crucial for improving translation quality.\n- Pre-reordering training data surprisingly degrades model performance.\n- Qualitative analysis reveals specific error patterns in NMT translations.\n\nSlide Title: Evolution of Machine Translation\nBullet Points:\n- Statistical Machine Translation (SMT) showed promising results but suffered from underfitting.\n- Recurrent Neural Networks (RNNs) improved prediction power in Natural Language Processing tasks.\n- Neural Machine Translation (NMT) samples translation results directly from RNNs.\n- Encoder-Decoder approach is a common architecture in NMT models.\n- Stacked Long Short-Term Memory (LSTM) units enhance NMT performance.\n\nSlide Title: Recurrent Neural Networks (RNNs)\nBullet Points:\n- RNNs model temporal data by updating internal state based on new input and previous state.\n- Backpropagation Through Time (BPTT) trains RNNs by unfolding the network through time.\n- RNNs struggle with capturing long-term dependencies due to error dilution.\n- Long Short-Term Memory (LSTM) units incorporate gates to control information flow.\n- LSTMs preserve information for long periods, overcoming RNN limitations.\n\nSlide Title: Gated Recurrent Units (GRUs)\nBullet Points:\n- GRUs proposed as a simpler alternative to LSTMs with fewer parameters.\n- GRUs control information flow with gating units, reducing computational complexity.\n- GRUs maintain one kind of internal state, unlike LSTMs with separate memory cells.\n- GRUs involve six weight parameters compared to LSTM's eleven.\n- GRUs offer a balance between performance and computational efficiency.\n\nSlide Title: Network Architectures in NMT\nBullet Points:\n- Encoder-Decoder approach encodes input sequence into vector representation.\n- Softmax function applied to output layer computes cross-entropy.\n- Trainable word embeddings replace one-hot embeddings for vocabulary tokens.\n- Soft-attention models address performance degradation in long input sentences.\n- Soft-attention mechanism allows focusing on specific input portions.\n\nSlide Title: Solutions for Unknown Words in NMT\nBullet Points:\n- Fixed vocabulary size in output layer poses challenges for NMT.\n- Model-specific approaches include noise-contrastive training and class-based models.\n- Translation-specific approaches use token alignment to recover unknown words.\n- Annotating unknown words with aligned source positions aids recovery.\n- Back-off systems replace unknown words using baseline SMT results.\n\nSlide Title: Experiment Setup and Evaluation\nBullet Points:\n- Experiments conducted on English-Japanese translation using ASPEC-JE corpus.\n- Training data filtered to 1.1M sentences with a maximum of 40 words.\n- Four NMT models evaluated with 1024 neurons in recurrent layers.\n- Perplexity used as a primary evaluation metric for model performance.\n- Soft-attention models with LSTM units outperform other architectures.\n\nSlide Title: Translation Performance and Insights\nBullet Points:\n- Soft-attention NMT models surpass SMT baselines in English-Japanese translation.\n- Pre-reordering training data degrades translation performance.\n- Unknown word recovery with back-off system improves BLEU scores.\n- Human evaluation reveals NMT models omit information, losing partial meaning.\n- Further improvements possible by addressing observed error patterns.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Systematic evaluation shows NMT models excel in English-Japanese translation.\n- Soft-attention models achieve substantial performance gains over SMT baselines.\n- NMT models perform well without pre-reordering data processing steps.\n- Unknown word recovery remains an open question in NMT.\n- Observed error patterns suggest potential for further model improvements.",
    "Slide Title: Introduction to Event Detection (ED)\nBullet Points:\n- Event Detection (ED) extracts event triggers and identifies event types from sentences.\n- Challenges arise as the same trigger can represent different events in varied contexts.\n- ED methods are categorized into feature-based and representation-based approaches.\n- Document-level information is crucial for ED, enhancing sentence-level context.\n- Existing methods face limitations like manual feature design and incomplete rule sets.\n\nSlide Title: Proposed DEEB-RNN Model\nBullet Points:\n- DEEB-RNN is a novel model for sentence-level Event Detection (ED).\n- It uses hierarchical, supervised attention-based Bi-RNN for document embeddings.\n- Word-level attention focuses on event triggers; sentence-level on event-containing sentences.\n- Document embeddings aid another Bi-RNN in identifying event triggers and types.\n- The model integrates attention and event type identification losses for learning.\n\nSlide Title: ED Oriented Document Embedding Learning (EDODEL)\nBullet Points:\n- EDODEL learns document embeddings using word and sentence-level Bi-GRU encoders.\n- Word-level embeddings combine word and entity type embeddings for Bi-GRU input.\n- Attention weights highlight trigger words, guiding sentence embedding creation.\n- Sentence-level embeddings use Bi-GRU to focus on event-containing sentences.\n- Document embedding is a weighted sum of sentence embeddings, emphasizing event sentences.\n\nSlide Title: Document-level Enhanced Event Detector (DEED)\nBullet Points:\n- DEED uses Bi-GRU and softmax to model Event Detection (ED) tasks.\n- Combines word, entity type, and document embeddings for Bi-GRU input.\n- Outputs probability vector for classifying words into event types.\n- Cross-entropy error measures loss between real and predicted event types.\n- Joint training integrates DEED and EDODEL modules for optimal performance.\n\nSlide Title: Joint Training of DEEB-RNN Model\nBullet Points:\n- DEEB-RNN jointly trains EDODEL and DEED modules for comprehensive learning.\n- Joint loss function balances attention and event type identification losses.\n- Hyper-parameters adjust the influence of word and sentence-level attention.\n- Training uses Stochastic Gradient Descent and dropout for regularization.\n- Pre-trained word embeddings initialize the model for improved performance.\n\nSlide Title: Experimental Setup and Baseline Models\nBullet Points:\n- Experiments conducted on ACE-2005 dataset with specific training, validation, and test sets.\n- Baseline models include feature-based and representation-based approaches.\n- DEEB-RNN compared against models like Sentence-level, Joint Local, and JRNN.\n- Performance evaluated using recall, precision, and F1-measure metrics.\n- DEEB-RNN consistently outperforms state-of-the-art methods in recall and F1-measure.\n\nSlide Title: Impacts of Different Attention Strategies\nBullet Points:\n- DEEB-RNN variants tested with different attention strategies on ACE-2005 dataset.\n- Document embeddings significantly enhance performance over basic Bi-GRU model.\n- DEEB-RNN2 improves precision by focusing on sentences with explicit event triggers.\n- DEEB-RNN3 achieves best performance using gold attention signals at both levels.\n- Document-level information proves beneficial for Event Detection (ED).\n\nSlide Title: Performance Comparison and Conclusions\nBullet Points:\n- DEEB-RNN outperforms existing methods in recall and F1-measure on ACE-2005.\n- Exploits document-level information using supervised attention for better trigger identification.\n- Automatically captures event types without manual rule design.\n- Hierarchical attention-based Bi-RNN enhances document embeddings for ED.\n- Future work aims to improve automatic weight determination for embeddings.",
    "Slide Title: Introduction to Semantic Role Labeling (SRL)\nBullet Points:\n- SRL captures predicate-argument relations like \"who did what to whom.\"\n- Recent SRL models use BIO-taggers for labeling argument spans per predicate.\n- Our end-to-end approach predicts predicates and argument spans in one pass.\n- Model uses learned, contextualized span representations for SRL graph prediction.\n- Overcomes limitations of semi-markov and BIO-based models by modeling overlapping spans.\n\nSlide Title: Model Overview\nBullet Points:\n- Considers all tokens as possible predicates and continuous spans as arguments.\n- Predicts relations between predicate-argument pairs, including no relation.\n- Uses random variables for each predicate-argument pair to determine relations.\n- Scoring function decomposed into unary scores and label-specific scores.\n- Final SRL output includes all non-empty predicate-argument relations.\n\nSlide Title: Learning and Beam Pruning\nBullet Points:\n- Minimizes negative log likelihood of the gold structure for learning.\n- Beam pruning addresses computational impracticality of evaluating all relations.\n- Defines beams for candidate arguments and predicates, ranked by unary scores.\n- Limits beam sizes and span widths to reduce relational factors.\n- Efficiently prunes unlikely argument spans and predicates.\n\nSlide Title: Neural Architecture\nBullet Points:\n- Builds contextualized representations using BiLSTM outputs and feedforward networks.\n- Word-level contexts use pre-trained embeddings and character-based representations.\n- Argument representation includes BiLSTM outputs, soft head word, and span width features.\n- Predicate representation is the BiLSTM output at the predicate's position.\n- Scoring functions implemented with feed-forward networks based on representations.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Experiments conducted on CoNLL 2005 and CoNLL 2012 benchmarks.\n- Evaluated on micro-averaged F1 for predicting predicate-argument-label tuples.\n- Joint model outperforms previous pipeline systems by significant F1 differences.\n- ELMo embeddings added to improve performance, orthogonal to main contribution.\n- Model predicts over 40% of sentences completely correctly across datasets.\n\nSlide Title: Analysis of Model Performance\nBullet Points:\n- Model efficiently prunes unlikely spans, reducing edges to consider.\n- Retains high recall of argument-bearing predicates with joint beam pruning.\n- Better at predicting long-distance arguments compared to ensemble models.\n- Achieves comparable syntactic agreement to previous syntax-based systems.\n- Faces challenges with global consistency due to strong independence assumption.\n\nSlide Title: Effectiveness of Beam Pruning\nBullet Points:\n- Beam pruning allows soft trade-off between efficiency and recall.\n- Model excels at predicting arguments farther from predicates.\n- Direct connections between predicates and arguments enhance performance.\n- Outperforms BIO taggers in handling long-distance dependencies.\n- Maintains high recall of gold argument-bearing predicates.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Proposed SRL model jointly predicts predicates and argument spans.\n- Supports joint predicate identification and incorporates span-level features.\n- Excels at long-range dependencies and syntactic boundary agreement.\n- Weaker at global consistency due to independence assumption.\n- Future work could explore higher-order inference and self-attention layers.",
    "Slide Title: Introduction to Presuppositions in Pragmatics\nBullet Points:\n- Presuppositions are assumptions shared by discourse participants, crucial for smooth communication.\n- Presupposition triggers include definite descriptions, factive verbs, and certain adverbs.\n- Adverbial triggers like \"again\" indicate event recurrence or continuation in discourse.\n- Presuppositions remain unaffected by negation, unlike entailment and implicature.\n- Study focuses on adverbial triggers: again, also, still, too, and yet.\n\nSlide Title: Importance of Adverbial Presupposition Triggers\nBullet Points:\n- Adverbial triggers are common in English journalistic texts, second to existential triggers.\n- Understanding triggers aids in language technology systems for better communication.\n- Context detection for triggers is complex, often spanning multiple sentences.\n- Appropriate use in language generation improves readability and coherence.\n- Datasets created from Penn Treebank and English Gigaword corpus for analysis.\n\nSlide Title: Research Objectives and Contributions\nBullet Points:\n- Introduce task of predicting adverbial presupposition triggers in discourse.\n- Develop datasets for detecting adverbial triggers, applicable to similar tasks.\n- Propose a new attention mechanism in RNN for better prediction performance.\n- Achieve 82.42% accuracy on predicting \"also\" in the Gigaword dataset.\n- Contributions include novel datasets and improved attention-based models.\n\nSlide Title: Related Work and Theoretical Background\nBullet Points:\n- Presupposition theory traces back to Frege and Strawson's philosophy of language.\n- No prior computational work directly on adverbial presupposition triggers.\n- Attention-based models show promise in pragmatic reasoning and prediction tasks.\n- Various linguistic studies explore adverbs like \"too\" and \"again\" as triggers.\n- Task setup resembles Cloze test, testing text comprehension via fill-in-the-blanks.\n\nSlide Title: Datasets and Data Extraction Process\nBullet Points:\n- Datasets extracted from Penn Treebank and English Gigaword corpus.\n- Focus on five adverbs: too, again, also, still, and yet for binary classification.\n- Positive samples contain adverbs triggering presupposition, negative samples do not.\n- Control for confounding factors like class balance and syntactic governor.\n- Randomization approaches used to control position-related confounding factors.\n\nSlide Title: Learning Model and Attention Mechanism\nBullet Points:\n- Model extends bidirectional LSTM with attention mechanism for presupposition detection.\n- Weighted-pooling neural network architecture computes correlations between hidden states.\n- Self-attention mechanism allows for capturing long-range dependencies.\n- Attention scores used to combine hidden states for presupposition prediction.\n- Model minimizes computational impact by not introducing new parameters.\n\nSlide Title: Experiments and Baseline Comparisons\nBullet Points:\n- WP model compared against most-frequent-class, logistic regression, LSTM, and CNN baselines.\n- Experiments conducted on both Penn Treebank and Gigaword datasets.\n- Impact of POS tags and attention mechanism on prediction accuracy investigated.\n- WP model outperforms baselines, showing promise in classification tasks.\n- Confusion matrix analysis shows balanced misclassification in presence/absence of triggers.\n\nSlide Title: Analysis and Conclusion\nBullet Points:\n- Examples illustrate complexity of detecting presuppositions in discourse.\n- Long-range dependencies motivate use of deep non-linear models and attention mechanisms.\n- WP model outperforms CNN and LSTM without additional parameters.\n- Future work to focus on data imbalance and applications in language generation.\n- Explore additional attention-based architectures for improved presupposition prediction.",
    "Slide Title: Introduction to Extractive Summarization\nBullet Points:\n- Focus on extractive summarization by extracting salient word sequences from source documents.\n- Supervised approaches often use human abstracts for annotations, facing challenges with accuracy.\n- Neural abstractive summarization allows copying or generating new words, but faces fidelity issues.\n- Study explores neural extractive summarization using Cloze-style comprehension questions.\n- Reinforcement learning is used to produce fluent, generic document summaries.\n\nSlide Title: Challenges in Extractive Summarization\nBullet Points:\n- Human abstracts are concise, leading to many source words being tagged as 0.\n- Mislabeling occurs when source words are paraphrases or generalizations of abstract words.\n- Ensuring summaries remain faithful to the original is a challenge for neural summarization.\n- System summaries should preserve salient content relevant to Cloze-style questions.\n- Attention mechanism locates summary segments relevant to given questions.\n\nSlide Title: Related Work in Summarization\nBullet Points:\n- Study focuses on generic summarization, differing from query-based summarization.\n- Cloze questions test system's reasoning and language understanding capabilities.\n- Approach draws on extracting (context, question, answer) triples from news articles.\n- Reinforcement learning applied to NLP tasks like dialog generation and machine translation.\n- Summaries encouraged to preserve content useful for answering questions.\n\nSlide Title: Our Approach to Extractive Summarization\nBullet Points:\n- System generates summaries by identifying consecutive word sequences from source documents.\n- Question-oriented reward encourages summaries to answer key questions about the document.\n- Bi-LSTM encodes questions and summaries to vectors for attention mechanism.\n- QA reward defined as log-likelihood of correctly predicting all answers.\n- System optimized to generate summaries preserving salient source content.\n\nSlide Title: Question-Focused Reward and QA Pairs\nBullet Points:\n- Summaries rewarded if they can answer important questions as document surrogates.\n- Questions converted from human abstracts using entities or keywords as answer tokens.\n- Extracted QA pairs ensure summaries cover comprehensive content.\n- System extracts at least one QA pair from each sentence in an abstract.\n- Adequacy of summary influences correctness of answering questions.\n\nSlide Title: Reinforcement Learning for Summarization\nBullet Points:\n- Policy optimized to generate extractive summaries maximizing expected reward.\n- Bi-LSTM encodes source document, LSTM tracks sampling decisions for fluency.\n- Pretraining with bigram-based method stabilizes training of full model.\n- Policy learned without reward function at test time for generic summaries.\n- Summaries generated by selecting words with highest probability.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Experiments conducted using CNN dataset with news articles and human-written highlights.\n- Hyperparameters tuned on validation set, including Bi-LSTM and LSTM encoder sizes.\n- Results show superior performance of EntityQ and KeywordQ methods over baselines.\n- R-1, R-2, and R-L scores indicate effectiveness of proposed approach.\n- Summaries stay true to original documents, aiding in browsing and comprehension.\n\nSlide Title: Future Work and Conclusion\nBullet Points:\n- Future work includes grouping summary segments into clusters for unique document aspects.\n- Usability study to test summary highlights' effectiveness in answering key questions.\n- Proposed method provides alternative strategy against extractive and abstractive baselines.\n- New training paradigm for extractive summarization using question-answer pairs.\n- Approach surpasses state-of-the-art systems, promoting concise and fluent summaries.",
    "Slide Title: Introduction to Neural Network Mappings\nBullet Points:\n- Neural network mappings bridge modalities in cross-modal retrieval and zero-shot learning.\n- Systems rely on mapping predicted vectors to target vectors' semantic structure.\n- Recent studies show input modality information propagates into predicted modality.\n- Introduced a measure to quantify similarity between neighborhood structures of vectors.\n- Experiments reveal mapped vectors resemble input vectors more than target vectors.\n\nSlide Title: Related Work and Motivation\nBullet Points:\n- Neural networks and linear mappings bridge modalities in cross-modal retrieval systems.\n- Text-to-image and image-to-text mappings used in various applications.\n- Cross-space neural mappings face challenges like \"hubness\" and \"pollution.\"\n- Mapped vectors often resemble input vectors more than target vectors.\n- Continuity in neural networks preserves topological properties of input.\n\nSlide Title: Proposed Approach for Cross-Modal Mapping\nBullet Points:\n- Two popular mappings: linear mapping and feed-forward neural network.\n- Linear mapping uses weights and biases to transform input vectors.\n- Feed-forward network includes non-linearity and hidden layers for mapping.\n- Loss functions include MSE, cosine, and max-margin for mapping evaluation.\n- Focus on cross-space mapping problem independently of class labels.\n\nSlide Title: Neighborhood Structure of Mapped Vectors (Experiment 1)\nBullet Points:\n- Mean nearest neighbor overlap (mNNO) measures similarity between vector sets.\n- mNNO quantifies shared nearest neighbors between paired vectors.\n- mNNO scales between 0 and 1, indicating semantic similarity.\n- Experiment reveals mapped vectors resemble input vectors more than target vectors.\n- Results consistent across different datasets and feature types.\n\nSlide Title: Mapping with Untrained Networks (Experiment 2)\nBullet Points:\n- Untrained networks used to study semantic structure preservation.\n- Concept similarity tasks assess semantic closeness of representations.\n- Random parameter mapping explores disruption or preservation of input structure.\n- Untrained mappings preserve semantic structure of input vectors.\n- Complements findings of Experiment 1 on semantic preservation.\n\nSlide Title: Experimental Setup and Datasets\nBullet Points:\n- Diverse cross-modal tasks at word, sentence, and document levels.\n- ImageNet, IAPR TC-12, and Wiki datasets used for experiments.\n- Text features include GloVe and word2vec embeddings.\n- Image features from VGG-128 and ResNet models.\n- Six benchmarks for concept similarity and relatedness tasks.\n\nSlide Title: Performance Metrics and Results\nBullet Points:\n- Spearman correlation evaluates similarity predictions against human ratings.\n- Statistical significance tested with Wilcoxon rank sum test.\n- Experiment 1 shows semantic structure of mapped vectors resembles input.\n- Experiment 2 confirms untrained mappings preserve input semantic structure.\n- Results highlight natural tendency of neural nets to preserve input semantics.\n\nSlide Title: Conclusions and Implications\nBullet Points:\n- Neural net mappings often produce vectors akin to input rather than target.\n- Proposed measure quantifies semantic similarity between vector sets.\n- Mean squared error may mislead; semantic structure is crucial in applications.\n- Findings encourage development of better architectures for cross-modal tasks.\n- Advocates for semantic-based criteria in evaluating predicted vectors.",
    "Slide Title: Introduction to Complaints in Natural Language\nBullet Points:\n- Complaining is a speech act expressing a negative mismatch between reality and expectations.\n- Understanding complaints is crucial for linguists, psychologists, and organizations.\n- Complaints differ from negative sentiment; they focus on breached expectations.\n- This study presents the first computational analysis of complaints in linguistics.\n- Contributions include a dataset, linguistic analysis, and predictive models for complaints.\n\nSlide Title: Related Work on Complaints\nBullet Points:\n- Complaints have been studied in linguistics and marketing research.\n- Defined as a breach of expectations, complaints are distinct speech acts.\n- Complaints can threaten both positive and negative face of the responsible party.\n- Previous studies examined complaints across cultures and socio-demographic traits.\n- Complaints often co-occur with other speech acts like warnings and suggestions.\n\nSlide Title: Data Collection and Annotation\nBullet Points:\n- No existing dataset for complaints; new dataset created from Twitter.\n- Twitter chosen for high self-expression and interaction with brands.\n- 1,971 tweets collected and annotated for complaints by experts.\n- Tweets categorized into domains for analysis and classifier transferability.\n- Annotation achieved substantial agreement with Cohen's Kappa of 0.731.\n\nSlide Title: Linguistic Features of Complaints\nBullet Points:\n- Features include unigrams, LIWC, word clusters, and part-of-speech tags.\n- Complaints use negations, possessive pronouns, and temporal references.\n- Distinctive patterns include possessive pronouns followed by nouns.\n- Complaints often formulated as questions, lacking exclamation marks.\n- Temporal references provide context for breached expectations.\n\nSlide Title: Predictive Models for Complaints\nBullet Points:\n- Feature-based approaches include Logistic Regression with Elastic Net.\n- Neural methods tested: Multi-layer Perceptron and Long-Short Term Memory.\n- Best performance with bag-of-word features, reaching 77.5 F1 score.\n- Combining all features boosts accuracy to 78 F1 and 0.864 AUC.\n- Neural networks comparable but not outperforming feature-based models.\n\nSlide Title: Enhancing Predictive Performance\nBullet Points:\n- Distant supervision used to identify extra complaint data.\n- Hashtags related to complaints collected for additional data.\n- Domain adaptation with EasyAdapt boosts F1 score to 79.\n- Pooling data without adaptation decreases predictive performance.\n- Domain adaptation beneficial for most domains, except apparel.\n\nSlide Title: Domain and Cross-Domain Experiments\nBullet Points:\n- Models trained on in-domain, out-of-domain, and combined data.\n- Domain adaptation improves performance in most domains.\n- Apparel domain shows different complaint features, affecting prediction.\n- Cross-domain experiments show consistent performance across domains.\n- 'Food & Beverage' domain shows lower performance; 'Other' higher.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- First computational approach to modeling complaints using linguistics and machine learning.\n- Dataset and code available for further research on complaints.\n- Predictive models useful for companies to analyze complaints efficiently.\n- Future work includes identifying complaint targets and improving performance.\n- Study role of complaints in personal and political conversations.",
    "Slide Title: Introduction to Word Embeddings\nBullet Points:\n- Word embeddings are crucial in natural language processing applications for representing words as vectors.\n- Analyzing embedding spaces helps understand their utility in downstream NLP tasks.\n- Neighborhood-based analysis is limited by unreliable nearest neighborhoods.\n- Geometric analysis offers insights into word embeddings' distribution and performance.\n- This work bridges semantic analysis and geometric performance analysis of embeddings.\n\nSlide Title: Related Work on Word Embeddings\nBullet Points:\n- Evaluating word embedding models' quality is challenging due to confounding factors.\n- Intrinsic evaluations like word similarity are intuitive but limited in predicting performance.\n- Nearest neighbors are used for semantic information but have stability issues.\n- Graph structures and visualizations offer alternative qualitative analysis methods.\n- This study provides a quantitative analysis of embedding spaces.\n\nSlide Title: Geometric Properties of Word Embeddings\nBullet Points:\n- Investigate geometric properties: position, feature distribution, global and local distances.\n- Transform embeddings to expose subsets of these attributes to models.\n- Use affine transformations, cosine distance encoding, and nearest neighbor encoding.\n- Affine transformations include translations, reflections, rotations, and homotheties.\n- Cosine distance encoding obfuscates feature distribution using pairwise distance matrices.\n\nSlide Title: Affine Transformations in Word Embeddings\nBullet Points:\n- Affine transformations change positions of points relative to the origin.\n- Include translations, reflections, rotations, and homotheties.\n- Translations shift vectors, reflections mirror over hyperplanes.\n- Rotations change orientation, homotheties scale vectors.\n- These transformations help analyze embedding spaces' geometric properties.\n\nSlide Title: Cosine Distance Encoding (CDE)\nBullet Points:\n- CDE obfuscates feature distribution by representing vectors as distance matrices.\n- Uses cosine distance to measure similarity between vectors.\n- Subset of distances to 10K most frequent words used for practicality.\n- Autoencoder maps distance matrix back to original dimensionality.\n- CDE transformation adds semantic noise, affecting performance.\n\nSlide Title: Nearest Neighbor Encoding (NNE)\nBullet Points:\n- NNE retains information about nearest neighborhoods, discarding global distances.\n- Sparse vector output forms a directed nearest neighbor graph.\n- Thresholded, weighted, and unweighted versions of NNE used.\n- Node2vec captures patterns in local graph structure for evaluation.\n- NNE transformation recovers performance lost in CDE transformation.\n\nSlide Title: Evaluation of Transformed Embeddings\nBullet Points:\n- Evaluate embeddings on intrinsic and extrinsic tasks to measure geometric contributions.\n- Intrinsic tasks include word similarity and relatedness using cosine similarity.\n- Extrinsic tasks involve relation classification, sentiment analysis, and inference.\n- CDE transformation causes largest performance drops in intrinsic tasks.\n- NNE transformation recovers performance in extrinsic tasks.\n\nSlide Title: Analysis and Discussion\nBullet Points:\n- Rotations, dilations, and reflections have minimal impact on task performance.\n- Translations affect intrinsic tasks due to reliance on cosine similarity.\n- Homothety leads to performance drops by altering vector distribution.\n- CDE transformation adds semantic noise, affecting nearest neighborhoods.\n- Focus on local geometric structure improves embedding tuning for tasks.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Quantitative analysis of geometric attributes reveals impact on task performance.\n- Intrinsic evaluations sensitive to absolute position, extrinsic tasks rely on local similarity.\n- Future work should focus on local geometric structure in embedding tuning.\n- Dynamic embedding methods like ELMo and BERT benefit from local structure focus.\n- Source code and results available online for further exploration.",
    "Slide Title: Introduction to Social Media and Influenza Detection\nBullet Points:\n- Social media platforms widely share personal information, especially on Twitter.\n- Twitter is a valuable resource for detecting events like influenza epidemics.\n- Over 30 Twitter-based influenza detection systems have been developed globally.\n- Systems assume real-world flu spread is reflected in flu-related tweets.\n- Noise in tweets, like suspicion or negation, affects precise influenza surveillance.\n\nSlide Title: Forecasting Words and Time Lag\nBullet Points:\n- Words like \"fever\" and \"headache\" are clues to upcoming influenza outbreaks.\n- A 16-day time lag exists between \"fever\" tweets and patient numbers.\n- Flu prevention words like \"shot\" show a 55-day time lag.\n- Estimating optimal time lag for each word is crucial for accurate predictions.\n- Time lags are incorporated into models for better influenza forecasting.\n\nSlide Title: Nowcasting and Forecasting Models\nBullet Points:\n- Nowcasting models estimate current patient numbers using time-shifted word frequencies.\n- Forecasting models predict future patient numbers based on time lags.\n- Lasso and Elastic Net regularization methods are used for parameter estimation.\n- Nowcasting models improve estimation accuracy with a correlation ratio of 0.93.\n- Forecasting models achieve successful predictions with a correlation ratio of 0.91.\n\nSlide Title: Dataset and Data Processing\nBullet Points:\n- 7.7 million influenza-related tweets collected from August 2012 to January 2016.\n- Noise filtered by removing retweets and tweets with links.\n- Japanese morphological parser used to extract stem forms of words.\n- Word frequency per day analyzed to build a word matrix.\n- IDSC reports used for evaluation, divided into three influenza seasons.\n\nSlide Title: Methodology for Time Shift Estimation\nBullet Points:\n- Cross Correlation used to find optimal time shift width for each word.\n- Time-shifted word matrix constructed to align with IDSC reports.\n- Algorithm decides optimal time shift width based on cross correlation.\n- Time shift operation arranges words to match influenza timelines.\n- Nowcasting model constructed by minimizing squared error with penalties.\n\nSlide Title: Experiment 1: Nowcasting Performance\nBullet Points:\n- Four linear methods compared: Lasso, Lasso+, ENet, ENet+.\n- Time-shifted models showed 0.1 point improvement over baseline models.\n- Overestimation observed due to news-related tweets affecting results.\n- Preprocessing crucial to prevent outlier words affecting model accuracy.\n- Lasso and ENet models showed similar performance in nowcasting.\n\nSlide Title: Experiment 2: Forecasting Performance\nBullet Points:\n- Lasso+ and ENet+ used for forecasting future influenza cases.\n- Forecasting accuracy superior to baseline up to 3 weeks into the future.\n- Accuracy for 1-week future prediction similar to current state estimation.\n- Forecasting performance decreased with increased time shift parameter.\n- Models overestimated before outbreaks and underestimated after peak.\n\nSlide Title: Discussion and Related Work\nBullet Points:\n- Time shift operation demonstrated basic feasibility for influenza prediction.\n- Inconsistency in time shifts affected forecasting performance.\n- Proposed approach fits within regression approaches for disease surveillance.\n- Study does not employ classification methods, leaving room for improvement.\n- Related work includes web-based surveillance for various diseases.\n\nSlide Title: Conclusions and Future Applications\nBullet Points:\n- Novel social media-based influenza surveillance system using forecasting words.\n- Optimal time lag explored for each word to maximize cross correlation.\n- Nowcasting and forecasting models predict influenza patient numbers.\n- Best nowcasting performance achieved with correlation ratio of 0.93.\n- Model has potential applications for predicting other events beyond influenza.",
    "Slide Title: Importance of Coherence in Text\nBullet Points:\n- Coherence enables a text to convey its communication purpose and meaning effectively.\n- A coherent text reduces reading time and cognitive load for readers.\n- Coherence involves integrating sentences around a central theme or topic.\n- Sentences in a coherent text exhibit discourse and rhetorical relations.\n- Textual organization and lexical continuity are crucial for coherence.\n\nSlide Title: Challenges in Computational Modelling of Coherence\nBullet Points:\n- Entity Grid model has bias towards original text ordering.\n- Supervised models suffer from data sparsity and domain dependence.\n- Entity Graph model addresses some limitations of supervised models.\n- Existing models focus on local cohesion, missing related-yet-not-identical entities.\n- Li and Hovy's approach captures semantic relations between sentences.\n\nSlide Title: Entity Grid Model Overview\nBullet Points:\n- Entity Grid evaluates local cohesion using Centering theory principles.\n- Text is represented as a matrix with entities as columns and sentences as rows.\n- Coherence is assessed by smooth transitions in the center of attention.\n- Salient entities are those mentioned frequently in the text.\n- Limitations include focus on local rather than global coherence.\n\nSlide Title: Entity Graph Model Overview\nBullet Points:\n- Entity Graph uses a bipartite graph to measure text coherence.\n- Graph relates nonadjacent sentences for global coherence assessment.\n- Sentences and entities are represented as vertices in the graph.\n- Weighted edges are based on entity roles in sentences.\n- Projection methods transform bipartite graph into directed projection graph.\n\nSlide Title: Semantic Similarity Graph Approach\nBullet Points:\n- Coherence is approached from a cohesion perspective using semantic similarity.\n- Text is encoded into a graph with sentences as vertices and semantic relations as edges.\n- Sentence vectors are created using pre-trained GloVe word vectors.\n- Three graph construction methods: PAV, SSV, and MSV.\n- Semantic similarity between sentences is key to establishing graph edges.\n\nSlide Title: Evaluation of Proposed Methods\nBullet Points:\n- Document discrimination task measures ability to identify original text.\n- Insertion task evaluates coherence by estimating original sentence position.\n- PAV method outperformed Entity Graph in both tasks.\n- Performance varied based on parameter settings and dataset characteristics.\n- Further refinement needed for capturing distant sentence relations.\n\nSlide Title: Results and Discussion\nBullet Points:\n- PAV method showed best performance in insertion task, outperforming baselines.\n- Local coherence is crucial for document discrimination task success.\n- Semantic similarity helps capture coherence beyond surface entity overlap.\n- OOV words not main reason for failures in insertion task.\n- Distant sentence relations contribute to text coherence.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Proposed unsupervised methods focus on sentence cohesion for coherence scoring.\n- PAV method outperformed unsupervised baseline in document discrimination task.\n- Further experiments needed with larger datasets for insertion task.\n- Refinement of sentence representation and similarity calculation is necessary.\n- Long-distance relations should be incorporated for improved coherence assessment.",
    "Slide Title: Introduction to BioASQ Challenge\nBullet Points:\n- BioASQ focuses on semantic indexing, QA, and information extraction in biomedical domain.\n- Our participation was in Task B, Phase B, focusing on biomedical QA.\n- Task involves returning answer candidates for given questions and gold-standard snippets.\n- Questions categorized into factoid, list, summary, and yes/no types.\n- Evaluation metrics include mean reciprocal rank for factoid and F1 score for list questions.\n\nSlide Title: Traditional vs. Neural QA Systems\nBullet Points:\n- Traditional QA systems use named-entity recognition, question classification, and answer scoring.\n- Require extensive resources and feature engineering specific to biomedical domain.\n- Our system uses a neural network architecture trained end-to-end on target task.\n- Based on FastQA, achieving state-of-the-art results on large QA benchmarks.\n- Trained on SQuAD dataset, fine-tuned on BioASQ training set.\n\nSlide Title: Neural Network Architecture\nBullet Points:\n- System inputs question and context, outputs start and end pointers to context tokens.\n- Uses FastQA with modified input vectors and output layer for list and factoid answers.\n- Context and question tokens mapped to high-dimensional word vectors.\n- Word vectors include GloVe, character, and biomedical Word2Vec embeddings.\n- One-hot encoding of question type concatenated to embedding vectors.\n\nSlide Title: Training and Optimization\nBullet Points:\n- Loss defined as cross-entropy of correct start and end indices.\n- Network trained in two phases: pre-training on SQuAD, fine-tuning on BioASQ.\n- Adam optimizer with exponentially decaying learning rate used for training.\n- BioASQ dataset preparation involves extracting answer spans from training data.\n- Inference phase retrieves top 20 answer spans via beam search.\n\nSlide Title: Model Ensemble and Implementation\nBullet Points:\n- Model ensemble built by training five single models using 5-fold cross-validation.\n- Ensemble combines models by averaging start and end scores before computing span probabilities.\n- Two systems submitted: best single model and model ensemble.\n- System implemented using TensorFlow, trained on NVidia GForce Titan X GPU.\n- Ensemble model performed slightly better on average compared to single model.\n\nSlide Title: Results and Discussion\nBullet Points:\n- Results reported for five test batches of BioASQ 5 (Task 5b, Phase B).\n- Performance numbers not final due to pending updates on gold-standard synonyms.\n- Single and ensemble systems performed similarly, with ensemble slightly better.\n- System successful on factoid questions, winning three out of five batches.\n- Performance on list questions varied significantly, expected due to pre-training dataset.\n\nSlide Title: Conclusion\nBullet Points:\n- System design for BioASQ 5B submission summarized, focusing on factoid and list questions.\n- Neural architecture trained end-to-end on QA task, novel for BioASQ challenges.\n- Achieved state-of-the-art results on factoid questions, competitive on list questions.\n- Approach demonstrates potential for neural networks in biomedical QA.\n- Future work could explore improvements for list and yes/no questions.",
    "Slide Title: Introduction to Stance Classification\nBullet Points:\n- Stance classification identifies user positions on specific targets from text.\n- Traditional approach involves target-specific classifiers trained on ground-truth data.\n- Cross-target classification adapts classifiers from related targets for new predictions.\n- Example: Using climate change stance to predict mining project approval stance.\n- Cross-target classification is challenging due to language model incompatibility.\n\nSlide Title: CrossNet Model Overview\nBullet Points:\n- CrossNet is a novel neural model for cross-target stance classification.\n- Utilizes self-attention mechanism to generalize between domain-related targets.\n- Learns domain-specific aspects from source target for destination target prediction.\n- Preliminary analysis shows improved classification performance in certain domains.\n- Focuses on capturing domain-specific information from stance-bearing sentences.\n\nSlide Title: CrossNet Model Architecture\nBullet Points:\n- CrossNet consists of four layers: Embedding, Context Encoding, Aspect Attention, Prediction.\n- Embedding Layer uses word embeddings to represent input sentence and target.\n- Context Encoding Layer captures contextual information using BiLSTM networks.\n- Aspect Attention Layer discovers domain-specific aspects for stance inference.\n- Prediction Layer uses domain aspect encoding to predict stance label.\n\nSlide Title: Aspect Attention Layer Details\nBullet Points:\n- Captures recurring domain aspects reflecting users' major concerns.\n- Utilizes self-attention to identify core parts of stance-bearing sentences.\n- Compatibility function scores semantic compatibility between sequence and hidden states.\n- Attention weights computed via softmax operation for domain aspect encoding.\n- Outputs domain aspect encoding for stance prediction.\n\nSlide Title: Model Training and Evaluation\nBullet Points:\n- Model trained using multi-class cross-entropy loss with L2-regularization.\n- Experiments conducted on datasets from Women's Rights, American Politics, Environments.\n- F1-score used to measure classification performance, considering class imbalances.\n- Transfer ratio metric evaluates effectiveness of target adaptation.\n- CrossNet shows significant improvement in cross-target task over baselines.\n\nSlide Title: Classification Performance Results\nBullet Points:\n- CrossNet outperforms baseline models in cross-target stance classification.\n- Aspect attention mechanism benefits target-level generalization.\n- Greater improvement observed in cross-target task compared to in-target task.\n- Transfer ratio results indicate importance of destination target-specific information.\n- Visualization shows model captures domain-specific information effectively.\n\nSlide Title: Visualization of Attention Mechanism\nBullet Points:\n- Self-attention highlights sentence parts related to domain aspects.\n- Examples show relevant words for domains like women's rights and politics.\n- Model captures central words for understanding stances on mining projects.\n- Domain-specific words contribute to stance prediction accuracy.\n- Visualization demonstrates model's ability to generalize across related targets.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- CrossNet extracts target-independent information for model generalization.\n- Achieves superior results over baselines in certain domains.\n- Future work includes selecting effective source targets for better generalization.\n- Consideration of multiple related sources for knowledge transfer.\n- Evaluation in multilingual scenarios to test generalization ability.",
    "Slide Title: Understanding Sentence Acceptability\nBullet Points:\n- Sentence acceptability measures how well-formed a sentence is to native speakers.\n- It includes semantic, syntactic, and pragmatic plausibility, plus non-linguistic factors.\n- Grammaticality focuses on syntactic well-formedness, a theoretical concept.\n- Research aims to predict acceptability judgments, considering document context.\n- Context's influence on acceptability is crucial for style and coherence assessment.\n\nSlide Title: Research Hypotheses and Objectives\nBullet Points:\n- Hypothesis 1: Document context boosts sentence acceptability judgments.\n- Hypothesis 2: Context aids language models in modeling acceptability.\n- Hypothesis 3: Models predict acceptability better with document context.\n- Research investigates context's impact on human acceptability judgments.\n- Study implications for language generation and coherence assessment.\n\nSlide Title: Experiment Design and Data Collection\nBullet Points:\n- Dataset includes sentences with acceptability ratings, judged with/without context.\n- 100 random English Wikipedia articles sampled for sentences and context.\n- Sentences translated to/from four languages to vary acceptability.\n- Amazon Mechanical Turk used for acceptability judgments on a 4-point scale.\n- 20 judgments per sentence, totaling 20,000 annotations collected.\n\nSlide Title: Impact of Context on Acceptability Ratings\nBullet Points:\n- Strong correlation found between ratings with and without context (Pearson's r = 0.80).\n- Context improves acceptability for ill-formed sentences, diminishes for well-formed.\n- Context compresses rating distribution, affecting correlation.\n- Workers focus on global coherence with context, reducing syntactic error impact.\n- Context influences ratings, shown in sample sentences with/without context.\n\nSlide Title: Language Models for Acceptability Prediction\nBullet Points:\n- Neural models outperform others in predicting acceptability across languages.\n- LSTM and topically-driven models (TDLM) used for experiments.\n- TDLM incorporates document context via topic and language components.\n- Four test variants: sentence-only and sentence-with-context for both models.\n- Context improves model performance, TDLM outperforms LSTM consistently.\n\nSlide Title: Model Performance and Human Ratings\nBullet Points:\n- SLOR measure outperforms others in predicting acceptability.\n- Context at test time enhances model performance across all human ratings.\n- TDLM better predicts acceptability than LSTM, even without context.\n- Lower correlation when predicting context-judged acceptability.\n- Models outperform average single-annotator correlation, promising for prediction.\n\nSlide Title: Related Work and Contextual Influence\nBullet Points:\n- Limited work on acceptability prediction compared to grammatical error detection.\n- Previous studies used unsupervised models, focusing on sentence-level acceptability.\n- Context improves acceptability for ill-formed sentences, reduces for well-formed.\n- Human agreement decreases with context, making ratings less predictable.\n- Future work to explore why context reduces acceptability for well-formed sentences.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- Context positively influences acceptability, especially for ill-formed sentences.\n- Incorporating context improves model performance in acceptability prediction.\n- Prediction declines with context-judged ratings, challenging initial hypothesis.\n- Human agreement decreases with context, affecting predictability.\n- Future research to explore context's impact on well-formed sentence acceptability.",
    "Slide Title: Introduction to Translation\nBullet Points:\n- Translation has been crucial in human civilization for law, religion, and trade.\n- Translators aim for invisibility, making translations seem original in the target language.\n- Cicero and St. Jerome emphasized translating sense, not word-for-word.\n- Recent years have seen standardization in translation quality (ISO 17100).\n- Translations exhibit unique characteristics, differing statistically from original texts.\n\nSlide Title: Universal Features of Translation\nBullet Points:\n- Simplification: Translators simplify complex source structures in the target language.\n- Standardization: Over-conforming to target language standards is common in translations.\n- Explicitation: Implicit source structures become explicit in the target language.\n- Interference: Source language \"fingerprints\" affect translation outcomes.\n- Shared isomorphic structures lead to more features in the target language.\n\nSlide Title: Hypotheses and Contributions\nBullet Points:\n- Hypothesis: Interference allows clustering source languages into families.\n- Phylogenetic trees can reflect true distances among source languages.\n- Trees improve with interference features, deteriorate with universal features.\n- Demonstrated interference phenomena can reconstruct intra-family ties.\n- Analyzed linguistic phenomena for language typology reconstruction.\n\nSlide Title: Related Work in Translation Studies\nBullet Points:\n- Historical linguistics uses bioinformatics for phylogenetic tree generation.\n- Translation studies distinguish originals from translations using machine learning.\n- Interference is a robust phenomenon, affecting sentence and morpheme levels.\n- Classifiers show lower accuracy with distant source-target language pairs.\n- Phylogenetic trees reconstructed from translations for the first time.\n\nSlide Title: Methodology and Dataset\nBullet Points:\n- Used Europarl corpus, EU Parliament proceedings translated into EU languages.\n- Focused on 17 source languages from Germanic, Romance, and Balto-Slavic families.\n- Translations into English and French analyzed for source language markers.\n- Equal-sized data chunks sampled for tree reconstruction experiments.\n- Feature sets include POS trigrams, function words, and cohesive markers.\n\nSlide Title: Phylogenetic Tree Reconstruction\nBullet Points:\n- Indo-European language family tree used as a gold standard.\n- Trees generated using agglomerative clustering of feature vectors.\n- Evaluation metrics include unweighted and weighted similarity to gold tree.\n- POS-trigrams feature set closely associated with source-language interference.\n- Function words and cohesive markers reflect grammatical constructs and explicitation.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- POS-trigrams produced trees closest to the gold standard.\n- Function words showed moderate results, better than random.\n- Cohesive markers were least accurate, reflecting universal explicitation.\n- English translations yielded better trees than French due to interference.\n- Interference remains strong despite intermediate distortions in translation.\n\nSlide Title: Analysis of Linguistic Phenomena\nBullet Points:\n- Overuse of definite articles in translations from Balto-Slavic languages.\n- Possessive constructions vary, with noun-'of'-NP common in Balto-Slavic.\n- Verb-particle constructions overused in Germanic language translations.\n- Tense and aspect expressed differently, with Germanic overusing perfect aspect.\n- Interference reveals source-language traces in translation products.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Translations reflect language typology, enabling phylogenetic tree reconstruction.\n- Interference is a dominant force in translation, surpassing universal features.\n- Historical connections among languages captured in translation trees.\n- Translations may influence language change, a potential research direction.\n- Future work to explore translations in different domains and languages.",
    "Slide Title: Introduction to Word Embeddings\nBullet Points:\n- Neural networks embed words into low-dimensional vector spaces, known as word embeddings.\n- Word embeddings enhance tasks like Machine Translation, syntactic parsing, and Question Answering.\n- A limitation is the inability to distinguish different meanings of the same word.\n- Previous methods induce word senses from corpora but lack semantic network knowledge.\n- SW2V model learns embeddings for words and senses using text corpora and semantic networks.\n\nSlide Title: Related Work on Word and Sense Embeddings\nBullet Points:\n- Word embeddings have evolved from models like word2vec and GloVe.\n- Lexical resources are underutilized in addressing word ambiguity.\n- Some models replace words with senses, limiting word-sense integration.\n- Shared vector spaces for words and senses are crucial for neural architectures.\n- SW2V proposes a joint training phase for words and senses in a common space.\n\nSlide Title: Connecting Words and Senses in Context\nBullet Points:\n- SW2V requires a corpus linking words to senses in context.\n- Manual sense annotation is costly; automatic systems face data and speed issues.\n- A shallow word-sense connectivity algorithm is proposed for semantic networks.\n- The algorithm connects words to senses based on network connections.\n- It is linear in corpus size, enabling fast training on large datasets.\n\nSlide Title: Joint Training of Words and Senses\nBullet Points:\n- SW2V extends word embedding models by integrating explicit knowledge.\n- The model uses the CBOW architecture, predicting words and senses in context.\n- Words and senses are updated together, maintaining a shared vector space.\n- The model predicts both words and senses, enhancing semantic coherence.\n- Different configurations for input and output layers are explored.\n\nSlide Title: Analysis of Model Components\nBullet Points:\n- SW2V configurations are tested on word similarity tasks.\n- Best results achieved with senses in input and both words and senses in output.\n- Shallow word-sense connectivity algorithm outperforms pre-disambiguated text.\n- The algorithm allows multiple senses per word, improving training robustness.\n- SW2V shows significant training speed and accuracy improvements.\n\nSlide Title: Evaluation of SW2V Model\nBullet Points:\n- SW2V evaluated on word similarity, sense clustering, and interconnectivity tasks.\n- Consistently outperforms sense-based models on similarity datasets.\n- Demonstrates superior sense clustering capabilities on ambiguous word sets.\n- Provides a coherent shared space for word and sense embeddings.\n- SW2V's MCS baseline shows strong performance in WSD tasks.\n\nSlide Title: Word and Sense Interconnectivity\nBullet Points:\n- SW2V builds a semantically coherent shared space for words and senses.\n- Evaluated on WSD tasks using a Most Common Sense baseline.\n- SW2V outperforms previous models in capturing predominant word senses.\n- Demonstrates effective clustering of semantically related words and senses.\n- Highlights the potential for resource-poor languages and large knowledge bases.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- SW2V learns word and sense embeddings in a single training phase.\n- Achieves a unified vector space using text corpora and semantic networks.\n- Outperforms state-of-the-art models in various semantic tasks.\n- Future work includes integrating WSD and Entity Linking systems.\n- Plans to explore multilingual and crosslingual applications.",
    "Slide Title: Introduction to Machine Translation Evaluation Metrics  \nBullet Points:  \n- Machine Translation (MT) metrics like BLEU often criticized for various reasons.  \n- Higher BLEU scores generally correlate with better human judgments in shared tasks.  \n- BLEU's correlation with human scores can be inconsistent, as shown in studies.  \n- Example of high BLEU score failing in manual evaluation in MT shared task.  \n- BLEU is a precision-based metric with brevity penalty for short translations.\n\nSlide Title: Understanding BLEU Score Calculation  \nBullet Points:  \n- BLEU calculates n-gram precision by matching hypothesis sentences with reference.  \n- Brevity penalty compensates for high precision translations that are too short.  \n- BLEU score ranges from 0 to 1, closer to 1 indicates better translation.  \n- High correlation with human judgments, especially for n-gram-based systems.  \n- Example shows BLEU's fallacy with subtle lexical differences affecting scores.\n\nSlide Title: Limitations of BLEU and Introduction to RIBES  \nBullet Points:  \n- BLEU lacks explicit consideration for reordering in translations.  \n- RIBES introduced to address reordering with rank correlation coefficient.  \n- Example shows RIBES penalizing wrongly ordered phrases in hypothesis.  \n- RIBES score presents wider difference than BLEU but still insufficient.  \n- Human evaluators preferred baseline translation despite higher RIBES score.\n\nSlide Title: Alternative MT Evaluation Metrics  \nBullet Points:  \n- Other metrics like METEOR and ROUGE-S improve upon BLEU's limitations.  \n- METEOR allows matching paraphrases and stems, not just surface strings.  \n- ROUGE-S uses skip-grams matches for better evaluation.  \n- Pre-trained models based on semantic similarity replace n-gram matching.  \n- Enriching n-gram matching handles variant translations but not BLEU's crudeness.\n\nSlide Title: Experimental Setup for WAT 2015 Shared Task  \nBullet Points:  \n- System submission for Korean to Japanese patent translation using JPO Patent Corpus.  \n- Phrase-based SMT implemented in Moses toolkit with specific settings.  \n- Language modeling trained using KenLM with 5-grams and Kneser-Ney smoothing.  \n- Minimum Error Rate Training (MERT) used for tuning decoding parameters.  \n- Human evaluations conducted as pairwise comparisons with baseline system.\n\nSlide Title: Human Evaluation and Pairwise Comparison  \nBullet Points:  \n- Human judgment scores acquired using Lancers crowdsourcing platform.  \n- Evaluators judged hypothesis and baseline translations for quality.  \n- Majority vote determined whether hypothesis won, lost, or tied baseline.  \n- HUMAN score calculated from wins, losses, and ties, ranging from -100 to +100.  \n- System with higher BLEU score received low human judgment score.\n\nSlide Title: Segment Level Meta-Evaluation  \nBullet Points:  \n- Meta-evaluation performed by calculating BLEU and RIBES score differences.  \n- Correlations between score differences and human judgments analyzed.  \n- High BLEU improvements often rated marginally better or tied with baseline.  \n- Disparity due to minor lexical differences and crowd-sourced preferences.  \n- Evaluation metric differences not reflecting major translation inadequacy.\n\nSlide Title: Conclusion and Implications for MT Evaluation  \nBullet Points:  \n- High BLEU and RIBES scores do not always correlate with better human judgment.  \n- Factors contributing to poor correlation identified in WAT 2015 task.  \n- Segment level meta-evaluation highlights disparity in metric improvements.  \n- Results aim to improve automatic translation evaluation metrics.  \n- Encourages development of metrics better aligned with human judgment.",
    "Slide Title: Introduction to BioASQ Challenge 2017\nBullet Points:\n- Overview of BioASQ challenge tasks and data from February to May 2017.\n- Evaluation of participating systems using state-of-the-art measures and manual assessment.\n- Detailed descriptions of systems available in workshop proceedings.\n- Conclusions summarize the outcomes of this year's challenge.\n\nSlide Title: Overview of BioASQ Challenge Tasks\nBullet Points:\n- Three main tasks: semantic indexing, question answering, and funding information extraction.\n- Task 5a: Classify PubMed documents into MeSH hierarchy concepts.\n- Task 5b: Large-scale question answering for biomedical questions.\n- Task 5c: Extract grant information from biomedical articles.\n\nSlide Title: Task 5a: Large-scale Semantic Indexing\nBullet Points:\n- Classify PubMed articles using MeSH hierarchy concepts.\n- Evaluation using flat and hierarchical information retrieval measures.\n- Online scenario with three independent batches of test sets.\n- 12,834,585 articles with 27,773 labels provided as training data.\n\nSlide Title: Task 5b: Biomedical Semantic Question Answering\nBullet Points:\n- Four types of questions: yes/no, factoid, list, and summary.\n- Two phases: retrieval of relevant elements and submission of exact answers.\n- 100 questions per batch with 24-hour response time.\n- Systems evaluated on document, snippet, and RDF triple retrieval.\n\nSlide Title: Task 5c: Funding Information Extraction\nBullet Points:\n- Extract grant information from biomedical articles.\n- Use of MEDLINE citations as golden data for training and evaluation.\n- Systems evaluated on grant ID, agency, and full-grant extraction.\n- 10 teams participated with 31 different systems submitted.\n\nSlide Title: Systems and Approaches for Task 5a\nBullet Points:\n- UIMA-based text and data mining workflow for document annotation.\n- Binary Relevance classification using TF-IDF and LDA models.\n- Sequence-to-sequence prediction with recurrent neural networks.\n- Document to vector and tf-idf feature embeddings for semantic indexing.\n\nSlide Title: Systems and Approaches for Task 5b\nBullet Points:\n- MetaMap for query expansion and BM25 model for document matching.\n- SAP HANA database with NLP components for answer processing.\n- Sequential dependence models and pseudorelevance feedback for query enrichment.\n- Extractive summarization and neural networks for ideal answer generation.\n\nSlide Title: Systems and Approaches for Task 5c\nBullet Points:\n- Linear SVM and regular expressions for grant information extraction.\n- Sequential learning models: CRFs, HMMs, and MaxEnt for snippet classification.\n- Ensemble methods to maximize recall and exploit model complementarity.\n- Focus on specific text sections for grant support information extraction.\n\nSlide Title: Results and Evaluation of BioASQ Challenge\nBullet Points:\n- Task 5a: DeepMeSH and AUTH systems outperform strong baselines.\n- Task 5b: Different systems excel in document and snippet retrieval.\n- Task 5c: Grant ID extraction is more challenging than agency extraction.\n- Overall performance indicates potential for system improvement.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- BioASQ challenge advances state-of-the-art in biomedical information systems.\n- Plans for more benchmark data and multi-batch scenario for Task 5c.\n- Increased participation highlights growth of BioASQ community.\n- BioASQ as a reference point for semantic indexing and question answering.",
    "Slide Title: Introduction to Simultaneous Interpretation\nBullet Points:\n- Simultaneous Interpretation (SI) is a challenging task with significant cognitive demands.\n- Interpreters aim to accurately and promptly render source speech into target language.\n- Strategies like generalization and summarization help convey messages efficiently.\n- Errors increase exponentially after minutes of interpreting due to memory limits.\n- Estimating interpreter performance can enhance Computer-Assisted Interpretation (CAI).\n\nSlide Title: Computer-Assisted Interpretation (CAI)\nBullet Points:\n- CAI systems provide live, interactive terminology support in interpreting booths.\n- Ideal support level depends on current interpreter performance to minimize distraction.\n- Quality estimation (QE) system evaluates interpreter output for optimal assistance.\n- CAI systems can adjust information flow based on interpreter's struggle level.\n- Accurate performance prediction can maximize interpreter output quality.\n\nSlide Title: Quality Estimation in Machine Translation\nBullet Points:\n- QE measures MT output quality without reference translations, using prediction tasks.\n- QuEst++ pipeline supports word, sentence, and document-level QE.\n- Feature extractor creates continuous feature vectors from source and translation.\n- Learning module predicts translation quality using trained example translations.\n- Support Vector Regression is default for sentence-level QE in QuEst++.\n\nSlide Title: Adapting QE for Simultaneous Interpretation\nBullet Points:\n- SI output differs from MT, with pauses, hesitations, and re-orderings.\n- Interpretation-specific features added to baseline QE feature set.\n- Pauses and hesitations indicate cognitive strategies and interpreter struggles.\n- Ratio of non-specific words helps manage memory constraints and cognitive load.\n- Cognates and utterance length features adapted for SI evaluation.\n\nSlide Title: Evaluation Metrics for Interpreter Output\nBullet Points:\n- METEOR metric considers paraphrases and content-function word distinctions.\n- Interpreter output differs from reference translations due to economizing strategies.\n- METEOR better handles disparities between MT and SI evaluation.\n- Fine-grained evaluation measures for interpreter performance are future work.\n- METEOR aligns with MT evaluation goals, despite lack of large annotated corpus.\n\nSlide Title: Interpretation Corpora and Data\nBullet Points:\n- EN-JA data from NAIST TED SI corpus with three interpreter proficiency levels.\n- EN-FR and EN-IT data from EPTIC corpus with European Parliament speeches.\n- Human transcribed SI output and written translations used for evaluation.\n- Google Translate used for more reliable METEOR scores in experiments.\n- Corpora provide diverse language pairs for comprehensive QE evaluation.\n\nSlide Title: Interpreter Quality Experiments\nBullet Points:\n- Pearson's r correlation used to evaluate QE system's prediction accuracy.\n- Baseline, trimmed, and proposed feature sets compared for performance.\n- Proposed model outperforms in all language settings, especially EN-JA(A-Rank).\n- Statistically significant gains achieved with interpreter-focused model.\n- Cross-validation and feature ablation used to refine QE system.\n\nSlide Title: Results Analysis and Conclusion\nBullet Points:\n- Ablation analysis shows importance of specific features for different languages.\n- Cognates aid EN-FR and EN-IT, while pauses help EN-FR and EN-IT models.\n- Proposed model captures interpretation-specific qualities like pauses and omissions.\n- QE application allows CAI systems to assist struggling interpreters effectively.\n- Future work includes creating fine-grained measures for interpreter performance.",
    "Slide Title: Introduction to Word Embeddings\nBullet Points:\n- Word embeddings enable generalization to unseen words using large unlabeled datasets.\n- Low-dimensional embeddings capture distributional similarity for shared word contexts.\n- Handling out-of-vocabulary (OOV) words is challenging, especially in low-resource languages.\n- MIMICK-RNN predicts vectors for OOV words using character-level RNNs.\n- MIMICK-RNN captures word-shape and lexical features for OOV words.\n\nSlide Title: Related Work on Word Embeddings\nBullet Points:\n- Morphology-based models predict embeddings using linguistic substructures.\n- Character-based models avoid issues with names and foreign words.\n- Our approach is a post-processing step applicable to any word embeddings.\n- Supervised subword models learn task-specific embeddings from labeled data.\n- MIMICK complements supervised models, improving low-resource language performance.\n\nSlide Title: MIMICK Word Embeddings Approach\nBullet Points:\n- MIMICK treats OOV embeddings as a generation problem using existing vocabulary.\n- Predictive function: Word Type Character Bi-LSTM for unseen word embeddings.\n- Model minimizes squared Euclidean distance between predicted and pre-trained embeddings.\n- Training outputs character embeddings matrix and neural network parameters.\n- MIMICK provides embeddings for new words using character sequences.\n\nSlide Title: MIMICK Polyglot Embeddings\nBullet Points:\n- Polyglot provides multilingual word embeddings for frequent words in various languages.\n- High OOV rates in many languages necessitate MIMICK's predictive model.\n- MIMICK algorithm applied to Polyglot embeddings for 23 languages.\n- Hyper-parameters set based on preliminary testing for optimal performance.\n- Nearest-neighbor examination validates MIMICK's protocol across languages.\n\nSlide Title: Joint Tagging of POS and Morphosyntactic Attributes\nBullet Points:\n- Universal Dependencies scheme supports tagging with language-specific features.\n- MIMICK extends POS tagging to morphosyntactic attributes using a projection layer.\n- Multilayer perceptron network outputs attribute-specific probability distributions.\n- Input to LSTM is a sequence of word embeddings, initialized from Polyglot or MIMICK.\n- Weighted sum loss computed for attribute sparsity across languages.\n\nSlide Title: Experimental Settings and Language Selection\nBullet Points:\n- Morphological complexity varies across languages, influencing MIMICK's performance.\n- Language family and script type considered in orthography-based approach.\n- 23 languages selected for evaluation based on typological properties.\n- Metrics include token-level accuracy and micro F1 for morphosyntactic attributes.\n- Models tested include No-Char, MIMICK, CHAR\u2192TAG, and Both.\n\nSlide Title: Results and Performance Analysis\nBullet Points:\n- MIMICK improves performance in low-resource and full-resource settings.\n- Significant gains in Slavic languages and Chinese for POS and morphosyntactic tagging.\n- MIMICK fares better than CHAR\u2192TAG in low-resource settings for several languages.\n- Statistically significant improvements in test-set tokens outside UD training set.\n- MIMICK initialization enhances representational mapping from orthography.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- MIMICK infers OOV word embeddings from limited-vocabulary models without corpus access.\n- Useful for low-resource languages and task-agnostic applications.\n- Improves performance on sequence-tagging tasks across diverse languages.\n- Future work may explore other subword units like morphemes or phonemes.\n- Acknowledgments to contributors and project support from Defense Threat Reduction Agency.",
    "Slide Title: Introduction to Span-Based Neural Parsing\nBullet Points:\n- Span-based neural parsing offers high accuracy and simplicity compared to other neural parsers.\n- Utilizes bidirectional RNNs for input sequence modeling, not output tree structures.\n- Combines merits of greedy and chart-based approaches for efficient parsing.\n- Achieves linear-time parsing with dynamic programming and beam search.\n- Introduces cube pruning to improve runtime complexity to O(nb log b).\n\nSlide Title: Span-Based Shift-Reduce Parsing\nBullet Points:\n- Maintains a stack of spans, adding new spans with shift or reduce actions.\n- Shift action pushes next singleton span onto the stack.\n- Reduce action combines top two spans to form a larger span.\n- Parsing continues until the entire sentence is represented as a span.\n- Uses Bi-LSTM features for efficient span representation.\n\nSlide Title: Dynamic Programming and Score Decomposition\nBullet Points:\n- Decomposes tree score into sum of span scores for efficient parsing.\n- Score of a span depends only on the span itself, not subtree information.\n- Shift-reduce setting involves 2n-1 steps for parsing.\n- Max over label X is independent of other spans, aiding parsing efficiency.\n- Graph-structured stack reformulation eliminates need for extra bookkeeping.\n\nSlide Title: Action-Synchronous Beam Search and Cube Pruning\nBullet Points:\n- Maintains top b parsing states at each step, pruning off the rest.\n- Cube pruning reduces complexity by considering top b states in a heap.\n- Avoids inserting all b\u00b2 reduced states, focusing on highest scoring left pointers.\n- Achieves linear runtime with O(n) parsing actions and cube pruning.\n- Enhances efficiency while maintaining parsing accuracy.\n\nSlide Title: Training with Structured SVM and Cross-Span Loss\nBullet Points:\n- Uses Structured SVM to ensure gold tree scores higher than others.\n- Loss-augmented decoding maximizes score difference between gold and predicted trees.\n- Cross-span loss penalizes spans crossing gold-tree spans for improved accuracy.\n- Max violation updates focus on largest violation along parse trajectory.\n- Training involves minibatches and dropout for regularization.\n\nSlide Title: Experiments on Penn Treebank and Discourse Parsing\nBullet Points:\n- Experiments conducted on Penn Treebank and PTB-RST discourse treebank.\n- Beam size of 20 achieves state-of-the-art performance with fast parsing.\n- Discourse parsing experiments focus on tractability of longer sequences.\n- Models represent end-to-end discourse parsing systems without preprocessing.\n- Larger beam sizes required for good discourse scores.\n\nSlide Title: Results and Performance Comparison\nBullet Points:\n- Achieves highest F1 score on Penn Treebank among single model systems.\n- Substantially faster for long sentences and end-to-end discourse parsing.\n- Beam size of 20 balances speed and performance effectively.\n- Out-of-the-box runtimes compared for discourse parsing.\n- Adaptation of split-point feature enhances discourse parsing results.\n\nSlide Title: Conclusions and Contributions\nBullet Points:\n- Developed a linear-time neural parser searching over exponentially large space.\n- Introduced cube pruning to improve runtime to O(nb log b).\n- New loss function and max-violation updates enhance training effectiveness.\n- Achieved state-of-the-art results in single-model end-to-end systems.\n- Provides insights into efficient parsing with minimal complexity.",
    "Slide Title: Introduction to Query Auto-Completion (QAC)\nBullet Points:\n- QAC provides suggested queries as users type, reducing time and cognitive load.\n- Most QAC systems extend the Most Popular Completion (MPC) algorithm.\n- MPC suggests completions based on popular queries matching the prefix.\n- Personalization improves QAC by considering user-specific query likelihoods.\n- Recent approaches use character LSTM models for unseen prefixes.\n\nSlide Title: Advancements in Personalized QAC\nBullet Points:\n- Personalized QAC combines personalization with language models for rare prefixes.\n- FactorCell model adapts language model weights for user-specific queries.\n- FactorCell allows more weight changes than traditional context vector methods.\n- Example: Sports-related queries change top completions for prefix \"ba.\"\n- Adaptive language models handle unseen user contexts effectively.\n\nSlide Title: Learning User Embeddings\nBullet Points:\n- User embeddings capture latent demographic factors for personalization.\n- Users with fewer than 15 queries are grouped as a single entity.\n- Embeddings are updated online during evaluation for unseen users.\n- User embeddings are updated via back-propagation with frozen model parameters.\n- Embeddings adjust recurrent layer weights for personalized query completion.\n\nSlide Title: Recurrent Layer Adaptation Techniques\nBullet Points:\n- Unadapted LM does no personalization, similar to Park and Chiba's model.\n- ConcatCell model adds user embedding to character embedding for personalization.\n- FactorCell model uses low-rank adaptation matrix for stronger personalization.\n- FactorCell transforms recurrent layer weights for user-specific adaptation.\n- FactorCell outperforms ConcatCell by adapting more parameters.\n\nSlide Title: Experimental Data and Implementation\nBullet Points:\n- Experiments use AOL Query data with 12 million queries from 173,000 users.\n- Models trained for six epochs using Adam optimizer with learning rate 10^-3.\n- Adadelta optimizer used for online user embedding updates.\n- Small and large model configurations tested with different FactorCell ranks.\n- Beam search with width 100 used for generating query completions.\n\nSlide Title: Results and Performance Evaluation\nBullet Points:\n- Neural models outperform MPC baseline, especially for seen prefixes.\n- Personalized models show better performance than unadapted models.\n- FactorCell model shows significant improvement with more user queries.\n- Longer prefixes lead to better completions; short queries benefit most.\n- FactorCell adapts quicker to repetitive search behaviors.\n\nSlide Title: Case Studies and Semantic Coherence\nBullet Points:\n- FactorCell identifies semantically coherent queries after related searches.\n- Example: High school-related queries yield entertainment and celebrity results.\n- FactorCell focuses on semantics, while ConcatCell focuses on orthography.\n- FactorCell adapts better to user-specific search patterns.\n- Semantic coherence enhances personalized query suggestions.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- Neural LM outperforms MPC even with rare prefix handling.\n- Personalization improves generative neural model approach.\n- RNNs used for next query suggestion and spelling correction integration.\n- Dynamic evaluation updates user embeddings for real-time adaptation.\n- Future work: Combine language model with MPC for improved results.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Personalization enhances LSTM model performance for QAC.\n- FactorCell model provides significant advantages in recurrent layer adaptation.\n- Adapted weight matrix computed once per query, reused in beam search.\n- Personalization effective for unseen users, benefits increase over time.\n- Future work: Extend model to include time as a conditioning factor.",
    "Slide Title: Introduction to Neural Vector Representations\nBullet Points:\n- Neural vector representations are crucial in natural language processing subfields.\n- Word vectors have been studied for linear substructures and semantic properties.\n- Sentence embeddings encode variable-length sentences into fixed-length vectors.\n- Evaluated using cosine similarities for semantic relatedness in competitions.\n- Limited understanding of specific semantic properties in sentence embeddings.\n\nSlide Title: Analysis of Sentence Embeddings\nBullet Points:\n- Minor sentence alterations can lead to significant meaning shifts.\n- Sentence triplets explore syntactic and semantic changes in embeddings.\n- Semantic relatedness assessed through sentence triplet comparisons.\n- High cosine similarity expected between semantically related sentences.\n- Automated analyses require more than just similarity comparisons.\n\nSlide Title: Sentence Modification Schemes\nBullet Points:\n- Not-Negation: Insert \"not\" before the first verb for negation.\n- Quantifier-Negation: Prepend \"there is no\" to original sentences.\n- Synonym Substitution: Replace verbs with appropriate synonyms.\n- Embedded Clause Extraction: Extract clauses from sentences with verbs like \"say.\"\n- Passivization: Convert active voice sentences to passive voice.\n\nSlide Title: Sentence Triplet Generation\nBullet Points:\n- Negation Detection: Original, Synonym Substitution, Not-Negation sentences.\n- Negation Variants: Quantifier-Negation, Not-Negation, Original sentence.\n- Clause Relatedness: Original, Embedded Clause Extraction, Not-Negation.\n- Argument Sensitivity: Original, Passivization, Argument Reordering.\n- Fixed Point Reorder: Original, Semantically equivalent, Fixed Point Inversion.\n\nSlide Title: Experiments and Datasets\nBullet Points:\n- Evaluation datasets created using triplet generation methods.\n- Source sentences from SICK, Penn Treebank WSJ, MSR Paraphrase corpus.\n- Human annotators ensure grammaticality and semantics in modified sentences.\n- Synonym substitution checked using Word-Net for context suitability.\n- Evaluation focuses on semantic properties of sentence embeddings.\n\nSlide Title: Embedding Methods Compared\nBullet Points:\n- GloVe Averaging: Averages word vectors, neglects word order.\n- Concatenated P-Mean Embeddings: Combines different p-means of word vectors.\n- Sent2Vec: Learns word and n-gram embeddings for sentence vectors.\n- Skip-Thought Vector: Predicts neighboring sentences using recurrent networks.\n- InferSent: Supervised by Stanford NLI dataset for semantic understanding.\n\nSlide Title: Results and Discussion\nBullet Points:\n- Negation Detection: InferSent and SkipThought distinguish negated from unnegated sentences.\n- Negation Variants: InferSent understands negation quantifiers better than others.\n- Clause Relatedness: Sent2Vec best at distinguishing embedded clauses from negations.\n- Argument Sensitivity: None adept at separating semantic from structural information.\n- Fixed Point Reorder: SkipThought and InferSent handle semantically equivalent structures well.\n\nSlide Title: Conclusion on Sentence Embeddings\nBullet Points:\n- Simple method proposed to inspect semantic properties of embeddings.\n- SkipThought and InferSent excel in distinguishing negation from synonymy.\n- InferSent better at identifying semantic equivalence despite word order.\n- SkipThought suitable for tasks where semantics align with structure.\n- Neural network-based embeddings outperform simple word averaging.",
    "Slide Title: Introduction to Weblio Pre-reordering SMT System\nBullet Points:\n- Overview of Weblio Pre-reordering Statistical Machine Translation (SMT) System and its experiments.\n- Utilizes pre-reordering method from word alignments and parse trees for translation.\n- Applied in English-Japanese translation subtask at WAT2014 workshop.\n- System outputs N-best reordering results for improved translation accuracy.\n- Incorporates N-best parse trees for enhanced automatic evaluation scores.\n\nSlide Title: Head-restructured CFG Parse Tree\nBullet Points:\n- Reorders SVO to SOV order by capturing long-distance reordering patterns.\n- Dependency trees help identify closely related words under a head word.\n- Not all sentence components need reordering; some structures maintain order.\n- Head-restructured CFG parse tree integrates sentence-level dependencies.\n- Explicit annotations in HRCFG tree aid in capturing reordering patterns.\n\nSlide Title: Reordering Model with Language Model Integration\nBullet Points:\n- Reordering model follows the approach in Zhu et al., 2014.\n- Language model identifies best order of node layers in target language.\n- Bilingual training data and word alignments used for node layer ordering.\n- Reordered nonterminal tags exported as new training data.\n- Language model estimates likelihood of tag sequence in target language.\n\nSlide Title: N-best Reordering Approach\nBullet Points:\n- Best sentence order comprises 1-best orders of node layers in parse tree.\n- N-best reordering results necessary for optimal translation outcomes.\n- Collects N-best reordering results for all node layers.\n- Ranks reordering results by accumulated language model score.\n- Example: \"The rocket is launched by NASA\" with multiple reordering options.\n\nSlide Title: Experimental Settings and Results\nBullet Points:\n- Baseline system uses 1-best parse trees for training and testing.\n- Stanford tokenizer and Berkeley parser used for CFG parse trees.\n- HRCFG trees built from CFG and dependency parse trees.\n- Experiments show improvements in BLEU and RIBES metrics.\n- N-best parse trees and reordering results enhance automatic evaluation scores.\n\nSlide Title: Evaluation of Pre-reordering Performance\nBullet Points:\n- Performance evaluated using Kendall's \u03c4 from word alignments.\n- Pre-reordering improves word order, increasing identical order sentences.\n- Average Kendall's \u03c4 improved from 0.30 to 0.71 after pre-reordering.\n- Graph shows intuitive picture of word order improvements.\n- Pre-reordering enhances translation quality by adjusting word order.\n\nSlide Title: Error Analysis and Context-aware Translation\nBullet Points:\n- Pre-reordering SMT system faces challenges with rare or unknown words.\n- Reordering model's flexibility leads to sparse model issues.\n- Context-aware translation tackles phrase selection problem in English-Japanese.\n- Domain-specific language models improve translation quality.\n- Interpolated language models achieve best perplexities with in-domain data.\n\nSlide Title: Conclusion and Future Research\nBullet Points:\n- Reordering model improves automatic evaluation scores with multiple candidates.\n- Human evaluation shows limitations in N-best parse tree incorporation.\n- Reordering model requires further improvement for general case handling.\n- Interpolating general and in-domain language models enhances translation quality.\n- Future research to explore performance limits of pre-reordering models.",
    "Slide Title: Introduction to Cross-Language Plagiarism\nBullet Points:\n- Plagiarism is a significant issue in higher education, exacerbated by internet and translation tools.\n- Cross-language plagiarism involves translating and plagiarizing text, complicating detection efforts.\n- Detecting cross-language plagiarism requires innovative similarity measures across different languages.\n- Word embeddings offer promising solutions for cross-lingual similarity and plagiarism detection.\n- This study explores using word embeddings to enhance cross-language plagiarism detection methods.\n\nSlide Title: Key Contributions of the Study\nBullet Points:\n- Augmented state-of-the-art methods with word embeddings for improved plagiarism detection.\n- Introduced syntax weighting in sentence representations for better textual similarity detection.\n- Combined methods to achieve an F1 score of 89.15% for English-French similarity detection.\n- Demonstrated significant improvement over existing methods with new approaches.\n- Evaluated methods on a challenging multilingual corpus with diverse text sources.\n\nSlide Title: Evaluation Dataset and Conditions\nBullet Points:\n- Used a multilingual dataset with French, English, and Spanish texts for evaluation.\n- Dataset includes cross-language alignment at document, sentence, and chunk levels.\n- Contains both human and machine-translated texts with varying named entity percentages.\n- Partially obfuscated dataset to challenge cross-language similarity detection.\n- Focused on French and English sub-corpora for this study.\n\nSlide Title: Overview of State-of-the-Art Methods\nBullet Points:\n- Cross-Language Character N-Gram (CL-CnG) compares text using 3-gram vectors.\n- CL-CTS measures semantic similarity using abstract concepts from lexical resources.\n- CL-ASA uses bilingual unigram dictionaries for translation-based similarity analysis.\n- CL-ESA represents document meaning with vectors derived from Wikipedia concepts.\n- Translation + Monolingual Analysis translates units for monolingual comparison.\n\nSlide Title: Proposed Methods Using Word Embeddings\nBullet Points:\n- Word embeddings project words into a continuous space based on context.\n- CL-CTS-WE replaces lexical resources with word embeddings for similarity detection.\n- CL-WES uses cosine similarity on sentence embeddings for cross-language comparison.\n- MultiVec toolkit used for computing continuous text representations.\n- Word embeddings capture synonymy and morphological closeness for improved detection.\n\nSlide Title: Improving Textual Similarity with Syntax Weighting\nBullet Points:\n- CL-WESS enhances CL-WES by incorporating syntax information into similarity measures.\n- Part-of-speech tagging and weighting optimize sentence representation vectors.\n- Condor algorithm used to determine optimal weights for improved F1 scores.\n- CL-WESS outperforms previous methods with significant performance gains.\n- Syntax weighting proves crucial for accurate cross-language similarity detection.\n\nSlide Title: Combining Methods for Enhanced Detection\nBullet Points:\n- Weighted fusion combines methods to improve cross-language similarity detection.\n- Decision tree fusion outperforms weighted fusion with higher F1 scores.\n- Methods show unique clustering capacities, enhancing classification accuracy.\n- Decision tree fusion involves CL-C3G, CL-WESS, and CL-CTS-WE for optimal results.\n- Fusion confirms complementarity of methods and utility of embeddings.\n\nSlide Title: Results and Discussion\nBullet Points:\n- Word embeddings improve CL-CTS performance, but CL-WESS shows the most promise.\n- CL-WESS significantly outperforms CL-C3G and other state-of-the-art methods.\n- Decision tree fusion achieves highest F1 scores at both chunk and sentence levels.\n- Fusion methods demonstrate the complementarity of different approaches.\n- Embeddings prove valuable for cross-language textual similarity detection.\n\nSlide Title: Conclusion and Future Perspectives\nBullet Points:\n- Augmented baseline approaches with word embeddings for improved detection.\n- CL-WESS emerges as the most effective method for cross-language similarity.\n- Fusion of methods significantly enhances detection performance.\n- Future work includes refining CL-WESS and exploring syntactic weight adaptations.\n- Participation in SemEval-2017 Task 1 for further evaluation and development.",
    "Slide Title: Introduction to Copy & Paste in NLP\nBullet Points:\n- Copy & paste is essential for document creation and analysis in NLP.\n- Archives enable the study of reuse and plagiarism in NLP papers.\n- Workshop on \"Rediscovering 50 Years of Discoveries in NLP\" initiated this research.\n- Automatic plagiarism detection aids in identifying potential sources for submitted work.\n- PAN international competition focuses on plagiarism detection annually.\n\nSlide Title: Objectives of the Study\nBullet Points:\n- Aim to explore entry-level detection of reuse and plagiarism in NLP.\n- Focus on comparing verbatim raw strings with linguistic parsing results.\n- Study practices and ascertainments in the NLP field.\n- Not focused on state-of-art metrics or algorithms for detection.\n- Extrinsic detection aims to find near-matches between texts.\n\nSlide Title: The NLP4NLP Corpus\nBullet Points:\n- NLP4NLP corpus contains 65,003 documents from conferences and journals.\n- Covers written and spoken language processing in NLP.\n- Documents span 50 years, from 1965 to 2015.\n- Preprocessing applied to represent sources in a common format.\n- Metadata transformed into a common BibTeX format under UTF8.\n\nSlide Title: Definitions and Terminology\nBullet Points:\n- \"Self-reuse\" involves cited copy & paste by the same authors.\n- \"Self-plagiarism\" involves uncited copy & paste by the same authors.\n- \"Reuse\" involves cited copy & paste by different authors.\n- \"Plagiarism\" involves uncited copy & paste by different authors.\n- Terms clarify citation practices in plagiarism detection.\n\nSlide Title: Algorithm for Detection\nBullet Points:\n- Word sequence comparison effective for detecting copy & paste.\n- Sliding windows of lemmas built for each document.\n- Focus index created for optimizing comparison phase.\n- Jaccard distance used to compute similarity scores.\n- Significant similarity scores filtered based on thresholds.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Linguistic processing strategy provides more pairs than raw text.\n- Caesura and lexical variations cause differences in results.\n- Self-reuse and self-plagiarism more common than reuse and plagiarism.\n- 18% of documents involve self-reuse or self-plagiarism.\n- Plagiarism cases are rare, representing only 0.23% of papers.\n\nSlide Title: Time Delay and Publication Patterns\nBullet Points:\n- 38% of similar papers published in the same year.\n- Average duration between publication and reuse is 1.22 years.\n- Conference papers often reused in journal papers within a year.\n- Self-reuse and self-plagiarism often occur in the same year.\n- Publication patterns influenced by conference and journal schedules.\n\nSlide Title: Discussion and Ethical Considerations\nBullet Points:\n- Self-reuse more prevalent than reusing others' content.\n- Legal issues arise if plagiarism violates copyright.\n- Ethical behavior maintained by following reuse and plagiarism principles.\n- Self-reuse justified by restating groundwork or addressing new evidence.\n- Further developments needed to identify altered copy & paste.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- First study on copy & paste in large NLP corpora.\n- Simple n-gram comparison method used for detection.\n- Self-reuse and self-plagiarism common due to publish-and-perish demand.\n- Plagiarism uncommon in the NLP community.\n- Future work to address graphical variations and non-native speaker practices.",
    "Slide Title: Introduction to Split-and-Rephrase Task\nBullet Points:\n- Processing long, complex sentences is challenging for humans and NLP tasks.\n- Split-and-Rephrase task aims to break complex sentences into simple ones.\n- Narayan et al. introduced a dataset and evaluation method for this task.\n- Two system setups: text-to-text and semantics-augmented with RDF triples.\n- Focus on text-to-text setup, achieving 77.5 BLEU score with SEQ2SEQ models.\n\nSlide Title: Challenges in Current Models\nBullet Points:\n- Manual inspection reveals unwanted behaviors in SEQ2SEQ outputs.\n- Unsupported facts: correct facts not mentioned in input sentences.\n- Repeated facts: same fact mentioned in multiple output sentences.\n- Missing facts: facts in input omitted in output sentences.\n- Model memorizes entity-fact pairs instead of learning to split and rephrase.\n\nSlide Title: Preliminary Experiments and Task Definition\nBullet Points:\n- Task: Split complex sentence into simple sentences conveying same information.\n- Dataset associates sentences with RDF triples for additional supervision.\n- Evaluation uses averaged individual multi-reference BLEU score.\n- Models trained without access to RDF triples during train or test time.\n- Comparison with baselines proposed by Narayan et al. (2017).\n\nSlide Title: Results and Analysis\nBullet Points:\n- SEQ2SEQ models outperform baseline with up to 28.35 BLEU improvement.\n- Single-layer models achieve better results than 3-layer models.\n- Analysis reveals unsupported facts, repetitions, and missing facts.\n- Attention weights focus heavily on first entity, causing hallucinated facts.\n- 97.16% of predicted sentences appear verbatim in training set.\n\nSlide Title: New Data-Split for Better Generalization\nBullet Points:\n- Original data-split susceptible to \"cheating\" by fact memorization.\n- New split divides 5,554 complex sentences into train, development, and test sets.\n- Ensures every RDF relation is represented in training set.\n- RDF triplets represented only in one of the splits.\n- New split balances challenge and feasibility for models.\n\nSlide Title: Copy-Augmented Model for Improved Performance\nBullet Points:\n- Augment SEQ2SEQ models with a copy mechanism for better performance.\n- Copy mechanism beneficial in tasks like abstractive summarization.\n- Biasing model towards copying improves performance on split-and-rephrase task.\n- Copy switch probability interpolates between target vocabulary and source tokens.\n- Copy-augmented models reduce unsupported and repeated sentences.\n\nSlide Title: Experiments and Results on New Data-Split\nBullet Points:\n- Models with larger capacities tend to memorize training data.\n- Experiments with copy-enhanced models of varying LSTM widths.\n- Models trained on original, proposed, and v1.0 data splits.\n- Copy-augmented models perform better on both data splits.\n- New competitive baseline established for split-and-rephrase task.\n\nSlide Title: Conclusions and Future Directions\nBullet Points:\n- SEQ2SEQ models achieve high scores without learning to split-and-rephrase.\n- New challenging data-split proposed to remedy this issue.\n- Copy-mechanism improves performance, but task remains unsolved.\n- Encourage future research on proposed split or version 1.0 dataset.\n- Larger dataset addresses overlap issues and offers new evaluation opportunities.",
    "Slide Title: Introduction to Bilingual Lexicon Induction\nBullet Points:\n- New methods for bilingual lexicon induction target low-resource language pairs with limited parallel data.\n- Multilingual word embeddings embed vocabularies in the same latent space for cross-lingual tasks.\n- Recent advances focus on unsupervised methods, reducing reliance on large seed dictionaries.\n- Linguistic features like edit distance and orthographic similarity can enhance performance.\n- This work extends modern embedding-based approaches with orthographic information for accuracy.\n\nSlide Title: Background and Methodology\nBullet Points:\n- Based on Artetxe et al. (2017), mapping matrices align source and target word embeddings.\n- Optimal mapping minimizes Euclidean distances between source and target embeddings.\n- Iterative self-learning framework refines mapping and dictionary until convergence.\n- Seed dictionary requirement reduced using numeral identity translations.\n- Two orthographic extension methods proposed to enhance the system.\n\nSlide Title: Orthographic Extension of Word Embeddings\nBullet Points:\n- Augments embeddings with vectors representing character counts from both languages' alphabets.\n- Character count vectors scaled and appended to base word embeddings.\n- Resulting augmented vectors normalized to magnitude 1 for consistency.\n- New matrices replace original embeddings in the self-learning process.\n- Enhances similarity detection between related languages.\n\nSlide Title: Orthographic Similarity Adjustment\nBullet Points:\n- Modifies similarity scores using normalized string edit distance between word pairs.\n- Levenshtein distance measures transformations needed to convert one word to another.\n- Orthographic similarity matrix added to standard similarity matrix for refinement.\n- Symmetric Delete algorithm identifies source-target pairs with similar orthographic features.\n- Reduces computational complexity by focusing on a subset of word pairs.\n\nSlide Title: Experiments and Datasets\nBullet Points:\n- Datasets include English-Italian, English-German, and English-Finnish language pairs.\n- Monolingual word embeddings trained with word2vec for each language.\n- Automatically-generated numeral identity translations used as input dictionary.\n- Hyperparameters tuned using a portion of the training set for development.\n- Experiments assess effectiveness of orthographic extensions and similarity adjustments.\n\nSlide Title: Results and Discussion\nBullet Points:\n- Orthographic extension increased embedding size by union of both languages' alphabets.\n- Orthographic similarity adjustment identified 2 million word pairs, reducing computation.\n- Scaling factors optimized based on development data results for best performance.\n- Identity translation component improved accuracy, especially for English-Finnish.\n- Extension method learns mapping between source and target letters, enhancing performance.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Two techniques improve bilingual lexicon induction using orthographic information.\n- Methods increase accuracy without parallel data, suitable for low-resource languages.\n- Combined and embedding extension methods outperform baseline systems.\n- Future work to explore related languages with different alphabets and transliteration.\n- Potential to extend methods to other unsupervised bilingual lexicon induction systems.",
    "Slide Title: Introduction to Semantic Graphs\nBullet Points:\n- Semantic graphs like WordNet encode language structure as human knowledge representation.\n- Local properties focus on specific semantic concept connections through relations like hypernymy.\n- Global properties encode emergent regularities but are rarely used in prediction settings.\n- Global graph features can aid local tasks like relation prediction by constraining predictions.\n- Exponential Random Graph Model (ERGM) helps estimate weights on local and global graph features.\n\nSlide Title: Extending ERGM for Semantic Graphs\nBullet Points:\n- ERGM framework extended with a margin-based objective for semantic graphs.\n- Max-Margin Markov Graph Model (M3GM) scales to WordNet's 10^5 nodes.\n- M3GM incorporates labeled motifs to address multi-relational nature of semantic graphs.\n- Combines M3GM with dyad-level model over word sense embeddings for improved predictions.\n- M3GM yields 3-4 point improvements on WN18RR dataset metrics.\n\nSlide Title: Related Work in Relation Prediction\nBullet Points:\n- Relation prediction often uses embedding-based methods without structural information.\n- Compositional models like bilinear tensors and translational models are common.\n- Recent works explore path prediction and automatic relation detection from text.\n- Graph Convolutional Networks consider high-order neighborhood properties for predictions.\n- M3GM introduces interaction features between relations for relation prediction.\n\nSlide Title: Graph Motifs as Features\nBullet Points:\n- Graph features include edge count, cycles, and paths of specific lengths.\n- Semantic graphs are multigraphs with multiple relation types over common nodes.\n- Combinatory motifs extracted for relation-sensitive features in semantic graphs.\n- Feature space expanded to include relation singletons, pairs, or triples.\n- Approximately 3,000 features generated for 11-relation WordNet version.\n\nSlide Title: Local Score Component and Parameter Estimation\nBullet Points:\n- Nodes in semantic graphs represent synsets with valuable information for predictions.\n- Association operator generalizes models like TransE, BiLin, and DistMult.\n- ERGM requires normalization over all possible graphs, approximated using MCMLE.\n- Margin loss objective ensures negative samples score lower than observed network.\n- Proposal distribution samples negative edges based on local association scores.\n\nSlide Title: Relation Prediction and WN18RR Dataset\nBullet Points:\n- Relation prediction task involves predicting hidden entities in labeled edge sets.\n- WN18RR dataset used for evaluation, addressing leakage in original WN18 dataset.\n- Metrics include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at k (H@k).\n- M3GM re-ranks top local candidates using graph score signal for improved results.\n- Synset embeddings initialized using methods like averaging and AutoExtend+Mimick.\n\nSlide Title: Results and Analysis\nBullet Points:\n- M3GM re-ranking improves performance on link prediction tasks.\n- Graph score weighted against association component per relation for best results.\n- Synset embedding initialization impacts performance, with FastText showing better results.\n- M3GM captures structural coherence, enhancing global graph structure understanding.\n- Analysis reveals M3GM's learning about WordNet's relation interactions and motifs.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- M3GM combines distributional coherence with structural coherence for semantic graphs.\n- Substantial improvements in link prediction through re-ranking method.\n- Potential for M3GM to score bundles of new edges for semantic graph extension.\n- Future work includes exploring multilingual applications and knowledge graph extensions.\n- Structural parameters from English WordNet may transfer to other languages.",
    "Slide Title: Introduction to Geolocation Prediction\nBullet Points:\n- Social media is a key source for analyzing public opinions and trends.\n- Geolocation prediction helps in disaster, disease, and political analysis.\n- Twitter is preferred for geolocation due to geotags and metadata.\n- Neural networks offer advanced methods for geolocation prediction.\n- Our model combines text, metadata, and user network with attention mechanisms.\n\nSlide Title: Related Works in Geolocation Prediction\nBullet Points:\n- Text-based approaches use word distributions for location estimation.\n- User-network approaches leverage social interactions for geolocation.\n- Metadata-based methods utilize location fields for prediction accuracy.\n- Combinatory approaches integrate multiple data types for better results.\n- Our model offers an alternative to existing ensemble methods.\n\nSlide Title: Proposed Neural Network Model\nBullet Points:\n- Combines text, metadata, and user network information.\n- Utilizes attention mechanisms for unified data representation.\n- Integrates user network information with city and user embeddings.\n- Processes text with RNN and attention layers for message representation.\n- Aims to outperform previous models in accuracy and efficiency.\n\nSlide Title: Text and Metadata Components\nBullet Points:\n- Text component uses RNN and attention for message timeline processing.\n- Metadata includes location, description, and timezone fields.\n- Shared word embeddings for text, location, and description.\n- Attention layers combine text and metadata for user representation.\n- Timezone embeddings enhance metadata integration.\n\nSlide Title: User Network Component\nBullet Points:\n- Constructs user networks from social media interactions.\n- Uses city and user embeddings for linked users and cities.\n- Attention layers create user network representations.\n- Reduces network size by filtering based on location data.\n- Enhances geolocation prediction with network information.\n\nSlide Title: Evaluation and Baseline Comparisons\nBullet Points:\n- Evaluated using TwitterUS and W-NUT datasets.\n- Baseline models include logistic regression and ensemble methods.\n- Our model shows significant accuracy improvements over baselines.\n- Metrics include accuracy, accuracy@161, and error distances.\n- Statistical tests confirm model's superior performance.\n\nSlide Title: Results and Performance Analysis\nBullet Points:\n- Achieved 3.8% increase in accuracy on W-NUT dataset.\n- Improved accuracy@161 by 6.6% over baseline models.\n- Analyzed attention probabilities for insight into model processes.\n- Attention patterns reveal effective data unification strategies.\n- Limitations include handling location changes and travel states.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Proposed model unifies text, metadata, and user network for geolocation.\n- Demonstrated significant improvements over state-of-the-art models.\n- Future work includes handling multiple locations and temporal states.\n- Potential applications in gender and age analysis on social media.\n- User network information crucial for diverse social media analyses.",
    "Slide Title: Introduction to NLP Learning Methods\nBullet Points:\n- NLP learning methods increasingly use end-to-end differentiable functions for gradient-based optimization.\n- Traditional NLP relied on modular processing stages forming a pipeline for text analysis.\n- Pipelines make discrete decisions, often seen as incompatible with neural learning.\n- Recent research shows benefits of linguistic preprocessing in neural architectures.\n- Solutions include reinforcement learning and structured attention for NLP tasks.\n\nSlide Title: Challenges with Current Methods\nBullet Points:\n- Reinforcement learning and structured attention replace argmax with sampling or marginalization.\n- Not all argmax operations have efficient sampling or marginalization methods.\n- Inspection of intermediate outputs is easier with hard decisions than posteriors.\n- Straight-through estimator treats discrete decisions as differentiable, ignoring argmax constraints.\n- Quality of intermediate representations degrades under the straight-through estimator.\n\nSlide Title: Introduction to SPIGOT\nBullet Points:\n- SPIGOT introduces a new method for structured projection of intermediate gradients.\n- Defines a proxy for gradient of loss function with respect to argmax input.\n- SPIGOT respects constraints in the argmax problem, unlike STE.\n- Applicable with any intermediate layer expressible as a constrained maximization problem.\n- Empirical results show SPIGOT's effectiveness even with approximate maximization and projection.\n\nSlide Title: Methodology of SPIGOT\nBullet Points:\n- SPIGOT allows structured argmax layer to be treated like a differentiable function.\n- Forward calculation calls parser, passing parse tree to next layer for processing.\n- Backward computation introduces a proxy for gradient of loss function.\n- SPIGOT uses projection of gradients to respect argmax constraints.\n- Designed to improve earlier methods that ignore argmax operation constraints.\n\nSlide Title: Relaxed Decoding and SPIGOT\nBullet Points:\n- Decoding problems decomposed into parts, each part included in output structure.\n- Scoring function decomposes into part-local, input-specific scores.\n- Argmax problem often NP-complete, relaxed to make decoding tractable.\n- SPIGOT views structured argmax as activation function for input-specific part scores.\n- Projection step keeps updated predictions within feasible set.\n\nSlide Title: Backpropagation through Pipelines\nBullet Points:\n- SPIGOT enables backpropagation through NLP pipelines with intermediate task outputs.\n- Parameters divided into intermediate task model and end task model.\n- SPIGOT applicable to various training scenarios, including latent structure induction.\n- Joint learning minimizes interpolation of end-task and intermediate-task loss functions.\n- Algorithm 1 describes forward and backward computation with SPIGOT.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Evaluated SPIGOT with syntactic tree structures in semantic dependency parsing.\n- Syntactic-then-semantic parsing uses intermediate syntactic parsing task.\n- SPIGOT outperforms baselines in semantic dependency parsing performance.\n- Semantic dependencies improve sentiment classification accuracy.\n- SPIGOT achieves stronger performance than state-of-the-art systems.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- SPIGOT offers a novel approach to backpropagating through neural architectures with discrete decisions.\n- Employs projection to respect constraints in intermediate tasks.\n- Demonstrated stronger performance in semantic parsing and sentiment classification.\n- Future work could explore more syntactic information for better performance.\n- SPIGOT implementation available at https://github.com/Noahs-ARK/SPIGOT.",
    "Slide Title: Introduction to Story Understanding\nBullet Points:\n- Understanding stories involves reasoning about causal links and characters' mental states.\n- Commonsense story cloze task highlights challenges for machine readers in story comprehension.\n- Language models struggle with abstracting complex implied dynamics in stories.\n- New annotation formalism labels mental states in commonsense short stories.\n- Dataset provides motivations and emotional reactions as pre- and post-conditions of events.\n\nSlide Title: Mental State Representations\nBullet Points:\n- Understanding actions, motivations, and emotions is a focus in philosophy and psychology.\n- Categorical labels derived from motivation and emotion theories annotate causal dynamics.\n- Maslow's hierarchy and Reiss's basic motives used for motivation categories.\n- Plutchik's wheel of emotions used for categorizing emotional dimensions.\n- Open text descriptions complement theory categories for mental state explanations.\n\nSlide Title: Annotation Framework and Challenges\nBullet Points:\n- Simple commonsense stories pose challenges for natural language understanding models.\n- Annotations require interpreting discourse and understanding implicit causal effects.\n- New annotation framework pipelines smaller tasks for efficient data collection.\n- Crowdsourced workers from Amazon Mechanical Turk used for annotations.\n- Unique challenges include understanding formal psychology theory categories.\n\nSlide Title: Annotation Pipeline Components\nBullet Points:\n- Entity Resolution identifies characters and sentences where they are mentioned.\n- Action Resolution determines if a character's action has an attributable motivation.\n- Affect Resolution identifies lines where characters have emotional reactions.\n- Motivation task annotates character motives using Maslow and Reiss categories.\n- Emotional Reaction task describes character emotions using Plutchik categories.\n\nSlide Title: Dataset Statistics and Insights\nBullet Points:\n- Dataset includes 300k annotations for motivation and emotion across 15,000 stories.\n- Over 150,000 character-line pairs annotated for motivations and emotional changes.\n- Theory category assignments are costly, so only a third of stories have them.\n- Training data uses open text descriptions for motivations and emotions.\n- Agreement analysis shows high scores for Reiss and Plutchik categories.\n\nSlide Title: Tasks and Baseline Models\nBullet Points:\n- Dataset supports multiple tasks: state classification, annotation classification, and explanation generation.\n- State classification predicts psychological states using Maslow, Reiss, and Plutchik labels.\n- Annotation classification predicts correct labels from emotional or motivation explanations.\n- Explanation generation trains models to describe psychological states in free text.\n- Baseline models use various encoders and classifiers for task evaluation.\n\nSlide Title: Experimental Results and Insights\nBullet Points:\n- Models outperform random baselines in categorizing Maslow, Reiss, and Plutchik states.\n- Contextual information improves model predictions of characters' mental states.\n- Pretraining on free-response data boosts performance in category prediction tasks.\n- Explanation generation models outperform baselines, indicating semantic closeness.\n- Dataset serves as a resource for evaluating models on emotional reactions and motivations.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Large-scale dataset for mental state tracking in commonsense stories presented.\n- Over 300k annotations for character motivations and emotional reactions included.\n- Modeling character-specific context and pretraining improve labeling performance.\n- Dataset can be a testbed for models learning common sense about emotions and motivations.\n- Future work may explore integrating additional resources for enhanced story understanding.",
    "Slide Title: Introduction to Satire and Satire Detection\nBullet Points:\n- Satire uses humor, irony, and exaggeration to entertain and criticize (Sulzer, 1771).\n- Satirical news mimics regular news without intending to deceive readers.\n- Satire detection distinguishes satirical from regular news, aiding knowledge base accuracy.\n- Automatic satire detection is challenging, even for human readers.\n- New model proposed to debias satire detection from publication characteristics.\n\nSlide Title: Challenges in Satire Detection\nBullet Points:\n- Previous models rely on publication source labels, affecting satire understanding.\n- Models may misinterpret publisher traits as satire characteristics.\n- Generalization to new sources is hindered by publisher-based labeling.\n- Adversarial training proposed to focus on satire, not publication traits.\n- New German dataset introduced for improved satire detection research.\n\nSlide Title: Previous Work in Satire Detection\nBullet Points:\n- Early models used handcrafted features like entity context and story coherence.\n- Recent models employ neural networks with word embeddings for satire detection.\n- Hierarchical models with attention over sentences were common.\n- Our model uses word-level attention, reducing parameters.\n- Adversarial training improves model robustness against domain-specific biases.\n\nSlide Title: Methods for Satire Classification\nBullet Points:\n- Previous datasets lacked publication source information, limiting analysis.\n- New corpus includes publication data to test hypothesis on source influence.\n- Model uses shared feature extractor for satire and publication classification.\n- Adversarial training controls for publication source bias in satire detection.\n- Word embeddings pretrained on corpus using word2vec for input representation.\n\nSlide Title: Model Architecture and Training\nBullet Points:\n- Feature extractor uses bidirectional LSTM and self-attention layer.\n- Satire detector performs binary classification on document level.\n- Publication identifier performs multi-class classification on publication source.\n- Adversarial training updates feature extractor to fool publication identifier.\n- Learning rate and gradient reversal weight tuned for optimal performance.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Satire detection evaluated using precision, recall, and F1 score.\n- Publication identification assessed with weighted macro precision, recall, and F1.\n- Adversarial training reduces publication identification accuracy.\n- Satire classification remains effective despite publication bias control.\n- Overlapping features for satire and publication classification observed.\n\nSlide Title: Analysis of Adversarial Training Impact\nBullet Points:\n- Baseline model predicts publication source accurately, affecting satire focus.\n- Adversarial model shifts focus to linguistic satire cues, ignoring publication traits.\n- Attention weights show adversarial model identifies humor in satire.\n- Adversarial training helps model focus on satire characteristics.\n- Further analysis needed on other confounding variables like time and region.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Adversarial training reduces publication bias in satire detection models.\n- Model maintains satire detection performance while controlling for source bias.\n- Future research to explore other confounding variables in satire detection.\n- Quantitative and qualitative analysis to enhance understanding of model behavior.\n- Potential for improved satire detection across diverse datasets and contexts.",
    "Slide Title: Introduction to Deep Learning Sentence Representations\nBullet Points:\n- Deep learning enables automatic learning of continuous sentence representations for various tasks.\n- Sentence representations can be task-specific or aim for universal applicability across tasks.\n- Representations can be learned unsupervised or supervised with annotated training sets.\n- Neural machine translation (NMT) models offer a different approach to sentence representations.\n- Attention mechanism in NMT allows dynamic focus on important parts of the source sentence.\n\nSlide Title: Advances in Neural Machine Translation\nBullet Points:\n- NMT has improved translation quality, impacting learned sentence representations.\n- Attention mechanism central to NMT, introduced by Vaswani et al. (2017).\n- Attention allows dynamic focus, removing static vector representation limitations.\n- Proposed novel encoder-decoder architecture with structured fixed-size input representation.\n- Evaluated on English-to-German and English-to-Czech translation tasks.\n\nSlide Title: Related Work on Sentence Representations\nBullet Points:\n- Continuous sentence representations have been a focus in neural machine translation research.\n- RNN sequence-to-sequence models explored semantic and syntactic structure in embeddings.\n- Hill et al. (2016) evaluated sentence representations across various models and tasks.\n- Schwenk and Douze (2017) aimed for language-independent sentence representations.\n- Attention mechanism not considered in some previous works.\n\nSlide Title: Proposed Model Architectures\nBullet Points:\n- Proposed models differ in encoder state processing, combination, and decoder usage.\n- Compound attention model incorporates attention in both encoder and decoder.\n- Inner attention used to compute weighted averages of encoder states.\n- Structured sentence embedding leveraged by attentive decoder.\n- Transformer model modified with inner attention for fixed-size sentence representation.\n\nSlide Title: Evaluation of Sentence Representations\nBullet Points:\n- Continuous sentence representations evaluated using classification and similarity tasks.\n- SentEval tool used for evaluation on 10 classification and 7 similarity tasks.\n- Paraphrase evaluation conducted using COCO and HyTER Networks datasets.\n- Metrics include cluster classification accuracy, nearest-neighbor retrieval, and Davies-Bouldin index.\n- Representation quality assessed in relation to translation quality.\n\nSlide Title: Experimental Results and Translation Quality\nBullet Points:\n- English-to-German and English-to-Czech NMT models trained and evaluated.\n- Translation quality estimated using BLEU score for single-reference case-sensitive evaluation.\n- ATTN Bahdanau et al. model performed best, followed by Transformer and ATTN-ATTN.\n- Larger representation size and more attention heads generally improved BLEU score.\n- Manual ranking confirmed automatic evaluation results.\n\nSlide Title: SentEval and Paraphrase Evaluation Results\nBullet Points:\n- SentEval results averaged for classification and similarity tasks.\n- More attention heads negatively impacted performance on cs models.\n- cs-ATTN-ATTN performed well with 1 attention head but poorly with more.\n- Supervised InferSent model outperformed others in most tasks.\n- Paraphrase evaluation showed InferSent as the best, followed by non-attentive cs models.\n\nSlide Title: Discussion and Conclusion\nBullet Points:\n- Representation metrics generally negatively correlated with BLEU score.\n- Multi-headed sentence representation less effective for paraphrase representation.\n- Proposed compound attention model did not perform well in meaning representation.\n- High BLEU score does not indicate good meaning representation.\n- Bilingual MT less likely to provide useful data for representation learning.",
    "Slide Title: Introduction to Neural Sequence-to-Sequence Models\nBullet Points:\n- SEQ2SEQ models excel in tasks like machine translation and syntactic parsing.\n- Require large parallel datasets, prompting research on non-parallel corpora.\n- Unsupervised SEQ2SEQ models proposed for summarization with minimal parallel data.\n- SEQ3 autoencoder introduced for end-to-end training via gradient optimization.\n- SEQ3 uses differentiable approximations, outperforming reinforcement learning.\n\nSlide Title: SEQ3 Model Overview\nBullet Points:\n- SEQ3 consists of two attentional encoder-decoder pairs: compressor and reconstructor.\n- Compressor generates a summary from input text, reconstructor reproduces input.\n- Communication through discrete words of the summary only.\n- Language model prior ensures human-readable summaries.\n- Topic loss rewards summaries with topic-indicating words.\n\nSlide Title: Compressor Functionality\nBullet Points:\n- Embedding layer projects source sequence to word embeddings.\n- Bidirectional RNN encodes embeddings, producing concatenated states.\n- Attentional RNN decoder generates summary with global attention.\n- Probability distribution over vocabulary computed for each summary word.\n- Output length controlled by sampling from a uniform distribution.\n\nSlide Title: Differentiable Word Sampling Techniques\nBullet Points:\n- Soft-Argmax used to pass weighted sum of word embeddings during training.\n- Gumbel-Softmax trick adds stochasticity, facilitating parameter exploration.\n- Straight-Through estimator discretizes embeddings in forward pass.\n- Continuous embeddings used in backward pass for gradient computation.\n- Techniques outperform scheduled sampling and reinforcement learning.\n\nSlide Title: Reconstructor and Decoder Initialization\nBullet Points:\n- Reconstructor operates on embeddings of summary words.\n- Decoder initialized with transformation of bidirectional encoder's last states.\n- Length vector includes target summary length and compression ratio.\n- Initialization ensures effective reconstruction of input text.\n- Similar initialization used for reconstructor's decoder.\n\nSlide Title: Loss Functions in SEQ3\nBullet Points:\n- Reconstruction loss measures log-likelihood of correctly reconstructed words.\n- Language model prior loss ensures readability of summaries.\n- Topic loss encourages preservation of topic-indicating words in summaries.\n- Length penalty helps predict end-of-sequence token accurately.\n- Loss functions work in synergy to improve model performance.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- SEQ3 trained on Gigaword dataset, tested on DUC-2003 and DUC-2004 tasks.\n- Outperforms unsupervised Pretrained Generator and LEAD-8 baseline.\n- Removal of LM prior or topic loss affects performance negatively.\n- SEQ3 shows extractive and abstractive capabilities in sampled outputs.\n- Results indicate potential for SEQ3 in other tasks like machine translation.\n\nSlide Title: Limitations and Future Work\nBullet Points:\n- Model tends to copy first words of input in compressed text.\n- Autoregressive reconstructor causes cascading errors in generated sequence.\n- Future work to explore SEQ3 in unsupervised machine translation and caption generation.\n- Potential modification to improve sequence generation and compression.\n- Continued exploration of SEQ3's capabilities in various NLP tasks.",
    "Slide Title: Introduction to Taylor's Law\nBullet Points:\n- Taylor's law describes variance growth with mean in event clustering systems.\n- Applied across domains: ecology, physics, finance, human dynamics, and more.\n- Limited analysis in natural language; Gerlach and Altmann (2014) notable exception.\n- Study applies Taylor's law to lexical fluctuation in texts.\n- Taylor exponent found universal across languages and text types.\n\nSlide Title: Related Work and Fluctuation Analysis\nBullet Points:\n- Quantifying text complexity involves entropy rate and scaling behaviors.\n- Zipf's and Heaps' laws are famous linguistic power laws.\n- Fluctuation analysis quantifies event clustering bias.\n- Long-range correlation analysis applied to text intervals.\n- Taylor's law shows vast applicability in written texts.\n\nSlide Title: Measuring the Taylor Exponent\nBullet Points:\n- Taylor's law holds when mean and standard deviation correlate by power law.\n- Exponent \u03b1 ranges from 0.5 to 1.0 across domains.\n- \u03b1 = 0.5 for i.i.d processes; \u03b1 = 1.0 for consistent word proportions.\n- Taylor exponent indicates systematic word co-occurrence.\n- Least-squares method used for exponent estimation.\n\nSlide Title: Taylor Exponents for Real Data\nBullet Points:\n- Natural language texts exhibit power-law distribution in word frequency.\n- Taylor exponent for texts around 0.58, indicating moderate coherence.\n- Exponent decreases slightly with larger text size.\n- Different data types show varying Taylor exponents.\n- Exponents for programming and music data higher than natural language.\n\nSlide Title: Data Source and Language Influence\nBullet Points:\n- Taylor exponent varies with data source: enwiki8, child-directed speech, programming.\n- Chinese texts show larger exponents than Indo-European languages.\n- Statistical tests confirm significant differences in language exponents.\n- Template-like data sources exhibit higher Taylor exponents.\n- Future work to explore Twitter data and adult utterances.\n\nSlide Title: Evaluating Machine-Generated Text\nBullet Points:\n- Machine-generated text often has Taylor exponent of 0.50.\n- Character-based LSTM models fail to capture word clustering.\n- Neural machine translation retains original text's clustering behavior.\n- Taylor exponent correlates with model perplexity.\n- Potential for Taylor exponent in evaluating language model quality.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Taylor's law quantifies word co-occurrence in natural language texts.\n- Exponent of 0.58 indicates complexity beyond random processes.\n- Application to machine-generated text reveals model limitations.\n- Future work to explore grammatical complexity and statistical tests.\n- Taylor's law offers insights into structural complexity of sequences.",
    "Slide Title: Introduction to Deep Learning in Question Answering\nBullet Points:\n- Deep learning applied to question answering tasks for images, tables, and text.\n- High accuracy models require representative test sets for real-world tasks.\n- Techniques proposed to analyze model sensitivity to question words.\n- Example: Visual Question Answering model's reliance on specific words.\n- Humans can identify important question words for model evaluation.\n\nSlide Title: Our Contributions and Analysis Workflow\nBullet Points:\n- Analyzed three question answering models using Integrated Gradients (IG).\n- Visualizations of attributions simplify model analysis.\n- Identified model weaknesses and crafted adversarial questions.\n- Developed overstability test for question answering networks.\n- Improved attack success rate using attribution insights.\n\nSlide Title: Visual Question Answering Analysis\nBullet Points:\n- Analyzed Kazemi and Elqursh's deep network for image-based questions.\n- Network relies heavily on images, ignoring many question words.\n- Accuracy drops significantly with content-free prefixes.\n- Overstability test shows reliance on generic words like \"color.\"\n- Attacks exploit network's reliance on unimportant words.\n\nSlide Title: Question Answering on Tables\nBullet Points:\n- Analyzed Neural Programmer (NP) for tabular data questions.\n- NP influenced by content-free words more than important ones.\n- Dropping content-free words reduces validation accuracy.\n- Overstability test shows reliance on few words for accuracy.\n- Attacks include question concatenation and stop word deletion.\n\nSlide Title: Reading Comprehension Model Analysis\nBullet Points:\n- Analyzed Yu et al.'s model for text-based question answering.\n- Adversarial attacks by Jia and Liang tested using attributions.\n- Successful attacks modify low-attribution contentful words.\n- High-attribution words missing in adversarial sentences lead to failures.\n- Attribution insights improve attack effectiveness.\n\nSlide Title: Integrated Gradients (IG) Technique\nBullet Points:\n- IG isolates question words used by deep learning systems.\n- Attributions sum to difference between input and baseline probabilities.\n- Uninfluential variables receive no attribution, influential ones do.\n- IG empirically validated via question perturbations.\n- IG helps craft attacks by identifying high-attribution terms.\n\nSlide Title: Related Work and Methodology\nBullet Points:\n- Inspired by Jia and Liang's work on reading comprehension systems.\n- Our method improves analysis by identifying high-attribution terms.\n- Focused on deep-learning-based systems for various tasks.\n- IG chosen for ease and efficiency of implementation.\n- Compared to other attribution methods and model-agnostic techniques.\n\nSlide Title: Conclusion and Reproducibility\nBullet Points:\n- Attributions reveal weaknesses in question answering models.\n- Workflow aids developers in improving model quality.\n- Under-reliance on important question terms is a significant issue.\n- Attribution-based methods gauge model weaknesses effectively.\n- Code for reproducing results available on GitHub.",
    "Slide Title: Introduction to Constituency Parsing\nBullet Points:\n- Fast and accurate constituency parsing is crucial in natural language processing tasks.\n- Parsing aids in tasks like relation extraction, paraphrase detection, and machine translation.\n- Neural network-based approaches have shown competitive results in constituency parsing.\n- Transition-based models decompose prediction into local decisions, leading to compounding errors.\n- Chart-based models use structured loss functions but have higher computational costs.\n\nSlide Title: Proposed Model for Constituency Parsing\nBullet Points:\n- Introduces a novel, fully-parallel model based on \"syntactic distance\" for parsing.\n- Model constructs parse trees by recursively splitting constituents using syntactic distances.\n- Syntactic distances determine the order of splits, defining hierarchical structure.\n- Model trained to reproduce ordering between split points using margin rank loss.\n- Parallel processing allows efficient computation on modern GPUs.\n\nSlide Title: Syntactic Distances and Parse Trees\nBullet Points:\n- Syntactic distances convert parse trees into compact representations for training.\n- Defined as vectors of scalars satisfying specific ranking conditions.\n- Algorithms convert parse trees to distance representations and back.\n- Binary parse trees are reconstructed using a divide-and-conquer approach.\n- Unary and n-ary nodes handled with special labels during binarization.\n\nSlide Title: Model Architecture and Learning\nBullet Points:\n- Neural networks estimate syntactic distances and constituent labels for sentences.\n- Uses LSTM and convolutional layers for capturing syntactical relations.\n- Word and POS tag embeddings are inputs for bidirectional LSTM layers.\n- Constituent labels predicted using softmax output from hidden state representations.\n- Syntactic distances calculated using a feed-forward network with no activation function.\n\nSlide Title: Training Objective and Loss Functions\nBullet Points:\n- Training objective combines prediction losses of syntactic distances and labels.\n- Uses softmax classifier with cross-entropy loss for constituent labels.\n- Mean-squared error loss over-penalizes by ignoring ranking equivalence.\n- Pair-wise learning-to-rank loss encourages correct ranking order reproduction.\n- Final loss is the sum of label and ranking distance losses.\n\nSlide Title: Experimental Evaluation\nBullet Points:\n- Evaluated on Penn Treebank and Chinese Treebank datasets for F1 score.\n- Model achieves strong performance without external data or ensembles.\n- Outperforms some recent chart-based and transition-based parsing models.\n- Ablation study shows importance of rank loss over MSE loss.\n- Parsing speed benefits from parallel computation on modern GPUs.\n\nSlide Title: Related Work in Neural Parsing\nBullet Points:\n- Neural network models have advanced dependency and constituency parsing.\n- Transition-based methods suffer from exposure bias during testing.\n- Chart-based parsing ensures structural consistency with exact inference.\n- Our model learns tree representation in parallel, avoiding exposure bias.\n- Recent works in translation and synthesis consider output variables conditionally independent.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Novel parsing scheme predicts syntactic distances for top-down split decisions.\n- Model achieves strong performance with efficient parallel processing.\n- Standard recurrent and convolution layers facilitate easy deployment.\n- Future work may explore sub-word features and character-level LSTM enhancements.\n- Potential for further improvements in parsing accuracy and efficiency.",
    "Slide Title: Introduction to Sentiment-to-Sentiment Translation\nBullet Points:\n- Sentiment-to-sentiment translation changes sentiment while preserving semantic content.\n- It is a style transfer task important in Natural Language Processing (NLP).\n- Applications include review sentiment transformation and news rewriting.\n- Lack of parallel training data is a major challenge.\n- Existing methods often fail to preserve semantic content.\n\nSlide Title: Proposed Cycled Reinforcement Learning Approach\nBullet Points:\n- Introduces a cycled reinforcement learning approach for sentiment translation.\n- Consists of a neutralization module and an emotionalization module.\n- Neutralization extracts non-emotional semantic content by filtering emotional words.\n- Emotionalization adds sentiment to neutralized content for translation.\n- Uses policy gradient to reward quality of generated text.\n\nSlide Title: Contributions of the Proposed Method\nBullet Points:\n- Enables training with unpaired data using only reviews and sentiment labels.\n- Separates sentiment information from semantic content explicitly.\n- Outperforms state-of-the-art systems in content preservation.\n- Uses cycled reinforcement learning to improve module performance.\n- Demonstrates effectiveness in sentiment-to-sentiment translation.\n\nSlide Title: Related Work in Style Transfer\nBullet Points:\n- Style transfer in computer vision involves mapping between image domains.\n- Unpaired language style transfer has been explored in recent studies.\n- Previous models struggle with preserving non-emotional semantic content.\n- Our method builds supervised training pairs by reconstructing original sentences.\n- Sentiment analysis is related, focusing on detecting sentiment in text.\n\nSlide Title: Overview of Cycled Reinforcement Learning\nBullet Points:\n- Approach includes neutralization and emotionalization modules.\n- Pre-training method uses self-attention based sentiment classifier.\n- Neutralization module filters out emotional information explicitly.\n- Emotionalization module adds sentiment using a bi-decoder framework.\n- Cycled training enhances module performance through feedback.\n\nSlide Title: Experimental Evaluation and Results\nBullet Points:\n- Evaluated on two review datasets with user ratings.\n- Automatic evaluation measures sentiment transformation and content preservation.\n- Human evaluation assesses sentiment and semantic similarity.\n- Proposed method achieves best overall performance on datasets.\n- Significant improvement in BLEU score indicates better content preservation.\n\nSlide Title: Analysis of Key Components\nBullet Points:\n- Emotionalization module baseline shows high BLEU but low sentiment accuracy.\n- Pre-training improves performance by providing initial learning ability.\n- Cycled reinforcement learning enhances module collaboration.\n- Neutralization mechanism improves sentiment transformation accuracy.\n- Error analysis highlights challenges with sentiment-conflicted and neutral sentences.\n\nSlide Title: Conclusions and Future Work\nBullet Points:\n- Proposed method outperforms state-of-the-art in semantic preservation.\n- Focuses on unpaired sentiment-to-sentiment translation.\n- Future work includes exploring fine-grained sentiment translation.\n- Aim to handle complex sentiment expressions and improve sentiment-aware decoders.\n- Potential to change sentiment strength in future research.",
    "Slide Title: Introduction to Domain-Specific Sentiment Analysis\nBullet Points:\n- Words expressing opinions vary across domains, affecting sentiment analysis accuracy.\n- Polarity of words can change between domains, e.g., \"unpredictable\" in movies vs. automobiles.\n- Supervised algorithms struggle with cross-domain sentiment analysis due to domain-specific vocabulary.\n- Domain adaptation techniques aim to transfer knowledge across domains.\n- Identifying words with consistent polarity across domains is crucial for effective sentiment analysis.\n\nSlide Title: Challenges in Cross-Domain Sentiment Analysis\nBullet Points:\n- Supervised learning requires retraining for each new domain, which is costly and time-consuming.\n- Transfer learning can reduce labeled data needs but struggles with polarity-changing words.\n- Identifying consistently significant words across domains is a complex task.\n- Existing methods like SCL and SFA focus on feature co-occurrence but have limitations.\n- Our approach uses \u03c7\u00b2 test and cosine-similarity to identify Significant Consistent Polarity (SCP) words.\n\nSlide Title: Proposed Approach for SCP Word Identification\nBullet Points:\n- SCP words are significant for sentiment classification with consistent polarity across domains.\n- \u03c7\u00b2 test identifies significant words in labeled source domains.\n- Cosine-similarity of context vectors assigns polarity in unlabeled target domains.\n- SCP words form a transferable knowledge set for cross-domain sentiment analysis.\n- Our method improves accuracy by focusing on SCP words rather than all features.\n\nSlide Title: Ensemble-Based Adaptation Algorithm\nBullet Points:\n- Combines classifiers trained on SCP words and target-specific features.\n- Classifier C_s uses SCP words from the source domain as a seed.\n- Classifier C_t learns from pseudo-labeled target domain instances.\n- Weighted ensemble of C_s and C_t enhances cross-domain performance.\n- Our approach outperforms SCL and common unigrams in sentiment classification.\n\nSlide Title: Experimental Setup and Results\nBullet Points:\n- Experiments conducted on Electronics, Kitchen, Books, and DVD domains.\n- SVM with linear kernel used for training classifiers.\n- SCP-based approach shows significant improvement over SCL and common unigrams.\n- Iterative process further enhances accuracy in target domains.\n- Strong positive correlation between SCP word identification and classification accuracy.\n\nSlide Title: Error Analysis and Domain Similarity\nBullet Points:\n- Higher accuracy achieved when source and target domains share more domain-specific words.\n- Electronics and Kitchen domains share more words than Electronics and DVD.\n- Common unique words percentage indicates domain similarity.\n- SCP words reduce errors in cross-domain sentiment classification.\n- Accurate SCP word identification leads to better classifier performance.\n\nSlide Title: Conclusion and Key Findings\nBullet Points:\n- SCP words represent transferable information for cross-domain sentiment classification.\n- Our approach achieves a strong positive correlation between SCP words and classification accuracy.\n- \u03c7\u00b2 test and cosine-similarity effectively identify SCP words.\n- Ensemble of SCP and target-specific features improves classification accuracy.\n- Our method outperforms existing approaches like SCL and common unigrams.\n\nSlide Title: Future Directions and Implications\nBullet Points:\n- Explore further improvements in SCP word identification techniques.\n- Investigate applicability to other domains and languages.\n- Potential for enhancing sentiment analysis in real-world applications.\n- SCP-based approach can reduce reliance on labeled data.\n- Encourages development of more robust cross-domain sentiment analysis models.",
    "Slide Title: Introduction to Search-based Structured Prediction\nBullet Points:\n- Models natural language structure generation as a search problem for various tasks.\n- Attracts research due to competitive accuracy and efficiency in performance.\n- Imitation learning used to train a policy, but faces ambiguity and discrepancy issues.\n- Ambiguities arise when multiple actions lead to optimal structures.\n- Discrepancy between training and testing harms generalization ability.\n\nSlide Title: Challenges in Search-based Structured Prediction\nBullet Points:\n- Ambiguities in reference policy lead to suboptimal training instances.\n- Discrepancy between training and testing phases affects performance.\n- Techniques like ensemble and exploration mitigate these issues.\n- Integrated knowledge distillation proposed to address these challenges.\n- Distillation combines reference and exploration to improve model performance.\n\nSlide Title: Background on Structured Prediction\nBullet Points:\n- Maps input to structural output with internal dependencies.\n- Formalized as a search problem with states, actions, and transitions.\n- Requires a reference policy to guide the search process.\n- Greedy policy often used, but can lead to error propagation.\n- Training involves imitating reference policy using negative log-likelihood loss.\n\nSlide Title: Knowledge Distillation in Structured Prediction\nBullet Points:\n- Transfers generalization ability from a cumbersome model to a smaller one.\n- Uses teacher model's output distribution as \"soft target\" for training.\n- Combines distillation loss with NLL loss for optimal learning.\n- Distillation from reference and exploration proposed for better performance.\n- Empirical results show significant improvements over baselines.\n\nSlide Title: Distillation from Reference and Exploration\nBullet Points:\n- Reference distillation learns from states produced by reference policy.\n- Exploration distillation learns from arbitrary states sampled by ensemble.\n- Combines both methods to enhance model's generalization ability.\n- Distillation from exploration shown to achieve comparable performance.\n- Workflow involves generating training states and optimizing distillation loss.\n\nSlide Title: Experiments and Results\nBullet Points:\n- Conducted on transition-based dependency parsing and neural machine translation.\n- Distilled models outperform strong baselines in both tasks.\n- Transition-based parsing shows significant improvement in LAS score.\n- Neural machine translation achieves higher BLEU score with distillation.\n- Combining reference and exploration distillation yields best results.\n\nSlide Title: Analysis of Distillation Method\nBullet Points:\n- Ensemble improves performance on ambiguous and non-optimal states.\n- Distillation model learned from exploration shows better generalization.\n- Larger \u03b1 in distillation loss leads to better performance.\n- Learning from distillation loss is stable and less sensitive to initialization.\n- Empirical analysis supports effectiveness of distillation approach.\n\nSlide Title: Related Work and Conclusion\nBullet Points:\n- Previous works apply knowledge distillation to various NLP problems.\n- Distillation focuses on action-level learning from reference and exploration.\n- Ensemble used in exploration studied in reinforcement learning.\n- Proposed method significantly improves single model's performance.\n- Future work includes extending distillation to semi-supervised learning.",
    "Slide Title: Introduction to Labeled Datasets\nBullet Points:\n- Traditional labeling involves human annotators providing a single label per example.\n- Challenge: Maximizing information from annotators' efforts in labeling.\n- Previous methods focused on labeling features or highlighting rationale phrases.\n- Some information types are hard to annotate, like absence or presence of words.\n- Proposal: Use natural language explanations for supervision in classifiers.\n\nSlide Title: BabbleLabble Framework Overview\nBullet Points:\n- BabbleLabble converts natural language explanations into a noisily-labeled training set.\n- Key components: semantic parser, filter bank, and label aggregator.\n- Semantic parser transforms explanations into logical forms for labeling functions.\n- Filter bank removes incorrect labeling functions without needing ground truth labels.\n- Label aggregator combines labels into a single probabilistic label per example.\n\nSlide Title: Semantic Parsing and Labeling Functions\nBullet Points:\n- Semantic parser generates logical forms from natural language explanations.\n- Rule-based parser used for coverage over many useful labeling functions.\n- Incorrect labeling functions can still have high end-task accuracy.\n- Parser is domain-independent, easily transferable to new domains.\n- Grammar rules guide the construction of logical forms for labeling.\n\nSlide Title: Filter Bank and Label Aggregator\nBullet Points:\n- Filter bank discards incorrect labeling functions using semantic and pragmatic filters.\n- Semantic filters check consistency with labeled examples.\n- Pragmatic filters remove constant, redundant, or correlated labeling functions.\n- Label aggregator combines multiple labels into a single probabilistic label.\n- Data programming models relationships between true labels and labeling functions.\n\nSlide Title: Discriminative Model and Experimental Setup\nBullet Points:\n- Noisily-labeled training set used to train a discriminative model.\n- Discriminative model incorporates features not identified by labeling functions.\n- Evaluated on three relation extraction tasks: Spouse, Disease, and Protein.\n- Datasets include news articles, biomedical abstracts, and neuroscience literature.\n- Experiments conducted using spaCy and Snorkel for parsing and classification.\n\nSlide Title: Experimental Results and High Bandwidth Supervision\nBullet Points:\n- BabbleLabble achieves high F1 scores with fewer user inputs than traditional methods.\n- Explanations applied to many unlabeled examples, contributing noisy labels.\n- Traditional supervision dominates with large labeled datasets.\n- BabbleLabble efficient in domains with more unlabeled than labeled data.\n- Incorrect parses filtered effectively, maintaining high end-task accuracy.\n\nSlide Title: Using Logical Forms as Functions or Features\nBullet Points:\n- Logical forms used as functions for weak supervision in data programming.\n- Data programming outperforms feature-based approach by utilizing unlabeled data.\n- Discriminative model benefits from additional patterns correlated with explanations.\n- Data programming improves with more unlabeled examples, constant logical forms.\n- Example: Disease task features not directly mentioned in explanations.\n\nSlide Title: Related Work and Discussion\nBullet Points:\n- Related themes: natural language explanations and weak supervision learning.\n- Previous work converted explanations into classifier features, we use labeling functions.\n- Weak supervision has been explored in distant supervision and multi-instance learning.\n- Natural language explanations can also interpret models, not just train them.\n- Future work: Extend framework to other tasks and interactive settings.",
    "Slide Title: Introduction to Human-Computer Conversation\nBullet Points:\n- Human-computer conversation is a critical task in AI and NLP with two research streams.\n- Task-oriented dialog helps complete specific tasks like buying tickets or shopping.\n- General purpose dialog aims for natural conversations on various open domain topics.\n- Generative conversational models learn from large-scale social conversation data.\n- General purpose dialog differs from machine translation due to varied input-response relations.\n\nSlide Title: Challenges in General Purpose Dialog\nBullet Points:\n- General responses like \"I don't know\" correspond to many input utterances.\n- Seq2Seq models favor high-frequency general responses, leading to trivial conversations.\n- Efforts to address this include Maximum Mutual Information (MMI) and topic guidance.\n- Latent responding factors are hard to interpret and decide in number.\n- Novel controlled response generation mechanism proposed to handle specificity.\n\nSlide Title: Specificity Controlled Seq2Seq Model (SC-Seq2Seq)\nBullet Points:\n- SC-Seq2Seq model simulates human control over response specificity.\n- Introduces specificity control variable to represent response purpose.\n- Usage representation of words interacts with specificity control through Gaussian Kernel.\n- Model parameters learned end-to-end on the same conversation corpus.\n- Distant supervision used to train SC-Seq2Seq with unknown specificity control variable.\n\nSlide Title: Distant Supervision Techniques\nBullet Points:\n- Normalized Inverse Response Frequency (NIRF) indicates response specificity level.\n- NIRF uses inverse frequency of a response in the corpus for specificity.\n- Normalized Inverse Word Frequency (NIWF) based on word collection specificity.\n- NIWF uses maximum inverse word frequency for response specificity.\n- Both methods aim to constrain specificity control variable within [0,1] range.\n\nSlide Title: Experiment and Dataset Description\nBullet Points:\n- Experiments conducted on Short Text Conversation (STC) dataset from Sina Weibo.\n- STC dataset contains 3.8 million post-comment pairs simulating utterance-response pairs.\n- Jieba Chinese word segmenter used for tokenization into sequences.\n- Development and test datasets each contain 10k pairs, remaining for training.\n- Comparison with state-of-the-art baselines like Seq2Seq-att and MMI-bidi.\n\nSlide Title: Evaluation Methodologies\nBullet Points:\n- Automatic evaluations include distinct-1 & distinct-2, BLEU, Average & Extrema.\n- Human evaluation conducted by labelers with Weibo experience.\n- Responses scored based on relevance, grammar, and informativeness.\n- Fleiss' kappa used to measure inter-rater consistency among labelers.\n- Evaluation results demonstrate effectiveness of controlled generation mechanism.\n\nSlide Title: Model Analysis and Baseline Comparison\nBullet Points:\n- SC-Seq2Seq NIWF achieves varying specificity levels by adjusting control variable.\n- SC-Seq2Seq NIWF outperforms baselines in specificity and fitting ground truth.\n- MARM generates specific but short responses, deviating from ground truth.\n- SC-Seq2Seq NIWF shows flexibility in generating informative responses.\n- Human evaluation confirms SC-Seq2Seq NIWF's effectiveness over baselines.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Novel controlled response generation mechanism handles specificity in utterance-response.\n- Specificity control variable interacts with usage representation for varied responses.\n- SC-Seq2Seq model outperforms state-of-the-art methods in generating specific responses.\n- Future work includes reinforcement learning for dynamic control variable adjustment.\n- Controlled generation mechanism enhances human-computer conversation quality.",
    "Slide Title: Introduction to Metaphor in Language\nBullet Points:\n- Metaphor enriches language, crucial for communication, cognition, and decision making.\n- One third of sentences in typical corpora contain metaphor expressions.\n- Metaphor uses words to represent concepts beyond their literal meanings.\n- Computational metaphor processing aids NLP tasks like Machine Translation and Sentiment Analysis.\n- Word embeddings popular for metaphor identification, avoiding hand-crafted knowledge.\n\nSlide Title: Challenges in Metaphor Identification\nBullet Points:\n- Existing models parse sentences into phrases, missing word-level metaphor identification.\n- Contextual information exclusion leads to metaphor identification failures.\n- Metaphors can appear literal when taken out of context.\n- Example: \"Climb the social ladder\" appears literal when parsed as \"climb ladder.\"\n- Need for models that operate at sentence level without preprocessing.\n\nSlide Title: Proposed Unsupervised Metaphor Processing Model\nBullet Points:\n- Model identifies and interprets linguistic metaphors at the word level.\n- Built on word embedding methods and WordNet for lexical relation acquisition.\n- Operates without labeled data, capturing metaphor in a data-driven manner.\n- Models distance between target word's literal and metaphorical senses.\n- Evaluated against strong baselines, outperforming in metaphor identification tasks.\n\nSlide Title: Related Work in Metaphor Processing\nBullet Points:\n- Various methods applied, including abstractness modeling and topic models.\n- Word embeddings used for metaphor identification, measuring cosine similarity.\n- Deep learning architectures introduced for improved metaphor detection.\n- Phrase-level models often fail due to exclusion of important contexts.\n- Our model improves performance by considering rich context at sentence level.\n\nSlide Title: Methodology of Metaphor Identification Framework\nBullet Points:\n- Hypothesis: Metaphorical words identified by domain differences in senses.\n- Word embeddings trained on Wikipedia for input and output vectors.\n- Candidate word set constructed using WordNet synonyms and hypernyms.\n- Best fit word represents literal sense in context, determined by cosine similarity.\n- Framework allows paraphrasing metaphors for improved NLP task support.\n\nSlide Title: Experimental Settings and Results\nBullet Points:\n- Compared against unsupervised and supervised baselines for metaphor identification.\n- Models trained on Wikipedia with context window of 5 for word vectors.\n- SIM-CBOW I+O model achieved highest F1 score in sentence-level evaluation.\n- SIM-SG I+O model performed best in phrase-level evaluation.\n- Translation accuracy improved significantly for metaphorical sentences.\n\nSlide Title: Metaphor Processing for Machine Translation\nBullet Points:\n- Evaluated metaphor processing's impact on English-Chinese machine translation.\n- Paraphrased metaphorical sentences improved translation accuracy.\n- Google and Bing Translators showed 11% and 9% overall accuracy improvements.\n- Context2Vec baseline also improved accuracy but was lower than our model.\n- Demonstrated effectiveness of metaphor processing in practical NLP tasks.\n\nSlide Title: Conclusion and Future Work\nBullet Points:\n- Proposed framework identifies and interprets metaphors with unsupervised learning.\n- Model outperforms baselines, improving translation systems' accuracy.\n- First study evaluating metaphor processing on Machine Translation.\n- Future work to introduce weighted CBOW and Skip-gram for positional information.\n- Metaphor processing in practical tasks holds real-world value beyond identification.",
    "Slide Title: Introduction to Controversial Content\nBullet Points:\n- Controversial content can spark debate, improving community health and engagement.\n- Detecting controversy aids both community members and moderators in managing discussions.\n- Controversiality varies by community context; topics differ in controversy across subreddits.\n- Early detection of controversy involves analyzing initial post content and responses.\n- Community-specific factors influence controversy, making prediction challenging.\n\nSlide Title: Research Approach and Objectives\nBullet Points:\n- Aim to predict controversy using early post and comment features.\n- Integrate structural and textual features from initial comment trees.\n- Study feature transferability across different subreddit communities.\n- Focus on early-detection methods for predicting controversial discussions.\n- Utilize a dataset from Reddit covering various subcommunities.\n\nSlide Title: Dataset and Subreddit Selection\nBullet Points:\n- Data sourced from Reddit, covering posts and comments from 2007 to 2014.\n- Focus on six subreddits: AskMen, AskWomen, Fitness, LifeProTips, personalfinance, relationships.\n- Each subreddit offers diverse styles and topics for analysis.\n- Reddit's upvote/downvote system helps identify controversial posts.\n- Posts with significant community attention are prioritized for study.\n\nSlide Title: Controversy Labeling and Validation\nBullet Points:\n- Assign binary labels: controversial vs. non-controversial based on upvote ratios.\n- Validate labels against Reddit's sort-by-controversy function.\n- High precision achieved in identifying controversial posts.\n- Community-specific patterns observed in controversial topics.\n- Language usage differences analyzed between controversial and non-controversial posts.\n\nSlide Title: Early Discussion Analysis\nBullet Points:\n- Focus on comments posted within one hour of original submission.\n- Analyze comment text and tree structure for predictive features.\n- Structural features of conversation trees show predictive capacity.\n- Community-specific differences in early discussion patterns.\n- Early comments provide significant information for controversy prediction.\n\nSlide Title: Predictive Models and Features\nBullet Points:\n- Compare text-only models for classifying controversial posts.\n- Use BERT features for high performance in text classification.\n- Incorporate post-time metadata to enhance prediction accuracy.\n- Early discussion features improve prediction over post-time baselines.\n- Textual content of comments offers significant predictive gain.\n\nSlide Title: Domain Transfer and Feature Transferability\nBullet Points:\n- Train models on one subreddit, test on another to assess transferability.\n- Text features are strong in-domain but brittle across communities.\n- Rate and structural comment tree features transfer better between communities.\n- Performance varies based on dataset size and community characteristics.\n- Aim to develop more expressive algorithms for better domain transfer.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Early discussion features effectively predict controversiality in Reddit communities.\n- Expressive feature sets of early discussions enhance prediction accuracy.\n- Future work to explore higher-quality textual representations for conversation trees.\n- Develop algorithms incorporating reply-structure relationships for improved performance.\n- Aim to make textual features less brittle and more transferable across domains.",
    "Slide Title: Introduction to Multimodal Social Media\nBullet Points:\n- Social platforms like Snapchat, Twitter, Instagram, and Pinterest enhance communication accessibility.\n- Social media is evolving from text-centric to multimodal, combining images, videos, audios, and texts.\n- Understanding textual components in social media is challenging due to multimodal nature.\n- Name tagging is crucial for language understanding and supports tasks like Question Answering.\n- Research in name tagging has focused on news articles, not multimodal social media.\n\nSlide Title: Challenges in Name Tagging on Social Media\nBullet Points:\n- Social media texts are often short, limiting context around names.\n- Linguistic variations, slangs, typos, and colloquial language are common.\n- Multimodal data increases difficulty compared to traditional newswire name tagging.\n- Visual context in images can aid in understanding textual components.\n- Example: \"Modern Baseball\" can refer to a band or sport, depending on image context.\n\nSlide Title: Proposed Model for Name Tagging\nBullet Points:\n- Attention-based model extracts visual features related to text in images.\n- Gate combines textual features from BLSTM with visual features for tag prediction.\n- Model evaluated on datasets from Snapchat and Twitter.\n- Outperforms state-of-the-art name taggers in multimodal social media.\n- Visual Attention component aids in model decision-making.\n\nSlide Title: BLSTM-CRF Sequence Labeling Model\nBullet Points:\n- Name tagging modeled as a sequence labeling problem using BLSTM.\n- BLSTM captures long-range dependencies of input for name tagging.\n- Character-level representation generated using another BLSTM.\n- CRFs learn constraints of labels in neighborhood for joint prediction.\n- Combines word embedding and character-level representation for final word representation.\n\nSlide Title: Visual Feature Representation and Attention Mechanism\nBullet Points:\n- CNNs, specifically ResNet, used to obtain image representations.\n- Visual attention mechanism focuses on image regions related to text.\n- Attention mechanism maps query and key-value pairs to an output.\n- Text query vector encoded using LSTM for semantic information.\n- Linear projection after summation used for attention implementation.\n\nSlide Title: Visual Modulation Gate\nBullet Points:\n- Modulation gate controls combination of visual features and word representation.\n- Visual features contribute differently to inferring tags of different words.\n- Gate dynamically controls visual and word feature combination before CRF layer.\n- Modulated word representations fed into CRF layer for tag prediction.\n- Experiments show impact of modulation gate on model performance.\n\nSlide Title: Datasets and Experimentation\nBullet Points:\n- Evaluated on Twitter and Snapchat datasets with named entities tagged.\n- Twitter dataset has longer sentences, more text-related images.\n- Snapchat dataset has shorter sentences, images often unrelated to text.\n- Proposed model outperforms baseline on both datasets.\n- Visual features validate entity types, e.g., person in image suggests person name.\n\nSlide Title: Results and Error Analysis\nBullet Points:\n- Model performs better on Twitter dataset due to longer sentence context.\n- Visual features benefit name tagging when images align with text.\n- Attention visualization shows model focusing on appropriate image regions.\n- Failed examples due to poor alignment, blur images, or wrong attention.\n- Visual features enhance name tagging, but challenges remain in social media.\n\nSlide Title: Related Work and Future Directions\nBullet Points:\n- Neural network architectures outperform traditional methods in name tagging.\n- Visual attention widely adopted in language and vision tasks.\n- Proposed model similar to VQA, finds image regions related to sentences.\n- Future work: expand model to fine-grained name tagging and entity linking.\n- Aim to encourage more research on multimodal social media.",
    "Slide Title: Introduction to Incremental Domain Adaptation\nBullet Points:\n- Incremental domain adaptation enhances neural machine translation (NMT) systems significantly.\n- Commonly used in computer-aided translation (CAT) scenarios for document-specific adaptation.\n- Evaluations often miss user experience aspects in collaborative translation.\n- Focus on measuring speed of lexical acquisition for in-domain vocabulary.\n- Ideal systems quickly adapt to new lexical items and generalize corrections.\n\nSlide Title: Measuring Immediate Adaptation\nBullet Points:\n- Perceived adaptation performance is crucial for interactive machine translation systems.\n- Lexical choice errors in domain-specific items can frustrate users.\n- Users expect correct translations after manual corrections.\n- Traditional metrics like BLEU may not capture immediate adaptation needs.\n- Focus on content words, setting aside syntax and word order.\n\nSlide Title: Proposed Metrics for Adaptation\nBullet Points:\n- Three metrics: one-shot, zero-shot, and combined vocabulary acquisition.\n- Metrics measure recall of target-language content words.\n- Content words exclude stopwords, focusing on nouns, verbs, adjectives, adverbs.\n- Metrics calculated automatically by comparing translation hypotheses to references.\n- Example calculation provided for understanding metric application.\n\nSlide Title: Related Work and Adaptation Techniques\nBullet Points:\n- Adaptation techniques evaluated using BLEU, TER, and user studies.\n- Regularization methods like group lasso improve space efficiency.\n- Online adaptation explored with various metrics and user feedback.\n- Document-level adaptation focuses on novel word accuracy.\n- Techniques trade off between corpus-level gains and immediate adaptation.\n\nSlide Title: Online Adaptation Methods\nBullet Points:\n- NMT systems adapted by fine-tuning with cross-entropy loss.\n- Full adaptation updates all parameters, but can be infeasible.\n- Bias adaptation introduces a second bias term in the output layer.\n- Fixed and top methods freeze parts of the model for adaptation.\n- Lasso method uses group lasso regularization for parameter selection.\n\nSlide Title: Experimental Results and Analysis\nBullet Points:\n- Performance evaluated on Autodesk dataset with various adaptation techniques.\n- Lasso outperforms others in zero-shot and combined recall.\n- Full adaptation shows high one-shot recall but low zero-shot recall.\n- Lasso improves zero-shot recall but requires multiple observations for acquisition.\n- Analysis confirms findings with novel words and subword combinations.\n\nSlide Title: Conclusions and Observations\nBullet Points:\n- Fine-tuning NMT models can degrade zero-shot recall despite corpus-level gains.\n- Regularization methods like lasso mitigate negative impacts on adaptation.\n- Proposed metrics R0, R1, and R0+1 measure immediate adaptation performance.\n- Lasso provides best tradeoff in performance across considered metrics.\n- Zero-shot recall often degraded by adaptation, while one-shot recall improves.\n\nSlide Title: Additional Results and Learning Curves\nBullet Points:\n- Additional datasets show similar patterns in adaptation performance.\n- Lasso consistently outperforms in zero-shot recall across datasets.\n- Learning curves reveal adaptation behavior over time.\n- Disparate signals observed between BLEU and proposed metrics.\n- Noise and repetition in segments impact adapted systems differently.",
    "Slide Title: Introduction to Speech-to-Text Translation\nBullet Points:\n- Speech-to-text translation (ST) has potential applications in low-resource languages like language documentation.\n- Traditional ST requires transcribed audio and parallel text, often unavailable for low-resource languages.\n- End-to-end ST is appealing for scenarios with source audio and target text translations.\n- Recent studies show end-to-end ST effectiveness but require substantial training resources.\n- Leveraging data from high-resource languages can improve low-resource ST performance.\n\nSlide Title: Leveraging High-Resource Languages\nBullet Points:\n- Training a single ASR model on multiple languages can benefit low-resource languages.\n- Transfer learning pre-trains a model on high-resource language pairs for low-resource tasks.\n- Pre-training on high-resource ASR tasks and fine-tuning improves low-resource ST.\n- Encoder parameter transfer is crucial for improving low-resource ST performance.\n- Pre-training on unrelated languages can still enhance low-resource ST.\n\nSlide Title: Methodology for ASR and ST\nBullet Points:\n- Encoder-decoder model with attention is used for both ASR and ST tasks.\n- Pre-trained English ASR model initializes Spanish-English ST model training.\n- Decoder shares vocabulary across ASR and ST tasks for high-resource target languages.\n- Transfer learning involves sharing encoder parameters for different language pairs.\n- Flexible transfer learning allows using any ASR language for ST tasks.\n\nSlide Title: Experimental Setup and Data Sets\nBullet Points:\n- English ASR uses Switchboard corpus with 300 hours of speech and transcripts.\n- French ASR uses GlobalPhone corpus with 20 hours of high-quality read speech.\n- Spanish-English ST uses Fisher Spanish corpus with 160 hours of telephone speech.\n- Mboshi-French ST uses a parallel corpus with 4 hours of Mboshi speech.\n- Preprocessing involves converting raw speech to MFCCs and normalizing text.\n\nSlide Title: Model Architecture and Training\nBullet Points:\n- Speech encoder uses CNN layers followed by bi-directional LSTM for feature extraction.\n- Text decoder predicts tokens using a softmax layer and attention mechanism.\n- Training involves dropout regularization and Adam optimizer for parameter optimization.\n- Gaussian noise and frame dropping enhance model robustness during training.\n- Evaluation metrics include BLEU scores and METEOR for precision and recall.\n\nSlide Title: Evaluation and Results\nBullet Points:\n- Transfer learning significantly improves BLEU scores for low-resource ST models.\n- Models trained on less than 10 hours of data benefit greatly from transfer learning.\n- Fine-tuning English ASR models yields results comparable to larger ST data sets.\n- Transfer learning enhances precision and recall in very low-resource scenarios.\n- Sample translations demonstrate improved word order and accuracy with transfer learning.\n\nSlide Title: Analysis and Insights\nBullet Points:\n- Transferring all parameters is most effective, with encoder parameters providing major gains.\n- Encoder learns language-independent acoustic features, normalizing speaker variability.\n- Transfer learning accelerates training, achieving higher BLEU scores quickly.\n- Increasing ASR data improves ST performance, especially with limited ST data.\n- Code-switching in data sets suggests benefits from English ASR models.\n\nSlide Title: Conclusion and Future Directions\nBullet Points:\n- Pre-training with high-resource ASR data significantly boosts low-resource ST performance.\n- Combining encoder and decoder parameters enhances results, with encoder transfer most beneficial.\n- Multilingual training and pre-trained multilingual features could further improve ST.\n- Future work includes exploring language model pre-training and shallow fusion techniques.\n- Strategies aim to optimize low-resource ST for practical applications like disaster relief.",
    "Slide Title: Introduction to Semantic Parsing\nBullet Points:\n- Semantic parsing maps language utterances to executable programs for visual reasoning tasks.\n- Early work relied on supervised learning with language-program pairs, challenging to scale.\n- Weakly-supervised parsing uses utterance-denotation pairs, easier to collect by non-experts.\n- Challenges include search through program space and spuriousness in denotation results.\n- CNLVR dataset aids in studying spuriousness with visual reasoning tasks.\n\nSlide Title: Problem Statement and Programming Language\nBullet Points:\n- Task: Map utterances to programs that yield correct denotations against a knowledge base.\n- CNLVR images described by objects' color, shape, size, and location.\n- Programming language supports spatial reasoning with functions for object properties.\n- Programs are sequences of function applications in prefix notation.\n- Set-theoretic reasoning extends language for complex visual tasks.\n\nSlide Title: Model Architecture\nBullet Points:\n- Model based on encoder-decoder architecture with bi-directional LSTM for encoding.\n- Decoder uses attention mechanism over encoder outputs for program generation.\n- Beam search applied to manage large program space during decoding.\n- Semantic typing system ensures syntactically valid program construction.\n- Discriminative re-ranking addresses label bias by scoring program candidates.\n\nSlide Title: Training from Weak Supervision\nBullet Points:\n- Programs treated as latent variables, approximately marginalized during training.\n- Objective maximizes probability of correct denotation across multiple images.\n- Spuriousness reduced by using multiple images per utterance for validation.\n- Abstract examples improve training by generalizing across similar utterances.\n- Data augmentation generates new training pairs from abstract examples.\n\nSlide Title: Learning from Abstract Examples\nBullet Points:\n- Abstract representations cluster language tokens and program constants.\n- Seven clusters defined for main semantic types in spatial reasoning domain.\n- Abstract examples improve search accuracy and reduce spuriousness.\n- High coverage achieved with a small set of abstract utterances.\n- Rule-based parser provides baseline by mapping abstract to real programs.\n\nSlide Title: Data Augmentation and Caching\nBullet Points:\n- Data augmentation generates non-abstract pairs from abstract examples.\n- Augmented data improves coverage and accuracy of semantic parser.\n- Caching mechanism tracks successful abstract utterance-program pairs.\n- Cache guides search by adding promising program prefixes to beam.\n- Combats spuriousness and search challenges in weak supervision.\n\nSlide Title: Experimental Evaluation\nBullet Points:\n- Evaluation on CNLVR dataset with accuracy and consistency metrics.\n- Weakly-supervised parser with re-ranking achieves state-of-the-art accuracy.\n- Ablation studies highlight importance of abstract examples and caching.\n- Error analysis reveals challenges with complex syntactic and semantic structures.\n- Model confidence correlated with probability of top-ranked program.\n\nSlide Title: Related Work and Discussion\nBullet Points:\n- Semantic parsers trained from denotations popular for scaling challenges.\n- Visual reasoning datasets like VQA and CLEVR provide context for CNLVR.\n- Spuriousness addressed with manual rules and crowdsourcing in prior work.\n- Abstract examples reduce search difficulty and improve parser performance.\n- Future work aims to automate lexicon learning for larger, variable domains."
]