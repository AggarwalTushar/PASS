[
    "SlideTitle: Introduction to Similarity Search\nBulletPoints:\n- Similarity search, also known as nearest-neighbor search, involves finding documents similar to a query document.\n- It is crucial for applications like plagiarism analysis, collaborative filtering, and content-based multimedia retrieval.\n- Semantic hashing is an effective approach for fast similarity search.\n\nSlideTitle: Challenges in Existing Methods\nBulletPoints:\n- Existing text hashing methods often require two-stage training procedures.\n- Binary constraints are typically handled ad-hoc, leading to information loss.\n- Training different modules separately requires additional hyperparameter tuning.\n\nSlideTitle: Proposed Neural Architecture\nBulletPoints:\n- A simple and generic neural architecture for text hashing is proposed.\n- Learns binary latent codes for documents in an end-to-end manner.\n- Utilizes neural variational inference (NVI) for text processing.\n\nSlideTitle: Generative Model Perspective\nBulletPoints:\n- Approach semantic hashing from a generative model perspective.\n- Binary codes are represented as deterministic or stochastic Bernoulli latent variables.\n- Inference and generative networks are optimized jointly.\n\nSlideTitle: Injecting Data-dependent Noise\nBulletPoints:\n- Data-dependent noise is injected into latent codes during decoding.\n- This accounts for the tradeoff between minimizing rate and distortion.\n- Provides a theoretical foundation for the effectiveness of the framework.\n\nSlideTitle: Experimental Setup and Datasets\nBulletPoints:\n- Experiments conducted on Reuters21578, 20Newsgroups, and TMC datasets.\n- TFIDF features used for training and evaluation.\n- Various baselines considered for comparison.\n\nSlideTitle: Experimental Results\nBulletPoints:\n- NASH variants outperform baseline methods in unsupervised semantic hashing.\n- Injection of noise improves robustness and retrieval performance.\n- Supervised NASH-DN-S model outperforms several baselines.\n\nSlideTitle: Semantic Hashing Evaluation\nBulletPoints:\n- Retrieval results tend to drop with longer hashing codes.\n- NASH models perform stronger than baselines even with longer codes.\n- Effective allocation of documents to meaningful hashing codes.\n\nSlideTitle: Conclusions\nBulletPoints:\n- First step towards end-to-end semantic hashing with effective gradient estimator.\n- Neural variational framework introduced for training.\n- Data-dependent noise injection enhances retrieval performance.",
    "SlideTitle: Introduction to Human Emotions\nBulletPoints:\n- Human emotions are complex and difficult to define.\n- Ekman's six basic emotions: anger, disgust, fear, guilt, joy, and sadness.\n- Importance of emotional phrase identification in NLP.\n- Extraction of seven emotional statements from a psychological corpus.\n- Use of Context Vectors and classification methods.\n\nSlideTitle: Related Work\nBulletPoints:\n- Analysis of emotion annotations by Mohammad et al. (2010).\n- Development of emotion lexicons for Bengali language.\n- SenticNet and ConceptNet as resources for emotion detection.\n- EmotiNet's advantage in classifying emotions using commonsense knowledge.\n- Contrast with previous approaches using Context Vectors.\n\nSlideTitle: Data Preprocessing Framework\nBulletPoints:\n- Emotional statements collected from the ISEAR database.\n- Tokenization into sentences and words.\n- Formation of trigrams as Context Windows.\n- Use of WordNet Affect for affect word identification.\n- Statistics of trigrams and stop words in Context Windows.\n\nSlideTitle: Context Vector Formation\nBulletPoints:\n- Mapping Context Windows into vector space.\n- Calculation of Affinity Score for Context Vectors.\n- Use of distance metrics: Chebyshev, Euclidean, and Hamming.\n- Formula for vectorization of Context Windows.\n\nSlideTitle: Feature Selection and Analysis\nBulletPoints:\n- Importance of feature selection in pattern classification.\n- POS tagging and extraction of Context Windows.\n- Calculation of TF and TF-IDF scores for emotion classes.\n- Variance as a measure of data spread.\n\nSlideTitle: Ranking Score of Context Windows\nBulletPoints:\n- Removal of duplicate Context Windows.\n- Calculation of ranking score using SentiWordnet lexicon.\n- Importance of ranking in identifying emotional phrases.\n- Examples of top-ranked context windows.\n\nSlideTitle: Result Analysis\nBulletPoints:\n- Classifier accuracies using test data and cross-validation.\n- BayesNet Classifier's high accuracy with Euclidean distance.\n- Comparison of J48, NaiveBayesSimple, and DecisionTree classifiers.\n- Best performance of DecisionTree with Chebyshev distance.\n\nSlideTitle: Conclusions and Future Works\nBulletPoints:\n- Vector formation and affinity score calculation inspired by Newton's law.\n- Use of various classifiers for emotion classification.\n- Future plans to incorporate more lexicons and associative learning.\n- Aim to identify important rules for classification.",
    "SlideTitle: Introduction to the Scientific Publication Lifecycle\nBulletPoints:\n- Main activities: literature review, research work, dissemination of results\n- Challenges faced by inexperienced researchers\n- Assistance from supervisors, experts, and librarians\n- Academic assistive systems to bridge expertise gap\n\nSlideTitle: Assistive Systems in Research\nBulletPoints:\n- Search systems with faceted user interfaces\n- Bibliometric tools for visualizing citation networks\n- Scientific paper recommender systems\n- Techniques for recommending articles for citation contexts\n\nSlideTitle: Rec4LRW System Overview\nBulletPoints:\n- Aimed at literature review and manuscript preparation\n- Tasks: building reading list, finding similar papers, shortlisting papers\n- Uses ACM Digital Library dataset\n- Beneficial for novice researchers\n\nSlideTitle: Unique UI Features of Rec4LRW\nBulletPoints:\n- Displays author-specified keywords, references count, short summary\n- Information cue labels: Popular, Recent, High Reach, Survey/Review\n- Option to view papers in parent cluster\n\nSlideTitle: Shortlisting Technique\nBulletPoints:\n- Identifies important and unique papers for citation\n- Uses Girvan-Newman algorithm for clustering\n- Considers article-type preference for shortlisting\n- Article-types: conference full paper, poster, generic research paper, case study\n\nSlideTitle: User Evaluation Study\nBulletPoints:\n- Conducted with 116 participants\n- Evaluated usefulness and effectiveness of tasks\n- Participants: research students, academic staff, research staff\n- Survey questions and subjective feedback collected\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- High agreement percentages for usefulness and importance\n- Students showed higher agreement than staff\n- Participants acknowledged usefulness in identifying important papers\n- Some reluctance in citing papers due to citation context\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Rec4LRW addresses gap in manuscript preparation assistance\n- Validates aim of identifying important papers for citation\n- Future work: expand article-type variations, include recent papers, enhance user control\n- Integrate with citation context recommendation task",
    "SlideTitle: Introduction to NMT Improvements\nBulletPoints:\n- Recent advancements in Neural Machine Translation (NMT)\n- Three categories: string-to-string, tree-to-string, string-to-tree systems\n- Tree-based systems offer syntactic information and prior knowledge\n- Focus on linearization methods for tree-based NMT\n\nSlideTitle: Challenges in Tree-Based NMT\nBulletPoints:\n- Major drawback: reliance on 1-best parse tree\n- Potential translation errors due to parsing mistakes\n- Forest-based methods in NMT are still being explored\n- Complexity in linearizing a forest\n\nSlideTitle: Proposed Forest-Based NMT Approach\nBulletPoints:\n- Inspired by tree-based NMT methods\n- Efficient forest-based NMT approach\n- Novel weighted linearization method for packed forests\n- Decoding under sequence-to-sequence framework\n\nSlideTitle: Preliminaries of NMT Systems\nBulletPoints:\n- Overview of sequence-to-sequence model\n- Use of RNNs for encoding and decoding sequences\n- Introduction of attention mechanism for long sequences\n- Different NMT systems: string-to-string, tree-to-string, string-to-tree\n\nSlideTitle: Linearization in Tree-Based NMT\nBulletPoints:\n- Linearization methods for tree-to-string and string-to-tree NMT\n- Utilization of syntax information in linearization\n- Current systems use only one tree for encoding or decoding\n- Proposal to utilize multiple trees (a forest)\n\nSlideTitle: Packed Forest Representation\nBulletPoints:\n- Compact representation of exponentially many parsing trees\n- Nodes and hyperedges in a packed forest\n- Score calculation for constituent trees\n- Challenges in linearizing a packed forest\n\nSlideTitle: Forest Linearization Algorithm\nBulletPoints:\n- Novel linearization algorithm for packed forests\n- Preserves word sequential and parent-child information\n- Use of different operators to combine tree parts\n- Weighted sequence with parser confidence scores\n\nSlideTitle: Encoding and Frameworks\nBulletPoints:\n- Linearized packed forest as encoder input\n- Two frameworks: Score-on-Embedding (SoE) and Score-on-Attention (SoA)\n- Differences in handling score and symbol sequences\n- Connection with conventional sequence-to-sequence NMT\n\nSlideTitle: Experimental Results and Analysis\nBulletPoints:\n- Evaluation on English-to-Chinese and English-to-Japanese tasks\n- Forest-based configurations yield better results than s2s baseline\n- Importance of edge scores in constituent trees or packed forests\n- Packed forests provide more syntactic information\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- New NMT framework utilizing packed forests\n- Efficient encoding of many linearized parsing trees\n- First attempt at using a forest in string-to-string NMT\n- Future work: elaborate structures for score layer integration",
    "SlideTitle: Introduction to Adversarial Attacks in Machine Learning\nBulletPoints:\n- Adversarial perturbations aim to mislead models by altering inputs.\n- Highlight vulnerabilities and brittleness of models.\n- Security concerns in safety-critical applications like self-driving cars.\n- Challenges in applying adversarial attacks to NLP due to discrete input space.\n\nSlideTitle: Adversarial Perturbations in NLP\nBulletPoints:\n- Meaning-preserving on the source side, meaning-destroying on the target side.\n- Importance of evaluating meaning preservation.\n- Method for formalizing meaning-preserving perturbations.\n- Use of gradient-based word substitution attacks with constraints.\n\nSlideTitle: Framework for Evaluating Adversarial Attacks\nBulletPoints:\n- Evaluation of adversarial attacks on seq2seq models.\n- Importance of target similarity and source similarity scores.\n- Reporting attack success using source and target scores.\n- Framework applicable to various tasks beyond meaning-preserving attacks.\n\nSlideTitle: Similarity Metrics for Evaluation\nBulletPoints:\n- Human judgment as the gold standard for semantic similarity.\n- Automatic metrics: BLEU, METEOR, and chrF.\n- chrF shows higher correlation with human judgment.\n- Importance of choosing the right metric for evaluation.\n\nSlideTitle: Gradient-Based Adversarial Attacks\nBulletPoints:\n- Overview of gradient-based attacks for word substitution.\n- Optimization problem for finding adversarial inputs.\n- Constraints like kNN and CharSwap to preserve meaning.\n- Examples of perturbations under different constraints.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluation of adversarial attacks using proposed framework.\n- chrF correlates better with human judgment than other metrics.\n- Constrained attacks preserve meaning better than unconstrained ones.\n- Transformer models less robust to small embedding perturbations.\n\nSlideTitle: Adversarial Training with Meaning-Preserving Attacks\nBulletPoints:\n- Adversarial training improves robustness to attacks.\n- CharSwap constraint maintains test performance.\n- Unconstrained attacks can degrade test performance.\n- CharSwap-adv more robust to constrained attacks.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Importance of meaning-preserving adversarial perturbations.\n- Proposed evaluation framework for adversarial attacks.\n- chrF as a reliable metric for evaluating meaning preservation.\n- Encouragement for consistent evaluation of meaning conservation in future research.",
    "SlideTitle: Introduction to Discourse in Machine Translation\nBulletPoints:\n- Importance of handling discourse phenomena in translation\n- Traditional systems translate sentences in isolation\n- Context-aware NMT models show performance improvements\n- Aim to enhance understanding of discourse phenomena in NMT\n\nSlideTitle: Context-Aware Neural Model\nBulletPoints:\n- Constructed a simple discourse-aware model\n- Achieves improvements over discourse-agnostic baseline\n- Uses Transformer architecture with context handling\n- Context information flows through a single attention layer\n\nSlideTitle: Neural Machine Translation Overview\nBulletPoints:\n- NMT models predict target sentence words sequentially\n- Encoder-decoder structure with attention mechanism\n- Transformer architecture avoids recurrence, uses self-attention\n- More parallelizable and faster to train than recurrent models\n\nSlideTitle: Context-Aware Model Architecture\nBulletPoints:\n- Based on Transformer architecture\n- Incorporates context information on the encoder side\n- Uses shared parameters for source and context encoders\n- Special token added to context sentences for differentiation\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Used OpenSubtitles2018 corpus for English and Russian\n- BLEU metric used for performance evaluation\n- Context-aware model shows 0.7 BLEU improvement\n- Previous sentence context more beneficial than next sentence\n\nSlideTitle: Analysis of Contextual Information\nBulletPoints:\n- Model relies on context for pronoun translation\n- High attention to context for ambiguous pronouns\n- Context especially helpful at the beginning of sentences\n- No straightforward dependence of BLEU improvement on sentence length\n\nSlideTitle: Pronoun Translation and Anaphora Resolution\nBulletPoints:\n- Context-aware model improves translation of ambiguous pronouns\n- Model implicitly captures anaphora without feature engineering\n- Attention mechanism performs latent anaphora resolution\n- Human evaluation confirms model's anaphora resolution capability\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- Context-aware NMT system outperforms baselines\n- Improvements prominent for sentences with ambiguous pronouns\n- Model induces anaphora relations through attention mechanism\n- Future work to explore other discourse phenomena in NMT systems",
    "SlideTitle: Introduction to Machine Translation\nBulletPoints:\n- Goal: Convert source language utterances into fluent target language utterances with the same meaning\n- Recent approaches use statistical techniques on parallel data\n- Unified semantic representations like Abstract Meaning Representation (AMR) are gaining interest\n- Deeper models allow for more parsimonious translation models\n\nSlideTitle: Related Work in Machine Translation\nBulletPoints:\n- Deep language representation has been utilized in NLP and MT\n- Early systems used transfer methods, parsing source sentences into target representations\n- AMR used for abstractive summarization and semantic graph exploitation\n- Discriminative models like Global lexicon model and phrase-sense disambiguation have been used\n\nSlideTitle: Semantic Representation\nBulletPoints:\n- Based on Logical Form (LF), represented as labeled directed graphs\n- Nodes correspond to content words; edges describe semantic relations\n- Additional linguistic information stored in graph nodes\n- Conversion to AMR specification possible, but not fully used in this work\n\nSlideTitle: Graph-to-String Translation\nBulletPoints:\n- Models developed for semantic-graph-to-string translation\n- Translation seen as a process of selecting target words based on source language representation\n- Assumes alignment of target words to source semantic graph nodes\n- Generative process involves transitioning through graph nodes to generate target-side words\n\nSlideTitle: Alignment of Semantic Graph Nodes\nBulletPoints:\n- Techniques for aligning source-side semantic graph nodes to target-side words\n- Gibbs sampling used for direct alignment\n- GIZA++ toolkit for word alignment\n- Composed alignments produce superior results\n\nSlideTitle: Discriminative Model and Features\nBulletPoints:\n- Alignment assumed to be given, using parser and GIZA++ alignments\n- Maximum entropy model for translation distribution\n- Rich feature set including lemma, part of speech, and semantic context\n- Online learning with stochastic gradient descent and L1 regularization\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Tested model in n-best re-ranking experiment for English\u2192French translation\n- Added six scores from the model to n-best lists\n- Modest but consistent improvement in BLEU scores\n- Gains expected to increase with direct decoder integration\n\nSlideTitle: Conclusion and Future Directions\nBulletPoints:\n- Initial attempt at including semantic features in statistical MT\n- Gains not large yet, but potential for improvement with feature engineering\n- Importance of semantic parser accuracy and consistency\n- Future research could explore robust features and target semantic language models",
    "SlideTitle: Introduction to Deep Neural Networks in NLP\nBulletPoints:\n- DNNs excel in supervised NLP tasks like dependency parsing, named entity recognition, and semantic role labeling.\n- Learning from unlabeled data under domain shift remains challenging.\n- Many domain adaptation approaches are task-specific and do not generalize well.\n- Classic semi-supervised learning algorithms can serve as strong baselines.\n\nSlideTitle: Contributions of the Study\nBulletPoints:\n- Proposal of a novel multi-task tri-training method.\n- Tri-training as a robust semi-supervised learning baseline for NLP models.\n- Extensive evaluation of bootstrapping algorithms against state-of-the-art approaches.\n- Analysis of task and data characteristics for optimal model performance.\n\nSlideTitle: Overview of Neural Bootstrapping Methods\nBulletPoints:\n- Introduction to self-training, tri-training, and tri-training with disagreement.\n- Novel multi-task tri-training method introduced.\n- Self-training uses model predictions on unlabeled data for training.\n- Tri-training reduces bias using agreement among three models.\n\nSlideTitle: Self-Training Method\nBulletPoints:\n- Leverages model's predictions on unlabeled data.\n- Adds examples with high confidence predictions to labeled data.\n- Calibration and online learning improve performance.\n- Main downside: error amplification under domain shift.\n\nSlideTitle: Tri-Training and Tri-Training with Disagreement\nBulletPoints:\n- Tri-training uses three models to reduce prediction bias.\n- Unlabeled data added if two models agree on the label.\n- Tri-training with disagreement requires one model to disagree.\n- More data-efficient and competitive results in some tasks.\n\nSlideTitle: Multi-Task Tri-Training (MT-Tri)\nBulletPoints:\n- Reduces time and space complexity of tri-training.\n- Shares parameters across models using multi-task learning.\n- Orthogonality constraint ensures diverse feature usage.\n- More efficient and robust to domain shift.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluated on POS tagging and sentiment analysis tasks.\n- Tri-training outperforms state-of-the-art on average.\n- MT-Tri excels in sentiment analysis, especially under strong domain shifts.\n- Classic tri-training superior in POS tagging, especially for OOVs and low-frequency tokens.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- Classic tri-training effective but complex in time and space.\n- MT-Tri offers a more efficient alternative, especially for sentiment analysis.\n- Importance of comparing neural approaches to strong baselines.\n- Future work to explore shared-private models for high-data setups.",
    "SlideTitle: Introduction to Conversational Interfaces\nBulletPoints:\n- Advances in speech recognition and language understanding\n- Common use cases: task completion and Q&A\n- Spoken Language Understanding (SLU) systems\n- Executable semantic parsers for complex structures\n\nSlideTitle: Unified Framework for SLU and Q&A\nBulletPoints:\n- Proposal of executable semantic parsing as a common framework\n- Parsing input utterances into logical forms\n- Advantages of parsing for handling complex linguistic phenomena\n\nSlideTitle: Challenges in Semantic Parsing\nBulletPoints:\n- Lack of annotated logical forms for training\n- Use of distant supervision and transfer learning\n- Multi-task learning (MTL) for improved generalization\n\nSlideTitle: Transition-based Parsing\nBulletPoints:\n- Definition and components of transition-based parsers\n- Neural transition-based parser with Stack-LSTMs\n- Use of attention mechanisms and copy mechanisms\n\nSlideTitle: Data Sets for Evaluation\nBulletPoints:\n- Overview of Q&A data sets: Overnight and NLmaps\n- SLU data sets: search, recipes, cinema, bookings, and closet\n- Challenges with data sparsity and vocabulary size\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Single-task semantic parsing experiments\n- Impact of attention and copy mechanisms\n- Comparison of delexicalization and copy mechanisms\n\nSlideTitle: Transfer Learning Approaches\nBulletPoints:\n- Pre-training and multi-task learning for low-resource domains\n- Results on Overnight and SLU domains\n- Preliminary experiments on cross-task transfer learning\n\nSlideTitle: Related Work and Conclusions\nBulletPoints:\n- Overview of related work in semantic parsing and SLU\n- Framing SLU as executable semantic parsing\n- Effectiveness of the proposed framework for Q&A and SLU\n- Future work in transfer learning across heterogeneous data sets",
    "SlideTitle: Introduction to Abstract Meaning Representation (AMR)\nBulletPoints:\n- AMR is a semantic graph representation abstracting syntactic realization.\n- Nodes represent concepts; edges represent semantic relations.\n- AMRs are graphs, not trees, due to reentrancies.\n- AMR-to-text generation involves generating natural language from AMR graphs.\n\nSlideTitle: Challenges in AMR-to-Text Generation\nBulletPoints:\n- Reentrancies are difficult to handle in parsing and generation.\n- Previous work often removed reentrancies or did not analyze their impact.\n- Explicit encoding of reentrancies' benefits is unclear.\n\nSlideTitle: Types of Encoders for AMR\nBulletPoints:\n- Sequential encoders reduce AMR graphs to sequences.\n- Tree encoders ignore reentrancies.\n- Graph encoders maintain reentrancies and encode structural information.\n\nSlideTitle: Input Representations for AMR\nBulletPoints:\n- AMRs are rooted, directed graphs with labeled edges.\n- Unlabeled edges reduce model parameters.\n- Tree-structured AMRs discard reentrancies.\n- Sequential AMRs linearize and anonymize graphs.\n\nSlideTitle: Encoder Architectures\nBulletPoints:\n- Recurrent Neural Network Encoders use BiLSTM for sequential input.\n- TreeLSTM Encoders process hierarchical structures, removing reentrancies.\n- Graph Convolutional Network (GCN) Encoders learn node embeddings from graph structure.\n\nSlideTitle: Stacking Encoders\nBulletPoints:\n- Structural encoders can be stacked on top of sequential encoders.\n- Sequence on top of structure refines input embeddings with graph information.\n- Structure on top of sequence applies structural encoders to BiLSTM outputs.\n\nSlideTitle: Experimental Results\nBulletPoints:\n- GCNSEQ model achieves state-of-the-art results on LDC2015E86 and LDC2017T10 datasets.\n- Graph encoders outperform sequential and tree encoders.\n- Explicitly encoding reentrancies benefits performance.\n\nSlideTitle: Handling Reentrancies and Long-Range Dependencies\nBulletPoints:\n- Graph encoders better handle reentrancies and long-range dependencies.\n- Manual inspection shows graph models generate more accurate pronouns and mentions.\n- Contrastive pairs used to analyze pronoun handling.\n\nSlideTitle: Conclusions\nBulletPoints:\n- Graph encoders improve AMR-to-text generation by encoding reentrancies.\n- Benefits are more significant with more reentrant structures and longer dependencies.\n- Future work will further explore graph encoders to enhance performance.",
    "SlideTitle: Introduction to Abstractive Sentence Summarization\nBulletPoints:\n- Exponential growth of online information necessitates automatic summarization systems\n- Focus on abstractive sentence summarization to generate shorter versions of sentences\n- Attentional seq2seq framework achieves state-of-the-art performance\n- Challenges include loss of control and lack of actual summarization\n\nSlideTitle: Limitations of Seq2Seq Models\nBulletPoints:\n- Seq2seq models depend solely on source text\n- Performance deteriorates with increased generation length\n- Tend to copy source words without summarization\n- Need for additional input beyond source sentence\n\nSlideTitle: Template-Based Summarization\nBulletPoints:\n- Traditional approach using manually defined templates\n- Templates are fluent and informative but time-consuming to create\n- Inspired by retrieve-based conversation systems\n- Soft templates as reference points for summarization\n\nSlideTitle: Re3Sum: A Hybrid Summarization System\nBulletPoints:\n- Combines seq2seq and template-based approaches\n- Consists of Retrieve, Rerank, and Rewrite modules\n- Uses Information Retrieval (IR) to find candidate soft templates\n- Jointly learns template saliency and summary generation\n\nSlideTitle: Methodology Overview\nBulletPoints:\n- Retrieve module finds candidate templates from training corpus\n- Rerank module measures template informativeness\n- Rewrite module generates summary using sentence and template hidden states\n- Uses BiRNN encoder and attentional RNN decoder\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Conducted on Annotated English Gigaword corpus\n- Evaluation using ROUGE metrics for informativeness\n- Re3Sum outperforms state-of-the-art seq2seq models\n- High-quality external summaries improve stability and readability\n\nSlideTitle: Linguistic Quality and Diversity\nBulletPoints:\n- Measures include length difference, copy ratio, and new named entities\n- Re3Sum generates more fluent and diverse summaries\n- Soft templates guide generation effectively\n- Promising prospects in generation diversity\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Soft templates improve readability and stability of summaries\n- Re3Sum generates informative and diverse summaries\n- Future work includes improving Retrieve module and testing on other tasks\n- Potential for document-level summarization and short text conversation",
    "SlideTitle: Introduction to Paraphrasing Diversity\nBulletPoints:\n- Importance of diversity in NLP tasks like data augmentation and conversational agents\n- Previous focus on semantic similarity, neglecting diversity\n- Reliance on top-k beam search leading to structurally similar paraphrases\n\nSlideTitle: Problem Formulation\nBulletPoints:\n- Aim to achieve highly diverse paraphrases without compromising quality\n- Novel approach using monotone submodular function maximization\n- Application in tasks like intent classification and paraphrase recognition\n\nSlideTitle: Related Work\nBulletPoints:\n- Previous methods focused on semantic similarity\n- Exploration of diversity in other NLP tasks\n- Use of determinantal point processes and Simultaneous Sparse Recovery for diversity\n\nSlideTitle: Submodular Function Maximization\nBulletPoints:\n- Benefits of submodular functions: flexibility, scalability, and near-optimal solutions\n- Introduction of Diverse Paraphraser using Submodularity (DiPS)\n- Extensive experiments demonstrating effectiveness\n\nSlideTitle: Methodology Overview\nBulletPoints:\n- SEQ2SEQ framework with diversity-inducing decoder\n- Use of submodular objective for high-quality paraphrases\n- Balance between fidelity and diversity in generation\n\nSlideTitle: Fidelity and Diversity Objectives\nBulletPoints:\n- Fidelity measured through syntactic and semantic similarity\n- Diversity encouraged through distinct n-grams and structural variations\n- Use of edit distance for structural diversity\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluation on datasets for paraphrase recognition and classification tasks\n- Comparison with baseline models and diversity schemes\n- DiPS outperforms in fidelity and diversity metrics\n\nSlideTitle: Data Augmentation and Applications\nBulletPoints:\n- Use of high-quality paraphrases for data augmentation\n- Improvement in accuracy for intent and question classification\n- Potential applications in conversational agents and text summarization\n\nSlideTitle: Conclusion\nBulletPoints:\n- DiPS generates diverse, high-quality paraphrases\n- Submodular approach offers control over fidelity and diversity\n- Effective for data augmentation and other NLG tasks",
    "SlideTitle: Introduction to Machine Question Answering\nBulletPoints:\n- Long-term goal of natural language processing\n- Challenge: Extracting answers from documents\n- Neural models show promise in paragraph-level QA\n- Need to adapt models for document-level input\n\nSlideTitle: Approaches to Document-Level QA\nBulletPoints:\n- Pipelined approach: Select a single paragraph\n- Confidence-based methods: Use multiple paragraphs\n- Importance of accurate confidence scores\n\nSlideTitle: Proposed Pipelined Method\nBulletPoints:\n- Linear classifier for paragraph selection\n- Distant supervision for training\n- Summed objective function to handle noise\n\nSlideTitle: Model Architecture\nBulletPoints:\n- Embedding with pretrained word vectors\n- Bi-directional GRU for processing\n- Attention mechanisms for context representation\n- Self-attention and prediction layers\n\nSlideTitle: Confidence Method for Multi-Paragraph QA\nBulletPoints:\n- Use un-normalized scores for confidence\n- Shared-normalization across paragraphs\n- No-answer option and merge approaches\n\nSlideTitle: Experimental Setup and Datasets\nBulletPoints:\n- Evaluation on TriviaQA and SQuAD datasets\n- Preprocessing and paragraph sampling techniques\n- Use of GloVe word vectors and Adadelta optimizer\n\nSlideTitle: Results and Performance\nBulletPoints:\n- Significant improvement on TriviaQA datasets\n- Shared-norm approach outperforms others\n- Effective on document-level SQuAD\n\nSlideTitle: Discussion and Error Analysis\nBulletPoints:\n- Challenges with multi-paragraph settings\n- Shared-norm approach resolves irrelevant text distraction\n- Error analysis suggests areas for improvement\n\nSlideTitle: Related Work and Conclusion\nBulletPoints:\n- Advances in reading comprehension datasets\n- Neural models with attention mechanisms\n- Importance of well-calibrated confidence scores\n- Contribution to open question answering systems",
    "SlideTitle: Introduction\nBulletPoints:\n- Time-critical analysis of social media data during sudden-onset events\n- Importance of timely access to social media information for humanitarian organizations\n- Challenge of classifying tweets in real-time during crisis events\n- Need for domain adaptation and semi-supervised learning\n\nSlideTitle: Proposed Model\nBulletPoints:\n- Novel model for domain adaptation and semi-supervised learning\n- Combines convolutional neural network with semi-supervised and domain adaptation networks\n- Learns high-level abstract representations invariant across domains\n- Uses stochastic gradient descent for simultaneous training\n\nSlideTitle: Model Architecture\nBulletPoints:\n- Input: Tweets mapped to distributed representations\n- Convolution and pooling layers for feature extraction\n- Three branches: supervised, semi-supervised, and domain adversarial\n- Loss components: classification, graph-based semi-supervised, and domain adversary\n\nSlideTitle: Semi-supervised Component\nBulletPoints:\n- Induces structural similarity between training instances\n- Graph-based approach for learning internal representations\n- Uses k-nearest neighbor graph for encoding similarity\n- Negative sampling for context prediction\n\nSlideTitle: Domain Adversarial Component\nBulletPoints:\n- Domain discriminator to distinguish between source and target domains\n- Sigmoid function for domain classification\n- Adversarial training to induce domain invariance\n- Balancing competing components for effective training\n\nSlideTitle: Experimental Settings\nBulletPoints:\n- Datasets: Nepal earthquake and Queensland floods Twitter data\n- Evaluation metrics: precision, recall, F-measure, AUC\n- Baselines: supervised, self-training, and graph-based semi-supervised\n- Domain adaptation with unsupervised adaptation\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Graph-based semi-supervised approach outperforms baselines\n- Significant improvements in F1 scores with semi-supervised learning\n- Domain adversarial training shows improvements in domain adaptation\n- Combined approach leverages unlabeled and labeled data effectively\n\nSlideTitle: Conclusions\nBulletPoints:\n- Deep learning framework for domain adaptation and semi-supervised learning\n- Combines supervised, semi-supervised, and domain adversarial training\n- Significant improvements over baselines in crisis-related tweet classification\n- Future work: utilizing small amounts of target domain labeled data",
    "SlideTitle: Introduction to Multilingual Corpora\nBulletPoints:\n- Useful for various NLP tasks like extracting parallel lexicons and feeding SMT systems\n- Importance of domain-specific corpora for high translation coverage\n- Challenges due to lack of large parallel text collections for all domains and languages\n- Wikipedia as a source for multilingual data\n\nSlideTitle: Background on Comparability in Multilingual Corpora\nBulletPoints:\n- Comparability is a fuzzy concept with various definitions\n- Ideal comparable corpus: similar texts in multiple languages\n- Four-class classification: Parallel, Strongly Comparable, Weakly Comparable, Non-comparable\n- Wikipedia's potential for multilingual text with different comparability levels\n\nSlideTitle: Challenges in Extracting Comparable Corpora from Wikipedia\nBulletPoints:\n- Wikipedia's chaotic taxonomy and arbitrary category links\n- Over-categorization and missing links in articles\n- Complexity of Wikipedia's category graph topology\n- Need for heuristic strategies to define domains\n\nSlideTitle: Domain-Specific Comparable Corpora Extraction\nBulletPoints:\n- Input: Top category of the domain (e.g., Sport)\n- Vocabulary definition from Wikipedia articles\n- Graph exploration using breadth-first search\n- Scoring categories to determine domain relevance\n\nSlideTitle: Parallel Sentence Extraction Technique\nBulletPoints:\n- Estimating similarity between cross-language sentence pairs\n- Use of cosine and length factor measures\n- Thresholds set to maximize F1 score\n- Non-supervised method generating a noisy parallel corpus\n\nSlideTitle: Evaluation: Statistical Machine Translation Task\nBulletPoints:\n- Validation of corpora quality through SMT impact\n- Comparison with Europarl corpus for English-Spanish translation\n- Testing on domain-specific and out-of-domain texts\n- Significant improvements in BLEU scores with Wikipedia data\n\nSlideTitle: Conclusions and Ongoing Work\nBulletPoints:\n- Model for automatic extraction of in-domain comparable corpora from Wikipedia\n- Application in extracting domain-specific parallel sentences\n- Significant improvement in machine translation quality\n- Future work on robust parallel sentence identification and evaluation schemes",
    "SlideTitle: Introduction to Event Relations\nBulletPoints:\nUnderstanding events is crucial for natural language understanding\nFocus on joint extraction of temporal and causal relations\nTemporal and causal relations often interact and influence each other\nProposed a joint framework for Temporal and Causal Reasoning (TCR)\n\nSlideTitle: Importance of Temporal and Causal Relations\nBulletPoints:\nTemporal and causal relations support applications like story completion and summarization\nTemporal relations can dictate causal relations and vice versa\nExample: Temporal relation dictated by causal relation (e.g., explosion before death)\n\nSlideTitle: Joint Framework for Temporal and Causal Reasoning\nBulletPoints:\nCombines temporal and causal extraction systems using a constrained conditional model (CCM)\nUses integer linear programming (ILP) to enforce constraints\nConstraints include cause-effect temporal precedence, symmetry, and transitivity\n\nSlideTitle: Challenges and Contributions\nBulletPoints:\nLimited work on joint extraction of temporal and causal relations\nDeveloped a jointly annotated dataset with dense temporal annotations\nImproved temporal extraction by incorporating explicit time expressions and knowledge-based rules\n\nSlideTitle: Related Work and Approaches\nBulletPoints:\nTemporal and causal relations represented by directed acyclic graphs\nExisting methods often break symmetric and transitive constraints\nAdopted ILP approach for global reasoning and consistent temporal graphs\n\nSlideTitle: Temporal and Causal Reasoning Framework\nBulletPoints:\nIntroduced temporal component with transitivity constraints\nIncorporated causal relations to complete the TCR framework\nConverted joint problem into an ILP for optimization\n\nSlideTitle: Experiments and Results\nBulletPoints:\nImproved temporal relation identification on TimeBank-Dense dataset\nDeveloped a new dataset with both temporal and causal relations\nJoint framework improved both temporal and causal performance\n\nSlideTitle: Discussion and Conclusion\nBulletPoints:\nObserved low precision and high recall on TB-Dense dataset\nJoint framework synthesizes temporal and causal signals effectively\nFuture work: better definition of events and handling temporal precedence exceptions\nPresented a novel joint framework for extracting temporal and causal relations",
    "SlideTitle: Introduction to Machine Translation Evaluation\nBulletPoints:\n- Automatic evaluation is more practical than manual evaluation for MT systems.\n- Manual evaluation is costly and time-consuming.\n- Automatic metrics facilitate cross-paper comparisons.\n- WMT Metrics Shared Task validates and develops new metrics annually.\n\nSlideTitle: Task Setup\nBulletPoints:\n- Participants provided with test sets for each language pair.\n- System-level metrics correlate with average human judgments.\n- Segment-level metrics correlate with human ranking of translations.\n- Participants choose language pairs and tracks to participate in.\n\nSlideTitle: Source and Reference Texts\nBulletPoints:\n- Source and reference texts from newstest2019.\n- Approximately 2,000 sentences per translation direction.\n- Exceptions for certain language pairs with fewer sentences.\n- Reference translations created in the same direction as MT systems.\n\nSlideTitle: System Outputs and Challenges\nBulletPoints:\n- Results affected by participating MT systems' quality.\n- Unsupervised MT systems for German\u2192Czech.\n- Domain adaptation for French\u2194German translations.\n- Evaluation based on 233 systems across 18 language pairs.\n\nSlideTitle: Manual Quality Assessment\nBulletPoints:\n- Direct Assessment (DA) used for human evaluation.\n- DA scores standardized per annotator.\n- Reference-based and reference-free evaluations.\n- Segment-level evaluation uses daRR judgements.\n\nSlideTitle: Baseline Metrics\nBulletPoints:\n- Baseline metrics prevent \"loss of knowledge.\"\n- Common metrics: BLEU, NIST, TER, WER, PER, CDER.\n- Moses scorer used for baseline metrics.\n- chrF and sacreBLEU metrics also evaluated.\n\nSlideTitle: Submitted Metrics Overview\nBulletPoints:\n- 24 metrics from 13 research groups.\n- Metrics include BEER, BERTr, CharacTER, EED, ESIM.\n- Metrics use various features like embeddings and edit distances.\n- Quality Estimation (QE) systems evaluated in \"QE as a Metric\" track.\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- System-level evaluation uses Pearson correlation.\n- Segment-level evaluation uses Kendall's Tau-like formulation.\n- Stability of metrics depends on underlying MT systems.\n- Semantic features in metrics show promise for future improvements.\n\nSlideTitle: Conclusion\nBulletPoints:\n- WMT19 Metrics Shared Task evaluates MT metrics' correlation with human judgment.\n- Best metrics achieve high Pearson correlation at system-level.\n- Segment-level evaluation shows varied correlations.\n- Future metrics may benefit from semantic understanding features.",
    "SlideTitle: Introduction to Zero-Shot Learning\nBulletPoints:\n- Fundamental problem in machine learning: automatic classification\n- Challenge: dynamic environments with emerging new classes\n- Zero-shot learning (ZSL) aims to classify unseen classes\n- Humans transfer knowledge from known to unknown domains\n- ZSL framework exploits semantic knowledge for unseen classes\n\nSlideTitle: Types of Semantic Knowledge in ZSL\nBulletPoints:\n- Semantic attributes: visual concepts and properties\n- Concept ontology: class hierarchy and knowledge graphs\n- Semantic word embeddings: implicit relationships between words\n- Few studies exploit combinations of these knowledge types\n\nSlideTitle: Proposed Framework Overview\nBulletPoints:\n- Novel two-phase framework for zero-shot text classification\n- Coarse-grained classification: seen vs. unseen classes\n- Fine-grained classification: final class decision\n- Utilizes convolutional neural networks (CNNs)\n- Trained using labelled data of seen classes only\n\nSlideTitle: Data and Feature Augmentation\nBulletPoints:\n- Data augmentation: topic translation technique\n- Feature augmentation: integrated semantic knowledge\n- Transfer knowledge from seen to unseen classes\n- Enhances framework's capability to detect unseen classes\n\nSlideTitle: Methodology and Notations\nBulletPoints:\n- Disjoint sets of seen (C_S) and unseen (C_U) classes\n- Coarse-grained classification: binary decision\n- Fine-grained classification: traditional or zero-shot classifier\n- Notations: word embeddings, class labels, relationship vectors\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Datasets: DBpedia ontology and 20newsgroups\n- Evaluation metrics: accuracy for binary and multi-class classification\n- Augmented data improved detection of unseen classes\n- Zero-shot classifier with integrated inputs achieved highest accuracy\n\nSlideTitle: Comparison with Baselines\nBulletPoints:\n- Count-based model and label similarity approaches\n- RNN Autoencoder and CNN+FC models\n- Proposed framework achieved highest overall accuracy\n- Effective knowledge transfer from seen to unseen classes\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Proposed CNN-based two-phase framework for ZSL\n- Data and feature augmentation improved accuracy\n- Highest overall accuracy compared to baselines\n- Future work: multi-label classification and larger datasets",
    "SlideTitle: Introduction to Multi-task Learning in NLP\nBulletPoints:\n- Supervised learning typically involves training individual models for each NLP task.\n- Multi-task Learning (MTL) allows knowledge transfer across related tasks.\n- MTL is effective in boosting performance, especially in low-resource languages.\n- Focus on enhancing models by borrowing knowledge from high-resource languages.\n\nSlideTitle: Challenges in Low-resource NLP\nBulletPoints:\n- Many languages lack sufficient labeled data for NLP tasks.\n- Annotating data for all 7,099 living languages is unattainable.\n- Rapid development needed for low-resource languages in special applications.\n- Data sparsity affects performance in low-resource settings.\n\nSlideTitle: Proposed Multi-lingual Multi-task Architecture\nBulletPoints:\n- Combines different transfer models through parameter sharing.\n- Shares character embeddings, CharCNN, and LSTM across models.\n- Uses different sharing strategies for various transfer schemes.\n- Aims to transfer universal knowledge among tasks and languages.\n\nSlideTitle: Model Basic Architecture\nBulletPoints:\n- Sequence labeling assigns categorical labels to tokens in sentences.\n- LSTM-CRFs-based models are suitable for multi-lingual multi-task learning.\n- Character embeddings transfer morphological and semantic information.\n- Highway networks enhance performance by transforming character feature vectors.\n\nSlideTitle: Multi-task Multi-lingual Architecture\nBulletPoints:\n- Focuses on enhancing low-resource tasks by transferring knowledge.\n- Shares character embeddings and CharCNN between similar scripts.\n- Shares bidirectional LSTM layer across models for feature extraction.\n- Uses shared linear and CRFs layers for task-specific knowledge transfer.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluated on Name Tagging and POS Tagging tasks with various languages.\n- Substantial performance gains over mono-lingual single-task models.\n- Cross-lingual transfer outperforms cross-task transfer in low-data scenarios.\n- Data augmentation with related languages boosts performance.\n\nSlideTitle: Qualitative Analysis and Ablation Studies\nBulletPoints:\n- Shared character-level networks transfer morphological and semantic information.\n- Cross-lingual transfer improves tag prediction and reduces overfitting.\n- Ablation studies show contributions of individual components.\n- Model performance is not heavily reliant on auxiliary task data.\n\nSlideTitle: Related Work and Future Directions\nBulletPoints:\n- MTL applied in various NLP areas like machine translation and text classification.\n- Previous models focused on different transfer schemes and supervision levels.\n- Future work includes applying architecture to other tasks like Event Extraction.\n- Explore integrating incremental learning for rapid adaptation to new tasks.",
    "SlideTitle: Introduction to Natural Language Inference (NLI)\nBulletPoints:\n- Focus on Natural Language Inference (NLI) task\n- Determine logical relationship: entailment, neutral, contradiction\n- Use of high-quality datasets like SNLI and MultiNLI\n- Importance of discourse markers in NLI\n\nSlideTitle: Discourse Marker Augmented Network\nBulletPoints:\n- Proposal of Discourse Marker Augmented Network for NLI\n- Transfer knowledge from Discourse Marker Prediction (DMP) task\n- Use of sentence encoder model from DMP\n- Incorporation of reinforcement learning for label confidence\n\nSlideTitle: Task Description\nBulletPoints:\n- NLI: Determine relationship between premise and hypothesis\n- DMP: Predict discourse marker linking two sentence halves\n- Use of BookCorpus for training sentence encoder\n\nSlideTitle: Sentence Encoder Model\nBulletPoints:\n- Use of Glove embeddings and bi-directional LSTM\n- Max pooling for feature extraction\n- Combination of hidden states for sentence representation\n- Softmax function for discourse marker prediction\n\nSlideTitle: Discourse Marker Augmented Network Architecture\nBulletPoints:\n- Encoding layer with word, character, POS, NER embeddings\n- Interaction layer with attention mechanism\n- Modeling layer using bi-directional LSTMs\n- Output layer with softmax for logical relation prediction\n\nSlideTitle: Training and Objective Function\nBulletPoints:\n- Use of log probabilities and reinforcement learning\n- Objective function combines cross-entropy loss and reward\n- REINFORCE algorithm for minimizing negative expected reward\n- Integration of annotator label diversity\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Use of BookCorpus, SNLI, and MultiNLI datasets\n- State-of-the-art results on SNLI and MultiNLI\n- Ensemble model performance: 89.6% on SNLI, 80.3% on MultiNLI\n- Ablation analysis to evaluate model components\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Discourse Marker Augmented Network enhances NLI task\n- Transfer learning from DMP augments semantic representation\n- Reinforcement learning optimizes model with annotator views\n- Future work: choice of discourse markers and transfer learning sources",
    "SlideTitle: Introduction\nBulletPoints:\n- Challenges in executing instruction sequences: language grounding, discourse dependencies, action generation\n- Example: Moving and mixing chemicals in beakers\n- Model maps instructions to actions considering previous utterances and world state\n- Focus on directly generating action sequences\n\nSlideTitle: Key Challenges\nBulletPoints:\n- Resolving references without explicit modeling\n- Learning sequences of actions for high-level tasks\n- Contextual cues from interaction history and world state\n- Continuous change in instruction history and world state\n\nSlideTitle: Proposed Model\nBulletPoints:\n- Attention-based model for instruction execution\n- Inputs: current instruction, previous instructions, initial and current world states\n- Predicts next action using attention encodings\n- Trained with rewards from environment exploration\n\nSlideTitle: Technical Overview\nBulletPoints:\n- Task: Map instructions to actions in a given domain\n- Example: ALCHEMY domain with beakers and actions like POP and PUSH\n- Execution: Sequence of state-action pairs until STOP action\n- Model uses RNN and attention for action prediction\n\nSlideTitle: Learning Approach\nBulletPoints:\n- Access to instruction sequences with start and goal states\n- Maximizes expected immediate reward using potential-based reward shaping\n- Addresses exploration problem with learning algorithm observing rewards for all actions\n\nSlideTitle: Evaluation\nBulletPoints:\n- Evaluated on SCONE with three domains: ALCHEMY, SCENE, TANGRAMS\n- Task completion accuracy: 89.1% for single instruction, 62.7% for complete sequences\n- Comparison with previous approaches using logical representations\n\nSlideTitle: Related Work\nBulletPoints:\n- Studies on executing instructions using SAIL corpus and navigation tasks\n- Focus on sequences of environment manipulation instructions\n- SCONE corpus designed for discourse context-dependence phenomena\n\nSlideTitle: Discussion and Future Work\nBulletPoints:\n- Model reasons about context-dependent instructional language\n- SESTRA learning algorithm overcomes learned biases in on-policy learning\n- Future work: Use intermediate world states from previous turns\n- Potential for using estimators to predict rewards for all actions",
    "SlideTitle: Introduction to Semantic Parsers\nBulletPoints:\n- Semantic parsers predict predicate-argument structures of sentences\n- Traditionally trained from difficult-to-gather datasets\n- Recent research explores non-expert training for semantic supervision\n- Crowdsourcing enables large-scale dataset creation for training parsers\n\nSlideTitle: QA-SRL Annotation Scheme\nBulletPoints:\n- QA-SRL uses question-answer pairs for predicate-argument relationships\n- Intuitive for non-experts and matches traditional annotation schemes\n- Used for tasks like Open IE\n- High precision but challenging to achieve high recall\n\nSlideTitle: Crowdsourcing QA-SRL\nBulletPoints:\n- New scalable approach for crowdsourcing QA-SRL\n- Streamlined web interface with autosuggest and quality control\n- Validation stage ensures high precision\n- Produced QA-SRL Bank 2.0 with 265,140 question-answer pairs in 9 days\n\nSlideTitle: Models for QA-SRL Parsing\nBulletPoints:\n- Pipeline approach: span detection and question generation\n- Best model uses span-based representation and custom LSTM\n- Trained directly from crowdsourced span labels\n- Achieved 82.2% span-detection F1 and 47.2% exact-match question accuracy\n\nSlideTitle: Data Annotation Process\nBulletPoints:\n- QA-SRL annotation involves question-answer pairs for each verbal predicate\n- Crowdsourcing pipeline with generation and validation steps\n- Autocomplete feature streamlines question writing\n- High precision and moderate agreement in quality judgments\n\nSlideTitle: Span Detection and Question Generation\nBulletPoints:\n- Span detection models: BIO sequence and span-based\n- Question generation models: local and sequence-based\n- Span-based model improves precision and recall\n- Sequence model achieves higher exact match accuracy\n\nSlideTitle: Data Expansion and Retraining\nBulletPoints:\n- Parser predictions validated to fill data gaps\n- Over-generation of QA pairs for already-annotated sentences\n- Expansion increased valid questions by 20%\n- Retrained model showed improvements in span detection and question generation\n\nSlideTitle: Final Evaluation and Results\nBulletPoints:\n- Human evaluation of models using crowdsourced validation\n- Sequence-based question generation models more accurate\n- Expanded data training resulted in more questions and spans\n- Final model achieved 82.64% question accuracy and 77.61% span-level precision\n\nSlideTitle: Conclusion\nBulletPoints:\n- QA-SRL scaled to large datasets for predicate-argument structures\n- New crowdsourcing approach enabled rapid data collection\n- First high-quality parser for QA-SRL developed\n- Validation stage and parser tuning increased dataset recall",
    "SlideTitle: Introduction to SIGHAN 2015 Bake-off\nBulletPoints:\n- Chinese spelling checkers are challenging due to lack of word delimiters and numerous characters.\n- Errors often arise from phonologically and visually similar words.\n- SIGHAN 2013 and 2014 provided benchmarks for spelling checkers.\n- SIGHAN 2015 focuses on essays by learners of Chinese as a Foreign Language (CFL).\n\nSlideTitle: Task Description\nBulletPoints:\n- Evaluate the capability of a Chinese spelling checker.\n- Input: Passage with/without spelling errors.\n- Output: Locations of incorrect characters and suggested corrections.\n- Format: \"pid [, location, correction]+\" for errors, \"pid, 0\" for no errors.\n\nSlideTitle: Data Preparation\nBulletPoints:\n- Learner corpus from TOCFL essays in Taiwan.\n- Errors manually annotated by native speakers.\n- Data split into Training, Dryrun, and Test Sets.\n- Focus on spelling error correction, not grammatical errors.\n\nSlideTitle: Performance Metrics\nBulletPoints:\n- Judged at detection and correction levels.\n- Metrics: True Positive, False Positive, True Negative, False Negative.\n- Importance of reducing false positive rate.\n- F1 score used to balance precision and recall.\n\nSlideTitle: Evaluation Results\nBulletPoints:\n- CAS achieved best error detection with F1 score of 0.6404.\n- CAS correction accuracy at 0.6918, highest among teams.\n- No system excelled in all metrics.\n- CAS and NCTU&NTUT provided best overall performance.\n\nSlideTitle: Approaches by Participant Teams\nBulletPoints:\n- CAS: Unified framework using HMM-based approach and two-stage filter.\n- NCTU&NTUT: Word vector/conditional random field based detector.\n- Focus on maximizing language model score for corrections.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- Overview of SIGHAN 2015 Bake-off and participant approaches.\n- Contributions to Chinese spell checker development.\n- Data sets and evaluation tools made publicly available.\n- Future focus on Chinese grammatical error correction and new language resources.",
    "SlideTitle: Introduction to Language Identification\nBulletPoints:\n- Language identification is crucial in natural language processing applications.\n- Improves accuracy in letter-to-phoneme conversion and machine transliteration.\n- Traditionally approached using character-level n-gram language models.\n- Proposal to use support vector machines (SVMs) for short text language identification.\n\nSlideTitle: Previous Work on Language Identification\nBulletPoints:\n- N-gram approaches popular for general text categorization.\n- Cavnar and Trenkle achieved 99.8% accuracy with n-gram models for longer texts.\n- Accuracy drops for shorter texts; SVMs achieve higher accuracy.\n- Konstantopoulos focused on proper nouns, achieving better results with tailored models.\n\nSlideTitle: Language Identification with SVMs\nBulletPoints:\n- SVMs use character n-gram occurrences as features.\n- Tested various kernels: linear, sigmoid, and radial basis function (RBF).\n- Optimal n-gram lengths found for single and full names.\n- SVMs outperform language models on specific data sets.\n\nSlideTitle: Intrinsic Evaluation: Transfermarkt Corpus\nBulletPoints:\n- European soccer player names annotated with 13 national languages.\n- SVMs outperform language models, especially on last names.\n- Data is noisy; player nationality doesn't always match name language.\n\nSlideTitle: Intrinsic Evaluation: CEJ Corpus\nBulletPoints:\n- Combined list of Chinese, English, and Japanese names.\n- Linear kernel used due to large corpus size.\n- SVMs reduce error rate by over 50% compared to language models.\n\nSlideTitle: Application to Machine Transliteration\nBulletPoints:\n- Language identification can determine word pronunciation.\n- Tested on English-Hindi corpus of names.\n- Language identification accuracy: 80% (linear), 84% (RBF), 83% (sigmoid).\n\nSlideTitle: Experimental Setup and Results\nBulletPoints:\n- Combined language identification with transliteration.\n- No significant improvement in accuracy with data splitting.\n- Reduction in data size affects training of separate models.\n- Language identification provides useful information to transliteration.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- SVMs with n-gram counts outperform traditional language models.\n- Language identification as preprocessing shows no significant accuracy change.\n- Future exploration: language identification probabilities as transliteration features.\n- Investigate non-disjoint data sets for transliteration model training.",
    "SlideTitle: Introduction to Retraction Citations\nBulletPoints:\n- Increase in retracted articles with rise in scientific publications\n- Retractions help remove erroneous or unethical research\n- Challenges posed by citations to retracted articles\n- Two types of retraction citations: pre-retraction and post-retraction\n- High pre-retraction citations often lead to post-retraction citations\n\nSlideTitle: Case Study: Scott S. Reuben\nBulletPoints:\n- Convicted of fabricating data in 25 studies\n- Articles remained popular post-retraction\n- Self-citation by authors contributes to perceived validity\n\nSlideTitle: Data Collection Methodology\nBulletPoints:\n- Accessed ScienceDirect in October 2014\n- Queried for \"RETRACTED\" in titles and notices\n- Selected top five articles cited over 20 times since 2015\n- Analyzed 109 citing documents from 2015 and 2016\n\nSlideTitle: Citation Categorization\nBulletPoints:\n- Positive: Cited as legitimate prior work\n- Negative: Mentioned as inappropriate\n- Neutral: Mentioned without judgment on validity\n\nSlideTitle: Case Study: S\u00e9ralini Affair\nBulletPoints:\n- Study on GM crops and tumors in rats\n- Retracted due to small sample size\n- Media frenzy and public debate on GM food safety\n- Republished in 2014, continued to receive citations\n\nSlideTitle: Case Study: Dipak Das\nBulletPoints:\n- Retracted due to data manipulation and falsification\n- Continued positive citations post-retraction\n- Popularity of \"authentic leadership\" in management studies\n\nSlideTitle: Discussion and Conclusions\nBulletPoints:\n- Retracted articles continue to be cited despite notices\n- Public and media interest can drive citations\n- Editors' approval of results may lead to continued citations\n- Invalid research appearing as valid is concerning\n\nSlideTitle: Recommendations\nBulletPoints:\n- Publishers should conduct reference checks for retracted citations\n- Request explanations for citing retracted articles\n- Delete retracted articles from publishers' websites\n- Retraction notices should remain, but articles should not be accessible",
    "SlideTitle: Introduction to Semantic Parsing and Surface Realization\nBulletPoints:\nIncreased interest in graph-structured semantic representations\nNeed for a formal framework for manipulating graphs\nIntroduction of graph grammar and DAG automata\nFocus on DAG transducers for natural language generation (NLG)\n\nSlideTitle: Challenges in DAG Transducers\nBulletPoints:\nExisting DAG transducers are limited in handling type-logical semantic graphs\nDifficulties in transforming hierarchical structures\nIssues with multi-rootedness and unconnected nodes\nNeed for a new approach to handle diverse internal structures\n\nSlideTitle: A New DAG Transducer Design\nBulletPoints:\nIntroduction of a novel DAG transducer for structure transformation\nUse of declarative programming for graph-to-string transformation\nOutput as a program rather than a direct data structure\nFlexibility in handling various linguistic structures\n\nSlideTitle: Declarative Programming Language for NLG\nBulletPoints:\nSyntax and semantics of the declarative programming language\nString concatenation and variable assignment\nExample program illustrating the transformation process\nConstruction of internal trees for program execution\n\nSlideTitle: DAG Transduction-based NLG Strategy\nBulletPoints:\nTwo-step strategy for meaning-to-text transformation\nPhase 1: Translate semantic graph into sequential lemmas\nPhase 2: Use neural sequence-to-sequence model for surface strings\nAdaptation for different morphosyntactic properties of languages\n\nSlideTitle: Inducing Transduction Rules\nBulletPoints:\nEmpirical study on DAG transduction-based NLG\nUse of EDS graphs and DeepBank data set\nConstraints for generating reasonable strings\nFine-to-coarse transduction strategy for robustness\n\nSlideTitle: Evaluation and Analysis\nBulletPoints:\nBLEU-4 scores for generated lemma sequences and sentences\nComparison with purely neural baseline\nEfficiency of the beam search decoder\nFull-coverage robustness for NLP applications\n\nSlideTitle: Conclusion\nBulletPoints:\nExtension of DAG automata work for flexible DAG transducer\nUse of declarative programming to reduce computation burden\nDevelopment of a practical system for semantic-graph-to-string task\nHigh accuracy, efficiency, and robustness in NLG applications",
    "SlideTitle: Introduction to Knowledge Base Question Answering\nBulletPoints:\n- Importance of large-scale knowledge bases like DBPedia and Freebase\n- Use of semantic parsing for mapping questions to formal meaning representations\n- Challenges with traditional semantic parsing approaches\n- Proposal of a new semantic parsing framework leveraging knowledge bases\n\nSlideTitle: Background and Knowledge Base Design\nBulletPoints:\n- Aim to map natural language questions to logical form queries\n- Knowledge base as a collection of subject-predicate-object triples\n- Use of Freebase as a large database with millions of topics and facts\n- Introduction of compound value type (CVT) entities\n\nSlideTitle: Query Graph Design\nBulletPoints:\n- Query graph as a restricted subset of \u03bb-calculus\n- Types of nodes: grounded entity, existential variable, lambda variable, aggregation function\n- Example of a query graph for a specific question\n- Differences from previous query graph designs\n\nSlideTitle: Staged Query Graph Generation\nBulletPoints:\n- Focus on generating query graphs with specific properties\n- Definition of states and actions for query graph generation\n- Use of a reward function to estimate the likelihood of correct parsing\n- Example of linking topic entities and identifying core inferential chains\n\nSlideTitle: Deep Convolutional Neural Networks for Semantic Similarity\nBulletPoints:\n- Use of Siamese neural networks for identifying core inferential chains\n- Mapping questions to patterns and comparing with candidate chains\n- Advantages of CNNs over lexical matching approaches\n- Training process and data generation for CNN models\n\nSlideTitle: Augmenting Constraints and Aggregations\nBulletPoints:\n- Expanding query graphs with constraints and aggregation functions\n- Use of simple rules to retain legitimate constraints\n- Examples of constraint and aggregation actions\n- Features for learning the reward function\n\nSlideTitle: Experimental Results and Analysis\nBulletPoints:\n- Use of WEBQUESTIONS dataset for evaluation\n- Comparison of system performance with existing methods\n- Contributions of each component to overall performance\n- Error analysis and areas for improvement\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Presentation of a semantic parsing framework for question answering\n- Definition of query graph as a meaning representation\n- Substantial improvement over previous methods on WEBQUESTIONS dataset\n- Future directions for extending query graph capabilities and exploring new models",
    "SlideTitle: Introduction to User Text Analysis\nBulletPoints:\n- Growth of online social networks enables broader context analysis\n- User text can infer attributes like location, gender, impact, and age\n- Language use reflects social status and occupational class\n- Study focuses on predicting occupational class from Twitter data\n\nSlideTitle: Standard Occupational Classification (SOC)\nBulletPoints:\n- SOC is a UK government system for classifying occupations\n- Jobs categorized hierarchically based on skill requirements\n- Includes 9 major groups, 25 sub-major groups, 90 minor groups, and 369 unit groups\n- Used for mapping Twitter users to occupations\n\nSlideTitle: Data Collection and Set Creation\nBulletPoints:\n- Created a new data set of Twitter users mapped to occupations\n- Used user description field for self-disclosed job titles\n- Retrieved accounts using Twitter's Search API and SOC unit groups\n- Final data set: 5,191 users from 55 minor groups, 10,796,836 tweets\n\nSlideTitle: Features for Occupational Class Prediction\nBulletPoints:\n- Two types of features: user level and textual features\n- User level features include followers, friends, retweets, etc.\n- Textual features derived from user's tweets using word embeddings and clusters\n- SVD and neural embeddings used for feature representation\n\nSlideTitle: Classification with Gaussian Processes\nBulletPoints:\n- Gaussian Processes (GP) used for classification\n- Bayesian non-parametric framework with kernel functions\n- GP offers interpretability via Automatic Relevance Determination\n- Used Expectation Propagation for inference\n\nSlideTitle: Experimental Results and Predictive Accuracy\nBulletPoints:\n- Compared accuracy of classification methods on held-out data\n- Textual features improved performance over user level features\n- Clusters outperformed embeddings in most cases\n- GP classifier showed strong predictive performance\n\nSlideTitle: Qualitative Analysis and Topic Divergence\nBulletPoints:\n- Word clusters used to extract qualitative insights\n- Topics like 'Corporate', 'Software Engineering', 'Health', etc. identified\n- Topic divergence observed between lower and higher skilled classes\n- Jensen-Shannon divergence used to quantify topic differences\n\nSlideTitle: Related Work and Conclusions\nBulletPoints:\n- Previous studies in psychology and economics on occupational prediction\n- Sociolinguistic studies highlight language use differences by social status\n- Our study is the first large-scale analysis of language use for occupational inference\n- Results show occupation influences text use, useful for future applications",
    "SlideTitle: Introduction to the ACL Anthology\nBulletPoints:\n- Open access to ACL conference proceedings and journal articles\n- Hosts third-party computational linguistics literature\n- Over 43,000 papers and 4,500 daily requests\n- Code available under Creative Commons licenses\n- Maintained by volunteers, coordinated by the Anthology editor\n\nSlideTitle: Current State of the Anthology\nBulletPoints:\n- Proposed by Steven Bird in 2001, launched in 2002\n- Provides access to papers in PDF and metadata formats\n- Daily administration by volunteers, with paid assistance for larger issues\n- Hosted at Saarland University, future hosting by ACL\n- Tech stack includes Ruby on Rails, Solr, PostgreSQL, Nginx, Debian GNU-Linux\n\nSlideTitle: Running the Anthology as a Community Project\nBulletPoints:\n- Contributors recruited from ACL conferences\n- New features developed by researchers using the Anthology\n- Challenges in preserving operational workflow knowledge\n- Importance of the Anthology editor role\n- Consideration of commercial support for publishing\n\nSlideTitle: Future Proofing the Anthology\nBulletPoints:\n- Code, documentation, and feature requests on GitHub\n- Improved documentation for easier volunteer recruitment\n- Use of Docker for virtualized environment and dependency management\n- Development versions available for testing and training\n- Security concerns with outdated software dependencies\n\nSlideTitle: Challenges for the Anthology\nBulletPoints:\n- Maintaining community buy-in by extending capabilities\n- Addressing the popularity of pre-prints with anonymous options\n- Leveraging Anthology data for reviewer matching\n- Mining reviewer interests from previous publications\n- Integration of solutions directly into the Anthology\n\nSlideTitle: Anonymous Pre-prints and Reviewer Matching\nBulletPoints:\n- Anonymous pre-prints to combat bias in publications\n- Collaboration with existing pre-print services or hosting within Anthology\n- Reviewer matching using Anthology data to find suitable reviewers\n- Mining text and metadata from submitted papers\n- Synergistic solutions for pre-prints and reviewer matching\n\nSlideTitle: Conclusion and Call to Action\nBulletPoints:\n- ACL Anthology as a key resource for NLP researchers\n- Behind-the-scenes software engineering and maintenance\n- Improved documentation and server setup for easier volunteer training\n- Invitation to community members to experiment with Anthology images\n- Open invitation for system administration, software development, and more\n\nSlideTitle: Get Involved with the ACL Anthology\nBulletPoints:\n- Contact the Anthology team for collaboration\n- Current needs in system administration, software development, and Docker integration\n- Encouragement for community members to contribute and support the Anthology's growth",
    "SlideTitle: Introduction to Machine Translation Systems\nBulletPoints:\n- Statistical Machine Translation (SMT) is costly due to the need for large parallel corpora.\n- Rule-Based Machine Translation (RBMT) uses a monolingual corpus for target word selection.\n- Open Asian parallel corpora are available for research.\n- Statistical Post Editing (SPE) applied to RBMT shows advantages in domain adaptation.\n\nSlideTitle: Overview of Toshiba System\nBulletPoints:\n- RBMT system is transfer-based with morphological and syntactic analysis.\n- High translation performance achieved through extensive rule sets.\n- Japanese-Korean translation omits syntactic analysis due to grammatical similarity.\n\nSlideTitle: Statistical Post Editing (SPE)\nBulletPoints:\n- SPE uses phrase-based SMT for domain adaptation.\n- Training involved translating source sentences with RBMT.\n- Language models trained using KenLM.\n- Distortion limits set based on language pair similarities.\n\nSlideTitle: System Combination using RNNLM\nBulletPoints:\n- SPE and SMT generate different translation candidates.\n- System combination achieved through n-best reranking with RNNLM.\n- RNNLMs trained with 500 hidden layers and 50 classes.\n- Improved translation results by combining SPE and SMT candidates.\n\nSlideTitle: Tuning and Pre/Postprocessing\nBulletPoints:\n- Technical term dictionaries selected for each task.\n- English word correction applied for misspelled words.\n- Japanese KATAKANA normalization improved BLEU scores.\n- Post-processing reduced unknown words in SMT results.\n\nSlideTitle: Experimental Results\nBulletPoints:\n- SPE improves RBMT results on BLEU and RIBES scores.\n- Reranking improves BLEU score by 0.3-0.5.\n- COMB shows better performance than other systems.\n- SPE accounts for 80% of translations in some tasks.\n\nSlideTitle: Official Results\nBulletPoints:\n- Toshiba systems ranked in top three in human evaluation.\n- COMB shows higher HUMAN score in ja-zh and zh-ja.\n- System combination improves human evaluation of Chinese translations.\n- BLEU scores of SPE and SMT nearly equal in ja-zh and zh-ja.\n\nSlideTitle: Conclusion\nBulletPoints:\n- Toshiba systems applied SPE and system combination with RNNLM.\n- Higher BLEU scores achieved than phrase-based SMT.\n- System combination improved BLEU in Japanese-English and Japanese-Chinese pairs.\n- Need to establish multi-system combinations for practical use.",
    "SlideTitle: Introduction to Phrase-based Statistical Machine Translation\nBulletPoints:\n- Phrase-based Statistical Machine Translation (PB-SMT) is a data-oriented approach.\n- Widely used for over 10 years.\n- Moses toolkit developed by the University of Edinburgh.\n- Training is the most crucial step in building a translation system.\n\nSlideTitle: Word or Phrase Alignment\nBulletPoints:\n- Alignment affects translation quality and is time-consuming.\n- Probabilistic approach determines best alignment links.\n- IBM models and HMM alignment models are widely used.\n- GIZA++ implements IBM Models for word alignment.\n\nSlideTitle: Associative Approaches\nBulletPoints:\n- Do not rely on alignment models but on statistical measures.\n- Examples include Dice coefficient and mutual information.\n- Anymalign and Cutnalign are associative approaches.\n- Anymalign samples sub-corpora for word occurrence distributions.\n\nSlideTitle: Bilingual Hierarchical Sub-sentential Alignment\nBulletPoints:\n- Cutnalign uses recursive binary segmentation for alignment.\n- Measures translation link strength between word pairs.\n- Computes optimal joint clustering of a bipartite graph.\n- Segments and aligns sentence pairs.\n\nSlideTitle: Data Used in Experiments\nBulletPoints:\n- Chinese-Japanese JPO Patent Corpus used.\n- Contains 1 million parallel sentences in four domains.\n- Sentences of 40 words or less used for training.\n- Stanford Segmenter and Juman used for segmentation.\n\nSlideTitle: Improvements in Alignment Methods\nBulletPoints:\n- Introduced multi-processing to accelerate alignment.\n- Re-implemented core of Cutnalign in C.\n- Approximations in N cut computation for speed-up.\n- Reduced search space in hierarchical alignment.\n\nSlideTitle: Experiment Settings and Results\nBulletPoints:\n- Experiments with GIZA++ or MGIZA.\n- Evaluated using BLEU and RIBES metrics.\n- Anymalign and Cutnalign used to build phrase tables.\n- Fastest training time achieved with specific settings.\n\nSlideTitle: Conclusion\nBulletPoints:\n- Accelerated development of SMT systems.\n- Significant reduction in training time.\n- Translation quality remains steady with different timeouts.\n- Fastest training time was 57 minutes, one fifth of GIZA++/MGIZA.",
    "SlideTitle: Introduction to Neural Machine Translation (NMT)\nBulletPoints:\n- NMT is a successful paradigm in recent years\n- More fluent than previous statistical systems\n- Adequacy remains a major concern\n- Common mistakes: dropping source words, repeating words\n- Various solutions proposed, including architectural changes and attention transformations\n\nSlideTitle: Proposed Solution\nBulletPoints:\n- New solution modifies attention transformation, not overall architecture\n- Replaces traditional softmax with transformations promoting sparsity or bounding attention\n- Introduces constrained sparsemax for sparse and bounded attention weights\n- End-to-end differentiable, suitable for gradient backpropagation\n\nSlideTitle: Contributions\nBulletPoints:\n- Formulation of constrained sparsemax with efficient algorithms\n- Empirical comparison of attention transformations\n- Error analysis with new metrics for coverage problems\n\nSlideTitle: Preliminaries\nBulletPoints:\n- Standard attentional encoder-decoder model\n- Use of Bi-LSTM encoder for source words representation\n- Conditional probability of target sentence computed with softmax output layer\n\nSlideTitle: Sparse and Constrained Attention\nBulletPoints:\n- Alternatives to softmax: sparsemax, constrained softmax, constrained sparsemax\n- Sparsemax yields sparse probability distribution\n- Constrained softmax bounds attention probabilities\n- Constrained sparsemax combines sparsity and bounded probabilities\n\nSlideTitle: Fertility Bounds and Strategies\nBulletPoints:\n- Three fertility setting methods: CONSTANT, GUIDED, PREDICTED\n- Use of <SINK> token for unbounded fertility\n- Exhaustion strategies to encourage attention to words with larger credit\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluated on three language pairs with small datasets\n- Implementation on OpenNMT-py toolkit\n- Baselines: softmax attention, COVPENALTY, COVVECTOR\n- Sparse models show higher BLEU and METEOR scores\n- Better REP and DROP scores suggest improved coverage\n\nSlideTitle: Conclusions\nBulletPoints:\n- New approach addresses coverage problem in NMT\n- Sparse and constrained attention transformations proposed\n- Efficient algorithms for constrained sparsemax\n- Fertility prediction model leads to sparse alignments and avoids repeated words",
    "SlideTitle: Introduction to Cross-Lingual Transfer Learning\nBulletPoints:\n- Supervised learning dominates NLP but requires large annotated corpora.\n- Many languages lack annotated text, limiting NLP progress.\n- Cross-lingual transfer learning transfers knowledge from high- to low-resource languages.\n- Traditional methods rely on a single source language, limiting transferability.\n- Our novel method uses multiple languages and accounts for transfer quality.\n\nSlideTitle: Proposed Method for Multilingual Transfer\nBulletPoints:\n- Inspired by truth inference in crowd-sourcing.\n- Adapts ideas to learn transfer quality and language-specific errors.\n- Uses a Bayesian graphical model for inference.\n- Can operate in unsupervised and supervised settings.\n\nSlideTitle: Approach to Multilingual Transfer\nBulletPoints:\n- Collection of models trained in high-resource settings.\n- Simple approaches include single model selection or ensemble voting.\n- Proposed methods: BEA uns for zero-shot and RaRe for few-shot transfer.\n- BEA uns learns model reliability without target language annotations.\n\nSlideTitle: Zero-Shot Transfer with BEA uns\nBulletPoints:\n- Selects a subset of reliable models for better ensemble performance.\n- Uses a probabilistic graphical model to infer model quality.\n- Differentiates reliable models through confusion matrices.\n- Applies mean-field variational Bayes for inference.\n\nSlideTitle: Few-Shot Transfer with BEA sup and RaRe\nBulletPoints:\n- BEA sup uses limited annotations to estimate model parameters.\n- RaRe ranks systems and retrains using top models and gold data.\n- Exploits character and structural features in the target language.\n- Fine-tuning corrects for misclassifications by transfer models.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluated on Wikiann NER corpus with 41 languages.\n- BEA uns and RaRe outperform majority voting and single best models.\n- Multilingual transfer shows significant improvements over single-source methods.\n- RaRe achieves further gains with limited target language annotations.\n\nSlideTitle: Analysis of Source Language Choice\nBulletPoints:\n- Performance affected by number and choice of source languages.\n- BEA and RaRe improve with more source models.\n- BEA uns\u00d72 effectively filters out bad models using unsupervised signals.\n- Supervised ranking further enhances performance.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Cross-lingual transfer requires careful model selection and aggregation.\n- Proposed methods outperform traditional ensembling and single-source models.\n- Light supervision enhances performance in low-resource settings.\n- Future work includes exploring more diverse language pairs and datasets.",
    "SlideTitle: Introduction to Neural Machine Translation (NMT)\nBulletPoints:\nEvaluation of NMT models in English-Japanese translation\nComparison with Statistical Machine Translation (SMT) baselines\nChallenges in recovering unknown words\nImpact of pre-reordered data on model performance\n\nSlideTitle: Background and Motivation\nBulletPoints:\nStatistical Machine Translation (SMT) limitations\nRecurrent Neural Networks (RNNs) in NLP\nIntroduction of Encoder-Decoder approach\nAdvancements with Long Short-Term Memory (LSTM) units\n\nSlideTitle: Recurrent Neural Networks (RNNs)\nBulletPoints:\nModeling temporal data with RNNs\nElman Network or Simple Recurrent Network\nBackpropagation Through Time (BPTT) for training\nChallenges in capturing long-term dependencies\n\nSlideTitle: Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU)\nBulletPoints:\nLSTM's ability to store long-term information\nIncorporation of gates in LSTM\nGRU's reduced computational complexity\nComparison of LSTM and GRU\n\nSlideTitle: Network Architectures in NMT\nBulletPoints:\nEncoder-Decoder approach\nUse of trainable word embeddings\nSoftmax function for cross-entropy computation\nRecursive translation process\n\nSlideTitle: Soft-Attention Models in NMT\nBulletPoints:\nLimitations of basic Encoder-Decoder approach\nIntroduction of soft-attention mechanism\nDifferentiable computational graph\nImproved performance with RNNsearch\n\nSlideTitle: Solutions for Unknown Words\nBulletPoints:\nChallenges with fixed vocabulary size\nModel-specific and translation-specific approaches\nUse of alignment for recovering unknown words\nBack-off system for unknown word recovery\n\nSlideTitle: Experiments and Evaluation\nBulletPoints:\nUse of ASPEC-JE corpus for training\nEvaluation based on perplexity\nComparison of different model architectures\nImpact of pre-reordered data on performance\n\nSlideTitle: Translation Performance and Results\nBulletPoints:\nEvaluation in The 2nd Workshop on Asian Translation\nComparison with SMT baselines\nEnsemble models for improved performance\nHuman evaluation results\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\nSoft-attention NMT models outperform SMT baselines\nNMT models effective without pre-reordering\nWorkaround for unknown words with back-off system\nOpportunities for further improvements in translation accuracy",
    "SlideTitle: Introduction to Event Detection (ED)\nBulletPoints:\n- ED is a subtask of event extraction focusing on extracting event triggers from sentences.\n- Challenges include identifying event types from isolated sentences due to context variability.\n- Existing methods are feature-based and representation-based, mainly using sentence-level context.\n- Document-level information is crucial for improving ED accuracy.\n\nSlideTitle: Limitations of Existing Methods\nBulletPoints:\n- Feature-based methods require manual feature design and may suffer from error propagation.\n- Representation-based methods may not capture event-specific information due to unsupervised training.\n- Document-level information is often underutilized in existing methods.\n\nSlideTitle: Proposed DEEB-RNN Model\nBulletPoints:\n- DEEB-RNN is a novel model for sentence-level ED using document embeddings.\n- Utilizes a hierarchical and supervised attention-based bidirectional RNN.\n- Focuses on word-level and sentence-level attention to identify event triggers and types.\n\nSlideTitle: ED Oriented Document Embedding Learning (EDODEL)\nBulletPoints:\n- Learns document embeddings using a word-level and sentence-level Bi-GRU encoder.\n- Applies attention mechanisms to focus on event triggers and sentences with events.\n- Constructs gold attention signals to supervise learning at both word and sentence levels.\n\nSlideTitle: Document-level Enhanced Event Detector (DEED)\nBulletPoints:\n- Uses Bi-GRU encoder and softmax layer to model ED task.\n- Concatenates word and entity type embeddings with document embeddings.\n- Computes probability vector for classifying event types.\n\nSlideTitle: Joint Training of DEEB-RNN\nBulletPoints:\n- Jointly trains EDODEL and DEED modules using a combined loss function.\n- Balances losses from attention mechanisms and event type identification.\n- Hyper-parameters \u03bb and \u00b5 adjust the influence of different loss components.\n\nSlideTitle: Experimental Results\nBulletPoints:\n- Evaluated on ACE-2005 dataset, outperforming state-of-the-art methods in recall and F1-measure.\n- DEEB-RNN variants with document embeddings show significant improvements.\n- Gold attention signals at both word and sentence levels enhance performance.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- DEEB-RNN effectively uses document-level information for improved ED.\n- Outperforms existing methods by leveraging hierarchical and supervised attention.\n- Future work includes automating the determination of sentence and document embedding weights.",
    "SlideTitle: Introduction to Semantic Role Labeling (SRL)\nBulletPoints:\n- SRL captures predicate-argument relations like \"who did what to whom.\"\n- Recent models are BIO-taggers, labeling argument spans for a single predicate.\n- Evaluated with gold predicates, requiring error-prone predicate identification models.\n- Proposed end-to-end approach predicts all predicates and argument spans in one pass.\n- Uses learned, contextualized span representations to predict SRL graphs.\n\nSlideTitle: Model Overview\nBulletPoints:\n- Considers all tokens as possible predicates and all continuous spans as arguments.\n- Predicts relations between each predicate-argument pair, including no relation.\n- Uses random variables for each predicate-argument pair, conditionally independent given input.\n- Scoring function decomposed into unary scores and label-specific scores.\n\nSlideTitle: Learning and Beam Pruning\nBulletPoints:\n- Minimizes negative log likelihood of the gold structure.\n- Beam pruning addresses computational impracticality of O(n^3|L|) possible relations.\n- Defines beams for candidate arguments and predicates, ranked by unary score.\n- Limits maximum width of spans to reduce computed unary factors.\n\nSlideTitle: Neural Architecture\nBulletPoints:\n- Builds contextualized representations for argument spans and predicate words using BiLSTM.\n- Word-level contexts use pre-trained word embeddings and character-based representations.\n- Argument representation includes BiLSTM outputs, soft head word, and span width features.\n- Predicate representation is the BiLSTM output at the predicate's position.\n\nSlideTitle: Scoring Functions\nBulletPoints:\n- Implemented with feed-forward networks based on predicate and argument representations.\n- Unary scores for arguments and predicates, and label-specific scores for relations.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Experiments on CoNLL 2005 and CoNLL 2012 benchmarks with end-to-end and gold predicates.\n- Evaluated on micro-averaged F1 for correctly predicting (predicate, argument span, label) tuples.\n- Joint model outperforms previous best pipeline system by F1 difference of 1.3 to 6.0.\n- Model predicts over 40% of sentences completely correctly on all datasets.\n\nSlideTitle: Analysis and Effectiveness\nBulletPoints:\n- Model efficiently prunes unlikely argument spans and predicates.\n- Better at predicting arguments farther from predicates compared to ensemble models.\n- Achieves comparable syntactic agreement to previous syntax-based systems.\n- Constrained decoding effective at eliminating core-role inconsistencies.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- New SRL model jointly predicts all predicates and argument spans.\n- Supports joint predicate identification and incorporates span-level features.\n- Better at long-range dependencies and syntactic boundaries, weaker at global consistency.\n- Future work could incorporate higher-order inference methods and self-attention layers.",
    "SlideTitle: Introduction to Presuppositions\nBulletPoints:\n- Presuppositions are assumptions or beliefs shared by discourse participants.\n- They are crucial for smooth communication in both spoken and written forms.\n- Presupposition triggers include definite descriptions, factive verbs, and certain adverbs.\n- Adverbial presupposition triggers indicate event recurrence or continuation.\n\nSlideTitle: Importance of Adverbial Presupposition Triggers\nBulletPoints:\n- Adverbial triggers are common in English journalistic texts.\n- They are the most frequent presupposition triggers after existential triggers.\n- Understanding these triggers can improve language technology systems.\n\nSlideTitle: Research Focus and Objectives\nBulletPoints:\n- Investigate the detection of contexts for adverbial presupposition triggers.\n- Develop datasets for binary classification tasks using Penn Treebank and English Gigaword corpora.\n- Focus on adverbs: too, again, also, still, and yet.\n\nSlideTitle: Methodology and Model Development\nBulletPoints:\n- Use of logistic regression and deep learning models (RNN, CNN) for classification.\n- Exploration of attention-based deep learning models for detecting adverbial triggers.\n- Introduction of a new weighted pooling attention mechanism.\n\nSlideTitle: Related Work and Theoretical Background\nBulletPoints:\n- Presupposition theory traced back to Frege and Strawson.\n- Previous studies on adverbial presupposition triggers in semantics and pragmatics.\n- Attention-based models have shown success in related tasks.\n\nSlideTitle: Datasets and Data Extraction\nBulletPoints:\n- Datasets extracted from Penn Treebank and English Gigaword corpora.\n- Focus on five target adverbs for binary classification tasks.\n- Data extraction involves identifying contexts with and without presupposition triggers.\n\nSlideTitle: Learning Model and Attention Mechanism\nBulletPoints:\n- Introduction of a weighted-pooling neural network architecture.\n- Use of self-attention to model word-pair interactions within sequences.\n- Attention scores used to combine hidden states for prediction.\n\nSlideTitle: Experimental Results and Analysis\nBulletPoints:\n- WP model outperforms baseline models in predicting presupposition triggers.\n- Analysis of model performance on test datasets.\n- Challenges in detecting long-range dependencies and coreferences.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Successful prediction of adverbial presupposition triggers using novel attention mechanism.\n- Model shows promise in classification tasks with complex contextual dependencies.\n- Future work to address data imbalance and explore applications in language generation.",
    "SlideTitle: Introduction to Extractive Summarization\nBulletPoints:\n- Focus on extractive summarization by extracting salient word sequences\n- Challenges with using human abstracts for annotations\n- Neural abstractive summarization issues with factual accuracy\n- New training paradigm using Cloze-style questions\n- Use of attention mechanism and reinforcement learning\n\nSlideTitle: Research Contributions\nBulletPoints:\n- Alternative training scheme for extractive summarization\n- Summaries encouraged to be semantically close to human abstracts\n- Comparison of methods to convert abstracts to Cloze-style questions\n- Improved performance on standard summarization dataset\n\nSlideTitle: Related Work\nBulletPoints:\n- Focus on generic summarization, different from query-based summarization\n- Use of Cloze questions in reading comprehension\n- Application of reinforcement learning in NLP tasks\n- Encouragement of summaries to preserve salient content\n\nSlideTitle: Our Approach\nBulletPoints:\n- System generates summaries by identifying word sequences\n- Question-oriented reward to encourage content useful for answering questions\n- Use of Bi-LSTM and attention mechanism\n- Reward function components for summary size, fluency, and bigram sharing\n\nSlideTitle: Reinforcement Learning for Summarization\nBulletPoints:\n- Optimization of policy for generating extractive summaries\n- Use of bidirectional LSTM for encoding source documents\n- Sampling decisions to improve fluency\n- Pretraining with bigram-based method\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Experiments conducted on CNN dataset\n- Comparison with state-of-the-art systems\n- Two variants: EntityQ and KeywordQ\n- Superior performance with R-1, R-2, and R-L scores\n\nSlideTitle: Impact of QA Pairs\nBulletPoints:\n- Varying number of QA pairs in reward function\n- Increased answer tokens with more QA pairs\n- Training and validation accuracy differences\n- Best results with K=1 QA pair\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- New training paradigm for extractive summarization\n- Use of question-answer pairs and reinforcement learning\n- Effective approach surpassing state-of-the-art systems\n- Future work on clustering summary segments and usability studies",
    "SlideTitle: Introduction to Neural Network Mappings\nBulletPoints:\n- Neural network mappings bridge modalities in cross-modal retrieval and zero-shot learning.\n- Success relies on predicted vectors being similar to target vectors in semantic structure.\n- Recent work shows input modality information propagates into predicted modality.\n- Introduce a measure to quantify similarity between neighborhood structures of vector sets.\n\nSlideTitle: Related Work and Motivation\nBulletPoints:\n- Neural networks and linear mappings are popular in cross-modal retrieval systems.\n- Zero-shot image classification uses image-to-text neural network mapping.\n- Cross-space neural mappings face challenges like \"hubness\" and \"pollution.\"\n- Mapped vectors often resemble input vectors more than target vectors semantically.\n\nSlideTitle: Proposed Approach for Cross-Modal Mapping\nBulletPoints:\n- Two popular mappings: Linear mapping and Feed-forward neural network.\n- Loss functions include MSE, cosine, and max-margin.\n- Focus on cross-space mapping problem independently of class labels.\n- Explore similarity between mapped input vectors and input/output vectors.\n\nSlideTitle: Neighborhood Structure of Mapped Vectors (Experiment 1)\nBulletPoints:\n- Measure similarity using mean nearest neighbor overlap (mNNO).\n- mNNO quantifies semantic similarity between paired vector sets.\n- Results show mapped vectors resemble input vectors more than target vectors.\n\nSlideTitle: Mapping with Untrained Networks (Experiment 2)\nBulletPoints:\n- Study semantic structure preservation using untrained networks.\n- Compare input vectors with their projection through random parameter mapping.\n- Untrained mappings preserve semantic structure of input vectors.\n\nSlideTitle: Experimental Setup and Datasets\nBulletPoints:\n- Experiment 1: Diverse cross-modal tasks at word, sentence, and document levels.\n- Experiment 2: Six benchmarks for concept similarity, relatedness, and visual similarity.\n- Use of varied dimensionality features and directions (text-to-image, image-to-text).\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Mapped vectors' semantic structure resembles input more than target vectors.\n- Untrained mappings preserve input semantic structure, complementing Experiment 1.\n- Proposed measure provides realistic estimate of semantic similarity.\n\nSlideTitle: Conclusions\nBulletPoints:\n- Neural net cross-modal mappings often produce vectors akin to input vectors.\n- Proposed measure quantifies similarity between neighborhood structures.\n- Semantic structure is crucial in cross-modal applications, beyond mean squared error.",
    "SlideTitle: Introduction to Complaints in Natural Language\nBulletPoints:\n- Complaining is a speech act expressing a negative mismatch between reality and expectations.\n- Importance for linguists, psychologists, organizations, and NLP applications.\n- Complaints differ from negative sentiment; focus on breached expectations.\n\nSlideTitle: Contributions of the Study\nBulletPoints:\n- First publicly available dataset of complaints from Twitter.\n- Quantitative analysis of linguistic features of complaints.\n- Predictive models achieving high performance in identifying complaints.\n- Use of distant supervision and domain adaptation to enhance performance.\n\nSlideTitle: Related Work on Complaints\nBulletPoints:\n- Complaints defined as breached expectations (Olshtain and Weinbach, 1987).\n- Considered a distinct speech act in pragmatics.\n- Complaints can threaten both positive and negative face.\n- Previous studies on complaints in various contexts and cultures.\n\nSlideTitle: Data Collection and Annotation\nBulletPoints:\n- New dataset of tweets annotated for complaints.\n- Twitter chosen for high self-expression and interaction.\n- Manual annotation of tweets directed at customer service accounts.\n- 1,971 tweets collected, with 62.4% identified as complaints.\n\nSlideTitle: Linguistic Features of Complaints\nBulletPoints:\n- Use of unigrams, LIWC, Word2Vec clusters, and part-of-speech tags.\n- Complaints characterized by negations, issues, and personal pronouns.\n- Temporal references and specific verb forms are distinctive.\n- Complaints often lack positive sentiment expressions.\n\nSlideTitle: Predictive Models for Complaints\nBulletPoints:\n- Feature-based approaches using Logistic Regression and neural methods.\n- Best performance with bag-of-word features and part-of-speech tags.\n- Neural networks comparable but not superior to feature-based models.\n- Distant supervision and domain adaptation improve performance.\n\nSlideTitle: Domain and Cross-Domain Experiments\nBulletPoints:\n- Models trained on in-domain and out-of-domain data.\n- Domain adaptation improves performance in most cases.\n- High predictive accuracy across domains, except transport.\n- Cross-domain models show consistent performance with some exceptions.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- First computational approach to modeling complaints in linguistics.\n- Dataset and code available for further research.\n- Potential applications in customer service and opinion analysis.\n- Future work on identifying complaint targets and sociolinguistic factors.",
    "SlideTitle: Introduction to Word Embeddings\nBulletPoints:\n- Word embeddings are vector representations of words used in NLP.\n- Analysis of embedding spaces is crucial for understanding their utility.\n- Neighborhood-based analysis is limited by unreliability.\n- Geometric analysis offers insights into embedding properties.\n\nSlideTitle: Bridging Semantic and Geometric Analysis\nBulletPoints:\n- Aim to bridge neighborhood-based semantic analysis and geometric performance analysis.\n- Transform pretrained embeddings to expose subsets of geometric components.\n- Evaluate changes in task performance using transformed embeddings.\n\nSlideTitle: Related Work on Word Embeddings\nBulletPoints:\n- Word embedding models analyzed from various angles.\n- Intrinsic evaluations like word similarity are intuitive but limited.\n- Nearest neighbors used as a proxy for semantic information.\n- Geometric analysis provides a quantitative counterpart.\n\nSlideTitle: Geometric Properties of Word Embeddings\nBulletPoints:\n- Consider attributes: position relative to origin, distribution of feature values, global and local pairwise distances.\n- Apply transformations to expose subsets of these attributes.\n- Use affine transformation, cosine distance encoding, nearest neighbor encoding, and random encoding.\n\nSlideTitle: Affine Transformations\nBulletPoints:\n- Affine transformations change positions of points relative to the origin.\n- Includes translations, reflections, rotations, and homotheties.\n- Used to analyze the impact on word embeddings.\n\nSlideTitle: Cosine Distance and Nearest Neighbor Encoding\nBulletPoints:\n- Cosine distance encoding obfuscates feature distribution.\n- Nearest neighbor encoding retains information about nearest neighborhoods.\n- Both methods transform embeddings for analysis.\n\nSlideTitle: Evaluation of Transformed Embeddings\nBulletPoints:\n- Evaluate on intrinsic tasks like word similarity and extrinsic tasks like sentiment classification.\n- Intrinsic evaluations sensitive to absolute position.\n- Extrinsic tasks rely on local similarity information.\n\nSlideTitle: Analysis and Discussion\nBulletPoints:\n- Models robust to simple linear transformations.\n- Large performance drops with CDE transformation.\n- Importance of local geometric structure in embedding learning.\n- Methods should focus on preserving local similarity.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Quantitative analysis of geometric attributes in embeddings.\n- Intrinsic evaluations sensitive to absolute position.\n- Downstream models rely on local similarity.\n- Future work should focus on local geometric structure in embeddings.",
    "SlideTitle: Introduction to Social Media-Based Influenza Detection\nBulletPoints:\n- Increased use of social media platforms for sharing personal information\n- Twitter as a valuable resource for personal information\n- Use of Twitter in event detection systems like earthquakes and disease outbreaks\n- Focus on detecting seasonal influenza epidemics using Twitter\n- Over 30 Twitter-based influenza detection systems developed\n\nSlideTitle: Challenges in Influenza Detection\nBulletPoints:\n- Differences between actual influenza tweets and noise\n- Importance of removing noise for precise surveillance\n- Use of forecasting words like \"fever\" and \"headache\"\n- Time lag between tweet frequency and patient numbers\n\nSlideTitle: Methodology Overview\nBulletPoints:\n- Estimating optimal time lag for forecasting words\n- Constructing a word frequency matrix with time shifts\n- Developing nowcasting and forecasting models\n- Use of regularization models like Lasso and Elastic Net\n\nSlideTitle: Dataset and Data Processing\nBulletPoints:\n- Collection of 7.7 million influenza-related tweets\n- Filtering of noise such as retweets and linked tweets\n- Use of Japanese morphological parser to extract words\n- Building a word matrix for analysis\n\nSlideTitle: Nowcasting Model\nBulletPoints:\n- Construction of a linear model for current influenza estimation\n- Use of time-shifted word matrix for parameter estimation\n- Comparison of Lasso and Elastic Net methods\n- Improved accuracy with time-shifted models\n\nSlideTitle: Forecasting Model\nBulletPoints:\n- Extension of nowcasting model to predict future patient numbers\n- Setting minimum time shift for future prediction\n- Evaluation of forecasting performance up to 3 weeks ahead\n- Superior accuracy compared to baseline models\n\nSlideTitle: Experimental Results\nBulletPoints:\n- Nowcasting model achieved correlation ratio of 0.93\n- Forecasting model achieved correlation ratio of 0.91 for 1-week prediction\n- Challenges with overestimation due to news-related tweets\n- Importance of preprocessing to handle outliers\n\nSlideTitle: Discussion and Future Work\nBulletPoints:\n- Basic feasibility of time shift operation demonstrated\n- Challenges with inconsistent time shifts across seasons\n- Need for techniques to distinguish forecasting words from noise\n- Potential application to prediction of other events\n\nSlideTitle: Conclusion\nBulletPoints:\n- Novel social media-based influenza surveillance system proposed\n- Use of forecasting words to predict epidemics\n- Best nowcasting performance to date achieved\n- Model enables prediction of future epidemics with room for further application",
    "SlideTitle: Introduction to Text Coherence\nBulletPoints:\n- Coherence is crucial for conveying communication purpose and meaning.\n- A coherent text reduces reading time and cognitive load.\n- Coherence involves integration, thematic centralization, and sentence relations.\n- Textual organization and semantic continuity are key for coherence.\n- Computational models like Entity Grid and Role Matrix have limitations.\n\nSlideTitle: Existing Coherence Models\nBulletPoints:\n- Entity Grid model evaluates local cohesion using Centering theory.\n- Entity Graph model measures global coherence using a bipartite graph.\n- Both models focus on entity distribution and syntactic roles.\n- Limitations include bias towards original text order and data sparsity.\n\nSlideTitle: Proposed Unsupervised Coherence Model\nBulletPoints:\n- Introduces Semantic Similarity Graph for text coherence.\n- Employs unsupervised learning to reduce data sparsity and domain dependence.\n- Encodes text into a graph with vertices as sentences and edges as semantic relations.\n- Utilizes GloVe word vectors for sentence vector representation.\n\nSlideTitle: Graph Construction Methods\nBulletPoints:\n- Preceding Adjacent Vertex (PAV): Establishes edges based on preceding sentence similarity.\n- Single Similar Vertex (SSV): Establishes edges based on most similar sentence.\n- Multiple Similar Vertex (MSV): Allows multiple edges based on similarity threshold.\n\nSlideTitle: Text Coherence Measurement\nBulletPoints:\n- Coherence measured by averaging weights of outgoing edges in the graph.\n- Proposed models allow bidirectional edges and consider semantic similarity.\n- Outperform Entity Graph by considering related-yet-not-identical entities.\n\nSlideTitle: Evaluation: Document Discrimination Task\nBulletPoints:\n- Task: Identify original text from permutated version.\n- PAV and MSV methods outperform Entity Graph.\n- Local coherence more crucial than long-distance relations in this task.\n\nSlideTitle: Evaluation: Insertion Task\nBulletPoints:\n- Task: Identify correct sentence insertion position.\n- PAV method outperforms both Entity Grid and Entity Graph.\n- Performance affected by word overlap and semantic similarity.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Proposed methods show promise in coherence scoring.\n- PAV method performs best but needs refinement for distant relations.\n- Future work: Improve sentence representation and similarity calculation.",
    "SlideTitle: Introduction to BioASQ Challenge\nBulletPoints:\n- BioASQ is a semantic indexing, question answering (QA), and information extraction challenge.\n- Our participation focused on Task B, Phase B: biomedical QA.\n- The challenge involves answering factoid and list questions.\n- Evaluation metrics include mean reciprocal rank (MRR) for factoid questions and F1 score for list questions.\n\nSlideTitle: Traditional vs. Neural QA Systems\nBulletPoints:\n- Traditional systems use a QA pipeline with named-entity recognition and question classification.\n- These systems require extensive resources and feature engineering.\n- Our system uses a neural network QA architecture trained end-to-end.\n- We build upon FastQA, an extractive factoid QA system.\n\nSlideTitle: Neural Network Architecture\nBulletPoints:\n- The system inputs a question and context, outputting start and end pointers to tokens.\n- Uses FastQA with modified input vectors and an output layer for list answers.\n- Word vectors include GloVe, character embeddings, and biomedical Word2Vec embeddings.\n- A one-hot encoding of the question type is concatenated to the embedding vectors.\n\nSlideTitle: Training and Optimization\nBulletPoints:\n- The network is trained in two phases: pre-training on SQuAD and fine-tuning on BioASQ.\n- Uses the Adam optimizer with an exponentially decaying learning rate.\n- Loss is defined as the cross-entropy of correct start and end indices.\n\nSlideTitle: BioASQ Dataset Preparation and Decoding\nBulletPoints:\n- Answer spans are extracted from BioASQ training data.\n- Inference phase retrieves top 20 answer spans via beam search.\n- For factoid questions, outputs top five answer strings.\n- For list questions, uses a probability cutoff threshold.\n\nSlideTitle: Model Ensemble\nBulletPoints:\n- Built a model ensemble using 5-fold cross-validation.\n- Combined models by averaging start and end scores.\n- Submitted the best single model and the model ensemble to the challenge.\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Reported results for five test batches of BioASQ 5.\n- Ensemble model performed slightly better on average.\n- Successful on factoid questions, winning three out of five batches.\n- Performance on list questions varied significantly.\n\nSlideTitle: Conclusion\nBulletPoints:\n- Summarized system design for BioASQ 5B submission.\n- Neural architecture trained end-to-end on the QA task.\n- Achieved state-of-the-art results on factoid questions.\n- Competitive results on list questions.",
    "SlideTitle: Introduction to Stance Classification\nBulletPoints:\n- Task of identifying users' positions on a specific target from text\n- Traditionally requires a target-specific classifier\n- Cross-target classification adapts classifiers from related targets\n- Focus on online users' stances on mining project approvals\n\nSlideTitle: Challenges in Cross-Target Classification\nBulletPoints:\n- Language models may not be compatible between different targets\n- Generalization possible through domain-related aspects\n- Example: \"reef destruction\" related to \"environment\" domain\n\nSlideTitle: CrossNet Model Overview\nBulletPoints:\n- Novel neural model for cross-target stance classification\n- Implements self-attention mechanism\n- Learns domain-specific aspects from a source target\n- Applies learned aspects to a destination target\n\nSlideTitle: CrossNet Model Architecture\nBulletPoints:\n- Consists of four layers: Embedding, Context Encoding, Aspect Attention, Prediction\n- Takes stance-bearing sentence and target as input\n- Outputs predicted stance label\n\nSlideTitle: Aspect Attention Layer\nBulletPoints:\n- Discovers domain-specific aspects for stance inference\n- Captures recurring core ideas in stance-bearing sentences\n- Utilizes self-attention to signal core parts of a sentence\n\nSlideTitle: Model Training and Evaluation\nBulletPoints:\n- Uses multi-class cross-entropy loss for training\n- Evaluated using F1-score and transfer ratio\n- Trained on source target, tested on destination target\n- Outperforms baseline models in cross-target tasks\n\nSlideTitle: Visualization and Learned Aspects\nBulletPoints:\n- Self-attention highlights domain-relevant sentence parts\n- Examples: \"feminist\", \"rights\", \"equality\" for women's rights\n- Model captures domain-specific and sentiment words\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- CrossNet extracts target-independent information for generalization\n- Superior results over baselines in certain domains\n- Future work: target selection, multilingual evaluation, knowledge transfer from multiple sources",
    "SlideTitle: Introduction to Sentence Acceptability\nBulletPoints:\n- Sentence acceptability defined as well-formedness or naturalness to native speakers\n- Encompasses semantic, syntactic, and pragmatic plausibility\n- Grammaticality is a theoretical concept, distinct from acceptability\n- Research focuses on predicting acceptability judgments\n\nSlideTitle: Research Motivation and Hypotheses\nBulletPoints:\n- Investigate impact of context on acceptability judgments\n- Test accuracy of language models incorporating document context\n- Hypotheses: \n  - H1: Document context boosts acceptability judgments\n  - H2: Context helps language models model acceptability\n  - H3: Models predict better with context than without\n\nSlideTitle: Methodology: Data Collection\nBulletPoints:\n- Construct dataset of sentences with acceptability ratings\n- Extracted 100 random articles from English Wikipedia\n- Used round-trip machine translation for sentence variation\n- Gathered judgments via Amazon Mechanical Turk\n\nSlideTitle: Experiment Design\nBulletPoints:\n- Sentences judged with and without document context\n- Context included one preceding and one succeeding sentence\n- Collected 20 judgments per sentence, totaling 20,000 annotations\n- Analyzed correlation between context and non-context ratings\n\nSlideTitle: Influence of Context on Acceptability\nBulletPoints:\n- Strong correlation (Pearson's r = 0.80) between context and non-context ratings\n- Context improves acceptability for ill-formed sentences\n- Context diminishes acceptability for well-formed sentences\n- Context compresses rating distribution, affecting correlation\n\nSlideTitle: Modelling Sentence Acceptability\nBulletPoints:\n- Experimented with neural models: LSTM and topically-driven language model (TDLM)\n- Models trained on 100K English Wikipedia articles\n- Context incorporated during training and testing\n- SLOR measure used for mapping sentence probability to acceptability\n\nSlideTitle: Model Performance and Findings\nBulletPoints:\n- Context improves model performance in acceptability prediction\n- TDLM outperforms LSTM in predicting acceptability\n- Lower correlation when predicting context-judged acceptability\n- Human agreement decreases with context, making ratings less predictable\n\nSlideTitle: Related Work and Future Directions\nBulletPoints:\n- Limited work on acceptability prediction compared to grammatical error detection\n- Previous studies focused on unsupervised models without context\n- Future work to explore why context reduces acceptability for well-formed sentences\n- Experiment with broader range of models, including sentence embeddings",
    "SlideTitle: Introduction to Translation\nBulletPoints:\n- Translation has been crucial in human civilization for law, religion, and trade.\n- Translators aim for invisibility, making translations seamless.\n- Translation quality has been standardized (ISO 17100).\n- Translations exhibit unique characteristics compared to originals.\n\nSlideTitle: Universal Features of Translation\nBulletPoints:\n- Simplification: Making complex structures simpler in the target language.\n- Standardization: Over-conforming to target language standards.\n- Explicitation: Making implicit source structures explicit in the target language.\n\nSlideTitle: Interference in Translation\nBulletPoints:\n- Interference reflects source language \"fingerprints\" on translations.\n- Language-pair specific phenomenon, influenced by isomorphic structures.\n- Hypothesis: Shared structures lead to shared features in translations.\n\nSlideTitle: Related Work and Methodology\nBulletPoints:\n- Historical linguistics uses bioinformatics methods for phylogenetic trees.\n- Translations exhibit unique properties, aiding in text classification.\n- Dataset: Europarl corpus, translations into EU languages.\n- Focus on 17 source languages from Germanic, Romance, and Balto-Slavic families.\n\nSlideTitle: Feature Sets for Analysis\nBulletPoints:\n- POS trigrams: Reflect shallow syntactic structure.\n- Function words: Indicate grammar in classification tasks.\n- Cohesive markers: Overrepresented in translated texts.\n\nSlideTitle: Phylogenetic Language Trees\nBulletPoints:\n- Indo-European family history is controversial.\n- Evaluation metric defined for tree quality assessment.\n- Trees generated using agglomerative clustering of feature vectors.\n\nSlideTitle: Evaluation and Results\nBulletPoints:\n- Unweighted and weighted metrics used for tree evaluation.\n- POS-trigrams feature set produced trees closest to the gold standard.\n- Function words showed moderate results, cohesive markers were least accurate.\n\nSlideTitle: Analysis of Linguistic Phenomena\nBulletPoints:\n- Definite articles overused in translations from Balto-Slavic languages.\n- Possessive constructions vary across language families.\n- Verb-particle constructions more common in Germanic translations.\n- Tense and aspect usage differs across language families.\n\nSlideTitle: Conclusion and Future Directions\nBulletPoints:\n- Translations reflect language typology, enabling phylogenetic tree reconstruction.\n- Interference is a dominant force in translation products.\n- Future research: Impact of translations on language evolution and change.",
    "SlideTitle: Introduction to Word Embeddings\nBulletPoints:\n- Neural networks embed words into low-dimensional vector spaces\n- Popular in NLP tasks like Machine Translation and Question Answering\n- Limitation: inability to discriminate among different meanings of the same word\n- Previous works induced word senses from corpora or bilingual data\n\nSlideTitle: Limitations of Current Approaches\nBulletPoints:\n- Solely based on statistics from text corpora\n- Do not exploit knowledge from semantic networks\n- Induced senses are not readily interpretable or mappable to lexical resources\n\nSlideTitle: SW2V Model Proposal\nBulletPoints:\n- SW2V: Senses and Words to Vectors\n- Exploits knowledge from text corpora and semantic networks\n- Learns embeddings for both words and senses\n- Features: same vector space for words and senses, flexible, scalable\n\nSlideTitle: Connecting Words and Senses\nBulletPoints:\n- SW2V requires a corpus with words connected to senses\n- Manual annotation is expensive; automatic systems are slow\n- Proposes an unsupervised shallow word-sense connectivity algorithm\n- Algorithm is linear on corpus size, fast, and accurate\n\nSlideTitle: Joint Training of Words and Senses\nBulletPoints:\n- Extends conventional word embedding models\n- Integrates explicit knowledge into architecture\n- Uses CBOW architecture, can be applied to Skip-Gram\n- Predicts both words and senses in context\n\nSlideTitle: Model Configurations and Evaluation\nBulletPoints:\n- Analyzes nine model configurations\n- Uses word similarity datasets for evaluation\n- Best configuration: only senses in input, both words and senses in output\n- Evaluates impact of shallow word-sense connectivity algorithm\n\nSlideTitle: Evaluation and Comparison\nBulletPoints:\n- Evaluates SW2V on word similarity, sense clustering, and interconnectivity\n- Outperforms state-of-the-art models in word similarity tasks\n- Demonstrates effective sense clustering capabilities\n- Tests interconnectivity in Word Sense Disambiguation tasks\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- SW2V learns vector representations for words and senses jointly\n- Achieves a semantically coherent vector space\n- Future work: integrate WSD and Entity Linking systems\n- Explore multilingual and crosslingual applications",
    "SlideTitle: Introduction to Machine Translation Evaluation\nBulletPoints:\n- Criticism of automatic MT evaluation metrics\n- BLEU scores correlate with human judgments in shared tasks\n- Example of BLEU score failures in translation quality\n\nSlideTitle: Understanding BLEU Scores\nBulletPoints:\n- BLEU is a precision-based metric\n- Incorporates brevity penalty for short translations\n- High BLEU score indicates closeness to reference translation\n- BLEU's correlation with human judgments\n\nSlideTitle: Example of BLEU Score Discrepancy\nBulletPoints:\n- Example from WAT 2015 MT shared task\n- High BLEU score but poor human evaluation\n- Lexical differences causing precision disparity\n\nSlideTitle: Limitations of BLEU in Reordering\nBulletPoints:\n- BLEU lacks explicit consideration for reordering\n- RIBES score introduced for rank correlation\n- Example of RIBES effectiveness in penalizing wrong order\n\nSlideTitle: Other MT Evaluation Metrics\nBulletPoints:\n- Metrics like METEOR and ROUGE-S\n- Use of paraphrases and skip-grams\n- Pre-trained models for semantic similarity\n\nSlideTitle: Experimental Setup for WAT 2015\nBulletPoints:\n- System submission for Korean to Japanese translation\n- Use of Moses toolkit and phrase-based SMT\n- Language modeling with KenLM\n\nSlideTitle: Human Evaluation Process\nBulletPoints:\n- Use of Lancers crowdsourcing platform\n- Pairwise comparison of translations\n- Calculation of HUMAN score\n\nSlideTitle: Results and Analysis\nBulletPoints:\n- High BLEU score but low human judgment score\n- Segment level meta-evaluation\n- Disparity between automatic metrics and human judgments\n\nSlideTitle: Conclusion\nBulletPoints:\n- High BLEU/RIBES scores don't always correlate with human judgment\n- Factors contributing to poor correlation\n- Need for improvements in automatic translation evaluation metrics",
    "SlideTitle: Introduction\nBulletPoints:\nOverview of BioASQ challenge 2017\nPresentation of participating systems and their evaluation\nBrief overview of tasks and challenge data\nEvaluation using state-of-the-art measures\n\nSlideTitle: Overview of the Tasks\nBulletPoints:\nThree main tasks: semantic indexing, question answering, funding information extraction\nTasks took place from February to May 2017\n\nSlideTitle: Large-scale Semantic Indexing (Task 5a)\nBulletPoints:\nClassify PubMed documents into MeSH hierarchy concepts\nTest sets included articles from all journals\nPerformance measured using flat and hierarchical measures\n21 hours to provide answers for each test set\n\nSlideTitle: Biomedical Semantic QA (Task 5b)\nBulletPoints:\nLarge-scale question answering for four types of biomedical questions\nTwo phases: retrieval of relevant elements and providing exact/ideal answers\nFive independent batches with 24-hour time gap between phases\n\nSlideTitle: Funding Information Extraction (Task 5c)\nBulletPoints:\nExtract grant information from biomedical articles\nUse of MEDLINE citations as golden data\nEvaluation on grant ID, agency, and full-grant extraction\n10 teams participated with 31 systems submitted\n\nSlideTitle: Systems and Approaches\nBulletPoints:\nVarious systems developed using different methodologies\nExamples include UIMA-based workflows, Binary Relevance classification, and neural networks\nBaselines included Medical Text Indexer (MTI) and OAQA system\n\nSlideTitle: Results and Evaluation\nBulletPoints:\nTask 5a: DeepMeSH and AUTH systems outperformed baselines\nTask 5b: Different systems excelled in different evaluation metrics\nTask 5c: Grant ID extraction was more challenging than agency extraction\n\nSlideTitle: Conclusion\nBulletPoints:\nBioASQ challenge advanced state-of-the-art in biomedical information systems\nIncrease in participation highlights community growth\nFuture editions to include more benchmark data and multi-batch scenarios",
    "SlideTitle: Introduction to Simultaneous Interpretation\nBulletPoints:\n- Simultaneous Interpretation (SI) is a challenging task with cognitive and attentional demands.\n- Interpreters use strategies like generalization and summarization to convey messages.\n- Errors increase with time due to memory and stamina limits.\n- Estimating interpreter performance can enhance Computer-Assisted Interpretation (CAI).\n\nSlideTitle: Computer-Assisted Interpretation (CAI)\nBulletPoints:\n- CAI systems like Interpret-Bank provide live terminology support.\n- Ideal support level depends on interpreter performance.\n- Quality estimation (QE) can help adjust information flow to interpreters.\n\nSlideTitle: Quality Estimation in Machine Translation\nBulletPoints:\n- QE measures MT output quality without reference translations.\n- QuEst++ is a popular QE pipeline with feature extraction and learning modules.\n- Supports word-level, sentence-level, and document-level QE.\n\nSlideTitle: Adapting QE for Interpretation\nBulletPoints:\n- SI output differs from MT with pauses, hesitations, and re-orderings.\n- Interpretation-specific features include pauses, non-specific words, and cognates.\n- Features help identify struggling interpreters.\n\nSlideTitle: Evaluation Metrics for Interpretation\nBulletPoints:\n- METEOR metric used for evaluating interpreter output.\n- METEOR considers paraphrases and content-function word distinctions.\n- Better suited for SI than traditional MT metrics like BLEU.\n\nSlideTitle: Data and Experiments\nBulletPoints:\n- EN-JA data from NAIST TED SI corpus; EN-FR and EN-IT from EPTIC corpus.\n- Experiments use Pearson's r correlation to evaluate QE system.\n- Proposed model outperforms baseline in all language settings.\n\nSlideTitle: Results and Analysis\nBulletPoints:\n- Proposed model shows significant gains, especially in EN-JA(A-Rank).\n- Ablation studies highlight importance of specific features.\n- Cognates and pauses aid model performance in different languages.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- QE application can help CAI systems assist struggling interpreters.\n- METEOR used for evaluation; future work could develop fine-grained measures.\n- Potential for improving interpreter performance and reducing cognitive load.",
    "SlideTitle: Introduction to Word Embeddings\nBulletPoints:\n- Word embeddings enable generalization to unseen words by embedding lexical features.\n- They capture distributional similarity, sharing information among words in similar contexts.\n- Handling out-of-vocabulary (OOV) words is challenging, especially in low-resource languages.\n- MIMICK-RNN model predicts vectors for OOV words using character-level RNNs.\n\nSlideTitle: MIMICK-RNN Model\nBulletPoints:\n- Trains a recurrent neural network (RNN) on character level with embedding as the target.\n- Predicts vectors for OOV words in downstream tasks.\n- Captures both word-shape and lexical features.\n- Shows improvement in tagging tasks with limited labeled data.\n\nSlideTitle: Related Work\nBulletPoints:\n- Compositional models use morphological or orthographic information for unseen words.\n- Character-based approaches avoid issues with names and foreign words.\n- Supervised subword models learn task-specific embeddings but struggle with low-resource languages.\n\nSlideTitle: MIMICK Word Embeddings\nBulletPoints:\n- Approaches OOV embeddings as a generation problem.\n- Uses a Word Type Character Bi-LSTM to predict embeddings.\n- Trains on existing vocabulary to predict embeddings for unseen words.\n- Minimizes squared Euclidean distance between predicted and pre-trained embeddings.\n\nSlideTitle: MIMICK Polyglot Embeddings\nBulletPoints:\n- Uses Polyglot multilingual word embeddings for experiments.\n- High OOV rates in many languages.\n- MIMICK algorithm applied to Polyglot embeddings for prediction models.\n- Nearest-neighbor examination shows robustness to typos and word compounding.\n\nSlideTitle: Joint Tagging of POS and Morphosyntactic Attributes\nBulletPoints:\n- Uses Universal Dependencies (UD) scheme for tagging.\n- Extends POS tagging model to morphosyntactic tagging.\n- Input is word embeddings, output is attribute-specific probability distributions.\n- Evaluates using micro F1 scores for morphosyntactic attributes.\n\nSlideTitle: Experimental Settings and Results\nBulletPoints:\n- Evaluates on 23 languages with varying morphological complexity.\n- Reports performance in low-resource and full-resource settings.\n- MIMICK shows significant improvements in Slavic and agglutinative languages.\n- MIMICK fares better than CHAR\u2192TAG in low-resource settings.\n\nSlideTitle: Conclusion\nBulletPoints:\n- MIMICK infers OOV word embeddings from pre-trained models without accessing the corpus.\n- Useful for low-resource languages and task-agnostic.\n- Improves performance on sequence-tagging tasks across various languages.\n- Future work may explore other subword units for training.",
    "SlideTitle: Introduction to Span-Based Neural Constituency Parsing  \nBulletPoints:  \n- High accuracy and simplicity compared to other neural parsers  \n- Uses bidirectional RNNs for input sequence modeling  \n- Efficient dynamic programming with CKY algorithm  \n- Limitations in search: greedy vs. chart-based parsers  \n\nSlideTitle: Proposed Approach  \nBulletPoints:  \n- Combines merits of greedy and chart-based approaches  \n- Linear-time span-based neural parser  \n- Left-to-right dynamic programming in action-synchronous style  \n- Running time of O(n^4) with beam search for practical runtime  \n\nSlideTitle: Contributions  \nBulletPoints:  \n- First neural parser with linear time and large search space  \n- Application of cube pruning to achieve O(nb log b) complexity  \n- Novel loss function penalizing wrong spans  \n- Faster than chart parsing baselines for long sentences  \n\nSlideTitle: Span-Based Shift-Reduce Parsing  \nBulletPoints:  \n- Maintains a stack of spans (i, j)  \n- Shift or reduce actions to form larger spans  \n- Top-most span labeled as constituent or null  \n- Parsing continues until entire sentence is formed  \n\nSlideTitle: Bi-LSTM Features and Score Decomposition  \nBulletPoints:  \n- Feature representation using bi-directional LSTM  \n- Eliminates need for complex feature engineering  \n- Score of a tree decomposed into sum of span scores  \n- Max over label X independent of other spans  \n\nSlideTitle: Graph-Structured Stack and Beam Search  \nBulletPoints:  \n- Reformulation as shift-reduce parser  \n- Step index for action-synchronous beam search  \n- Top b parsing states maintained at each step  \n- Cube pruning to improve runtime complexity  \n\nSlideTitle: Training with Structured SVM  \nBulletPoints:  \n- Model scores gold tree higher than others by margin  \n- Loss-augmented decoding for training  \n- Cross-span loss function for better accuracy  \n- Max violation updates for training efficiency  \n\nSlideTitle: Experiments and Results  \nBulletPoints:  \n- Experiments on Penn Treebank and PTB-RST discourse treebank  \n- Use of dropout and Adam optimizer for training  \n- State-of-the-art results with beam size of 20  \n- Tractability of parsing on longer sequences  \n\nSlideTitle: Conclusions  \nBulletPoints:  \n- New neural parser with linear time and large search space  \n- Cube pruning for improved runtime  \n- New loss function for training  \n- Achieves state-of-the-art results among single-model systems",
    "SlideTitle: Introduction to Query Auto-Completion (QAC)\nBulletPoints:\n- QAC provides suggested queries as users type\n- Saves time and reduces cognitive load\n- Most approaches extend the Most Popular Completion (MPC) algorithm\n- Personalization improves QAC by considering user-specific query likelihoods\n\nSlideTitle: Advances in QAC with Language Models\nBulletPoints:\n- Park and Chiba (2017) introduced character LSTM language models for QAC\n- Handles unseen prefixes and reduces memory usage\n- Combines personalization with language models for better query handling\n\nSlideTitle: Personalized QAC with FactorCell Model\nBulletPoints:\n- FactorCell model adapts language model weights for personalization\n- Uses user embeddings to transform recurrent layer weights\n- Allows stronger adaptation compared to traditional methods\n\nSlideTitle: Learning User Embeddings\nBulletPoints:\n- Embeddings capture latent demographic factors\n- Users with fewer queries grouped as a single entity\n- Online updating of embeddings for unseen users during evaluation\n\nSlideTitle: Recurrent Layer Adaptation Techniques\nBulletPoints:\n- Unadapted LM, ConcatCell, and FactorCell models\n- ConcatCell adds user embedding to input, adjusting bias\n- FactorCell uses low-rank adaptation matrix for stronger personalization\n\nSlideTitle: Experimental Setup and Data\nBulletPoints:\n- Experiments use AOL Query data from 2006\n- Models trained with Adam optimizer, evaluated with Adadelta\n- Small and large model configurations tested\n\nSlideTitle: Results and Performance\nBulletPoints:\n- Neural models outperform MPC baseline\n- FactorCell model shows best performance, especially for seen prefixes\n- Personalized models improve with more user queries\n\nSlideTitle: Case Studies and Insights\nBulletPoints:\n- FactorCell adapts quicker to repetitive search behaviors\n- Case studies show semantically coherent query adaptations\n- FactorCell focuses on user-specific search patterns\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Personalization enhances LSTM model performance\n- FactorCell model provides computational efficiency\n- Future work to explore combining language models with MPC and time-based factors",
    "SlideTitle: Introduction to Neural Vector Representations\nBulletPoints:\n- Ubiquity in natural language processing subfields\n- Importance of studying word vector properties\n- Sentence embeddings encode variable-length sentences into fixed-length vectors\n- Evaluated using cosine similarities and semantic relatedness\n\nSlideTitle: Analysis of Sentence Embeddings\nBulletPoints:\n- Generation of phenomena-specific evaluation datasets\n- Minor sentence alterations can shift meaning\n- Use of sentence triplets to assess semantic relatedness\n- Importance of semantic relatedness over structural similarity\n\nSlideTitle: Sentence Modification Schemes\nBulletPoints:\n- Not-Negation: Inserting \"not\" before the first verb\n- Quantifier-Negation: Prepending \"there is no\"\n- Synonym Substitution: Replacing verbs with synonyms\n- Embedded Clause Extraction: Extracting clauses from sentences\n- Passivization and Argument Reordering\n\nSlideTitle: Sentence Triplet Generation\nBulletPoints:\n- Five evaluation datasets created\n- Negation Detection: Original, Synonym Substitution, Not-Negation\n- Negation Variants: Quantifier-Negation, Not-Negation, Original\n- Clause Relatedness: Original, Embedded Clause Extraction, Not-Negation\n- Argument Sensitivity and Fixed Point Reorder\n\nSlideTitle: Experiments and Datasets\nBulletPoints:\n- Evaluation datasets from SICK, Penn Treebank WSJ, MSR Paraphrase corpus\n- Human annotators ensure grammaticality and semantics\n- Use of WordNet for synonym substitution\n\nSlideTitle: Embedding Methods\nBulletPoints:\n- GloVe Averaging: Average of word vectors\n- Concatenated P-Mean Embeddings: Multiple p-means of word vectors\n- Sent2Vec: Word and n-gram embeddings\n- Skip-Thought Vector: Neighbour prediction with recurrent neural networks\n- InferSent: Supervision from Stanford NLI dataset\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Negation Detection: InferSent and SkipThought distinguish negation\n- Negation Variants: InferSent understands negation quantifiers\n- Clause Relatedness: Sent2Vec fares best in distinguishing embedded clauses\n- Argument Sensitivity: Difficulty in distinguishing semantic from structural information\n- Fixed Point Reorder: Challenges in encoding word order information\n\nSlideTitle: Conclusion\nBulletPoints:\n- Simple method to inspect sentence embeddings\n- SkipThought and InferSent distinguish negation from synonymy\n- InferSent better at identifying semantic equivalence\n- SkipThought suitable for structure-based semantics\n- Neural network embeddings outperform word averaging methods",
    "SlideTitle: Introduction to Weblio Pre-reordering SMT System\nBulletPoints:\nOverview of Weblio Pre-reordering Statistical Machine Translation (SMT) System\nApplication of pre-reordering method from Zhu et al., 2014\nIntegration of statistical language model in pre-reordering\nFocus on English-Japanese translation in WAT2014\n\nSlideTitle: Head-restructured CFG Parse Tree\nBulletPoints:\nReordering SVO to SOV order is crucial for translation\nDependency trees capture reordering patterns but have limitations\nIntroduction of Head-restructured CFG (HRCFG) parse tree\nHRCFG tree maintains local grammatical structures\n\nSlideTitle: Reordering Model with Language Model Integration\nBulletPoints:\nReordering model follows Zhu et al., 2014 approach\nLanguage model identifies best order of node layers\nUse of bilingual training data and GIZA++ for word alignments\nExport reordered nonterminal tags as new training data\n\nSlideTitle: N-best Reordering Approach\nBulletPoints:\nBest order comprised of 1-best orders of node layers\nN-best reordering results improve translation accuracy\nImplementation of N-best reordering in MT system\nRanking reordering results by accumulated language model score\n\nSlideTitle: Experimental Settings and Results\nBulletPoints:\nBaseline system uses 1-best parse trees\nUse of Stanford tokenizer and Berkeley parser\nIncorporation of N-best parse trees and reordering results\nImprovements in BLEU and RIBES metrics observed\n\nSlideTitle: Evaluation of Pre-reordering\nBulletPoints:\nPerformance evaluation using Kendall's \u03c4\nComparison of natural and adjusted order\nIncrease in identical word order after pre-reordering\nGraph of Kendall's \u03c4 distribution provides insights\n\nSlideTitle: Error Analysis and Context-aware Translation\nBulletPoints:\nIssues with reordering model flexibility and sparsity\nExamples of reordering errors in ASPEC test corpus\nEfforts to utilize context information in translation\nInterpolation of general and in-domain language models\n\nSlideTitle: Conclusion and Future Research\nBulletPoints:\nSummary of reordering model and N-best results\nAutomatic evaluation improvements, but human evaluation challenges\nNeed for further improvement in reordering model stability\nFuture exploration of pre-reordering models and language pairs",
    "SlideTitle: Introduction to Plagiarism and Cross-Language Plagiarism\nBulletPoints:\nPlagiarism is a significant issue in higher education.\nCross-language plagiarism involves translation-based plagiarism.\nChallenge: Detecting plagiarism when the source and suspicious document are in different languages.\nWord embeddings can help propose new cross-lingual similarity measures.\n\nSlideTitle: Main Contributions\nBulletPoints:\nAugmented state-of-the-art methods with word embeddings.\nIntroduced syntax weighting in sentence representations.\nAchieved an F1 score of 89.15% for English-French similarity detection at chunk level.\n\nSlideTitle: Evaluation Conditions and Dataset\nBulletPoints:\nUsed a multilingual dataset with French, English, and Spanish texts.\nDataset includes cross-language alignment at document, sentence, and chunk levels.\nContains both human and machine-translated texts.\nPartially obfuscated to complicate similarity detection.\n\nSlideTitle: Overview of State-of-the-Art Methods\nBulletPoints:\nCL-CnG: Uses character n-grams for similarity detection.\nCL-CTS: Measures semantic similarity using conceptual thesaurus.\nCL-ASA: Uses bilingual unigram dictionary for alignment-based similarity.\nCL-ESA: Represents document meaning using Wikipedia-derived concepts.\nT+MA: Translates units for monolingual comparison.\n\nSlideTitle: Proposed Methods Using Word Embeddings\nBulletPoints:\nWord embeddings project words into a continuous space based on context.\nCL-CTS-WE: Replaces lexical resources with word embeddings.\nCL-WES: Direct comparison using cosine similarity on sentence embeddings.\nCL-WESS: Introduces syntax weighting to improve CL-WES.\n\nSlideTitle: Combining Multiple Methods\nBulletPoints:\nWeighted fusion assigns weights to similarity scores of each method.\nDecision tree fusion uses C4.5 algorithm for optimal method combination.\nCL-C3G, CL-WESS, and CL-CTS-WE are key methods in decision tree fusion.\n\nSlideTitle: Results and Discussion\nBulletPoints:\nCL-CTS-WE improves performance over CL-CTS.\nCL-WESS significantly boosts performance over CL-WES.\nDecision tree fusion achieves an F1 score of 89.15% at chunk level.\nFusion methods outperform individual methods in similarity detection.\n\nSlideTitle: Conclusion and Future Perspectives\nBulletPoints:\nWord embeddings enhance baseline approaches for similarity detection.\nCL-WESS is the most promising method with syntactically weighted representations.\nFusion of methods significantly improves cross-language similarity detection.\nFuture work: Improve CL-WESS by analyzing syntactic weights and adapt to plagiarist's stylometry.",
    "SlideTitle: Introduction to Copy & Paste in NLP\nBulletPoints:\n- Copy & paste is essential for document creation\n- Archives allow study of reuse and plagiarism in NLP\n- Focus on written and spoken materials\n\nSlideTitle: Context and Background\nBulletPoints:\n- Study initiated at ACL's 50th anniversary workshop\n- Importance of automatic plagiarism detection\n- PAN international plagiarism detection competition\n\nSlideTitle: Objectives of the Study\nBulletPoints:\n- Focus on extrinsic detection of near-matches\n- Compare verbatim raw strings vs. linguistic parsing\n- Study practices in the NLP field\n\nSlideTitle: The NLP4NLP Corpus\nBulletPoints:\n- Large corpus of NLP research from 1965 to 2015\n- Contains 65,003 documents from various sources\n- Preprocessing to standardize formats and metadata\n\nSlideTitle: Definitions and Terminology\nBulletPoints:\n- Self-reuse vs. self-plagiarism\n- Reuse vs. plagiarism\n- Importance of proper source citation\n\nSlideTitle: Algorithm and Methodology\nBulletPoints:\n- Comparison of word sequences for detection\n- Use of sliding windows and Jaccard distance\n- Linguistic parsing for higher-level comparison\n\nSlideTitle: Results and Findings\nBulletPoints:\n- Self-reuse and self-plagiarism are common\n- Reuse and plagiarism are rare\n- Time delay between publication and reuse\n\nSlideTitle: Discussion and Conclusions\nBulletPoints:\n- Self-reuse often due to author style and publication practices\n- Ethical considerations in reuse and plagiarism\n- Limitations and future directions for improvement",
    "SlideTitle: Introduction to Split-and-Rephrase Task\nBulletPoints:\n- Processing long, complex sentences is challenging for both humans and NLP tasks.\n- The \"Split-and-Rephrase\" task involves breaking complex sentences into simpler ones.\n- Narayan et al. (2017) introduced a dataset and evaluation method for this task.\n- Two system setups: text-to-text and semantics-augmented.\n- Focus on text-to-text setup with SEQ2SEQ models achieving 77.5 BLEU score.\n\nSlideTitle: Challenges in SEQ2SEQ Models\nBulletPoints:\n- Unwanted behaviors in outputs: unsupported facts, repetitions, and missing facts.\n- Model memorizes entity-fact pairs instead of learning to split and rephrase.\n- High BLEU score due to memorization and small number of distinct simple sentences.\n\nSlideTitle: Proposed Improvements\nBulletPoints:\n- Introduce a more challenging data split to aid further research.\n- Establish a stronger baseline with a copy mechanism in SEQ2SEQ models.\n- Copy mechanism shown to be helpful in similar tasks like abstractive summarization.\n\nSlideTitle: Preliminary Experiments\nBulletPoints:\n- Task: Split a complex sentence into simple ones without RDF triples.\n- Evaluation: Averaged individual multi-reference BLEU score.\n- Models trained with different LSTM cell sizes (128, 256, 512).\n\nSlideTitle: Results and Analysis\nBulletPoints:\n- Models outperform SEQ2SEQ baseline with up to 28.35 BLEU improvement.\n- Manual inspection reveals common mistakes: unsupported facts, repetitions, missing facts.\n- Attention weights show model's focus on entities rather than cues.\n\nSlideTitle: New Data-Split Strategy\nBulletPoints:\n- Original data-split susceptible to fact memorization.\n- New split ensures every RDF relation is represented in training set.\n- Aims to balance challenge and feasibility for split-and-rephrase models.\n\nSlideTitle: Copy-Augmented Model\nBulletPoints:\n- Augment SEQ2SEQ models with a copy mechanism for better performance.\n- Copy mechanism biases model towards copying relevant words from complex sentences.\n- Experiments with varying LSTM widths (128, 256, 512).\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- SEQ2SEQ models achieve high scores without learning to split-and-rephrase.\n- New data-split proposed to address memorization issues.\n- Copy-mechanism improves performance, establishing a new baseline.\n- Encourage future research on proposed split or version 1.0 of the dataset.",
    "SlideTitle: Introduction\nBulletPoints:\n- New methods for bilingual lexicon induction for low-resource language pairs\n- Importance of reducing reliance on expensive parallel data\n- Use of multilingual word embeddings and recent unsupervised methods\n- Lack of linguistic feature utilization in modern methods\n- Proposal to extend embedding-based approach with orthographic information\n\nSlideTitle: Background\nBulletPoints:\n- Based on Artetxe et al. (2017) framework\n- Word embedding matrices for source and target languages\n- Objective to find a mapping matrix for aligned embeddings\n- Iterative self-learning framework to reduce seed dictionary size\n\nSlideTitle: Orthographic Extension of Word Embeddings\nBulletPoints:\n- Augment embeddings with orthographic information\n- Append letter count vector to each word's embedding\n- Normalize augmented vectors for self-learning framework\n- Use of combined alphabets from both languages\n\nSlideTitle: Orthographic Similarity Adjustment\nBulletPoints:\n- Modify similarity score with orthographic similarity\n- Use normalized string edit distance for similarity measure\n- Calculate similarity for a subset of word pairs\n- Adaptation of Symmetric Delete spelling correction algorithm\n\nSlideTitle: Experiments\nBulletPoints:\n- Datasets: English-Italian, English-German, English-Finnish\n- Use of numeral identity translations for dictionary\n- Tunable hyperparameters for method optimization\n- Significant computation savings with orthographic similarity adjustment\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Embedding extension increased accuracy\n- Orthographic similarity adjustment identified 2 million word pairs\n- Best results with combined methods and identity translation\n- Different performance trends for English-Finnish due to language family\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Two techniques for improving bilingual lexicon induction\n- Use of orthographic information without parallel data\n- Significant gains over baseline system\n- Future work on related languages with different alphabets and other systems",
    "SlideTitle: Introduction to Semantic Graphs\nBulletPoints:\n- Semantic graphs encode structural qualities of language\n- WordNet is an example of a semantic graph\n- Local properties focus on specific semantic concepts and relations\n- Global properties encode emergent regularities\n- Global features can aid in local tasks like relation prediction\n\nSlideTitle: Motivation and Approach\nBulletPoints:\n- Local models may misinterpret relations due to lack of global context\n- Global graph features can constrain local predictions\n- Use of Exponential Random Graph Model (ERGM) for feature estimation\n- Introduction of Max-Margin Markov Graph Model (M3GM) for scalability\n\nSlideTitle: Related Work\nBulletPoints:\n- Embedding-based methods for relation prediction\n- Compositional models for path prediction\n- Graph Convolutional Networks for high-order neighborhood properties\n- Novelty of modeling relation interaction explicitly\n\nSlideTitle: Graph Motifs as Features\nBulletPoints:\n- Selection of graph features like edge count, cycles, and paths\n- Semantic graphs as multigraphs with multiple relation types\n- Expansion of feature space to include relation-sensitive motifs\n- Combinatory features for relation prediction\n\nSlideTitle: Max-Margin Markov Graph Model (M3GM)\nBulletPoints:\n- Adaptation of ERGM for semantic graphs\n- Incorporation of node-specific information\n- Use of local association scores and global graph features\n- Margin loss objective for parameter estimation\n\nSlideTitle: Relation Prediction and Evaluation\nBulletPoints:\n- Evaluation on WN18RR dataset\n- Metrics: Mean Rank (MR), Mean Reciprocal Rank (MRR), Hits@k\n- Comparison of association models and M3GM re-ranker\n- Improvements in prediction accuracy with M3GM\n\nSlideTitle: Synset Embeddings and Initialization\nBulletPoints:\n- Use of pre-trained word embeddings for synset representation\n- Methods: Averaging, AutoExtend retrofitting, Mimick\n- Importance of distributional signal in semantic setup\n- Performance comparison of embedding methods\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- M3GM combines distributional and structural coherence\n- Substantial improvements in link prediction performance\n- Potential for multilingual applications and knowledge graph extension\n- Exploration of structural parameter transferability across languages",
    "SlideTitle: Introduction to Geolocation Prediction in Social Media\nBulletPoints:\n- Social media as a source for analyzing public opinions\n- Importance of geolocation prediction for various analyses\n- Twitter's suitability for geolocation prediction\n- Proposal of a neural network model for Twitter geolocation prediction\n\nSlideTitle: Contributions of the Proposed Model\nBulletPoints:\n- Neural network model with unified text, metadata, and user network representations\n- Outperformance of previous ensemble approaches\n- Insight into the unification processes of the model\n- Applicability to other social media analyses\n\nSlideTitle: Related Works: Text-based Approach\nBulletPoints:\n- Use of word distributions for geolocation prediction\n- Success of maximum likelihood and language modeling approaches\n- Supervised machine learning methods for text-based prediction\n\nSlideTitle: Related Works: User-network and Metadata-based Approaches\nBulletPoints:\n- User interactions as network links for geolocation prediction\n- Use of friendship and follower information in models\n- Metadata fields as clues for geolocation prediction\n- Combinatory approaches to enhance prediction accuracy\n\nSlideTitle: Proposed Neural Network Model\nBulletPoints:\n- Integration of user network information with city and user embeddings\n- Text component using RNN and attention layers\n- Text and metadata component with shared word embeddings\n- User network component with linked cities and users\n\nSlideTitle: Evaluation and Baseline Comparisons\nBulletPoints:\n- Implementation of baseline models for comparison\n- Pre-training of embeddings and neural network optimization\n- Evaluation metrics: accuracy, accuracy@161, median and mean error distances\n- Performance improvements over baseline models\n\nSlideTitle: Results and Discussion\nBulletPoints:\n- Significant improvements in accuracy and accuracy@161\n- Analysis of attention probabilities and unification strategies\n- Typical attention patterns identified through clustering\n- Limitations in city prediction and handling location changes\n\nSlideTitle: Conclusion and Future Directions\nBulletPoints:\n- Summary of the proposed model's achievements\n- Plans to expand the model for multiple locations and temporal states\n- Application to other social media analyses like gender and age analysis\n- Potential for unifying text and user network information in future studies",
    "SlideTitle: Introduction to NLP Learning Methods\nBulletPoints:\n- NLP learning methods are increasingly dominated by end-to-end differentiable functions.\n- Traditional NLP used modular stages forming a pipeline.\n- Pipelines make discrete decisions, seemingly incompatible with neural learning.\n- Pipelines can be treated as layers in neural architectures for NLP tasks.\n\nSlideTitle: Challenges with Current Methods\nBulletPoints:\n- Reinforcement learning and structured attention replace argmax with sampling or marginalization.\n- Not all operations have efficient sampling or marginalization methods.\n- Inspection of intermediate outputs is more straightforward for hard decisions.\n\nSlideTitle: Introduction to SPIGOT\nBulletPoints:\n- SPIGOT is a new method for optimizing intermediate gradients.\n- It defines a proxy for the gradient of a loss function with respect to argmax input.\n- SPIGOT respects constraints in the argmax problem.\n- Applicable to any intermediate layer expressible as a constrained maximization problem.\n\nSlideTitle: Methodology\nBulletPoints:\n- SPIGOT allows structured argmax layers to be treated like differentiable functions.\n- Introduces a proxy for the gradient of the loss function.\n- Uses projection of gradients to respect constraints.\n- Applicable to structured intermediate tasks followed by end tasks.\n\nSlideTitle: Relaxed Decoding\nBulletPoints:\n- Decoding problems decomposed into parts, such as arcs in a dependency tree.\n- Scoring function decomposes into part-local, input-specific scores.\n- Linear constraints specify the output space.\n- Relaxation transforms discrete problems into linear objective optimization.\n\nSlideTitle: From STE to SPIGOT\nBulletPoints:\n- Argmax viewed as an activation function.\n- SPIGOT introduces a projection step to keep updated values in the feasible set.\n- Projection alters parameter updates for gradient proxy.\n- SPIGOT limits gradient space to satisfy constraints.\n\nSlideTitle: Backpropagation through Pipelines\nBulletPoints:\n- SPIGOT enables backpropagation through NLP pipelines.\n- Intermediate task output used as features for end task.\n- Joint learning minimizes loss functions for both tasks.\n- SPIGOT applicable to various training scenarios.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- Evaluated on syntactic-then-semantic parsing and sentiment classification.\n- SPIGOT outperforms state-of-the-art semantic dependency parsers.\n- Improves sentiment classification accuracy.\n- Joint training with SPIGOT enhances performance over pipelined systems.\n\nSlideTitle: Conclusion\nBulletPoints:\n- SPIGOT provides a novel approach to backpropagating through discrete structured decisions.\n- Employs a projection to respect constraints in intermediate tasks.\n- Achieves stronger performance than baselines in experiments.\n- Implementation available at https://github.com/Noahs-ARK/SPIGOT.",
    "SlideTitle: Introduction to Story Understanding\nBulletPoints:\n- Understanding stories requires reasoning about causal links between events and characters' mental states.\n- Commonsense story cloze task highlights the difficulty for machine readers.\n- Language models often lack the ability to model complex implied dynamics.\n- New annotation formalism developed for labeling mental states in stories.\n\nSlideTitle: Dataset Properties\nBulletPoints:\n- Provides a fully-specified chain of motivations and emotional reactions for each character.\n- Captures implied effects not directly stated in the story.\n- Includes formal labels from psychology theories and open text descriptions.\n- Encompasses 15k stories with 300k annotations for 150k character-line pairs.\n\nSlideTitle: Mental State Representations\nBulletPoints:\n- Draws from philosophy and psychology to derive categorical labels.\n- Uses Maslow's hierarchy of needs and Reiss's basic motives for motivation categories.\n- Plutchik's wheel of emotions used for emotion categorization.\n- Open text descriptions complement theory categories for better accessibility.\n\nSlideTitle: Annotation Framework and Challenges\nBulletPoints:\n- Annotates simple commonsense stories with rich character interactions.\n- First study to annotate full chains of mental state effects for story characters.\n- Challenges include interpreting discourse, understanding implicit effects, and formal psychology categories.\n- New annotation framework introduced using crowdsourced workers.\n\nSlideTitle: Annotation Pipeline\nBulletPoints:\n- Entity Resolution: Identifies characters and sentences they appear in.\n- Action Resolution: Determines if a character's action is intentional.\n- Affect Resolution: Identifies lines where characters have emotional reactions.\n- Motivation and Emotional Reaction: Annotates character motives and emotional responses.\n\nSlideTitle: Dataset Statistics and Insights\nBulletPoints:\n- Tasks involving theory category assignments are the most expensive.\n- Dataset includes 300k annotations across 15,000 stories.\n- Quality control measures implemented for annotator reliability.\n- Provides insights into annotator agreement and confusion matrices.\n\nSlideTitle: Tasks and Baseline Models\nBulletPoints:\n- New tasks include state classification, annotation classification, and explanation generation.\n- Baseline models use various encoders like TF-IDF, GloVe, LSTM, REN, and NPN.\n- Models trained to predict psychological states and generate explanations.\n- Pretraining on free-response data boosts performance.\n\nSlideTitle: Experimental Results and Conclusion\nBulletPoints:\n- Models outperform random baselines in state classification tasks.\n- Contextual information improves model predictions.\n- Explanation generation models produce semantically closer outputs.\n- Dataset serves as a resource for training and evaluating mental state tracking in stories.",
    "SlideTitle: Introduction to Satire\nBulletPoints:\n- Satire is an art form used to criticize in an entertaining manner.\n- Utilizes humor, irony, sarcasm, exaggerations, parody, or caricature.\n- Satirical news mimics regular news without intending to deceive.\n- Satire detection is crucial for distinguishing satirical from regular news.\n\nSlideTitle: Challenges in Satire Detection\nBulletPoints:\n- Distinguishing satire from regular news is challenging, even for humans.\n- Automatic systems can aid in understanding satire characteristics.\n- Previous models often focus on publisher characteristics rather than satire.\n\nSlideTitle: Previous Work on Satire Detection\nBulletPoints:\n- Early methods used handcrafted features like entity context and story coherence.\n- Recent models use word embeddings and neural networks.\n- Adversarial training is explored for domain adaptation and debiasing.\n\nSlideTitle: Limitations of Previous Methods\nBulletPoints:\n- Previous datasets lacked publication source information.\n- Classifiers may learn to associate articles with specific publishers.\n- New German corpus includes publication sources for better analysis.\n\nSlideTitle: Proposed Model for Satire Classification\nBulletPoints:\n- Two classification tasks: satire detection and publication identification.\n- Shared feature extractor with adversarial training.\n- Focus on word embeddings and bidirectional LSTM with self-attention.\n\nSlideTitle: Adversarial Training Approach\nBulletPoints:\n- Adversarial training controls for publication source bias.\n- Parameters updated to optimize satire detection and fool publication identifier.\n- Experiment with different hyperparameter values for optimal performance.\n\nSlideTitle: Results and Analysis\nBulletPoints:\n- Adversarial training reduces publication identification accuracy.\n- Satire detection performance remains comparable.\n- Attention weights shift focus from publication-specific words to satire cues.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Adversarial training helps focus on linguistic characteristics of satire.\n- Future work to explore other confounding variables like time and region.\n- Propose more quantitative and qualitative analysis for model behavior understanding.",
    "SlideTitle: Introduction to Deep Learning for Sentence Representations\nBulletPoints:\n- Deep learning enables automatic learning of continuous sentence representations.\n- Representations can be task-specific or aim for \"universal\" performance across tasks.\n- Learned representations can be unsupervised or supervised.\n- NMT advancements, like \"attention,\" have improved translation quality.\n- Proposed a novel encoder-decoder architecture for static sentence representations.\n\nSlideTitle: Related Work in Sentence Representations\nBulletPoints:\n- Continuous sentence representations are crucial in neural machine translation.\n- RNN sequence-to-sequence models reflect semantic and syntactic structures.\n- Evaluations include sentence classification and semantic similarity tasks.\n- Language-independent representations explored using NMT systems.\n\nSlideTitle: Model Architectures Overview\nBulletPoints:\n- Proposed models differ in encoder state processing and decoder usage.\n- RNN-based models and Transformer model variations explored.\n- Compound attention is the main proposed architecture.\n- Inner attention used to compute weighted averages of encoder states.\n\nSlideTitle: Compound Attention Mechanism\nBulletPoints:\n- Incorporates attention in both encoder and decoder.\n- Uses bidirectional recurrent network with GRU for input processing.\n- Inner attention computes multiple weighted averages of encoder states.\n- Structured sentence embedding used in decoder attention.\n\nSlideTitle: Constant Context and Transformer Model\nBulletPoints:\n- Compound attention benefits from context vectors in every decoding step.\n- Baseline models replace context vectors with entire sentence embedding.\n- Transformer model modified with inner attention for fixed-size representation.\n- Decoder attention operates on components of this representation.\n\nSlideTitle: Evaluation of Sentence Representations\nBulletPoints:\n- Evaluated using classification and similarity tasks from SentEval.\n- Paraphrase evaluation using COCO and HyTER Networks.\n- Metrics include cluster classification accuracy and nearest-neighbor retrieval.\n- Inverse Davies-Bouldin index measures cluster separation.\n\nSlideTitle: Experimental Results and Translation Quality\nBulletPoints:\n- Trained English-to-German and English-to-Czech NMT models.\n- Translation quality estimated using BLEU scores.\n- Compound attention models show varied performance across tasks.\n- Relation between representation metrics and BLEU scores analyzed.\n\nSlideTitle: Conclusion and Observations\nBulletPoints:\n- Proposed compound attention model evaluated for meaning representation.\n- Better BLEU scores often correlate with poorer meaning representation.\n- Bilingual MT may not provide useful data for representation learning.\n- High BLEU scores may lead to systems ignoring sentence meaning.",
    "SlideTitle: Introduction to SEQ2SEQ Models\nBulletPoints:\n- Neural sequence-to-sequence models excel in NLP tasks like machine translation and parsing.\n- Require large parallel datasets; non-parallel corpora can boost performance.\n- Unsupervised SEQ2SEQ models proposed for summarization tasks.\n- SEQ3 autoencoder introduced for end-to-end training via gradient optimization.\n\nSlideTitle: SEQ3 Model Overview\nBulletPoints:\n- SEQ3 is a sequence-to-sequence-to-sequence autoencoder.\n- Comprises two attentional encoder-decoder pairs: compressor (C) and reconstructor (R).\n- C generates a summary from input text; R reconstructs input from summary.\n- Employs differentiable approximations for sampling from categorical distributions.\n\nSlideTitle: Compressor Component\nBulletPoints:\n- Embedding layer projects source sequence to word embeddings.\n- Bidirectional RNN encodes embeddings, producing hidden states.\n- Attentional RNN decoder generates summary using global attention and input feeding.\n- Summary length sampled from a uniform distribution for varying compression ratios.\n\nSlideTitle: Differentiable Word Sampling\nBulletPoints:\n- Soft-Argmax used to pass weighted sum of word embeddings during training.\n- Gumbel-Softmax reparametrization trick for low variance approximation of sampling.\n- Straight-Through estimator used to discretize embeddings during training.\n\nSlideTitle: Reconstructor Component\nBulletPoints:\n- Operates like the compressor but on summary word embeddings.\n- Decoder initialized using transformation of encoder's last hidden states.\n- Reconstruction loss drives compressor to produce sentences increasing target word likelihood.\n\nSlideTitle: Loss Functions\nBulletPoints:\n- Reconstruction Loss: Negative log-likelihood of reconstructed words.\n- LM Prior Loss: Ensures summaries are readable using pretrained language model.\n- Topic Loss: Encourages preservation of topic-indicating words in summary.\n- Length Penalty: Helps predict end-of-sequence token at target summary length.\n\nSlideTitle: Experimental Results\nBulletPoints:\n- SEQ3 trained on Gigaword dataset; tested on DUC-2003 and DUC-2004 tasks.\n- Outperforms unsupervised Pretrained Generator and LEAD-8 baseline.\n- LM prior and topic loss work in synergy for better performance.\n- Demonstrates extractive and abstractive capabilities in generated summaries.\n\nSlideTitle: Limitations and Future Work\nBulletPoints:\n- Model tends to copy first words of input sentence.\n- Errors early in sequence have cascading effects due to autoregressive reconstructor.\n- Future work includes exploring SEQ3 in tasks like unsupervised machine translation and caption generation.",
    "SlideTitle: Introduction to Taylor's Law\nBulletPoints:\n- Taylor's law describes the variance growth of events with respect to the mean, forming a power law.\n- It quantifies clustering behavior in systems across various domains.\n- Limited analysis of Taylor's law in natural language studies.\n- Applied Taylor's law to texts, considering lexical fluctuation and word alignment.\n- Found Taylor exponent as a universal characteristic across texts and languages.\n\nSlideTitle: Related Work\nBulletPoints:\n- Study quantifies complexity in texts using measures like entropy rate and scaling behaviors.\n- Known power laws in linguistic data include Zipf's and Heaps' laws.\n- Fluctuation analysis quantifies bias in event occurrence timing.\n- Long-range correlation analysis applied to text by transforming it into intervals.\n- Taylor's law shows vast applicability and stability for written texts.\n\nSlideTitle: Measuring the Taylor Exponent\nBulletPoints:\n- Taylor's law holds when mean and standard deviation are correlated by a power law.\n- Taylor exponent \u03b1 ranges from 0.5 to 1.0 across various domains.\n- \u03b1 = 0.5 for i.i.d processes; \u03b1 = 1.0 for consistent word co-occurrence.\n- Method involves plotting mean and standard deviation in log-log coordinates.\n- Least-squares method used for fitting points to estimate exponent.\n\nSlideTitle: Taylor Exponents for Real Data\nBulletPoints:\n- Taylor exponent indicates consistent co-occurrence among words.\n- Exponent value of 0.58 suggests words in natural language texts are not strongly coherent.\n- Exponent slightly decreases with increased text size.\n- Different data sources show varying Taylor exponents.\n- Exponents for natural language texts differ from programming language and music data.\n\nSlideTitle: Analysis of Different Data Sources\nBulletPoints:\n- Taylor exponent varies according to data source.\n- Natural language texts have lower exponents compared to structured data like programming code.\n- Exponent distinguishes between different kinds of data.\n- Statistical tests show significant differences in exponents across languages.\n\nSlideTitle: Evaluation of Machine-Generated Text\nBulletPoints:\n- Taylor exponent of 0.50 for i.i.d processes and standard language models.\n- Character-based LSTM models produce text with exponent of 0.50.\n- Neural machine translation models maintain burstiness of original text.\n- Taylor exponent can evaluate quality of machine-generated text.\n\nSlideTitle: Conclusion\nBulletPoints:\n- Proposed method analyzes natural language text using Taylor's law.\n- Written texts follow Taylor's law with exponents around 0.58.\n- Exponents differ for other data sources, indicating complexity differences.\n- Taylor's law applicable for evaluating machine-generated text.\n- Future work includes analysis of other data types and relation to grammatical complexity.",
    "SlideTitle: Introduction to Deep Learning in Question Answering\nBulletPoints:\n- Application of deep learning in various question answering tasks\n- Importance of evaluating model accuracy on representative test sets\n- Proposal of techniques to analyze model sensitivity to question words\n\nSlideTitle: Our Contributions\nBulletPoints:\n- Analysis workflow for understanding question answering models\n- Use of Integrated Gradients (IG) for attribution of predictions\n- Identification of model weaknesses and crafting adversarial questions\n- Introduction of an overstability test for question answering networks\n\nSlideTitle: Visual Question Answering (VQA)\nBulletPoints:\n- Task involves answering questions about images\n- Analysis of Kazemi and Elqursh (2017) model\n- Model's reliance on image over question words\n- Attacks exploiting model's reliance on generic words\n\nSlideTitle: Question Answering on Tables\nBulletPoints:\n- Analysis of Neural Programmer (NP) model\n- Model's reliance on content-free words\n- Overstability and table-specific default programs\n- Attacks using question concatenation and stop word deletion\n\nSlideTitle: Reading Comprehension\nBulletPoints:\n- Task involves answering questions from text paragraphs\n- Analysis of Yu et al. (2018) model\n- Effectiveness of adversarial attacks by Jia and Liang (2017)\n- Use of attributions to predict attack success\n\nSlideTitle: Integrated Gradients (IG) Technique\nBulletPoints:\n- Attribution technique to isolate influential question words\n- Empirical validation via question perturbations\n- Use of IG to craft targeted attacks\n\nSlideTitle: Related Work\nBulletPoints:\n- Comparison with Jia and Liang (2017) approach\n- Use of crowdsourcing for adversarial example generation\n- Discussion of task-specific related work\n\nSlideTitle: Conclusion and Reproducibility\nBulletPoints:\n- Identification of model weaknesses using attributions\n- Importance of addressing under-reliance on important question terms\n- Availability of code for reproducing results and generating attributions",
    "SlideTitle: Introduction to Constituency Parsing\nBulletPoints:\n- Importance of fast and accurate constituency parsing in NLP\n- Applications in relation extraction, paraphrase detection, and more\n- Recent success of neural network-based approaches\n\nSlideTitle: Parsing Approaches\nBulletPoints:\n- Transition-based parsing: sequential parse tree production\n- Chart-based parsing: non-linear potentials and structured inference\n- Challenges: compounding errors and computational cost\n\nSlideTitle: Proposed Model\nBulletPoints:\n- Novel fully-parallel model based on \"syntactic distance\"\n- Top-down parsing approach using syntactic distances\n- Parallel computation for efficiency\n\nSlideTitle: Syntactic Distances\nBulletPoints:\n- Definition and role in parse tree construction\n- Conversion algorithms for parse tree representation\n- Efficient mapping from distances to parse tree\n\nSlideTitle: Model Architecture\nBulletPoints:\n- Use of neural networks to predict syntactic distances\n- Combination of LSTM and convolutional layers\n- Prediction of constituent labels and syntactic distances\n\nSlideTitle: Training and Objective\nBulletPoints:\n- Training with modified hinge loss for ranking\n- Use of softmax classifier for constituent labels\n- Objective: sum of prediction losses for distances and labels\n\nSlideTitle: Experimental Results\nBulletPoints:\n- Evaluation on Penn Treebank and Chinese Treebank datasets\n- Achieved state-of-the-art performance for single-model parsing\n- Comparison with other models and benchmarks\n\nSlideTitle: Parsing Speed and Efficiency\nBulletPoints:\n- Parallel computation of syntactic distances\n- O(n log n) complexity for tree conversion\n- Faster parsing speed compared to other models\n\nSlideTitle: Conclusion\nBulletPoints:\n- Novel parsing scheme using syntactic distances\n- Strong performance and efficiency\n- Straightforward deployment with standard deep learning frameworks",
    "SlideTitle: Introduction to Sentiment-to-Sentiment Translation\nBulletPoints:\nChange underlying sentiment while preserving semantic content\nImportant in NLP for tasks like review sentiment transformation\nChallenges due to lack of parallel training data\nProposed cycled reinforcement learning approach\n\nSlideTitle: Challenges in Sentiment Translation\nBulletPoints:\nExisting methods fail to preserve semantic content\nImplicit separation of emotional and semantic information\nLack of supervised parallel data\n\nSlideTitle: Proposed Cycled Reinforcement Learning Approach\nBulletPoints:\nNeutralization module extracts non-emotional content\nEmotionalization module adds sentiment to content\nCycled training with policy gradient for optimization\n\nSlideTitle: Related Work\nBulletPoints:\nStyle transfer in computer vision and language\nPrevious models focus on style accuracy, not content preservation\nBack reconstruction in machine translation\n\nSlideTitle: Neutralization and Emotionalization Modules\nBulletPoints:\nNeutralization module filters out emotional words using LSTM\nEmotionalization module uses bi-decoder framework\nPre-training with self-attention based sentiment classifier\n\nSlideTitle: Cycled Reinforcement Learning Mechanism\nBulletPoints:\nNeutralization module as first agent, emotionalization as second\nReinforcement learning with policy gradient\nRewards based on sentiment confidence and BLEU score\n\nSlideTitle: Experimental Results\nBulletPoints:\nTested on Yelp and Amazon review datasets\nProposed method outperforms state-of-the-art in content preservation\nImproved BLEU scores indicate better semantic preservation\n\nSlideTitle: Human and Automatic Evaluation\nBulletPoints:\nHuman evaluation shows improved semantic preservation\nAutomatic evaluation uses sentiment classifier and BLEU score\nAgreement between human and automatic evaluations\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\nCycled reinforcement learning improves sentiment translation\nFuture work: fine-grained sentiment translation\nExplore better sentiment-aware decoders",
    "SlideTitle: Introduction to Domain-Specific Sentiment Analysis\nBulletPoints:\n- Words used to express opinions vary by domain\n- Some words change polarity across domains\n- Supervised algorithms struggle with cross-domain generalization\n- Domain adaptation techniques transfer knowledge across domains\n\nSlideTitle: Challenges in Cross-Domain Sentiment Analysis\nBulletPoints:\n- Words like \"unpredictable\" change polarity between domains\n- Transfer learning can negatively impact performance\n- Need to identify consistent polarity words across domains\n\nSlideTitle: Proposed Approach\nBulletPoints:\n- Identify Significant Consistent Polarity (SCP) words\n- Use \u03c7\u00b2 test and cosine-similarity for SCP word identification\n- Develop an ensemble-based adaptation algorithm\n\nSlideTitle: Related Work\nBulletPoints:\n- Structured Correspondence Learning (SCL) and Structured Feature Alignment (SFA)\n- Deep learning approaches for feature mapping\n- Our approach focuses on consistent polarity words\n\nSlideTitle: Extracting SCP Features\nBulletPoints:\n- Use \u03c7\u00b2 test for labeled source domain\n- Frequency threshold for significance in target domain\n- Context vector and cosine-similarity for polarity in target domain\n\nSlideTitle: Cross-Domain Adaptation Algorithm\nBulletPoints:\n- Train classifier on SCP words in source domain\n- Use pseudo-labeled instances for target domain classifier\n- Combine classifiers in a weighted ensemble\n\nSlideTitle: Experimental Results\nBulletPoints:\n- SCP-based approach outperforms SCL and common unigrams\n- Weighted ensemble improves cross-domain performance\n- Strong correlation between SCP identification and classification accuracy\n\nSlideTitle: Conclusion\nBulletPoints:\n- SCP words are effective for cross-domain sentiment classification\n- Ensemble approach reduces errors from individual classifiers\n- SCP identification leads to more accurate target domain classifiers",
    "SlideTitle: Introduction to Search-based Structured Prediction\nBulletPoints:\n- Models natural language structure generation as a search problem\n- Competitive performance in accuracy and running time\n- Challenges include ambiguities in reference policy and training-testing discrepancies\n\nSlideTitle: Problems in Search-based Structured Prediction\nBulletPoints:\n- Ambiguities in reference policy lead to suboptimal training instances\n- Discrepancy between training and testing phases\n- Harmful to generalization ability and performance\n\nSlideTitle: Proposed Solution: Integrated Knowledge Distillation\nBulletPoints:\n- Distill a single model from an ensemble of baselines\n- Match ensemble's output distribution on reference and exploration states\n- Improves model performance by combining distillation from reference and exploration\n\nSlideTitle: Background on Structured Prediction\nBulletPoints:\n- Maps input to structural output with internal dependencies\n- Formalized as a search problem with states, actions, and policies\n- Requires constructing a reference policy for training\n\nSlideTitle: Knowledge Distillation in Structured Prediction\nBulletPoints:\n- Transfers generalization ability from a cumbersome model to a smaller one\n- Uses teacher model's output distribution as \"soft target\"\n- Combines distillation loss with NLL loss for optimization\n\nSlideTitle: Distillation from Reference and Exploration\nBulletPoints:\n- Distillation from reference uses states produced by reference policy\n- Distillation from exploration samples actions from ensemble\n- Combining both methods enhances model performance\n\nSlideTitle: Experimental Results\nBulletPoints:\n- Conducted on transition-based dependency parsing and neural machine translation\n- Distilled models outperform strong baselines\n- Significant improvements in LAS and BLEU scores\n\nSlideTitle: Analysis and Conclusion\nBulletPoints:\n- Ensemble improves performance on ambiguous and non-optimal states\n- Distillation loss leads to more stable learning\n- Proposed method significantly enhances single model performance in structured prediction tasks",
    "SlideTitle: Introduction to Labeled Datasets\nBulletPoints:\n- Human annotators provide labels for datasets, typically offering one bit of information per example.\n- Challenge: Extract more information per example after annotator effort.\n- Previous methods: Labeling features, highlighting rationale phrases, marking image regions.\n- New approach: Use natural language explanations for supervision.\n\nSlideTitle: Natural Language Explanations\nBulletPoints:\n- Framework for annotators to provide explanations for each label.\n- Explanations parsed into logical forms representing labeling functions (LFs).\n- LFs map examples to labels, creating a large, weakly-supervised training set.\n- Simple rule-based semantic parser suffices for parsing explanations.\n\nSlideTitle: The BabbleLabble Framework\nBulletPoints:\n- Converts natural language explanations and unlabeled data into a labeled training set.\n- Key components: Semantic parser, filter bank, label aggregator.\n- Semantic parser converts explanations into logical forms.\n- Filter bank removes incorrect LFs without ground truth labels.\n\nSlideTitle: Semantic Parser and Filter Bank\nBulletPoints:\n- Semantic parser generates multiple LFs from explanations.\n- Filter bank discards incorrect LFs using semantic and pragmatic filters.\n- Filters ensure LFs are consistent with examples and not redundant.\n\nSlideTitle: Label Aggregator and Discriminative Model\nBulletPoints:\n- Label aggregator combines multiple labels into a single probabilistic label.\n- Uses data programming to model relationships between true labels and LFs.\n- Discriminative model trained on noisily-labeled data for improved accuracy.\n- Incorporates features not identified by LFs.\n\nSlideTitle: Experimental Setup and Results\nBulletPoints:\n- Evaluated on three relation extraction tasks: Spouse, Disease, Protein.\n- BabbleLabble achieves high F1 scores with fewer user inputs than traditional methods.\n- Filter bank removes 86% of incorrect parses, maintaining high accuracy.\n- Data programming approach outperforms feature-based methods.\n\nSlideTitle: Related Work and Discussion\nBulletPoints:\n- Related to modeling natural language explanations and weak supervision.\n- Previous works: Learning from natural language, distant supervision, multi-instance learning.\n- BabbleLabble leverages natural language for high-bandwidth communication.\n- Potential to extend framework to other tasks and interactive settings.\n\nSlideTitle: Reproducibility and Resources\nBulletPoints:\n- Code, data, and experiments available on CodaLab platform.\n- Encourages reproducibility and further exploration of the framework.\n- Link: https://worksheets.codalab.org/worksheets/0x900e7e41deaa4ec5b2fe41dc50594548/",
    "SlideTitle: Introduction to Human-Computer Conversation\nBulletPoints:\n- Critical and challenging task in AI and NLP\n- Two major research streams: task-oriented and general-purpose dialog\n- General-purpose dialog aims for natural, meaningful conversations\n- Seq2Seq models used for generative conversational models\n\nSlideTitle: Challenges in General-Purpose Dialog\nBulletPoints:\n- General responses like \"I don't know\" are common\n- Previous models favor high-frequency, trivial responses\n- Efforts to address trivial responses include MMI and topic guidance\n- Need for controlled response generation\n\nSlideTitle: Specificity Controlled Seq2Seq Model (SC-Seq2Seq)\nBulletPoints:\n- Novel mechanism to control response specificity\n- Inspired by human conversation control of specificity\n- Introduces specificity control variable in Seq2Seq framework\n- Usage representation of words interacts with specificity control\n\nSlideTitle: Distant Supervision for Specificity Control\nBulletPoints:\n- Specificity control variable is unknown in raw data\n- Two methods: Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF)\n- NIRF based on response frequency, NIWF on word frequency\n- Normalized values constrain specificity control variable\n\nSlideTitle: Experiment and Dataset\nBulletPoints:\n- Experiments on Short Text Conversation (STC) dataset\n- STC contains 3.8 million post-comment pairs\n- Comparison with state-of-the-art response generation methods\n- Evaluation through automatic and human assessments\n\nSlideTitle: Evaluation Results\nBulletPoints:\n- SC-Seq2Seq NIWF outperforms baselines in specificity\n- Best performance with control variable s set to 0.5\n- Human evaluation shows SC-Seq2Seq NIWF generates informative responses\n- Statistically significant improvements over baselines\n\nSlideTitle: Case Study and Analysis\nBulletPoints:\n- SC-Seq2Seq NIWF generates specific responses with high control variable\n- Responses become general with lower control variable\n- Usage representations differ from semantic embeddings\n- Visualization shows distinct distributions for usage and semantic spaces\n\nSlideTitle: Conclusion\nBulletPoints:\n- Novel controlled response generation mechanism\n- Specificity control variable enhances Seq2Seq model\n- Generates both general and specific responses\n- Outperforms state-of-the-art methods in response generation",
    "SlideTitle: Introduction to Metaphor Processing\nBulletPoints:\n- Metaphor enriches language and plays a significant role in communication and cognition.\n- About one third of sentences in typical corpora contain metaphor expressions.\n- Computational metaphor processing improves NLP tasks like Machine Translation and Sentiment Analysis.\n- Challenges in metaphor identification include word-level detection and contextual understanding.\n\nSlideTitle: Proposed Metaphor Processing Model\nBulletPoints:\n- An unsupervised model built on word embeddings and WordNet for lexical relation acquisition.\n- Operates at the sentence level without preprocessing like dependency parsing.\n- Captures metaphor in a data-driven manner without labeled data.\n- Evaluated against strong baselines, showing significant improvements.\n\nSlideTitle: Related Work in Metaphor Processing\nBulletPoints:\n- Various methods applied, including abstractness modeling and topic models.\n- Word embedding approaches measure cosine similarity for metaphor identification.\n- Deep learning architectures introduced for improved metaphor detection.\n- Contextual information is crucial for accurate metaphor identification.\n\nSlideTitle: Methodology Overview\nBulletPoints:\n- Hypothesis 1: Metaphorical words come from different domains than their literal senses.\n- Hypothesis 2: Literal senses are more common in corpora than metaphoric senses.\n- Framework involves training word embeddings and identifying best fit words.\n- Cosine similarity used to determine metaphoricity of target words.\n\nSlideTitle: Experimental Settings and Results\nBulletPoints:\n- Compared against unsupervised and supervised baselines.\n- Models trained on Wikipedia dump with specific settings for word embeddings.\n- SIM-CBOW I+O model achieved highest F1 score in sentence-level identification.\n- Performance stable across different thresholds for metaphor identification.\n\nSlideTitle: Metaphor Processing for Machine Translation\nBulletPoints:\n- Evaluated how metaphor processing supports English-Chinese MT.\n- Paraphrasing metaphorical words improves translation accuracy.\n- Significant improvements observed in Google and Bing Translators.\n- Overall translation performance improved by 11% and 9% respectively.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- Proposed framework effectively identifies and interprets metaphors.\n- Contributes to improved translation accuracy in MT systems.\n- First study to evaluate metaphor processing on MT.\n- Future work to introduce weighted CBOW and Skip-gram for positional information.",
    "SlideTitle: Introduction to Controversial Content\nBulletPoints:\nControversial content can improve community health by sparking debate\nDetecting controversial content is useful for users and moderators\nControversiality is community-specific and dynamic\n\nSlideTitle: Early Detection Approach\nBulletPoints:\nProposes using content and structure of initial comments for early detection\nIncorporates structural and textual features of comment trees\nImproves predictive performance quickly\n\nSlideTitle: Datasets and Communities\nBulletPoints:\nData drawn from Reddit, covering posts and comments from 2007 to 2014\nFocus on six subreddits: AskMen, AskWomen, Fitness, LifeProTips, personalfinance, relationships\nEach subreddit has a large number of posts and comments\n\nSlideTitle: Assigning Controversy Labels\nBulletPoints:\nBinary labels: controversial vs. non-controversial\nProcess involves filtering and sorting posts by upvote percentage\nHigh precision in matching Reddit's controversy ranking\n\nSlideTitle: Early Discussion Threads Analysis\nBulletPoints:\nFocus on comments posted within one hour of original submission\nComment text and tree structure analyzed for predictive capacity\nStructural features of conversation trees are predictive\n\nSlideTitle: Early Prediction of Controversy\nBulletPoints:\nTools predict controversy using early discussion features\nIncorporates text, rate, and tree features from early comments\nText content of comments provides significant predictive gain\n\nSlideTitle: Domain Transfer Experiments\nBulletPoints:\nExamines transferability of features across subreddits\nText features are community-specific and brittle\nRate and tree features transfer better between communities\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\nEarly discussion features predict controversiality effectively\nExpressive feature set of early discussions is crucial\nFuture work could focus on higher-quality textual representations for conversation trees",
    "SlideTitle: Introduction to Multimodal Name Tagging\nBulletPoints:\n- Social platforms are increasingly multimodal, combining images, videos, audios, and texts.\n- Name tagging is crucial for language understanding in social media.\n- Challenges include short text, linguistic variations, and the need for visual context.\n\nSlideTitle: Challenges in Name Tagging on Social Media\nBulletPoints:\n- Textual components are often short, limiting context.\n- Common use of slangs, typos, and colloquial language.\n- Visual context is often necessary for accurate name tagging.\n\nSlideTitle: Proposed Model for Name Tagging\nBulletPoints:\n- Extends state-of-the-art models to incorporate visual information.\n- Uses an attention mechanism to extract relevant visual features.\n- Combines textual and visual features using a modulation gate.\n\nSlideTitle: Model Components\nBulletPoints:\n- BLSTM-CRF sequence labeling model for word prediction.\n- Visual Attention Model to focus on relevant image regions.\n- Modulation Gate to combine visual and textual features at word level.\n\nSlideTitle: Visual Feature Representation\nBulletPoints:\n- Uses CNNs, specifically ResNet, to extract image features.\n- Global and regional visual features are considered.\n- Visual attention mechanism helps focus on relevant image parts.\n\nSlideTitle: Datasets for Evaluation\nBulletPoints:\n- Two datasets from Twitter and Snapchat.\n- Contains named entities: Location, Person, Organization, Miscellaneous.\n- Twitter dataset has longer text; Snapchat focuses more on images.\n\nSlideTitle: Experiment and Results\nBulletPoints:\n- Tokenization and labeling schema used for training.\n- Models optimized with fine-tuning and early stopping.\n- Proposed model outperforms baselines with 3%-4% F-score gain.\n\nSlideTitle: Attention Visualization and Error Analysis\nBulletPoints:\n- Successful attention visualization aligns with text context.\n- Challenges include poor alignment, blur images, and wrong attention.\n- Visual features enhance name tagging when well-aligned with text.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- Gated Visual Attention improves name tagging in multimodal social media.\n- Encourages further research on multimodal social media.\n- Future work includes fine-grained name tagging and entity linking.",
    "SlideTitle: Introduction to Incremental Domain Adaptation\nBulletPoints:\n- Incremental domain adaptation improves machine translation systems.\n- Used in computer-aided translation (CAT) scenarios.\n- User translations serve as new training examples.\n- Focus on measuring speed of lexical acquisition for in-domain vocabulary.\n\nSlideTitle: Measuring Immediate Adaptation\nBulletPoints:\n- Perceived adaptation performance is crucial for user acceptance.\n- Lexical choice errors are particularly salient.\n- Users expect correct translation of in-domain vocabulary after manual correction.\n- Traditional metrics like BLEU and TER may not capture desired behaviors.\n\nSlideTitle: Proposed Metrics for Adaptation\nBulletPoints:\n- Three metrics: one-shot, zero-shot, and combined vocabulary acquisition.\n- Metrics measure recall of target-language content words.\n- Content words defined as nouns, main verbs, adjectives, or adverbs.\n- Metrics calculated automatically without models or word alignments.\n\nSlideTitle: Related Work in Adaptation\nBulletPoints:\n- Estimating adaptability using repetition rate.\n- NMT brought focus on discourse-level and fine-grained evaluation.\n- Online adaptation explored using BLEU, effort estimation, and user studies.\n- Regularization methods like group lasso improve adaptation efficiency.\n\nSlideTitle: Online Adaptation Techniques\nBulletPoints:\n- NMT systems adapted by fine-tuning with cross-entropy loss.\n- Full adaptation updates all parameters, but can be infeasible.\n- Bias adaptation introduces a second bias term in the output layer.\n- Fixed and top adaptation freeze parts of the model.\n\nSlideTitle: Experiments and Results\nBulletPoints:\n- English\u2192German NMT system adapted using various techniques.\n- Performance evaluated on Autodesk dataset.\n- Lasso outperforms in zero-shot recall, but has lower one-shot recall.\n- Full adaptation shows high one-shot recall but degrades zero-shot recall.\n\nSlideTitle: Analysis of Adaptation Results\nBulletPoints:\n- Analysis conducted on novel words and subword combinations.\n- Lasso improves learning of new subword combinations.\n- Zero-shot recall degraded by adaptation, one-shot recall improved.\n- Lasso provides best tradeoff in performance across metrics.\n\nSlideTitle: Conclusions and Future Work\nBulletPoints:\n- Fine-tuning NMT models can degrade zero-shot recall.\n- Regularization methods mitigate negative effects to some extent.\n- Proposed metrics useful for measuring immediate adaptation performance.\n- Future work needed to develop more effective immediate adaptation techniques.",
    "SlideTitle: Introduction to Speech-to-Text Translation\nBulletPoints:\n- Potential applications for low-resource languages\n- Challenges with traditional ST methods\n- End-to-end ST as a promising approach\n- Importance of leveraging high-resource language data\n\nSlideTitle: Methodology Overview\nBulletPoints:\n- Use of encoder-decoder model with attention\n- Pre-training with high-resource ASR models\n- Transfer learning for low-resource ST\n- Shared vocabulary for ASR and ST tasks\n\nSlideTitle: Experimental Setup\nBulletPoints:\n- Data sets used: English ASR, French ASR, Spanish-English ST, Mboshi-French ST\n- Preprocessing of speech and text data\n- Model architecture for ASR and ST\n\nSlideTitle: Evaluation Metrics\nBulletPoints:\n- Use of BLEU scores for model evaluation\n- Limitations of BLEU in low-resource settings\n- Additional metrics: word-level unigram precision and recall\n- Naive baseline models for comparison\n\nSlideTitle: Results: Spanish-English ST\nBulletPoints:\n- Impact of transfer learning on BLEU scores\n- Improvements in precision and recall with transfer learning\n- Effectiveness in very low-resource scenarios\n- Sample translations and attention analysis\n\nSlideTitle: Analysis of Transfer Learning\nBulletPoints:\n- Importance of transferring encoder parameters\n- Impact of ASR data amount on ST performance\n- Code-switching effects in Spanish-English ST\n- Benefits of using French ASR for Spanish-English ST\n\nSlideTitle: Results: Mboshi-French ST\nBulletPoints:\n- Challenges with very low-resource data\n- Comparison of naive baselines and pre-trained models\n- Best results with combined English and French parameters\n- Flexibility of the transfer learning framework\n\nSlideTitle: Conclusion and Future Directions\nBulletPoints:\n- Significant gains with ASR pre-training for low-resource ST\n- Faster training and improved BLEU scores\n- Potential for multilingual training and improved features\n- Future exploration of language model pre-training strategies",
    "SlideTitle: Introduction to Semantic Parsing\nBulletPoints:\n- Semantic parsing maps language utterances to executable programs.\n- Early work required expert annotators for training data.\n- Weakly-supervised semantic parsing uses utterance-denotation pairs.\n- Challenges include search complexity and spuriousness.\n\nSlideTitle: Visual Reasoning with CNLVR\nBulletPoints:\n- CNLVR corpus helps investigate spuriousness in semantic parsing.\n- Task involves determining truth of statements about images.\n- Images paired with complex natural language statements.\n- Semantic parsing translates language to programs for correctness.\n\nSlideTitle: Problem Statement and Programming Language\nBulletPoints:\n- Goal: Learn a parser mapping utterances to programs for correct denotations.\n- CNLVR images described by objects with attributes.\n- Programming language supports spatial reasoning functions.\n- Programs are sequences of function applications in prefix notation.\n\nSlideTitle: Model Architecture\nBulletPoints:\n- Based on encoder-decoder architecture with bi-directional LSTM.\n- Decoder uses attention mechanism over encoder outputs.\n- Beam search used to navigate large program space.\n- Semantic typing system prunes search space for valid programs.\n\nSlideTitle: Training from Weak Supervision\nBulletPoints:\n- Programs treated as latent variables, approximately marginalized.\n- Objective maximizes probability of correct denotations.\n- Spuriousness reduced by using multiple images per utterance.\n- Discriminative re-ranking improves model accuracy.\n\nSlideTitle: Learning from Abstract Examples\nBulletPoints:\n- Abstract representations help handle language compositionality.\n- Small lexicon maps lexical items to typed program constants.\n- Abstract examples improve search and reduce spuriousness.\n- Data augmentation generates new training examples.\n\nSlideTitle: Caching and Search Optimization\nBulletPoints:\n- Cache tracks abstract utterance-program pairs with correct denotations.\n- Full program and prefix retrieval guide search process.\n- Caching combats spuriousness and search challenges.\n- Abstract examples shared across training iterations.\n\nSlideTitle: Experimental Evaluation and Results\nBulletPoints:\n- Weakly-supervised parser with re-ranking achieves 84.0% accuracy.\n- Abstract examples improve coverage and accuracy.\n- Consistency metric evaluates correct denotation across images.\n- Error analysis highlights challenges with complex utterances.\n\nSlideTitle: Conclusion and Future Work\nBulletPoints:\n- First semantic parser for CNLVR dataset.\n- Abstract examples improve training from delayed supervision.\n- Future work aims to automate lexicon learning for larger domains.\n- Approach establishes new state-of-the-art in semantic parsing."
]