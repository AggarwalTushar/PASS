[
    "Slide Topic: Introduction to Sentiment-to-Sentiment Translation  \nSummary:\n- Sentiment-to-sentiment translation changes sentiment while preserving semantic content.\n- It is a style transfer task in NLP with applications like review transformation and news rewriting.\n- A major challenge is the lack of parallel training data.\n- Existing methods often fail to maintain semantic content when altering sentiment.\n- Example: Transforming \"The food is delicious\" to \"What a bad movie\" changes sentiment but not the topic.\n- This issue arises from difficulty in separating emotional from semantic information in dense vectors.\n\nSlide Topic: Challenges in Sentiment-to-Sentiment Translation  \nSummary:\n- Lack of parallel data complicates sentiment modification.\n- Models struggle to preserve non-emotional semantic content.\n- Emotional and semantic information often mixed in hidden layers.\n- Results in poor content preservation during sentiment change.\n- Examples show sentiment change without topic preservation.\n\nSlide Topic: Proposed Cycled Reinforcement Learning Approach  \nSummary:\n- Introduces a cycled reinforcement learning approach to address lack of parallel data.\n- Consists of a neutralization module and an emotionalization module.\n- Neutralization module extracts non-emotional semantic content by filtering out emotional words.\n- Emotionalization module adds sentiment to the neutral content.\n- Enables training with unpaired data and improves content preservation.\n- Separates sentiment from semantic content for better results.\n\nSlide Topic: Neutralization and Emotionalization Modules  \nSummary:\n- The neutralization module uses LSTM and self-attention to identify non-emotional words.\n- It filters out emotional words to create a neutralized sequence.\n- The emotionalization module employs a bi-decoder framework to add sentiment to neutral content.\n- Pre-training methods enhance initial learning ability of these modules.\n- This improves performance in sentiment-to-sentiment translation.\n\nSlide Topic: Cycled Reinforcement Learning Mechanism  \nSummary:\n- Trains neutralization and emotionalization modules to enhance each other.\n- Neutralization module extracts semantic content.\n- Emotionalization module reconstructs sentences with original sentiment.\n- Uses policy gradient to train the neutralization module.\n- Rewards based on sentiment confidence and BLEU score.\n- Aims to better identify non-emotional words.\n\n- Experiments conducted on Yelp and Amazon review datasets.  \n- User ratings indicate sentiment for text-sentiment pairs.  \n- Focus on sentence-level sentiment translation.  \n- Datasets split into training, validation, and testing sets.  \n- Sentiment classifier filters low-confidence pairs.  \n- Hyper-parameters tuned based on validation performance.  \n- Modules pre-trained before cycled training.  \n\nSlide Topic: Evaluation Metrics and Baselines  \nSummary:\n- Evaluation includes both automatic and human assessments.\n- Automatic evaluation uses sentiment accuracy and BLEU score.\n- Baselines include CAAE and MDAL models.\n- Human evaluation scores text on sentiment and semantic similarity.\n- Proposed method shows improved performance over baselines.\n\nSlide Topic: Results and Analysis  \nSummary:  \n- The proposed method outperforms baselines in content preservation.  \n- Achieves higher BLEU scores, indicating improved performance.  \n- Automatic evaluations show significant improvements in semantic preservation.  \n- Human evaluations confirm higher semantic scores for the proposed method.  \n- Effectively separates emotional from semantic content.  \n- Enhances overall translation quality.  \n\nSlide Topic: Error Analysis and Future Work  \nSummary:  \n- Identified issues with sentiment-conflicted and neutral sentences.  \n- Sentiment not fully removed or added in some cases.  \n- Need to handle complex sentiment expressions better.  \n- Aim to improve sentiment-aware decoders.  \n- Future work to explore fine-grained sentiment translation.  \n- Plan to adjust sentiment strength and reversal.  ",
    "Slide Topic: Introduction to Constituency Parsing and Its Importance  \nSummary:\n- Constituency parsing is crucial in natural language processing.\n- It aids tasks like relation extraction and paraphrase detection.\n- Recently, it's been vital for natural language inference and machine translation.\n- Neural network approaches have improved parsing accuracy.\n- Dense input representations yield competitive results in parsing.\n\nSlide Topic: Challenges in Transition-Based and Chart-Based Parsing Models  \nSummary:  \n- Transition-based models use a sequence of local decisions for structured prediction.  \n- They allow fast greedy decoding but can lead to compounding errors.  \n- Solutions involve complex training with beam search and dynamic oracles.  \n- Chart-based models use structured loss functions and exact inference via CYK algorithm.  \n- They face higher computational costs during decoding.  \n\nSlide Topic: Proposed Fully-Parallel Model Using Syntactic Distances  \nSummary:\n- Introduces a novel, fully-parallel model for constituency parsing.\n- Utilizes 'syntactic distance' to determine split point ordering.\n- Trained using margin rank loss to match ground-truth distances.\n- Each split point's distance is estimated independently.\n- Enables easy parallelization on modern computing architectures like GPUs.\n\nSlide Topic: Algorithm for Mapping Syntactic Distances to Parse Trees  \nSummary:\n- Introduce two algorithms for parse tree conversion and mapping.\n- First algorithm converts parse trees into compact representations using word distances.\n- Second algorithm maps inferred representations back to complete parse trees.\n- The representation is used for supervised training.\n- Mapping from syntactic distances to trees is efficient at O(n log n).\n- This efficiency makes the decoding process computationally attractive.\n\nSlide Topic: Model Architecture and Learning Syntactic Distances  \nSummary:\n- Neural networks estimate syntactic distance vectors for sentences.\n- Syntactic distances and constituent labels are predicted using a neural network.\n- The architecture combines recurrent (LSTM) and convolutional layers.\n- Bidirectional LSTM layers capture long-term syntactic relations.\n- The final loss is the sum of individual losses for syntactic distances and labels.\n\nSlide Topic: Experimental Evaluation on PTB and CTB Datasets  \nSummary:\n- Evaluated model on PTB and CTB datasets.\n- Achieved strong performance in single-model constituency parsing.\n- Set new state-of-the-art for single-model parsing.\n- Achieved 86.5 F1 score on CTB test set.\n- Detailed statistics are available in the tables.\n\n**Slide Topic: Ablation Study and Parsing Speed Comparison**\n\nSummary:\n- Conducted ablation study by removing components from the best-trained network.\n- Re-trained the ablated version from scratch for analysis.\n- Syntactic distance prediction can be batched on modern GPUs.\n- Distance-to-tree conversion uses an O(n log n) divide-and-conquer algorithm.\n- Achieved parsing speed of 111.1 sentences per second.\n- Outperforms other state-of-the-art neural parsers.",
    "Slide Topic: Introduction to Translation and Its Historical Context  \nSummary:  \n- Translation has been crucial since the rise of law, religion, and trade in multilingual societies.  \n- Evidence of translations dates back four millennia, including references in the Bible.  \n- Translators have historically aimed for invisibility, making translations appear original.  \n- Cicero emphasized preserving style and force over word-for-word translation.  \n- St. Jerome echoed this, advocating for translating sense for sense.  \n\nSlide Topic: Translation Universals and Interference Phenomena  \nSummary:\n- Translations have unique characteristics distinct from non-translated texts.\n- These differences are due to statistical phenomena, not poor translation.\n- Universal features of translation include simplification, standardization, and explicitation.\n- Interference reflects the influence of the source language on the translation.\n- Source text features often transfer to the target text, creating interference.\n\nSlide Topic: Hypotheses on Language Distance and Phylogenetic Trees  \nSummary:  \n- Languages with shared structures likely share features in translation.  \n- Language distance can be partially retained in translations to a third language.  \n- Extracting structures from translations can generate phylogenetic trees.  \n- These trees reflect 'true' distances among source languages.  \n- Tree quality improves with features corresponding to interference phenomena.  \n\nSlide Topic: Methodology and Dataset Description  \nSummary:  \n- The study uses the Europarl corpus, containing European Parliament proceedings and translations.  \n- Data spans from 1999 to 2011, with translation directions validated cross-lingually.  \n- Original speeches were translated into all EU languages using English as a pivot.  \n- Focus is on 17 source languages from Germanic, Romance, and Balto-Slavic families.\n\nSlide Topic: Feature Sets for Translation Analysis  \nSummary:\n- Texts are represented as feature vectors for analysis.\n- Feature choice affects source-language interference in translations.\n- Key feature sets: top-1,000 frequent POS trigrams, function words, cohesive markers.\n- POS trigrams and function words are linked to interference.\n- Cohesive markers are considered universal and explicit.\n\nSlide Topic: Evaluation of Phylogenetic Trees  \nSummary:  \n- Defined a metric to assess distance from the 'true' tree.  \n- Reconstructed trees using agglomerative clustering of feature vectors.  \n- Used variance minimization algorithm with Euclidean distance.  \n- Evaluated trees from English and French translations.  \n- POS-trigrams produced trees closest to the gold standard.  \n- Confirmed hypothesis on source language interference.  \n\nSlide Topic: Analysis of Linguistic Phenomena in Translations  \nSummary:  \n- Source-language traces are dominant in translation products.  \n- These traces help reconstruct the history of source languages.  \n- Languages differ in articles, possessive constructions, verb-particle constructions, and tense/aspect.  \n- We computed pattern frequencies in English translations from each language.  \n- Results show significant overuse of 'the' in translations from Balto-Slavic languages.  \n- Some overuse of 'the' is also noted in translations from Romance languages.  \n\nSlide Topic: Conclusions and Implications for Language Typology  \nSummary:\n- Translations are systematic, not random, distortions of original texts.\n- They reflect language typology, enabling phylogenetic tree reconstruction.\n- This is possible using a monolingual corpus of multiple translations.\n- High-quality translations by professional translators are key.\n- Translations into English and French capture historical language elements.",
    "Slide Topic: Introduction to Cross-Language Plagiarism Detection  \nSummary:\n- Plagiarism is a major issue, especially in higher education.\n- Monolingual plagiarism detection is well-researched.\n- Cross-language plagiarism involves translating text, complicating detection.\n- Detection is challenging due to language differences between source and document.\n- The paper explores using word embeddings for cross-lingual similarity measures.\n\nSlide Topic: Contributions and Innovations in Plagiarism Detection  \nSummary:  \n- Enhanced methods using word embeddings.  \n- Introduced syntax weighting for better sentence representation.  \n- Improved textual similarity detection.  \n- Combined methods to verify complementarity.  \n- Achieved an F1 score of 89.15% for English-French detection.  \n- Outperformed individual methods significantly.  \n\nSlide Topic: Evaluation Conditions and Dataset Characteristics  \nSummary:  \n- Utilizes dataset by Ferrero et al. (2016) for cross-language textual similarity.  \n- Multilingual content includes French, English, and Spanish texts.  \n- Provides cross-language alignment at document, sentence, and chunk levels.  \n- Contains both human and machine-translated texts.  \n- Features varying percentages of named entities and a mix of obfuscated and clear texts.  \n- Authored by individuals with different expertise levels.  \n\nSlide Topic: Overview of State-of-the-Art Methods  \nSummary:  \n- Reviews methods for cross-language textual similarity detection.  \n- Includes CL-CnG, CL-CTS, CL-ASA, CL-ESA, and T+MA.  \n- CL-CnG uses character n-grams for analysis.  \n- CL-CTS employs conceptual thesauri for similarity.  \n- CL-ASA focuses on alignment-based analysis.  \n- CL-ESA utilizes semantic vectors; T+MA combines translation with monolingual analysis.  \n\nSlide Topic: Proposed Methods Using Word Embeddings  \nSummary:\n- Utilize word embeddings for effective plagiarism detection.\n- Capture synonymy and morphological closeness with embeddings.\n- Employ MultiVec toolkit for continuous text representations.\n- Use models like word2vec and bilingual distributed representations.\n- Train embeddings on News Commentary parallel corpus.\n- Apply CBOW model with specific vector, window, and sampling parameters.\n\nSlide Topic: Improving Textual Similarity with Syntax Weighting  \nSummary:\n- Introduced CL-CTS-WE, using word embeddings instead of lexical resources.\n- Developed CL-WES, applying cosine similarity on sentence embeddings.\n- Enhanced CL-WES with CL-WESS by adding syntax weighting.\n- Syntax weighting uses part-of-speech tags to adjust word contributions.\n- This syntactic enhancement significantly boosts performance.\n- CL-WESS is the top-performing single method evaluated.\n\n**Slide Topic: Combining Multiple Methods for Enhanced Detection**\n\nSummary:\n- Explores enhancing detection through weighted and decision tree fusion.\n- Weighted fusion assigns scores based on method similarity.\n- Decision tree fusion uses the C4.5 algorithm for method optimization.\n- Decision tree fusion achieves superior performance with an F1 score of 89.15%.\n- Demonstrates the complementarity and effectiveness of combined approaches.\n\nSlide Topic: Results and Discussion on Method Performance  \nSummary:\n- Word embeddings enhance method performance.\n- CL-CTS-WE outperforms traditional methods.\n- CL-WESS, with syntax weighting, achieves best results.\n- Decision tree fusion boosts performance further.\n- Achieves F1 score of 89.15% at chunk level.\n- Confirms complementarity and utility in cross-language similarity detection.",
    "Slide Topic: Introduction to Taylor's Law in Text Analysis  \nSummary:\n- Taylor's Law describes how event variance grows with the mean, forming a power law.\n- It quantifies clustering behavior in systems.\n- Widely studied in ecology, life sciences, physics, finance, and human dynamics.\n- Limited application in natural language analysis.\n- This study applies Taylor's Law to text, focusing on lexical fluctuations.\n\nSlide Topic: Application of Taylor's Law to Natural Language  \nSummary:\n- Taylor exponent is a universal characteristic of texts across languages.\n- Analysis includes over 1100 single-author texts in 14 languages and large-scale newspaper data.\n- Taylor exponents differ for symbolic sequential data like child-directed speech, code, and music.\n- These differences help distinguish various data sources.\n- Taylor exponent can categorize and quantify language's structural complexity.\n\nSlide Topic: Measuring the Taylor Exponent in Texts  \nSummary:\n- Define a discrete time series X of words from a set W.\n- Segment the series X by a positive integer length \u2206t.\n- Count occurrences of a specific word wk in each segment.\n- Calculate the mean (\u00b5k) and standard deviation (\u03c3k) for word occurrences.\n- Taylor's law holds if \u00b5 and \u03c3 are correlated by a power law: \u03c3 \u221d \u00b5^\u03b1.\n\nSlide Topic: Taylor Exponents Across Different Data Types  \nSummary:  \n- Analysis shows Taylor exponents vary by data source.  \n- Distributions differ from natural language texts.  \n- Exponents are significantly larger in these data sets.  \n- Indicates frequent fixed-form expressions.  \n- Data types may be distinguishable by Taylor exponent.  \n\n**Slide Topic: Impact of Data and Segment Size on Taylor Exponent**\n\nSummary:\n- Taylor exponent is slightly affected by data size.\n- Increasing data size tends to slightly decrease the exponent.\n- Exponent varies with segment size (\u2206t).\n- Larger segment sizes lead to a growth in the exponent.\n- Indicates self-similarity in word co-occurrences in natural language.\n\n**Slide Topic: Comparison of Taylor Exponents in Machine-Generated Text**\n\nSummary:\n- Character-based LSTM models generate text with a Taylor exponent of 0.5.\n- This exponent highlights a limitation in the LSTM model's performance.\n- State-of-the-art word-level models achieve exponents greater than 0.50.\n- These exponents are still lower than those of the training dataset.\n- Indicates capability in modeling text burstiness and potential for improvement.\n\nSlide Topic: Implications for Language Model Evaluation  \nSummary:\n- The Taylor exponent is effective for evaluating machine-generated text.\n- Neural machine translation models maintain the burstiness of original text.\n- Translated text retains the clustering behavior of the original.\n- The Taylor exponent of translated text matches that of the original text.",
    "Slide Topic: Introduction and Motivation  \nSummary:  \n- Focus on real-time analysis of social media data during sudden events.  \n- Social media provides timely reports on injuries, damage, and urgent needs.  \n- Humanitarian organizations use this data to reduce human and economic losses.  \n- Task involves classifying tweets into predefined categories during crises.  \n- Challenge: Lack of large labeled datasets for effective DNN training.  \n- Solution: Use domain adaptation and semi-supervised learning for data leverage.  \n\nSlide Topic: Proposed Model and Framework  \nSummary:\n- Introduces a unified deep learning framework for domain adaptation and semi-supervised learning.\n- Combines a task-solving network with two additional networks for specific learning tasks.\n- The semi-supervised component predicts contextual nodes in a graph to learn internal representations.\n- Domain adaptation is achieved through adversarial training with a domain discriminator.\n- Aims to learn high-level abstract representations that are discriminative yet domain-invariant.\n\nSlide Topic: Model Components and Architecture  \nSummary:\n- Input: Tweets from a finite vocabulary.\n- First layer: Maps words to distributed representations using an embedding matrix.\n- Look-up layer output: Matrix processed through convolution and pooling layers.\n- Network splits into three branches, each contributing a separate loss.\n- Components: Supervised classification loss, graph-based semi-supervised loss, and adversary loss for domain invariance.\n\nSlide Topic: Semi-supervised Learning Approach  \nSummary:\n- Induces structural similarity between source and target events.\n- Utilizes a graph-based semi-supervised deep learning framework.\n- Shows impressive gains over existing methods.\n- A 'similarity' graph encodes relations between training instances.\n- The network learns internal representations from shared inputs.\n- Graph constructed using a k-nearest neighbor approach with k=10.\n\nSlide Topic: Domain Adaptation with Adversarial Training  \nSummary:\n- The network learns abstract features for classification tasks.\n- Goal: Make features invariant across domains or events.\n- Achieved through domain adversarial training of neural networks.\n- A domain discriminator distinguishes between input domains.\n- Discriminator uses a sigmoid function and negative log-probability loss.\n- Adversarial training updates parameters to balance network components.\n\nSlide Topic: Experimental Setup and Datasets  \nSummary:  \n- Utilized two Twitter datasets from the 2015 Nepal earthquake and 2013 Queensland floods.  \n- Collected millions of tweets via Twitter streaming API using event-specific keywords/hashtags.  \n- Employed crowdsourcing to label data into relevant and non-relevant classes.  \n- Split labeled data: 60% training, 30% test, 10% development.  \n- Followed word2vec preprocessing steps for data preparation.  \n\nSlide Topic: Results and Analysis  \nSummary:  \n- Table 2 shows results from supervised, self-training, and graph-based semi-supervised experiments.  \n- Graph-based semi-supervised approach outperforms baselines with 4% to 13% F1 score improvements.  \n- Domain adversarial training improves F1 scores by 1.8% to 4.1% across both events.  \n- Combining semi-supervised learning and unsupervised domain adaptation yields 5% to 7% F1 score gains.\n\nSlide Topic: Related Work and Comparisons  \nSummary:  \n- Two research areas related to our work: semi-supervised learning and domain adaptation.  \n- Semi-supervised learning models include self-training and co-training approaches.  \n- Graph-based semi-supervised methods use nodes for labeled/unlabeled instances and edge weights for similarity.  \n- Domain adaptation methods include supervised, semi-supervised, and unsupervised techniques.  \n- Adversarial training of neural networks has recently impacted fields like computer vision.  \n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Presented a deep learning framework for domain adaptation using adversarial training and graph-based semi-supervised learning.  \n- Leveraged labeled and unlabeled data from related events.  \n- Evaluated on two crisis-related tweet datasets, showing significant improvements over baselines.  \n- Demonstrated batch-wise incremental training for the graph-based approach.  \n- Provided approximation of labeled examples needed for acceptable performance at event onset.  ",
    "Slide Topic: Introduction to Abstract Meaning Representation (AMR)  \nSummary:  \n- AMR is a semantic graph representation abstracting sentence syntax.  \n- Nodes represent concepts; edges represent semantic relations.  \n- AMRs are graphs, not trees, due to reentrancies from co-references.  \n- Reentrancies occur when nodes have multiple parents.  \n- AMR-to-text generation uses encoder/decoder architectures.  \n- This process is similar to Neural Machine Translation.  \n\nSlide Topic: Challenges in AMR-to-Text Generation  \nSummary:\n- Reentrancies are central to AMR and pose significant challenges.\n- Parsing and generation are both affected by reentrancies.\n- Previous work often removes reentrancies for simpler data structures.\n- The impact of maintaining reentrancies on performance is unclear.\n- Explicit encoding of reentrancies may not benefit generation.\n- State-of-the-art parsers struggle with predicting reentrant structures.\n\nSlide Topic: Comparison of Encoder Types for AMR  \nSummary:  \n- Compares three encoder types: sequential, tree, and graph.  \n- Sequential encoders reduce AMR graphs to sequences.  \n- Tree encoders ignore reentrancies.  \n- Graph encoders excel in handling reentrancies and long-range dependencies.  \n- Graph encoder model achieves state-of-the-art results on LDC2015E86 and LDC2017T10 datasets.  \n\nSlide Topic: Graph-Structured AMRs and Their Encoding  \nSummary:  \n- AMRs are represented as rooted, directed graphs with labeled edges.  \n- Labeled edges are transformed into unlabeled edges to reduce model parameters.  \n- This transformation creates an equivalent Levi graph.  \n- The process allows for more efficient encoding of AMR graphs.  \n- Efficiency improvements have been demonstrated in previous studies.  \n\nSlide Topic: Tree-Structured and Sequential AMR Representations  \nSummary:  \n- Tree-structured AMRs are created by removing reentrancies.  \n- Nodes with multiple incoming edges are replaced with identically labeled nodes.  \n- Sequential AMRs linearize and anonymize AMR graphs.  \n- Conversion to sequences is done through depth-first traversal.  \n- This method loses reentrancy information.  \n- It helps reduce data sparsity.  \n\nSlide Topic: Stacking Encoders: Structure and Sequence Integration  \nSummary:\n- The study examines combining structural encoders (TreeLSTMs, GCNs) with sequential encoders (BiLSTMs).\n- Two methods are tested: structure on top of sequence and sequence on top of structure.\n- The sequence-on-structure approach refines input embeddings with structural encoders before BiLSTMs.\n- This method proves more effective, emphasizing the value of structural information integration.\n\n**Slide Topic: Experimental Results and Evaluation Metrics**\n\nSummary:\n- Evaluation metrics used: BLEU and Meteor.\n- Results reported on LDC2015E86 and LDC2017T10 datasets.\n- GCNSEQ model integrates graph encoders with BiLSTMs.\n- Achieves state-of-the-art results, proving structural encoders' efficacy.\n- Graph encoder excels in handling reentrancies and long-range dependencies.\n\nSlide Topic: Handling Reentrancies and Long-Range Dependencies  \nSummary:\n- Graph encoders explicitly model reentrancies.\n- They outperform tree and sequential encoders.\n- Graph models excel with more reentrancies and longer dependencies.\n- Manual inspection shows better handling of pronouns and control structures.\n- Explicit encoding of reentrancies enhances accuracy.",
    "Slide Topic: Introduction to Temporal and Causal Relations  \nSummary:  \n- Understanding events is key for natural language applications.  \n- Focus on joint extraction of temporal and causal relations.  \n- Temporal relations can be influenced by causal interactions.  \n- One event causing another determines their temporal order.  \n- Essential for tasks like story completion and summarization.  \n\nSlide Topic: Joint Framework for Temporal and Causal Reasoning (TCR)  \nSummary:\n- Introduces a joint framework for Temporal and Causal Reasoning (TCR).\n- Utilizes a constrained conditional model (CCM) with an integer linear programming (ILP) objective.\n- Enforces constraints like cause must precede effect, symmetry, and transitivity.\n- Leverages the one-dimensional nature of time and causality.\n- CCM is a suitable choice for addressing temporal and causal reasoning challenges.\n\n**Slide Topic: Challenges in Jointly Annotating Temporal and Causal Data**\n\nSummary:\n- Lack of jointly annotated data hinders studying temporal and causal relations.\n- A new dataset with dense temporal annotations is developed.\n- This dataset augments the existing Event-Causality dataset.\n- The joint framework improves extraction of both temporal and causal relations.\n- Statistically significant improvements are achieved with the proposed approach.\n\n**Slide Topic: Empirical Results and Improvements in Temporal Extraction**\n\nSummary:\n- The study shows improvements in temporal extraction.\n- Enhancements include explicit time expressions and high-precision rules.\n- These modifications are integrated into the ILP objective.\n- Tested on the TimeBank-Dense dataset.\n- Results indicate benefits for ILP-based methods.\n- Leads to better temporal relation identification.\n\nSlide Topic: Related Work and Existing Approaches  \nSummary:  \n- Temporal and causal relations are modeled using directed acyclic graphs.  \n- Early methods relied on local classification models.  \n- These models often violated symmetry and transitive constraints.  \n- Recent approaches, like CAEVO, use global reasoning to enforce constraints.  \n- The paper enhances these methods with an ILP approach for joint extraction.  \n\nSlide Topic: Transitivity and Linguistic Constraints in Temporal Reasoning  \nSummary:\n- Transitivity constraints enhance temporal consistency across event pairs.\n- Linguistic rules are derived from syntactic or semantic patterns.\n- These rules are integrated as high-precision constraints.\n- The approach improves global temporal reasoning.\n- The design focuses on enforcing linguistic rules effectively.\n\n**Slide Topic: Joint Performance on New Dataset with Temporal and Causal Annotations**\n\n**Summary:**\n- Evaluates joint framework on a new dataset with temporal and causal annotations.\n- Joint framework improves both temporal and causal performance.\n- Indicates mutual benefits of temporal and causal signals.\n- Dataset created by augmenting existing data with dense temporal annotations.\n- Ensures comprehensive evaluation of the joint approach.\n\nSlide Topic: Discussion on Temporal and Causal Interactions  \nSummary:\n- Temporal and causal information are complementary.\n- Temporal relations focus on local context.\n- Causal relations encode global knowledge.\n- A joint framework synthesizes these signals for better event understanding.\n- Challenges exist when causes occur after effects.\n- Improved event definitions are needed to address these challenges.",
    "Slide Topic: Introduction to High-Bandwidth Supervision  \nSummary:  \n- Traditional labeling provides one bit of information per example.  \n- This approach uses natural language explanations for richer data.  \n- Annotators give explanations parsed into logical labeling functions.  \n- Functions map examples to labels, creating a large, weakly-supervised set.  \n- Enhances annotator efficiency and accelerates the training process.  \n\nSlide Topic: The BabbleLabble Framework  \nSummary:\n- Transforms natural language explanations and unlabeled data into a noisily-labeled training set.\n- Consists of three components: semantic parser, filter bank, and label aggregator.\n- Semantic parser converts explanations into logical forms.\n- Filter bank removes incorrect logical forms.\n- Label aggregator combines labels into a single probabilistic label per example.\n- Enables training of a discriminative model, enhancing labeling efficiency.\n\nSlide Topic: Semantic Parsing and Logical Forms  \nSummary:\n- Semantic parsing converts natural language into logical forms.\n- A simple rule-based parser effectively filters incorrect forms and tolerates noise.\n- It uses grammar rules to generate logical forms from explanations.\n- This method covers many useful labeling functions.\n- The approach is domain-independent and versatile.\n- It applies across various tasks without task-specific training.\n\nSlide Topic: Filter Bank and Label Aggregation  \nSummary:  \n- The filter bank discards incorrect logical forms without extra labels.  \n- It uses semantic and pragmatic filters for consistency and redundancy removal.  \n- The label aggregator combines remaining labels into a single probabilistic label.  \n- Data programming models relationships between true labels and labeling functions.  \n- This process enhances the accuracy and reliability of generated labels.  \n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Evaluated Babble Labble on Spouse, Disease, and Protein tasks.  \n- Achieved significant improvement in supervision efficiency.  \n- Comparable F1 scores with fewer user inputs than traditional methods.  \n- Effectively utilized unlabeled data for high-bandwidth supervision.  \n- Outperformed traditional methods in domains with abundant unlabeled data.  \n\nSlide Topic: Utility of Incorrect Parses  \nSummary:  \n- Babble Labble's filter bank removes over 95% of incorrect logical forms.  \n- The final set is 86% correct on average.  \n- End-task accuracy difference between correct and incorrect parses is under 2.5%.  \n- Incorrect parses still provide valuable signals.  \n- Framework filters effectively maintain high accuracy.  \n\n**Using Logical Forms as Functions or Features**\n\n- Logical forms from natural language can be used as functions or features.\n- Babble Labble framework uses them as functions for weak supervision.\n- This approach creates larger training sets via data programming.\n- Using logical forms as functions outperforms using them as features.\n- It leverages unlabeled data to enhance performance.\n- Data programming exposes classifiers to additional patterns, boosting predictive capabilities.",
    "**Slide Topic: Introduction to Abstractive Sentence Summarization**\n\nSummary:\n- Growing online information demands effective summarization systems.\n- Abstractive summarization creates shorter versions of sentences.\n- It preserves original meaning while condensing content.\n- Useful for applications like crafting appealing headlines.\n- Attentional seq2seq framework achieves state-of-the-art results.\n- Challenges remain in generating longer text summaries.\n\nSlide Topic: Challenges with Seq2Seq Models  \nSummary:\n- Seq2Seq models struggle with longer text generation.\n- They can produce summaries that are too short or repetitive.\n- This affects the informativeness and readability of outputs.\n- Models often copy source words instead of summarizing.\n- Relying solely on source sentences is insufficient.\n\nSlide Topic: Template-Based Summarization Approaches  \nSummary:\n- Template-based summarization uses predefined sentence structures.\n- It produces fluent and informative summaries.\n- Construction is time-consuming and requires domain knowledge.\n- Limited by the inability to cover all domains.\n- Soft templates offer flexibility using existing summaries as references.\n\nSlide Topic: Re3Sum: A Hybrid Summarization System  \nSummary:  \n- Re3Sum combines seq2seq and template-based summarization approaches.  \n- It consists of three modules: Retrieve, Rerank, and Rewrite.  \n- Uses an Information Retrieval platform to find candidate soft templates.  \n- Extends seq2seq model to learn template saliency and generate summaries.  \n- Improves informativeness and readability over traditional seq2seq models.\n\nSlide Topic: Retrieve, Rerank, and Rewrite Modules  \nSummary:\n- Retrieve module identifies candidate templates from a large corpus.\n- Assumes similar sentences share summary patterns.\n- Rerank module measures informativeness using a BiRNN encoder.\n- Selects the most relevant template.\n- Rewrite module generates summaries by combining input sentence and template.\n- Leverages seq2seq model for faithful and informative rewriting.\n\nSlide Topic: Experimental Setup and Evaluation Metrics  \nSummary:  \n- Experiments conducted on the Annotated English Gigaword corpus.  \n- ROUGE used as the primary evaluation metric.  \n- ROUGE measures summary quality by lexical overlap.  \n- Additional metrics: length difference, copy ratio, new named entities.  \n- Metrics assess readability, informativeness, and faithfulness.  \n- Open-NMT seq2seq framework used as a baseline for comparison.  \n\nSlide Topic: Results and Analysis  \nSummary:  \n- Re3Sum outperformed state-of-the-art seq2seq models in informativeness and readability.  \n- Demonstrated stability and reduced copy ratio, shifting from compression to abstraction.  \n- Rerank module improved template selection effectively.  \n- Rewrite module ensured summaries were faithful to the input.  \n- Showed potential for generating diverse summaries, valuable for practical applications.  \n\nSlide Topic: Future Work and Conclusion  \nSummary:  \n- Introducing soft templates enhances seq2seq summarization.  \n- Future work includes improving the Retrieve module.  \n- Testing on tasks like document-level summarization is planned.  \n- The model generates diverse and informative summaries.  \n- Promising applications are anticipated in various domains.  ",
    "Slide Topic: Introduction to Event Detection and Challenges  \nSummary:  \n- Event Detection (ED) identifies event triggers and determines their types.  \n- Context-dependent nature of triggers poses a significant challenge.  \n- Triggers can represent different events in varying contexts.  \n- Sentence-level context is commonly used, but document-level context is crucial.  \n- Document-level context provides thematic coherence across sentences.  \n- Example: The trigger 'leave' can indicate different events based on context.  \n\nSlide Topic: Existing Methods and Their Limitations  \nSummary:  \n- Current methods are feature-based and representation-based.  \n- Feature-based methods rely on manually designed features, leading to errors and being time-consuming.  \n- Representation-based methods use distributed representations but miss event-specific details.  \n- Document-level information is underutilized in these methods.  \n- Existing methods struggle with capturing inter-event information effectively.  \n- Complex rule-based systems are often required.  \n\nSlide Topic: Proposed DEEB-RNN Model  \nSummary:\n- Introduces a novel approach for sentence-level event detection.\n- Utilizes hierarchical, supervised attention-based bidirectional RNN.\n- Focuses on word-level attention for event triggers.\n- Employs sentence-level attention for event-containing sentences.\n- Integrates attention-based loss functions to enhance event identification.\n- First study to use hierarchical attention for event detection-oriented embeddings.\n\nSlide Topic: ED Oriented Document Embedding Learning (EDODEL) Module  \nSummary:\n- Utilizes a hierarchical attention network with word-level and sentence-level Bi-GRU encoders.\n- Word embeddings are created by combining word and entity type embeddings, followed by attention-weighted summation.\n- Sentence embeddings focus on sentences with events, using a similar method.\n- Constructs gold attention signals to supervise learning.\n- Prioritizes attention on event-related words and sentences.\n\nSlide Topic: Document-level Enhanced Event Detector (DEED) Module  \nSummary:\n- Utilizes a Bi-GRU encoder and softmax output layer for event detection.\n- Handles multi-word event triggers effectively.\n- Classifies each word in a sentence to an event type using embeddings.\n- Employs cross-entropy error for the loss function.\n- Leverages context for accurate event type classification.\n\nSlide Topic: Joint Training of DEEB-RNN Model  \nSummary:\n- DEEB-RNN modules are trained jointly.\n- Uses a combined loss function.\n- Balances attention-based and classification losses.\n- Optimizes model parameters.\n- Enhances event detection capability.\n- Integrates information at multiple levels effectively.\n\nSlide Topic: Experimental Setup and Baseline Models  \nSummary:  \n- DEEB-RNN model evaluated on ACE-2005 dataset.  \n- Training set: 529 documents; Validation set: 30; Test set: 40.  \n- Baseline models include feature-based and representation-based approaches.  \n- Models: Sentence-level, Joint Local, JRNN, Skip-CNN, ANN-S2, Cross-event, PSL, DLRNN.  \n- Baselines serve as benchmarks to validate DEEB-RNN effectiveness.  \n\nSlide Topic: Performance Comparison and Analysis  \nSummary:  \n- DEEB-RNN outperforms state-of-the-art methods in recall and F1-measure.  \n- Utilizes document-level embeddings and supervised attention mechanisms.  \n- Different attention strategies impact precision and recall variably.  \n- DEEB-RNN3 excels using gold attention signals at word and sentence levels.  \n- Captures event types automatically without manual rules, offering a key advantage.  ",
    "Slide Topic: Introduction to Neural Machine Translation and Its Advantages  \nSummary:\n- Neural Machine Translation (NMT) uses RNNs for direct translation sampling.\n- It employs an Encoder-Decoder architecture for effective translation.\n- NMT outperforms phrase-based SMT in tasks like English-French and English-German.\n- Stacked LSTM units enhance performance in both encoder and decoder roles.\n\n**Slide Topic: Recurrent Neural Networks and Their Limitations**\n\nSummary:\n- RNNs are designed for modeling temporal data.\n- They update internal states based on new inputs and previous states.\n- RNNs produce outputs recursively.\n- They struggle with capturing long-term dependencies.\n- Error information dilutes over time in RNNs.\n- LSTM and GRU architectures address these limitations.\n\n**Slide Topic: Long Short-Term Memory and Gated Recurrent Units**\n\nSummary:\n- LSTM units use gates to control information flow and capture long-term dependencies.\n- They utilize memory cells for storing information, which can be updated linearly.\n- GRUs are similar but maintain only one kind of internal state, reducing computational complexity.\n- Both LSTM and GRU improve NMT model performance by managing information flow effectively.\n\nSlide Topic: Encoder-Decoder Architecture in NMT  \nSummary:  \n- Encodes input sequence into a vector representation.  \n- Decodes vector to generate output sequence.  \n- Utilizes trainable word embeddings.  \n- Employs softmax function for cross-entropy computation.  \n- Features stacked multilayer recurrent units.  \n- Enhances ability to handle complex translations.  \n\nSlide Topic: Soft-Attention Mechanism in NMT  \nSummary:  \n- Enhances basic Encoder-Decoder models, especially for long sentences.  \n- Computes weights for all input positions to determine focus.  \n- Creates a weighted summary of encoder hidden states.  \n- Allows the model to concentrate on relevant input parts.  \n- Improves translation quality without performance loss on longer sentences.  \n\nSlide Topic: Handling Unknown Words in NMT  \nSummary:  \n- Unknown words pose a challenge due to fixed vocabulary size in NMT.  \n- Solutions include model-specific approaches like noise-contrastive training.  \n- Translation-specific methods use token alignments for better handling.  \n- Annotating unknown words with aligned source positions aids recovery.  \n- These techniques help NMT models outperform state-of-the-art SMT systems.  \n\nSlide Topic: Experiments and Evaluation of NMT Models  \nSummary:  \n- Conducted experiments on English-Japanese translation using the ASPEC-JE corpus.  \n- Evaluated models based on perplexity.  \n- Soft-attention models outperformed simple Encoder-Decoder models.  \n- Highlighted challenges in translating between structurally different languages.  \n- Emphasized the importance of capturing long-range dependencies.  \n\nSlide Topic: Qualitative Analysis and Error Patterns in NMT Translations  \nSummary:  \n- NMT models often omit information, leading to incomplete translations.  \n- Translations are fluent but may not preserve full source text meaning.  \n- Error patterns indicate a need for improved translation accuracy.  \n- Addressing these errors can enhance overall translation quality.",
    "Slide Topic: Introduction to Knowledge Base Question Answering  \nSummary:\n- Large-scale knowledge bases like DBpedia and Freebase are vital for open-domain QA.\n- State-of-the-art KB-QA uses semantic parsing to map questions to formal representations.\n- This process translates questions into KB queries to retrieve answers.\n- It enhances understanding, aiding in error analysis and answer justification.\n- Traditional semantic parsing faces challenges like ontology matching and predicate selection.\n\nSlide Topic: Challenges in Semantic Parsing for QA  \nSummary:\n- Traditional semantic parsing is often disconnected from the knowledge base.\n- Generic meaning representations face ontology matching issues.\n- Logical forms may use predicates not found in the knowledge base.\n- Even with schema alignment, finding correct predicates is challenging.\n- An integrated approach is needed to better leverage the knowledge base.\n\nSlide Topic: Proposed Semantic Parsing Framework  \nSummary:\n- Introduces a semantic parsing framework integrating the knowledge base.\n- Defines a query graph mapped to a logical form in \u03bb-calculus.\n- Relates semantically to \u03bb-DCS for logical representation.\n- Reduces semantic parsing to query graph generation.\n- Formulates as a search problem with staged states and actions.\n- Focuses on efficient search in promising areas for correct query graphs.\n\nSlide Topic: Query Graph Design and Execution  \nSummary:\n- Our query graph design is inspired by previous works with key differences.\n- Nodes and edges resemble exact entities and predicates from the knowledge base.\n- This allows straightforward translation to a directly executable logical form query.\n- The graph includes grounded entities, existential and lambda variables, and aggregation functions.\n- Directed edges are labeled with predicates, connecting the components.\n- The design enables efficient execution against the knowledge base to retrieve answers.\n\nSlide Topic: Staged Query Graph Generation  \nSummary:\n- Focus on generating query graphs with specific properties.\n- Rooted at a topic entity with a core inferential chain.\n- Formalized as a search problem with staged states and actions.\n- Enables efficient exploration of possible query graphs.\n- Staged design prunes search space and biases exploration order.\n- Increases likelihood of generating correct semantic parse.\n\nSlide Topic: Deep Convolutional Neural Networks for Relation Matching  \nSummary:\n- Utilize Siamese neural networks to handle semantic variations in questions.\n- Map questions to patterns and compare with candidate chains using CNNs.\n- Overcome natural language and KB predicate mismatches.\n- Leverage continuous-space representations for improved accuracy.\n- Enhance relation matching beyond traditional lexical methods.\n\nSlide Topic: Augmenting Constraints and Aggregations  \nSummary:\n- A graph with only the inferential chain forms the simplest query graph.\n- Additional constraints and aggregation functions refine the answer set.\n- Constraints are added by attaching entities to variable nodes.\n- Aggregation functions manage constraints over the entire answer set.\n- Simple rules ensure only legitimate constraints are retained.\n- This approach enhances the precision of the query graph.\n\nSlide Topic: Experimental Results and Analysis  \nSummary:  \n- Our semantic parsing approach achieves 52.5% F1 on the WEBQUESTIONS dataset.  \n- This represents a 7.2% absolute gain over the previous best method.  \n- System performance is evaluated by the ratio of correctly answered questions.  \n- Precision, recall, and F1 are computed for each question.  \n- Results highlight the effectiveness of our approach with significant improvements.  ",
    "Slide Topic: Introduction to Retraction Citations  \nSummary:  \n- Retractions of scientific articles are increasing relative to overall publications.  \n- Retractions help remove erroneous or unethical research from literature.  \n- Retraction citations pose challenges to scientific integrity.  \n- Two types of retraction citations: pre-retraction and post-retraction.  \n- Both citation types jeopardize the scientific process when cited as legitimate.\n\nSlide Topic: Data Collection and Methodology  \nSummary:\n- Accessed ScienceDirect database in October 2014.\n- Queried for 'RETRACTED' in article titles and retraction notices.\n- Retrieved 1,203 results; 988 were retracted articles.\n- Selected top five articles cited over 20 times since 2015.\n- Inspected each citing document for mentions of retracted articles.\n- Categorized mentions as positive, negative, or neutral.\n\nSlide Topic: Case Study: Donmez et al. Article  \nSummary:  \n- Published in 2010 in Cell; retracted in 2014 due to data irregularities.  \n- Retraction due to graph and image misrepresentation, not result validity.  \n- Editors cited inadequate figure preparation standards.  \n- Citations mostly positive despite retraction.  \n- One negative mention in a Journal of Korean Medical Science letter.  \n- Highlighted concerns about altered graphics causing bias in biomedicine.  \n\nSlide Topic: Case Study: S\u00e9ralini Affair  \nSummary:\n- Published in 2012, the study sparked debate over its findings and methodology.\n- It involved a 2-year study on rats fed genetically modified crops, showing increased tumors.\n- The article was retracted due to the low number of animals used.\n- Post-retraction, citations were divided, with many negative ones highlighting media frenzy.\n- The study was republished in 2014, receiving mostly positive mentions.\n\nSlide Topic: Case Study: Mukherjee Article  \nSummary:  \n- Dipak Das and his lab were investigated for scientific misconduct.  \n- The article was retracted in 2014 due to data manipulation and falsification.  \n- Despite retraction, the article continued to be cited positively.  \n- Findings were used to support other studies.  \n- Popularity of 'authentic leadership' in management studies may explain positive citations.  \n\nSlide Topic: Case Study: Walumbwa Article  \nSummary:\n- The article, published in 1999, was retracted due to duplication.\n- Authors reused data from a version published two years earlier.\n- Despite retraction, the article is still cited positively.\n- Recent citation analysis shows it is often viewed as legitimate.\n- The case highlights issues in academic citation practices.\n\nSlide Topic: Discussion on Continuous Citations  \nSummary:  \n- Retracted articles are often cited years after retraction.  \n- Public or media interest can drive continuous citations.  \n- The S\u00e9ralini article sparked debate on GM foods.  \n- Mukherjee's article on resveratrol remains of public interest.  \n- Walumbwa's work on 'authentic leadership' is popular in management.  \n- Some articles, like Donmez's, are cited despite retraction reasons.  \n\nSlide Topic: Recommendations for Publishers  \nSummary:\n- Implement reference checks to identify citations of retracted articles.\n- Remove or request explanations for such citations from authors.\n- Ensure explanations appear in the published paper.\n- Delete retracted articles from publishers' websites.\n- Retain retraction notices but remove articles from platforms like ScienceDirect.\n- Prevent authors from downloading, reading, and citing retracted articles.",
    "**Slide Topic: Introduction to Word Embeddings and Their Analysis**\n\nSummary:\n- Word embeddings are vector representations of words used in NLP.\n- They are crucial for various NLP applications.\n- Analyzing embedding spaces helps understand their utility in tasks.\n- Nearest neighborhoods are often used to infer semantic information.\n- This method is limited by the unreliability of nearest neighborhoods.\n\nSlide Topic: Limitations of Neighborhood-Based Analysis  \nSummary:\n- Neighborhood-based analysis relies on unreliable nearest neighbors.\n- It evaluates semantic content, not feature space characteristics.\n- Nearest neighbors are used as proxies for semantic information.\n- Their instability across samples limits their effectiveness.\n- They do not always correlate with linguistic analyses.\n\nSlide Topic: Geometric Analysis of Word Embeddings  \nSummary:  \n- Geometric analysis provides insights into word embeddings' properties.  \n- It examines distribution and correlation with downstream performance.  \n- Neighborhood-based semantic characterizations are enhanced.  \n- Focuses on the continuous feature space of embeddings.  \n- Geometric features link to neural models' continuity assumptions.\n\nSlide Topic: Transformations of Word Embeddings  \nSummary:  \n- Analyze four components of word embeddings' geometry.  \n- Transform pretrained embeddings to highlight specific components.  \n- Apply transformations to Word2Vec, GloVe, and FastText embeddings.  \n- Evaluate transformed embeddings using standard performance tests.  \n- Findings indicate focus on local similarity in embedding development.  \n\nSlide Topic: Affine Transformations and Their Impact  \nSummary:\n- Affine transformations are used in post-processing word embeddings.\n- Artexe et al. align multilingual embedding spaces with matrix transforms.\n- Faruqui et al. apply linear sparsification for better lexical semantics.\n- These transformations alter point positions relative to the origin.\n- Unlike linear transformations, affine transformations do not fix the origin.\n- We explore the broader class of affine transformations for enhanced flexibility.\n\n**Slide Topic: Cosine Distance Encoding and Its Implications**\n\nSummary:\n- Cosine distance encoding transforms word vectors into a pairwise distance matrix.\n- This method obfuscates feature distribution, enhancing data privacy.\n- It addresses the non-interpretability of embedding features.\n- Enables comparison of embeddings based on relative organization.\n- Utilizes distances to embeddings of the 10K most frequent words.\n\n**Slide Topic: Nearest Neighbor Encoding and Graph Structures**\n\nSummary:\n- Nearest neighbor encoding discards most global pairwise distance information.\n- It retains only information about nearest neighborhoods.\n- The transformation outputs a sparse vector.\n- Nearest neighborhoods serve as a proxy for semantic information.\n- Combines outputs for each word vector into a sparse adjacency matrix.\n- This matrix describes a directed nearest neighbor graph.\n\n**Evaluation of Transformed Embeddings**\n\n- Assess contributions of geometric aspects to word embeddings' utility.\n- Evaluate transformed embeddings using intrinsic and extrinsic methods.\n- Intrinsic evaluations model linguistic information in vector space.\n- Extrinsic evaluations use embeddings as input for downstream models.\n- Intrinsic tests include word similarity and relatedness via cosine similarity.\n\nSlide Topic: Analysis and Discussion of Results  \nSummary:  \n- Affine transformations include rotations, dilations, and reflections.  \n- These transformations have minimal impact on intrinsic and extrinsic tasks.  \n- Models show robustness to simple linear transformations.  \n- Extrinsic evaluations are relatively insensitive to translations.  \n- Bias terms can model translations effectively.  \n- Intrinsic tasks are more sensitive to shifts due to reliance on cosine similarity.  \n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Traditional analysis of word embeddings focuses on qualitative aspects.  \n- Our study adopts a quantitative approach to examine geometric attributes.  \n- We assess the impact of geometric properties on task performance.  \n- Findings highlight the importance of local geometric structure.  \n- Future work should focus on tuning embeddings with improved sampling and evaluation methods.",
    "Slide Topic: Introduction to the Problem and Solution  \nSummary:  \n- Teaching machines to answer user-generated questions is a key NLP goal.  \n- Current methods can find relevant documents but struggle with answer extraction.  \n- Neural models show promise but need adaptation for longer texts.  \n- This paper proposes an improved method achieving state-of-the-art results.  \n- It introduces training for accurate per-paragraph confidence scores.  \n- Enhanced performance is achieved through this new approach.  \n\n**Slide Topic: Pipelined Method for Question Answering**\n\nSummary:\n- The system selects a single paragraph for a paragraph-level model.\n- For a single source, the paragraph with the smallest TF-IDF cosine distance to the question is chosen.\n- For multiple sources, a linear classifier using TF-IDF scores and other features is used.\n- This method improves answer selection accuracy.\n- Accuracy increased from 83.1% to 85.1% on TriviaQA web.\n\nSlide Topic: Handling Noisy Labels in Distant Supervision  \nSummary:\n- Noisy supervision can cause models to select incorrect answer spans.\n- A summed objective function optimizes the negative log-likelihood of correct spans.\n- This approach helps the model focus on the most relevant spans.\n- It enhances the model's ability to manage noise in distantly supervised setups.\n\nSlide Topic: Model Architecture and Design  \nSummary:\n- The model incorporates embedding and pre-processing with a bi-directional GRU.\n- It utilizes attention mechanisms, including self-attention.\n- Variational dropout is employed for regularization.\n- A linear layer with ReLU activations is used.\n- The attention mechanism creates a query-aware context representation.\n- The prediction layer computes answer start and end scores using softmax for probabilities.\n\n**Slide Topic: Confidence Method for Multi-Paragraph Settings**\n\nSummary:\n- The model uses un-normalized scores as confidence measures.\n- Errors arise when trained only on answer-containing paragraphs.\n- Poor confidence scores result from limited training data.\n- Shared-normalization approach ensures comparable scores.\n- This method improves performance across multiple paragraphs.\n\nSlide Topic: Experimental Setup and Datasets  \nSummary:  \n- Evaluated on datasets: TriviaQA unfiltered, TriviaQA wiki, TriviaQA web, and SQuAD.  \n- Preprocessing includes restructuring documents and marking answer spans.  \n- Sampling involves selecting paragraphs from context during training.  \n- Model trained with Adadelta optimizer.  \n- Utilizes GloVe word vectors.  \n- Specific dimensionalities set for GRUs and linear layers.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:  \n- Ablation study on TriviaQA web shows method effectiveness.  \n- Shared-norm approach outperforms others, especially with many paragraphs.  \n- Model significantly improves over prior work on TriviaQA web and wiki test sets.  \n- On SQuAD, shared-norm model adapts well to document-level tasks.  \n- Outperforms previous results on SQuAD.  \n\nSlide Topic: Discussion and Error Analysis  \nSummary:  \n- Models trained only on answer-containing paragraphs struggle in multi-paragraph settings.  \n- Shared-norm approach reduces distraction from irrelevant text.  \n- Error analysis on TriviaQA shows issues with insufficient evidence and complex syntax.  \n- Multi-sentence reading errors are also identified.  \n- Improvements should target sentence and paragraph-level comprehension.  \n- Handling document-level coreferences is crucial for better performance.  ",
    "Slide Topic: Introduction to Diversity in Paraphrasing  \nSummary:\n- Diversity in paraphrasing is crucial for NLP applications like data augmentation and conversational agents.\n- Traditional methods focus on semantic similarity, often leading to structurally similar sentences.\n- The goal is to generate diverse paraphrases without sacrificing quality.\n- A novel approach uses monotone submodular function maximization.\n- This method is effective for tasks such as intent classification and paraphrase recognition.\n\nSlide Topic: Monotone Submodular Function Maximization  \nSummary:  \n- Enhances paraphrasing by maintaining fidelity to the source.  \n- Offers flexibility in balancing diversity and fidelity.  \n- A simple greedy algorithm provides near-optimal solutions.  \n- Applicable in various real-world scenarios.  \n- Shares properties with convex functions, enabling operations like mixtures and truncation.  \n\nSlide Topic: Diverse Paraphraser using Submodularity (DiPS)  \nSummary:\n- Introduces DiPS, a tool for generating diverse paraphrases.\n- Maximizes a novel submodular objective for effective paraphrasing.\n- Ensures structural diversity without losing fidelity.\n- Demonstrates utility in data augmentation for tasks like intent and question classification.\n- Outperforms several diversity-inducing schemes in generating high-quality paraphrases.\n\nSlide Topic: Fidelity and Diversity in Paraphrase Generation  \nSummary:\n- Fidelity and diversity are crucial in paraphrase generation.\n- Fidelity is achieved by maximizing semantic similarity.\n- Monotone submodular functions ensure fidelity.\n- Diversity is promoted by rewarding distinct n-grams and variations.\n- A parameterized submodular objective balances fidelity and diversity.\n- This balance results in high-quality paraphrase generation.\n\nSlide Topic: Methodology and Implementation  \nSummary:\n- Paraphrase generation is treated as a sequence-to-sequence learning problem.\n- The framework combines a sentence encoder with a diversity-inducing decoder.\n- A modified decoder with a submodular objective replaces greedy decoding and standard beam search.\n- High-quality paraphrases are generated by optimizing the submodular objective.\n- The process involves selecting probable subsequences until the desired output length is achieved.\n\nSlide Topic: Experiments and Datasets  \nSummary:  \n- Experiments conducted on multiple datasets categorized by task.  \n- Datasets evaluate the method's effectiveness in generating diverse paraphrases.  \n- Baseline models compared for producing diverse responses in dialog systems.  \n- Aim to determine if DiPS generates diverse paraphrases without losing fidelity.  \n- Assess the usefulness of generated paraphrases in data augmentation.  \n\nSlide Topic: Intrinsic Evaluation and Results  \nSummary:  \n- Intrinsic evaluation assesses fidelity using metrics like BLEU, METEOR, and TER-Plus.  \n- Diversity is measured by calculating distinct n-grams.  \n- DiPS outperforms baseline models in fidelity metrics.  \n- It achieves substantial gains in diversity without compromising fidelity.  \n- Paraphrase recognition models are evaluated with augmented training data.  \n- High-quality paraphrases yield better results, demonstrating DiPS's effectiveness.  \n\nSlide Topic: Data Augmentation Applications  \nSummary:  \n- Data augmentation is used in intent and question classification tasks.  \n- DiPS offers high structural variations without losing fidelity.  \n- Outperforms other augmentation methods like SBS and DBS.  \n- Enhances accuracy scores in classification models.  \n- Highlights the importance of high-quality paraphrases.  \n- Proven effective through experiments on standard classifier models.  ",
    "Slide Topic: Introduction to Geolocation Prediction in Social Media  \nSummary:  \n- Social media is a key source for analyzing public opinions.  \n- Automated methods are essential due to the vast data volume.  \n- Geolocation prediction helps determine user or content location.  \n- Location data is vital for disaster, disease, and political analyses.  \n- It enhances sentiment and user attribute analyses regionally.  \n- Platforms like Twitter are favored for geolocation prediction.  \n\nSlide Topic: Proposed Neural Network Model for Geolocation Prediction  \nSummary:\n- Introduces a neural network model for geolocation prediction on Twitter.\n- Integrates text, metadata, and user network data using a complex neural network.\n- Utilizes an attention mechanism for unified data representation.\n- Outperforms previous ensemble methods on two open datasets.\n- Analyzes model components to understand the unification process.\n\nSlide Topic: Text-Based Approaches in Geolocation Prediction  \nSummary:  \n- Word probability distributions help estimate user geolocations.  \n- Maximum likelihood and language models minimize KL-divergence for location prediction.  \n- Topic modeling links latent topics with geographical regions using word distributions.  \n- Supervised machine learning methods use word features for geolocation prediction.  \n- Popular methods include Multinomial Naive Bayes, logistic regression, and neural networks.\n\nSlide Topic: User-Network-Based Approaches and Their Integration  \nSummary:\n- Social media interactions form user networks.\n- User network data is used to predict geolocation.\n- Facebook friendship data aids in location prediction.\n- Twitter friend and follower data enhance location algorithms.\n- Mention information in Twitter uses label propagation models.\n- Proposed model integrates user network data with text and metadata.\n\n**Slide Topic: Metadata-Based Approaches and Combinatory Methods**\n\nSummary:\n- Metadata, like location fields, effectively predicts geolocation.\n- Decent geolocation accuracy is achievable using location fields.\n- Combining metadata with text extends text-based approaches.\n- Combinatory methods include dynamically weighted ensemble, polygon stacking, stacking, and average pooling with neural networks.\n- These methods strengthen geolocation prediction.\n- User-network-based approaches are combined with others to enhance prediction accuracy.\n\n**Slide Topic: Evaluation and Comparison with Baseline Models**\n\nSummary:\n- The proposed model outperforms earlier models.\n- It is a neural network with similarities to previous models.\n- Key differences include integrating user network information.\n- Combines text and metadata using an attention mechanism.\n- Evaluation metrics: accuracy, accuracy@161, median error distance, mean error distance.\n- Significant improvements in accuracy and accuracy@161 over baseline models.\n\nSlide Topic: Analysis of Attention Mechanisms in the Proposed Model  \nSummary:\n- The model effectively unifies text, metadata, and user network representations.\n- Improvements are observed across four key metrics.\n- Analysis of two attention layers provides insights into the unification process.\n- Higher probabilities are assigned to timeline representations in Twitter US compared to W-NUT.\n- Strong preference for network representation in Twitter US due to more user network edges.\n- Clustering attention probabilities reveals typical patterns matching estimated probability density functions.",
    "**Slide Topic: Criticism of BLEU as an MT Evaluation Metric**\n\nSummary:\n- BLEU is a widely used MT evaluation metric but faces criticism.\n- Higher BLEU scores often correlate with better human judgments.\n- Notable failures occur when translations with the same BLEU score differ in quality.\n- Callison-Burch et al. (2006) found high correlation excluding an outlier system.\n- Including the outlier system resulted in poor correlation with human scores.\n- A low BLEU score does not always indicate poor human judgment.\n\nSlide Topic: Real-World Example of BLEU's Limitations  \nSummary:\n- WAT 2015 task showed high BLEU scores with poor manual evaluation.\n- Hypothesis translation scored 64.03 BLEU, higher than baseline's 43.29.\n- Human evaluators preferred the baseline translation.\n- Minor lexical differences led to large BLEU score disparities.\n- Example highlights BLEU's limitations in assessing translation quality.\n\n**Slide Topic: Technical Details of BLEU Calculation**\n\nSummary:\n- BLEU is a precision-based metric using n-gram precision.\n- It includes a brevity penalty to simulate recall.\n- BLEU score formula: BLEU = BP \u00d7 exp(\u03a3(w_n log p_n)).\n- BP represents the brevity penalty; p_n is n-gram precision.\n- Scores range from 0 to 1, with higher scores indicating better alignment with reference translations.\n\nSlide Topic: RIBES as an Alternative to BLEU  \nSummary:  \n- RIBES addresses BLEU's lack of reordering consideration.  \n- It uses a rank correlation coefficient to evaluate translations.  \n- RIBES penalizes wrongly ordered phrases more effectively than BLEU.  \n- In WAT 2015, RIBES failed to account for significant adequacy errors.  \n- A case showed a higher RIBES score but less accurate translation than baseline.\n\nSlide Topic: Other Automatic MT Evaluation Metrics  \nSummary:  \n- METEOR and ROUGE-S address BLEU's limitations.  \n- METEOR supports paraphrase and stem matches.  \n- ROUGE-S utilizes skip-grams for evaluation.  \n- These metrics enhance syntactic fluency and semantic adequacy.  \n- They still don't fully resolve BLEU's issues, like content omission and recall calculation.  \n\n**Slide Topic: Experimental Setup for WAT 2015 Shared Task**\n\nSummary:\n- Task: Translate Korean to Japanese patents using JPO Patent Corpus.\n- System: Utilized Moses toolkit with specific configurations.\n- Word Alignment: Employed MGIZA++.\n- Reordering Model: Used bi-directional lexicalized model.\n- Language Modeling: Implemented KenLM.\n- Tuning: Applied Minimum Error Rate Training (MERT).\n- Evaluation: Compared against baseline phrase-based MT system.\n\nSlide Topic: Human Evaluation Methodology  \nSummary:  \n- Human evaluations for WAT 2015 used the Lancers crowdsourcing platform.  \n- Evaluators compared hypothesis and baseline translations.  \n- Judgments determined which translation was better or if they were tied.  \n- HUMAN score ranged from -100 to +100 based on these judgments.  \n- Evaluators were non-experts, affecting judgment precision.  \n- Precision issues were notable in patent translations.  \n\nSlide Topic: Segment Level Meta-Evaluation and Disparities  \nSummary:  \n- Segment-level meta-evaluation revealed disparities in BLEU/RIBES vs. human judgments.  \n- High BLEU improvements often did not align with better human scores.  \n- Disparities arose from minor lexical differences and crowd-sourced vs. expert preferences.  \n- Metric differences failed to reflect major translation inadequacies.  \n- Findings highlight limitations of automatic metrics in capturing true translation quality.  ",
    "Slide Topic: Introduction to Controversial Content Detection  \nSummary:  \n- Controversial content attracts both positive and negative feedback.  \n- It can benefit community health by sparking debate.  \n- Detection helps users refine posts and alerts moderators to monitor discussions.  \n- Some platforms sort content by controversy, highlighting its importance.  \n- Controversiality is context-dependent, varying across communities.  \n- Community-specific detection methods are necessary due to this variability.  \n\nSlide Topic: Community-Specific Controversiality  \nSummary:\n- Controversial topics vary by community context.\n- Example: 'Crossfit' is controversial in the Fitness subreddit.\n- Moderators struggle to predict controversy due to unpredictable factors.\n- Early opinions can heavily influence discussion dynamics.\n- Community-specific factors are crucial in detecting controversial content.\n\nSlide Topic: Early-Detection Approach and Methodology  \nSummary:\n- Proposes an early-detection approach using initial post content and comment structure.\n- Integrates separate research streams for improved predictive performance.\n- Incorporates structural and textual features of early comment trees.\n- Adding features from the first 15 minutes of discussion boosts prediction accuracy.\n- Explores feature transferability across communities.\n- Text features are strong predictors within a community; structural features transfer better across communities.\n\nSlide Topic: Datasets and Data Collection  \nSummary:  \n- Dataset sourced from Reddit, covering 2007 to 2014.  \n- Focus on six text-based subreddits with high activity.  \n- Includes Q&A subreddits like AskMen and AskWomen.  \n- Covers special-interest and advice communities like Fitness and LifeProTips.  \n- Enables exploration of community-specific controversy.  \n- Tests early-detection approach across diverse communities.  \n\nSlide Topic: Assigning and Validating Controversy Labels  \nSummary:\n- Assign binary controversy labels based on upvote and downvote proportions.\n- Posts with high numbers of both upvotes and downvotes are labeled controversial.\n- Labeling method aligns with Reddit's sort-by-controversy function.\n- Achieves high precision in identifying controversial posts.\n- Validate labels by comparing with Reddit's controversial ranking.\n- Ensures method reflects community perceptions of controversy.\n\nSlide Topic: Analysis of Early Discussion Threads  \nSummary:  \n- Analyzed comments posted within the first hour to understand discussion dynamics.  \n- Examined comment text and tree structure for insights.  \n- Structural features of conversation trees predict controversy.  \n- Max-depth ratio and Wiener index are key features in various communities.  \n- Early discussion features offer insights beyond comment rate.  \n- Analysis helps predict potential post controversy.  \n\nSlide Topic: Early Prediction of Controversy  \nSummary:\n- Early prediction models use features available soon after a post is made.\n- Key features include comment text and tree structure.\n- Models outperform post-time baselines, showing the value of early discussion features.\n- Comment text is a major source of predictive gain.\n- Both post and comment text are crucial for controversy prediction.\n- Controversy prediction models learn distinct patterns from popularity prediction models.\n\n**Slide Topic: Domain Transfer and Feature Transferability**\n\nSummary:\n- Text features are strong predictors within a single community.\n- Text features are less effective across different communities.\n- Rate and structural features transfer better between communities.\n- Text features capture community-specific nuances.\n- Structural features provide more generalizable insights.\n- Findings suggest potential for robust cross-domain algorithms.",
    "Slide Topic: Introduction to Automatic Evaluation in Machine Translation  \nSummary:\n- Automatic evaluation is more practical than manual evaluation in MT.\n- Manual evaluation is costly and time-consuming.\n- Automatic metrics are essential for hyperparameter tuning and architecture search.\n- Metrics provide objective, reproducible results for cross-paper comparisons.\n- The WMT Metrics Shared Task validates and develops evaluation metrics annually.\n\n**Slide Topic: Task Setup and Language Pairs**\n\nSummary:\n- Provided one test set per language pair: source texts, MT outputs, and reference translation.\n- System-level metrics correlate with average human judgments of translation quality.\n- Participants chose language pairs and tracks freely.\n- Source, reference texts, and MT outputs from the News Translation Task.\n- Involved translations between English and Czech, German, Finnish, Gujarati, Kazakh, Lithuanian, Russian, and Chinese (15 pairs, excluding cs-en).\n\nSlide Topic: Manual Quality Assessment and Golden Truth  \nSummary:\n- Direct Assessment (DA) is used as the 'golden truth' for metric evaluation.\n- DA involves collecting numerous quality assessments for all MT system outputs.\n- Scores are standardized per annotator for consistency.\n- WMT19 scores are reference-based for some language pairs, reference-free for others.\n- System-level manual scores differ from segment-level scores.\n- Segment-level evaluation uses DA judgments converted to relative rankings due to feasibility issues.\n\nSlide Topic: Baseline Metrics and Their Implementation  \nSummary:  \n- Baseline metrics validate and compare popular metrics, preventing 'loss of knowledge.'  \n- Moses scorer aggregates several useful MT evaluation metrics over time.  \n- Reported baselines include BLEU, NIST (using mteval-v13a.pl), and TER, WER, PER, CDER (via Moses scorer).  \n- Baselines are used for system and segment-level evaluations.  \n- System-level metrics: BLEU, TER, WER, PER, CDER, sacreBLEU-BLEU, sacreBLEU-chrF.  \n- Segment-level metrics: sentBLEU; chrF is used for both levels.  \n\nSlide Topic: Submitted Metrics and Their Characteristics  \nSummary:\n- BEER combines sub-word features and global word order using a linear model.\n- BERTr uses contextual word embeddings for MT output comparison.\n- CharacTER is a character-level metric inspired by Translation Edit Rate (TER).\n- EED is a character-based metric building on CDER.\n- ESIM is a neural model adapted for MT evaluation from Natural Language Inference.\n- hLEPOR_baseline combines length penalty, precision, recall, and position difference penalty.\n\n**Slide Topic: System-Level Evaluation and Results**\n\nSummary:\n- Pearson correlation is used for system-level metric evaluation.\n- It measures correlation between human scores and predicted scores.\n- YiSi metrics achieve highest correlations across several language pairs.\n- YiSi metrics are not significantly outperformed by others in most cases.\n- New metric ESIM excels in 5 languages, winning 11 out of 16 language pairs.\n\nSlide Topic: Segment-Level Evaluation and Results  \nSummary:  \n- Segment-level evaluation uses manual judgments from the News Translation Task.  \n- Quality of metrics' scores is measured against daRR golden truth using a Kendall's Tau-like formula.  \n- YiSi-1_srl is the top performer for all language pairs it participated in.  \n- ESIM achieved 6 'winners' out of 18 language pairs.  \n- Both YiSi and ESIM are based on neural networks, confirming last year's trend.  \n\n- Human data collected from reference-based and reference-free evaluations this year.\n- Metric performance depends on underlying texts and systems in the News Translation Task.\n- Distinguishing outputs of top MT systems is challenging, even for human assessors.\n- Metrics correlate better with human judgments when systems vary widely in performance.\n- Weak correlation observed among top state-of-the-art systems across several language pairs.\n- Metrics like BERTr, ESIM, YiSi use more semantic features than n-gram/char-gram baselines.",
    "Slide Topic: Introduction to the Split-and-Rephrase Task  \nSummary:  \n- The task involves breaking complex sentences into simpler ones.  \n- Original meaning must be retained during the process.  \n- Introduced by Narayan et al. (2017) with a specific dataset and evaluation method.  \n- Achieved a BLEU score of 48.9 for text-to-text systems.  \n- Achieved a BLEU score of 78.7 for RDF-aware systems.  \n- The task is challenging due to the need to split and rephrase without losing information.  \n\nSlide Topic: Challenges in Processing Complex Sentences  \nSummary:  \n- Processing complex sentences is challenging for humans and NLP systems.  \n- Complexity impacts tasks like parsing and machine translation.  \n- An automatic system to simplify sentences is desirable.  \n- Current models often memorize rather than understand content.\n\nSlide Topic: Evaluation of SEQ2SEQ Models  \nSummary:\n- Vanilla SEQ2SEQ models with attention excel in the Split-and-Rephrase task.\n- Achieved a BLEU score of 77.5, surpassing previous text-to-text baselines.\n- Models often generate unsupported facts and repeat information.\n- Tend to omit details, indicating memorization over learning.\n- Highlight the need for improved learning strategies in these models.\n\nSlide Topic: Issues with Current Models and Data Splits  \nSummary:  \n- Current models, especially SEQ2SEQ, memorize entity-fact pairs.  \n- Overlap in training and test data leads to memorization.  \n- High BLEU scores are achieved but generalization is poor.  \n- Original data split allows 'cheating' through memorization.  \n- 99% of simple sentences in validation and test sets appear in training set.  \n\nSlide Topic: Proposed New Data Split for Better Generalization  \nSummary:  \n- Introduce a new data split to enhance generalization.  \n- Ensure RDF triplets are unique to each split.  \n- Prevent models from memorizing facts.  \n- Encourage learning to identify and rephrase relations.  \n- Maintain the original 80%/10%/10% data division.  \n- Reduce simple sentence overlap.  \n\nSlide Topic: Copy-Augmented SEQ2SEQ Models  \nSummary:  \n- Introduces a copy mechanism to enhance SEQ2SEQ models.  \n- Improves handling of the Split-and-Rephrase task.  \n- Allows models to copy relevant parts of the input sentence.  \n- Shows improved performance in reducing unsupported facts.  \n- Decreases repeated facts in model outputs.  \n\nSlide Topic: Experimental Results and Analysis  \nSummary:  \n- Copy-augmented models outperform vanilla SEQ2SEQ models.  \n- Performance is notably better on the new data split.  \n- These models distribute attention more evenly across input symbols.  \n- They reduce unsupported and repeated facts.  \n- Challenges remain with model performance on the new split.  \n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- SEQ2SEQ models achieve high scores but struggle with the Split-and-Rephrase task.  \n- New data split and copy-augmented models offer a more robust baseline.  \n- The task remains unsolved despite improvements.  \n- Future research should focus on the new data split.  \n- Updated dataset version 1.0 is crucial for addressing challenges.  ",
    "Slide Topic: Introduction to Neural Network Mappings in Cross-Modal Retrieval  \nSummary:\n- Neural network mappings bridge different modalities in cross-modal retrieval.\n- They are used in zero-shot learning and building multimodal representations.\n- Networks are trained to predict vectors of one modality from another.\n- Retrieval and labeling are based on nearest neighbors of predicted vectors.\n- Success depends on making predicted vectors similar to target vectors.\n- The effectiveness of these mappings is not fully explored.\n\nSlide Topic: Challenges in Achieving Semantic Similarity in Neural Network Mappings  \nSummary:\n- Recent studies show input modality information often influences predicted modality.\n- Predicted vectors in cross-modal mappings retain significant input vector information.\n- Language-to-vision mappings can outperform original visual vectors in concept similarity.\n- Mapped vectors often resemble input vectors more than target vectors semantically.\n- This challenges the goal of achieving true cross-modal semantic similarity.\n\nSlide Topic: Proposed Measure for Quantifying Neighborhood Structure Similarity  \nSummary:\n- Introduce the mean nearest neighbor overlap measure (mNNO K) for comparing neighborhood structures.\n- mNNO K quantifies semantic similarity by calculating nearest neighbor overlap.\n- The normalizing constant K scales mNNO K between 0 and 1.\n- This scaling makes mNNO K independent of the choice of K.\n- The measure implicitly performs retrieval for similarity assessment.\n- Provides a realistic estimate of semantic similarity.\n\nSlide Topic: Experimental Setup and Datasets for Cross-Modal Tasks  \nSummary:\n- Diverse cross-modal tasks are tested at word, sentence, and document levels.\n- Datasets used include ImageNet, IAPR TC-12, and Wiki.\n- Various image and text features ensure results are feature-independent.\n- Experiments cover both text-to-image and image-to-text directions.\n- Extracted features are publicly available for further research.\n\nSlide Topic: Results and Analysis of Neighborhood Structure in Mapped Vectors  \nSummary:  \n- Mapped vectors' semantic structure resembles input vectors more than target vectors.  \n- This pattern is consistent across various mappings, embeddings, and directions.  \n- Test mean nearest neighbor overlap supports this resemblance to input vectors.  \n- Highlights a previously neglected phenomenon in cross-modal mappings.\n\n**Slide Topic: Impact of Untrained Networks on Semantic Structure Preservation**\n\nSummary:\n- Untrained linear and neural net mappings preserve input semantic structure.\n- This complements findings from trained mappings.\n- Neural nets naturally tend to preserve semantic information.\n- Preservation occurs regardless of target vectors and loss function.\n- Results show semantic structure is maintained even without training.\n\n**Slide Topic: Conclusions and Future Directions for Cross-Modal Neural Network Research**\n\nSummary:\n- Neural net cross-modal mappings resemble input vectors more than target vectors semantically.\n- A new measure quantifies similarity between neighborhood structures of vector sets.\n- The study highlights the need for improved architectures to bridge modalities.\n- Advocates for using semantic-based criteria to evaluate predicted vector quality.\n- Future research should focus on enhancing cross-modal mapping accuracy.",
    "Slide Topic: Introduction to NLP Pipelines and Neural Architectures  \nSummary:\n- NLP learning is now dominated by end-to-end differentiable functions.\n- Traditional NLP used modular stages forming a processing pipeline.\n- Pipelines involved tokenization, tagging, parsing, and semantic analysis.\n- Discrete decisions in pipelines seem incompatible with neural learning.\n- Some researchers find value in linguistic preprocessing.\n- Pipelines can be integrated as layers in neural architectures for NLP tasks.\n\nSlide Topic: Challenges with Discrete Decisions in Neural Networks  \nSummary:\n- Solutions include reinforcement learning (e.g., REINFORCE) and structured attention.\n- These methods replace argmax with sampling or marginalization.\n- Downsides: not all argmax operations have efficient sampling/marginalization.\n- Hard decisions allow easier inspection for error analysis than posteriors.\n- Straight-through estimator (STE) treats discrete decisions as differentiable.\n- STE is fast and effective but ignores constraints like unique syntactic parents.\n\n**Slide Topic: Structured Projection of Intermediate Gradients Optimization Technique (SPIGOT)**\n\nSummary:\n- Introduces SPIGOT, a new optimization method.\n- Defines a gradient proxy for loss function input to argmax.\n- Respects constraints in the argmax problem, unlike STE.\n- Applicable to any intermediate layer as a constrained maximization problem.\n- Feasible set must be projectable.\n- Empirical results show effectiveness with approximate maximization and projection.\n\nSlide Topic: Application of SPIGOT in NLP Architectures  \nSummary:\n- Introduce two architectures using structured argmax as an intermediate layer.\n- First architecture: semantic parsing with syntactic parsing in the middle.\n- Second architecture: sentiment analysis with semantic parsing in the middle.\n- Both architectures are trained with a joint objective.\n- Training uses separate datasets for intermediate and end tasks.\n- Intermediate task parameters are influenced by both datasets.\n\nSlide Topic: Empirical Evaluation of SPIGOT  \nSummary:\n- SPIGOT improves over state-of-the-art semantic dependency parsers.\n- It outperforms pipeline, SA, and STE in performance.\n- Semantic parsing enhances sentiment classification, especially with SPIGOT.\n- SPIGOT shows better results than BiLSTM and alternatives.\n- Table 1 shows SPIGOT surpasses STE on DM by over 0.3% in labeled F1, both in-domain and out-of-domain.\n\nSlide Topic: Analysis of Intermediate Model Behavior  \nSummary:\n- Examines impact of end-task training on intermediate model predictions.\n- Investigates if end-task signals can override intermediate predictions.\n- Uses syntactic-then-semantic parsing model as a case study.\n- Compares pipelined system with jointly trained system using SPIGOT.\n- Analyzes development set instances with both syntactic and semantic annotations.\n- Partitions instances based on agreement (SAME) or disagreement (DIFF) in syntactic predictions.\n\nSlide Topic: Related Work in Joint Learning and Differentiable Optimization  \nSummary:\n- Joint learning in NLP aims to reduce cascading errors.\n- Joint inference can be costly in terms of resources.\n- Advances in representation learning enable parameter sharing across tasks.\n- Differentiable optimization reviewed by Gould et al. focuses on bi-level optimization.\n- Amos and Kolter extend this to subdifferentiable quadratic programs.\n- Both approaches require an invertible Hessian, limiting NLP applications.\n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Introduced SPIGOT for backpropagation in neural networks with discrete decisions.  \n- SPIGOT uses a proxy for gradients via a constraint-respecting projection.  \n- Evaluated on two architectures: semantic parser and sentiment classifier.  \n- Demonstrated superior performance over baselines in both settings.  \n- Outperformed state-of-the-art systems in semantic dependency parsing.",
    "Slide Topic: Introduction to Discourse Phenomena in Translation  \nSummary:\n- Discourse phenomena are crucial for coherence and resolving ambiguities in translation.\n- Traditional machine translation often ignores these, translating sentences in isolation.\n- Earlier research focused on specific issues like pronoun translation and discourse connectives.\n- Neural Machine Translation (NMT) can incorporate extended context, improving performance.\n- Identifying which discourse phenomena NMT handles effectively remains challenging.\n\nSlide Topic: Context-Aware Neural Machine Translation  \nSummary:\n- Develops a discourse-aware model to enhance NMT understanding.\n- Utilizes Transformer architecture for independent encoding of source and context sentences.\n- Employs a single attention layer for context-aware representation.\n- Outperforms simple sentence concatenation by capturing contextual information.\n- Shows improvements over discourse-agnostic baselines, especially in pronoun translation.\n\nSlide Topic: Transformer Architecture and Context Integration  \nSummary:  \n- Transformer architecture excels in NMT with high parallelizability and training efficiency.  \n- It utilizes stacked self-attention and fully connected layers.  \n- Our model modifies the Transformer's encoder to incorporate context information.  \n- Parameters are shared between source and context encoders.  \n- Context integration is crucial for handling discourse phenomena in translation tasks.  \n\nSlide Topic: Experiments and Data Setting  \nSummary:  \n- Utilized OpenSubtitles 2018 corpus for English and Russian.  \n- Selected 2 million training instances with subsets for development and testing.  \n- Sentences encoded using byte-pair encoding with ~32,000 tokens.  \n- Aimed to verify improvements from context integration.  \n- Evaluated performance using BLEU scores.  \n\nSlide Topic: Results and Analysis of Contextual Information  \nSummary:  \n- Context-aware model shows a 0.7 BLEU improvement over discourse-agnostic model.  \n- Improvement driven by discourse phenomena, not topic effects.  \n- Performance drops when real context is replaced with random sentences.  \n- Model uses context to translate ambiguous pronouns.  \n- Indicates implicit learning of coreference phenomena.  \n\nSlide Topic: Analysis of Pronoun Translation  \nSummary:  \n- The model relies on contextual history for translating ambiguous pronouns.  \n- Pronouns like 'it', 'yours', 'you', and 'I' are challenging in Russian.  \n- BLEU scores improve with co-referential pronouns in test sets.  \n- Improvements are notable when antecedents are nouns.  \n- The model effectively uses context to resolve pronoun ambiguities.  \n- This enhances overall translation quality.  \n\nSlide Topic: Latent Anaphora Resolution  \nSummary:  \n- The model's attention mechanism performs latent anaphora resolution.  \n- Attention weights align well with coreference systems.  \n- Captures anaphora without explicit feature engineering.  \n- Human evaluation confirms effectiveness in resolving anaphora.  \n- Outperforms simple heuristics in anaphora resolution.  \n\nSlide Topic: Related Work and Comparison  \nSummary:  \n- Previous work used external coreference systems with mixed results.  \n- Our context-aware NMT model learns coreference without feature engineering.  \n- Other models lack increased attention between pronouns and antecedents.  \n- Our approach effectively captures discourse phenomena.\n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Introduced a context-aware NMT system using Transformer architecture.  \n- Outperformed baselines in handling ambiguous pronouns and anaphora relations.  \n- Future work to explore specialized features in the attention model.  \n- Aim to further improve anaphora handling.  \n- Investigate other discourse phenomena like elliptical constructions and discourse relations.",
    "Slide Topic: Introduction to Weblio Pre-reordering SMT System  \nSummary:\n- Describes the Weblio Pre-reordering Statistical Machine Translation (SMT) System.\n- Utilizes the pre-reordering method from Zhu et al., 2014.\n- Automatically learns pre-reordering models from word alignments and parse trees.\n- Integrates a statistical language model to reorder nodes in parse trees.\n- Applied primarily to English-Japanese translation in WAT 2014.\n\nSlide Topic: Head-restructured CFG Parse Tree for Reordering  \nSummary:\n- Reordering SVO to SOV requires handling long-distance dependencies.\n- Zhu et al. (2014) propose a solution using CFG parse trees.\n- Sentence-level dependencies are injected into these trees.\n- The resulting structure is called a 'Head-restructured CFG parse tree' (HRCFG tree).\n- HRCFG trees explicitly show subject, object, and verb parts.\n- Figure 1 illustrates an example of an HRCFG tree.\n\nSlide Topic: Integration of Language Model in Reordering  \nSummary:  \n- Our reordering model is based on Zhu et al. (2014).  \n- A language model helps determine the optimal node order for the target language.  \n- Bilingual training data and GIZA++ word alignments are used.  \n- We identify the best order for each node layer in parse trees.  \n- Example: 'nsubj dobj hits' sequence scores highest, aligning with Japanese order.  \n\nSlide Topic: N-best Reordering and Translation Hypotheses  \nSummary:\n- The best sentence order uses 1-best orders from each parse tree node layer.\n- N-best reordering is crucial for optimal translation results.\n- Our MT system collects N-best reordering results for all node layers.\n- Results are ranked by accumulated language model scores.\n- Table 1 illustrates two reordering results for \"The rocket is launched by NASA.\"\n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Baseline system uses 1-best parse trees for training and testing.  \n- Experiments conducted with N-best parse trees and reordering results.  \n- Table 2 lists automatic evaluation scores for different system settings.  \n- N-best approach improved BLEU and RIBES metrics.  \n- System 4 excelled in automatic evaluations but underperformed in human evaluation compared to System 3.  \n\nSlide Topic: Evaluation of Pre-reordering Performance  \nSummary:\n- Evaluated pre-reordering performance using Kendall's \u03c4.\n- Method based on Isozaki et al. (2010) for word alignments.\n- Analyzed 1.5M sentences from the ASPEC corpus.\n- Average Kendall's \u03c4: 0.30 (natural order), 0.71 (adjusted order).\n- Figure 5 illustrates improvements in word order.\n\nSlide Topic: Error Analysis and Challenges in Reordering  \nSummary:\n- Pre-reordering SMT system improves translation over baseline SMT systems.\n- Reordering model still has defects affecting translation quality.\n- Model is sparse due to sequences mixed with nonterminal tags and words.\n- Struggles with reordering sentences with rare or unknown words.\n- Table 4 illustrates reordering errors in the ASPEC test corpus.\n\nSlide Topic: Context-aware Machine Translation Efforts  \nSummary:\n- Utilizes context information for improved translation.\n- Focuses on English-Japanese phrase selection challenges.\n- English words often have multiple Japanese translations.\n- Figure 6 illustrates translation distribution across domains.\n- Experiments interpolate general and in-domain language models.\n- Figure 7 shows interpolated models achieve optimal perplexity.",
    "Slide Topic: Introduction to Semantic Parsing and QA-SRL  \nSummary:  \n- Semantic parsing predicts predicate-argument structures in sentences.  \n- Training typically requires hard-to-gather datasets.  \n- Recent research explores non-expert semantic supervision.  \n- Crowdsourcing can create large-scale, cost-effective datasets.  \n- QA-SRL is intuitive for non-experts and useful for tasks like Open IE.  \n\nSlide Topic: Crowdsourcing Approach for QA-SRL Annotation  \nSummary:\n- Introduced a scalable crowdsourcing method for QA-SRL.\n- Developed a streamlined web interface with auto-suggest and quality control.\n- Implemented a validation stage to ensure high precision.\n- Created QA-SRL Bank 2.0 with 133,479 verbs from 64,018 sentences.\n- Achieved 265,140 question-answer pairs in 9 days.\n- Data shows high precision and good recall, though not exhaustive.\n\nSlide Topic: Data Annotation and Quality Control  \nSummary:  \n- QA-SRL annotation involves creating question-answer pairs for each verb in a sentence.  \n- Answers are contiguous spans from the sentence.  \n- A crowdsourcing pipeline is used for rapid, cost-effective, large-scale annotation.  \n- The pipeline includes a generation step and a validation step.  \n- In the generation step, a worker creates QA-SRL questions for a marked verb.  \n- In the validation step, multiple workers answer or validate each question.  \n\n**Model Architecture and Span Detection**\n\n- The parser constructs tuples in a three-step pipeline.\n- Step 1: Identify verbal predicates.\n- Step 2: Use unlabeled span detection to select argument spans for each verb.\n- Step 3: Generate a question for each span.\n- Two models for span detection: sequence-tagging with BIO encoding and span-based model.\n- Span-based model outperforms BIO model in precision and recall.\n\nSlide Topic: Question Generation Techniques  \nSummary:\n- Introduce two question generation models.\n- Models generate questions from a span representation.\n- Local model predicts words for each slot independently.\n- Sequence model uses RNN to share information between slots.\n- Sequence model has higher exact match accuracy.\n- Word-level accuracy is comparable between models.\n\nSlide Topic: Data Expansion and Bootstrapping  \nSummary:  \n- Our trained parser generates full QA-SRL annotations.  \n- Predictions are validated using the original annotation process.  \n- Focused annotation efforts help fill potential data gaps.  \n- Low probability cutoffs detect spans, over-generating QA pairs.  \n- Expansion increased valid questions in train/dev partitions by 20%.  \n\nSlide Topic: Evaluation Metrics and Results  \nSummary:  \n- Span detection evaluated using modified precision and recall.  \n- Predicted spans are correct if they match any labeled spans.  \n- Span-based model outperforms BIO model in precision and recall.  \n- Sequential model shows higher exact match accuracy in question generation.  \n- Human evaluation confirms sequence-based models are more accurate than local models.  \n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Demonstrated scalability of QA-SRL to large datasets.  \n- Introduced a new methodology for large-scale predicate-argument labeling.  \n- Developed a scalable crowdsourcing approach for QA-SRL.  \n- Collected QA-SRL Bank 2.0 with over 250,000 QA pairs from 64,000 sentences in 9 days.  \n- Trained the first parser to produce high-quality QA-SRL structures.  ",
    "Slide Topic: Introduction to Human-Computer Conversation  \nSummary:\n- Human-computer conversation is a key challenge in AI and NLP.\n- Two main research areas: task-oriented and general-purpose dialog.\n- Task-oriented dialog assists in completing specific tasks like shopping.\n- General-purpose dialog focuses on natural conversations across topics.\n- Recent focus on general-purpose dialog for developing AI systems.\n- Generative models from social data are widely used for this purpose.\n\nSlide Topic: Challenges in General Purpose Dialog  \nSummary:\n- General-purpose dialog differs from machine translation.\n- Machine translation has a 1-to-1 semantic relationship.\n- Dialog responses can match many input utterances.\n- Seq2Seq models favor frequent, general responses.\n- These responses are safe but often boring and trivial.\n- They may lead to a quick end of conversation.\n\nSlide Topic: Existing Solutions and Their Limitations  \nSummary:  \n- Li et al. (2016a) used Maximum Mutual Information to reduce general responses.  \n- Xing et al. (2017) guided Seq2Seq models with pre-defined topics from an external corpus.  \n- Ensuring topic consistency between external and conversation corpora is challenging.  \n- This inconsistency can introduce additional noise into the system.  \n- Latent responding factors model multiple mechanisms but are hard to interpret.  \n- Deciding the number of latent factors remains difficult.  \n\nSlide Topic: Proposed Specificity Controlled Seq2Seq Model  \nSummary:  \n- Introduce a novel controlled response generation mechanism.  \n- Inspired by everyday human conversation.  \n- Utilize a Seq2Seq framework with a specificity control variable.  \n- The control variable interacts with word usage via a Gaussian Kernel layer.  \n- Guides the model to generate responses at varying specificity levels.  \n- Named the Specificity Controlled Seq2Seq model (SC-Seq2Seq).  \n\nSlide Topic: Model Architecture and Mechanisms  \nSummary:  \n- The SC-Seq2Seq architecture is illustrated in Figure 2.  \n- The encoder uses a bi-directional GRU to map input into a compact vector capturing essential topics.  \n- The decoder generates responses based on hidden representations and a control variable, s.  \n- Word generation probability combines semantic and specificity-based probabilities.  \n- A Gaussian Kernel layer defines specificity-based generation probability, interacting with word usage representations.  \n\nSlide Topic: Distant Supervision Techniques  \nSummary:  \n- Distant supervision is used to train the SC-Seq2Seq model.  \n- Specificity control variable is unknown in raw data.  \n- Two methods to acquire distant labels: NIRF and NIWF.  \n- Normalized values restrict specificity control to a defined range.  \n- Each range end has a clear meaning for specificity.  \n\nSlide Topic: Experimental Setup and Evaluation  \nSummary:  \n- Conducted experiments on the public Short Text Conversation (STC) dataset.  \n- Compared the SC-Seq2Seq model against state-of-the-art baselines.  \n- Used automatic and human evaluations, including distinct-1 & distinct-2, BLEU, Average & Extrema.  \n- Model trained on a Tesla K80 GPU card; results reported on test dataset.  \n- SC-Seq2Seq with NIWF generates responses from general to specific, as measured by distinct metrics.  \n\nSlide Topic: Analysis and Case Studies  \nSummary:  \n- Conducted case studies to evaluate model performance.  \n- Baseline models often produce general and short responses.  \n- SC-Seq2Seq NIWF with high control values generates long, specific responses.  \n- Lowering control values results in more general, shorter responses.  \n- Analyzed word usage representations in the model.  \n- Found significant differences between usage and semantic embeddings.  ",
    "Slide Topic: Introduction to Neural Machine Translation and Coverage Issues  \nSummary:\n- Neural Machine Translation (NMT) offers more fluent translations than previous systems.\n- Adequacy issues persist, such as dropping and repeating words.\n- Solutions include coverage and length penalties during beam search.\n- Coverage vectors track attention history to improve translation.\n- Gating architectures, adaptive attention, and reconstruction loss are explored.\n- Challenges remain, driving the search for new solutions.\n\nSlide Topic: Proposed Solution: Constrained Sparsemax Transformation  \nSummary:\n- Introduces a novel attention transformation without altering architecture.\n- Combines attention sparsity with bounding attention a word can receive.\n- Transformation is end-to-end differentiable for gradient backpropagation.\n- Formulates constrained sparsemax and derives efficient algorithms.\n- Provides detailed empirical comparison of various attention transformations.\n\nSlide Topic: Sparse and Constrained Attention Mechanisms  \nSummary:\n- Traditional softmax attention can be inefficient and repetitive.\n- Sparsemax projects scores onto the probability simplex for sparse distributions.\n- Constrained softmax uses fertility values to bound attention probabilities.\n- This leads to sparsity over time steps.\n- Constrained sparsemax combines these features for sparse, bounded probabilities.\n- Enhances interpretability and efficiency of attention mechanisms.\n\nSlide Topic: Fertility Bounds and Exhaustion Strategies  \nSummary:\n- Explore strategies for setting fertility bounds: CONSTANT, GUIDED, and PREDICTED.\n- GUIDED strategy uses a word aligner based on observed values.\n- PREDICTED strategy relies on model predictions.\n- Implement an exhaustion strategy to prevent missing source words.\n- Adjust pre-attention scores to favor words with remaining credit.\n- Ensure comprehensive coverage of source words.\n\n**Slide Topic: Experimental Setup and Evaluation Metrics**\n\nSummary:\n- Focus on small datasets prone to coverage mistakes.\n- Evaluate attention transformations on language pairs: DE-EN, JA-EN, RO-EN.\n- Use datasets: IWSLT 2014, KFTT, WMT 2016.\n- Evaluation metrics: tokenized BLEU, METEOR, REP-score, DROP-score.\n- Implementation based on OpenNMT-py toolkit.\n- Compare methods against baseline models with softmax attention and coverage models.\n\nSlide Topic: Results and Analysis of Attention Transformations  \nSummary:  \n- Sparse models outperform csoftmax and softmax on BLEU and METEOR scores.  \n- They also achieve better REP and DROP scores, addressing coverage issues.  \n- Csparsemax effectively reduces repetitions with sparse alignments.  \n- Attention maps and translation examples demonstrate these improvements.\n\nSlide Topic: Comparison of Fertility Strategies  \nSummary:  \n- Predicted fertility strategy outperforms Constant and Guided strategies.  \n- Achieves higher BLEU and METEOR scores in DE-EN language pair.  \n- Predicting fertilities based on model insights enhances translation accuracy.  \n- Highlights the importance of selecting appropriate fertility strategies.  \n- Optimizes translation quality and coverage.\n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- Proposed a new approach to address the coverage problem in NMT.  \n- Replaced softmax attentional transformation with sparse, constrained alternatives.  \n- Introduced constrained sparsemax transformation with efficient implementation algorithms.  \n- Improved translation quality by avoiding repeated words and providing sparse alignments.  \n- Future work includes exploring fertility prediction models.  \n- Plan to extend the approach to larger datasets and diverse language pairs.  ",
    "Slide Topic: Introduction to Multilingual Corpora and Comparability  \nSummary:\n- Multilingual corpora are vital for various NLP tasks.\n- Initially used for extracting parallel lexicons.\n- Later supported statistical machine translation (SMT) systems.\n- SMT systems need bilingual texts for statistical models.\n- Domain-specific corpora are crucial but scarce.\n- Wikipedia offers valuable comparable and parallel corpora.\n\nSlide Topic: Challenges in Extracting Comparable Corpora from Wikipedia  \nSummary:\n- Wikipedia's taxonomy is dense and chaotic, complicating corpus extraction.\n- Arbitrary category links lead to over-categorization and cycles.\n- Defining domains accurately is challenging due to this complexity.\n- Heuristic strategies are needed, like using characteristic vocabulary for queries.\n- Exploring category graphs from a root category can help mitigate issues.\n- Despite challenges, interlanguage links and multilinguality offer great potential.\n\nSlide Topic: Domain-Specific Comparable Corpora Extraction Methodology  \nSummary:\n- Start from a top category of interest on Wikipedia.\n- Define domain vocabulary by collecting and preprocessing articles.\n- Control graph exploration with constraints to prevent loops.\n- Score categories based on domain likelihood using vocabulary terms.\n- Tested on English and Spanish Wikipedia in domains like Computer Science, Science, and Sports.\n- Resulted in domain vocabularies of 100-400 lemmas.\n\nSlide Topic: Parallel Sentence Extraction Techniques  \nSummary:\n- Estimate similarity between cross-language sentence pairs.\n- Use character n-grams, pseudo-cognates, and translated word 1-grams.\n- Set thresholds to extract parallel sentences, resulting in a noisy corpus.\n- Quality of similarity measures affects corpus purity.\n- SMT systems can handle some noise in word alignment.\n- Character 3-grams and monolingual translations perform well, with length factor measures enhancing F1 scores.\n\nSlide Topic: Evaluation of Extracted Corpora in Statistical Machine Translation  \nSummary:\n- Evaluated corpora based on their impact on SMT performance.\n- Wikipedia-derived corpora improved BLEU scores significantly over Europarl.\n- Best results achieved by combining different similarity measures.\n- BLEU scores doubled in some cases with this approach.\n- Even noisy corpora outperformed general-purpose ones in domain-specific tasks.\n- Confirmed the utility of Wikipedia-derived corpora for enhancing translation quality.\n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Developed a model for extracting in-domain comparable corpora from Wikipedia.  \n- Enhanced machine translation quality for domain-specific tasks using parallel sentences.  \n- Model applicable to various language pairs and domains with corresponding Wikipedia editions.  \n- Future work includes improving parallel sentence identification and evaluation methods.  \n- Prototype operational in multiple languages, showcasing its versatility.",
    "Slide Topic: Introduction to Bilingual Lexicon Induction  \nSummary:\n- New methods for bilingual lexicon induction target low-resource language pairs.\n- These methods minimize the need for expensive parallel data.\n- A common approach uses multilingual word embeddings in a shared latent space.\n- Many methods still rely on a large seed dictionary for cross-lingual signals.\n- Recent research aims to reduce dependency on large seed dictionaries.\n\nSlide Topic: Challenges in Low-Resource Language Pairs  \nSummary:  \n- Recent advances in unsupervised bilingual lexicon induction show promise.  \n- Current methods do not utilize linguistic features of the languages.  \n- Earlier methods leveraged features like edit distance and orthographic similarity.  \n- Incorporating linguistic features improved performance in related languages.  \n- Modern methods have yet to apply these beneficial linguistic features.  \n\nSlide Topic: Modern Embedding-Based Approaches  \nSummary:\n- Extends Artetxe et al. (2017) with orthographic information.\n- Leverages language similarities for better bilingual lexicon induction.\n- Uses a small, auto-generated seed lexicon of identical numerals.\n- Employs an iterative self-learning framework for linear mapping.\n- Reduces need for large seed dictionaries through self-learning.\n\nSlide Topic: Orthographic Extension of Word Embeddings  \nSummary:\n- Augments embeddings for words in both languages.\n- Appends a vector to each word's embedding.\n- Vector length equals the union of both languages' alphabets.\n- Each position in the vector represents a letter count in the word.\n- Resulting vector is normalized to magnitude 1.\n\nSlide Topic: Orthographic Similarity Adjustment  \nSummary:  \n- Adjusts similarity scores for word pairs in dictionary induction.  \n- Based on the self-learning framework by Artetxe et al. (2017).  \n- Original similarity uses dot product of word embeddings.  \n- Adds orthographic similarity using normalized string edit distance.  \n- Normalized edit distance is Levenshtein distance divided by longer word length.  \n\nSlide Topic: Experimental Setup and Datasets  \nSummary:  \n- Utilized datasets from Artetxe et al. (2017) with three language pairs: English-Italian, English-German, and English-Finnish.  \n- Each dataset includes monolingual word embeddings for both languages.  \n- Contains a bilingual dictionary divided into training and test sets.  \n- Training set is not used as the input dictionary for the system.  \n- An automatically-generated dictionary with numeral identity translations is used instead.  \n\nSlide Topic: Results and Discussion  \nSummary:  \n- Word embeddings were extended by the union size of both languages' alphabets.  \n- The heuristic identified about 2 million word pairs per language pair from 40 billion possibilities.  \n- This approach resulted in significant computational savings.  \n- Table 1 compares our methods with Artetxe et al. (2017) using selected scaling factors.  \n- The extension method learns mappings between source and target letters.  \n- This capability may explain its improved performance.  \n\n**Conclusion and Future Work**\n\n- Presented two techniques for embedding-based bilingual lexicon induction.\n- Methods utilize orthographic information without parallel data.\n- Achieved increased accuracy for low-resource language pairs.\n- Both combined and embedding extension methods outperformed baseline.\n- Future work includes applying methods to languages with different alphabets.\n- Plan to extend to other unsupervised bilingual lexicon systems.",
    "Slide Topic: Introduction to the Scientific Publication Lifecycle  \nSummary:\n- The lifecycle includes literature review, research work, and result dissemination.\n- Activities are shared through conferences and journals.\n- Each stage involves specific sub-activities requiring expertise.\n- Inexperienced researchers often face challenges.\n- Assistance from supervisors, experts, and librarians is crucial for skill development.\n\n**Slide Topic: Challenges Faced by Novice Researchers**\n\nSummary:\n- Novice researchers often struggle due to lack of experience.\n- They need help selecting appropriate information sources.\n- Formulating effective search queries is a common challenge.\n- Difficulty in browsing and evaluating search results is prevalent.\n- These issues highlight the expertise gap with experienced researchers.\n\nSlide Topic: Existing Academic Assistive Systems  \nSummary:  \n- Academic assistive systems bridge the expertise gap between experts and novices.  \n- They include search systems with faceted user interfaces.  \n- Bibliometric tools visualize citation networks.  \n- Scientific paper recommender systems are available.  \n- Techniques recommend articles for citation contexts in manuscripts.  \n- Systems suggest prospective conference venues for manuscript publication.  \n\nSlide Topic: Unexplored Areas in Manuscript Preparation  \nSummary:  \n- Identifying key papers for citation is an unexplored area in manuscript preparation.  \n- Citation choices depend on research type and article type.  \n- Article type imposes constraints like recency and quantity on citations.  \n- Our research helps by shortlisting papers from researchers' reading lists.  \n- Shortlisting is tailored to article-type preferences.\n\nSlide Topic: Overview of the Rec4LRW System  \nSummary:  \n- Assists researchers in literature review and manuscript preparation.  \n- Builds an initial reading list and finds similar papers.  \n- Shortlists papers for manuscript inclusion based on article type.  \n- Utilizes a corpus from the ACM Digital Library.  \n- Especially beneficial for novice researchers and those exploring new topics.  \n\nSlide Topic: Technical Details and Dataset of Rec4LRW  \nSummary:  \n- Utilizes ACM Digital Library snapshot with 103,739 articles and 2,320,345 references.  \n- Recommendation mechanisms based on seven bibliographic features.  \n- Girvan-Newman algorithm clusters citation network to identify key papers.  \n- UI includes information cue labels and parent cluster paper viewing options.\n\nSlide Topic: User Evaluation Study and Results  \nSummary:  \n- Conducted with 116 participants, including research students and staff.  \n- Evaluated the usefulness and effectiveness of the Rec4LRW system.  \n- High agreement on measures like Importance and Shortlisting_Feature.  \n- Demonstrated task usefulness in identifying important papers.  \n- Participants appreciated information cue labels and viewing papers in parent clusters.  \n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Rec4LRW system fills a gap in manuscript preparation assistance.  \n- It introduces a novel task for shortlisting articles.  \n- User evaluation confirmed the system's effectiveness and quality.  \n- Future work will expand the task's scope for various article types.  \n- Plans include integration with citation context recommendation tasks.  \n- More user control features will be developed.  ",
    "Slide Topic: Introduction and Context of Reuse and Plagiarism in NLP  \nSummary:  \n- Copy & paste ease has increased document proliferation.  \n- Archives enable studying reuse and plagiarism in NLP.  \n- Research builds on studies from ACL's 50th anniversary.  \n- Automatic detection tools are crucial for identifying sources.  \n- Study focuses on LREC papers from 1998 to 2014.  \n\n**Slide Topic: Objectives and Methodology of the Study**\n\nSummary:\n- Explore entry-level detection of reuse and plagiarism.\n- Compare verbatim raw strings with linguistic parsing results.\n- Focus on extrinsic detection to find near-matches between texts.\n- Analyze practices within the NLP field.\n- Compare word sequences to detect copy & paste operations.\n- Use a focus index and Jaccard distance for optimized comparisons.\n\nSlide Topic: Corpus and Data Preprocessing Techniques  \nSummary:\n- The NLP4NLP corpus includes 65,003 documents from 1965 to 2015.\n- Documents were sourced from various conferences and journals.\n- Metadata was standardized into BibTeX format under UTF8.\n- Text extraction from PDFs used PDFBox and Tesseract OCR.\n- Quality filters ensured content was over 900 characters with minimal out-of-vocabulary words.\n- The corpus contains English and French texts; German and Russian were excluded.\n\nSlide Topic: Definitions and Terminology Clarification  \nSummary:  \n- 'Self-reuse': Copy & paste with citation by the same authors.  \n- 'Self-plagiarism': Copy & paste without citation by the same authors.  \n- 'Reuse': Citing a source with different authors.  \n- 'Plagiarism': Copying without citation from different authors.  \n- Study examines copy & paste operations in both directions within NLP4NLP.  \n\nSlide Topic: Algorithm and Technical Implementation  \nSummary:\n- Build sliding windows of lemmas and record in a focus index.\n- Compute similarity using Jaccard distance and filter pairs by threshold.\n- Address variations like hyphenation, case, and spelling differences.\n- Apply linguistic parsing to normalize text.\n- Tune parameters like window size and threshold using PAN contest articles.\n- Validate through manual examination of results.\n\nSlide Topic: Results and Analysis of Self-Reuse and Self-Plagiarism  \nSummary:\n- Self-reuse and self-plagiarism are common, found in 18% of cases.\n- 61% of authors did not quote the source paper.\n- Large conferences like ISCA and IEEE-ICASSP are frequent sources.\n- Papers often transition from conferences to journals.\n- Similarity scores range from 4% to 97%.\n- Approximately 4,500 pairs have a similarity score of 10% or higher.\n\nSlide Topic: Findings on Reuse and Plagiarism  \nSummary:\n- Reuse and plagiarism are relatively uncommon.\n- 261 papers reused fragments from other authors.\n- 60% of these did not quote the source.\n- Manual checks showed many involved shared authorship or collaboration.\n- Only three cases were true plagiarism.\n- Similarity scores ranged from 4% to 42%.\n\nSlide Topic: Discussion on Ethical Implications and Further Developments  \nSummary:  \n- Self-reuse is more common than reusing others' content.  \n- Source papers are quoted in only 39% of cases.  \n- Plagiarism raises legal issues but often complies with the right to quote.  \n- Self-reuse and self-plagiarism stem from authors' writing styles and submission practices.  \n- Future developments focus on addressing 'rogeting' and improving altered text identification.  ",
    "Slide Topic: Introduction to Story Understanding and Mental State Reasoning  \nSummary:\n- Story understanding involves reasoning about causal links and characters' mental states.\n- These relationships are often implicit and not directly stated in the text.\n- Humans find this reasoning intuitive, but it is challenging for machines.\n- Current language models excel at local fluency but struggle with abstract reasoning.\n- They often fail to infer complex dynamics like characters' mental states or future actions.\n\nSlide Topic: Annotation Formalism and Dataset Properties  \nSummary:\n- Introduces a new annotation formalism for labeling mental states in short stories.\n- Dataset provides a complete chain of motivations and emotional reactions for characters.\n- Captures state changes for entities not directly mentioned in sentences.\n- Combines formal labels from multiple psychology theories with open text descriptions.\n- Corpus includes 15k stories with 300k annotations for 150k character-line pairs.\n\nSlide Topic: Theories of Motivation and Emotion  \nSummary:\n- Two key motivation theories: Maslow's and Reiss's.\n- Maslow's hierarchy includes five categories from basic needs to spiritual growth.\n- Reiss identifies 19 detailed motivation categories for deeper insights.\n- Emotion theory focuses on Plutchik's 'wheel of emotions.'\n- Plutchik's model includes eight basic emotional dimensions.\n\n**Slide Topic: Annotation Framework and Pipeline**\n\nSummary:\n- Introduces a new annotation framework with isolated tasks.\n- Key components: entity, action, affect, motivation, and emotional reaction resolution.\n- Annotations collected via Amazon Mechanical Turk crowdsourcing.\n- Designed to manage complex mental state effects in story characters.\n\nSlide Topic: Dataset Statistics and Insights  \nSummary:  \n- Dataset includes 300k low-level annotations for motivation and emotion.  \n- Covers 150k character-line pairs across 15,000 stories.  \n- 56k pairs have annotated motivation; 105k have annotated emotion changes.  \n- Theory category tasks are the hardest and most expensive.  \n- Theory category labels obtained for one-third of annotated stories.  \n- These labels are assigned to development and test sets.  \n\nSlide Topic: Experimental Setup and Baseline Models  \nSummary:\n- Train state, annotation, and explanation models.\n- State classification categorizes characters' psychological states.\n- Tasks use label sets from dev and test dataset splits.\n- Baseline models: TF-IDF, GloVe, LSTM, REN, NPN.\n- Each model has specific architectures for encoding.\n\nSlide Topic: Experimental Results and Analysis  \nSummary:  \n- All models outperform the random baseline in categorizing Maslow, Reiss, and Plutchik states.  \n- Adding entity-specific contextual information enhances model performance.  \n- Models learn to condition on a character's previous experience for mental state classification.  \n- Pretraining encoder parameters with free response annotations boosts performance.  \n- Performance improvements are observed across all models and prediction tasks.  \n\nSlide Topic: Related Work and Conclusion  \nSummary:  \n- Previous projects have explored emotion theories in NLP tasks.  \n- Our work introduces a new dataset for tracking emotional reactions in stories.  \n- We provide a large-scale dataset for training and evaluating mental state tracking.  \n- The dataset focuses on characters in short commonsense stories.  \n- Modeling character-specific context enhances labeling performance.  \n- Pretraining on free-response data further boosts performance.  ",
    "Slide Topic: Introduction to Similarity Search and Semantic Hashing  \nSummary:  \n- Similarity search, or nearest-neighbor search, identifies items most similar to a query.  \n- It is crucial for applications like plagiarism detection, collaborative filtering, and multimedia retrieval.  \n- Semantic hashing accelerates similarity search by using binary hash codes.  \n- Each item is represented by a similarity-preserving hash code.  \n- Similarity is evaluated by calculating Hamming distances between hash codes.\n\nSlide Topic: Challenges in Existing Text Hashing Approaches  \nSummary:\n- Existing methods often require two-stage training procedures.\n- Approaches are divided into two categories: unsupervised binary code learning and supervised binary classifier training.\n- Continuous text representations are inferred and then binarized during testing.\n- Lack of end-to-end learning can lead to suboptimal local optima.\n- Two-stage strategies may not fully optimize model parameters.\n\nSlide Topic: Proposed Neural Architecture for End-to-End Text Hashing  \nSummary:\n- Introduces a simple, generic neural architecture for text hashing.\n- Learns binary latent codes in an end-to-end manner.\n- Inspired by advances in neural variational inference for text processing.\n- Approaches semantic hashing using a generative model perspective.\n- Represents binary codes as deterministic or stochastic Bernoulli variables.\n- Jointly optimizes encoder and decoder networks by maximizing a variational lower bound.\n\nSlide Topic: Neural Variational Inference Framework  \nSummary:\n- Inspired by the success of variational autoencoders in NLP.\n- Focus on training discrete (binary) latent variables.\n- Define a generative model for encoding and decoding distributions.\n- Use inference and generative networks for approximation.\n- Cyclic strategy enhances latent variable generalization.\n\nSlide Topic: Rate-Distortion Theory and Data-Dependent Noise  \nSummary:\n- Introduce data-dependent noise in latent codes during decoding.\n- Adaptively balance rate and distortion during training.\n- Connect method to rate-distortion theory.\n- Provide theoretical foundation for framework's effectiveness.\n\nSlide Topic: Supervised Hashing with NASH  \nSummary:\n- NASH extends to supervised hashing by learning a mapping from latent variables to labels.\n- This mapping is parameterized by a two-layer MLP and a fully-connected softmax layer.\n- The model balances maximizing the variational lower bound and minimizing discriminative loss.\n- A joint training objective is employed to achieve this balance.\n\nSlide Topic: Experimental Setup and Evaluation Metrics  \nSummary:  \n- Utilized three datasets: Reuters 21578, 20 Newsgroups, and TMC.  \n- Employed a feedforward neural network with 2 hidden layers.  \n- Used ReLU activation function for non-linearity.  \n- Trained model with Adam optimizer, learning rate of 1 \u00d7 10\u207b\u00b3.  \n- Evaluated hashing codes using each test set entry as a query.  \n- Measured performance with precision metric.  \n\nSlide Topic: Ablation Study and Qualitative Analysis  \nSummary:  \n- Stochastic sampling enhances retrieval results across all datasets.  \n- Increased stochasticity in latent variables improves learned representations.  \n- Stacking MLP layers in encoder networks boosts empirical results.  \n- Non-linear decoder networks significantly reduce retrieval precision.  \n- Decoder networks act as a similarity measure between latent variables and word embeddings.",
    "Slide Topic: Introduction to Domain Adaptation Challenges  \nSummary:  \n- Deep neural networks excel in supervised NLP tasks with labeled data.  \n- Learning from unlabeled data under domain shift is challenging.  \n- Domain shift occurs when training and test data distributions differ.  \n- Many domain adaptation methods are task-specific and lack generalization.  \n- Some approaches risk overfitting by evaluating on limited datasets.  \n\nSlide Topic: Re-evaluation of Bootstrapping Algorithms  \nSummary:\n- Re-evaluate bootstrapping algorithms for deep neural networks (DNNs).\n- These are semi-supervised, general-purpose algorithms.\n- They treat models as black boxes, compatible with current NLP models.\n- Originally designed for in-domain performance.\n- Effectiveness in domain adaptation remains unexplored.\n- Re-evaluate self-training, tri-training, and tri-training with disagreement on two NLP tasks.\n\nSlide Topic: Self-training and Its Variants  \nSummary:\n- Self-training is a foundational bootstrapping approach.\n- It uses the model's predictions on unlabeled data for training.\n- Most confident predictions are used as additional training data.\n- Calibration is crucial due to poorly calibrated neural network outputs.\n- Top n confident predictions are added to labeled data each epoch.\n- Throttling variant outperforms classic selection in experiments.\n\n**Tri-training and Tri-training with Disagreement**\n\n- Tri-training reduces prediction bias using three independently trained models.\n- An unlabeled data point is added if two models agree on its label.\n- Tri-training with Disagreement requires the third model to disagree on the prediction.\n- This method enhances data efficiency.\n- Achieves competitive results in part-of-speech tagging.\n\nSlide Topic: Proposed Multi-task Tri-training Method  \nSummary:\n- Introduces Multi-task Tri-training (MT-Tri) to reduce tri-training complexity.\n- Utilizes multi-task learning to share knowledge across models.\n- Models collaborate to learn a joint representation, enhancing convergence.\n- Model-specific softmax layers are updated only for respective inputs.\n- Orthogonality constraint ensures diversity among models.\n\nSlide Topic: Experiments and Evaluation on NLP Tasks  \nSummary:\n- Evaluated on two unsupervised domain adaptation datasets.\n- Tasks include sequence labeling and classification.\n- Used SANCL 2012 dataset for POS tagging.\n- Compared results in low and high-data conditions.\n- Evaluated sentiment analysis on Amazon reviews dataset.\n- Model: MLP with one hidden layer, compared against baselines.\n\nSlide Topic: Analysis of POS Tagging and Sentiment Analysis Results  \nSummary:\n- Tri-training achieves the best average results for sentiment analysis in two target domains.\n- MT-Tri outperforms the state of the art in 3 out of 4 domains.\n- For POS tagging, tri-training with disagreement excels in low-data scenarios with 89.70% accuracy.\n- In high-data setups, classic tri-training significantly outperforms the baseline in 4 out of 5 cases.\n\nSlide Topic: Comparison with State-of-the-Art and Related Work  \nSummary:  \n- Our methods are compared to top results on both datasets, including several baselines.  \n- The multi-task tri-training model excels in sentiment analysis over traditional and recent alternatives.  \n- For POS tagging, classic tri-training remains superior.  \n- Emphasize the importance of comparing neural approaches to strong baselines.  \n- Results are reported across several runs for accuracy and reliability.  \n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- Re-evaluated traditional bootstrapping algorithms for neural networks in semi-supervised learning.  \n- Classic tri-training outperforms recent state-of-the-art methods in NLP tasks.  \n- Proposed a more efficient multi-task tri-training model for sentiment analysis.  \n- New model surpasses both traditional tri-training and recent alternatives.  \n- Future work could explore shared-private models for high-data scenarios.  ",
    "Slide Topic: Introduction to Cross-Lingual Transfer Learning  \nSummary:\n- Supervised learning requires large annotated corpora, which most languages lack.\n- Cross-lingual transfer learning transfers knowledge from high-resource to low-resource languages.\n- Traditional methods include annotation projection and transferable representations.\n- Most methods rely on a single source language, limiting transfer potential.\n- Some approaches use multiple source languages but often treat all sources equally, affecting quality.\n\nSlide Topic: Challenges in Multilingual Transfer  \nSummary:\n- Most methods rely on a single source language, limiting knowledge transfer.\n- Target languages may share similarities with multiple sources, enhancing transfer.\n- Existing methods often treat all source languages equally, ignoring quality differences.\n- Uniform weighting can lead to suboptimal performance.\n- Diverse characteristics of source languages are not adequately addressed.\n\nSlide Topic: Proposed Zero-Shot Multilingual Transfer Method  \nSummary:\n- Introduces a novel zero-shot multilingual transfer method inspired by truth inference in crowdsourcing.\n- Estimates model reliability and mistake patterns for multilingual transfer.\n- Learns transfer quality and language-specific errors using a Bayesian graphical model.\n- Enables accurate label inference in target languages despite noise.\n- Outperforms uniform ensembling and single best models.\n\nSlide Topic: Bayesian Graphical Model for Transfer Quality  \nSummary:\n- Inspired by Kim and Ghahramani (2012), designed for zero-shot transfer settings.\n- Generates predictions from 'true' labels corrupted by transfer models.\n- Uses a confusion matrix to describe label corruption.\n- Differentiates reliable from unreliable models through efficient inference.\n- Employs mean-field variational Bayes to optimize the evidence lower bound.\n- Demonstrated effectiveness in NER tasks by filtering unreliable models.\n\nSlide Topic: Few-Shot Transfer with Limited Annotations  \nSummary:\n- Introduce two models: BEAsup and RaRe for limited annotations.\n- BEAsup estimates Bayesian model parameters using labeled data.\n- RaRe ranks systems and retrains using top-ranked outputs.\n- Captures regularities like common words missed by cross-lingual transfer.\n- RaRe includes a distillation step with large unannotated datasets.\n- Fine-tuning with small supervised datasets enhances low-resource performance.\n\nSlide Topic: Experimental Setup and Data  \nSummary:  \n- Evaluated using a subset of the Wikiann NER corpus with 41 languages.  \n- Languages selected for overlap with multilingual word embedding resources.  \n- Additional evaluation on CoNLL 2002 and 2003 datasets for comparison.  \n- Utilized fastText 300-dimensional Wikipedia embeddings.  \n- Embeddings mapped to English space using Procrustes rotation.  \n- BiLSTM-CRF model used, tuned for high and low-resource settings.\n\nSlide Topic: Results and Analysis of Transfer Methods  \nSummary:\n- Multilingual methods outperform majority voting, especially in zero-shot learning.\n- BEA ent uns surpasses BEA tok uns, proving entity-level inference effectiveness.\n- Limited annotation access boosts performance; RaRe and BEA ent sup achieve F1 scores of 77.4 and 74.8.\n- Majority voting works well for Romance and Germanic languages but fails for Slavic languages.\n- RaRe outperforms BEA ent sup for isolated languages, highlighting data aggregation limitations.\n\n**Comparison with Existing Methods**\n\n- Our methods are competitive with benchmark methods on CoNLL-02/03 datasets.\n- RaReI shows improvements over unsupervised models.\n- Majority voting, especially MV ent, performs well, comparable to BEA.\n- Using 100 annotated sentences in the target language provides significant improvements.\n- Multilingual transfer outperforms state-of-the-art unsupervised single-source models like BWET.\n- Our methods can combine multiple source models to outperform majority voting.",
    "Slide Topic: Introduction to Instruction Execution Challenges  \nSummary:\n- Agents must ground language in their observed environment.\n- Reasoning about discourse dependencies is crucial.\n- Generating actions to achieve high-level goals is a key challenge.\n- Contextual cues from interaction history and world state are essential.\n- Instructions often refer to previously mentioned objects or actions.\n- The world state determines available actions and object references.\n\nSlide Topic: Proposed Model and Learning Approach  \nSummary:  \n- The model maps sequences of instructions to actions.  \n- It considers previous utterances and world state to select actions.  \n- It learns to combine simple actions to achieve complex goals.  \n- Training is possible using the ALCHEMY domain.  \n- The model generates action sequences directly, resolving references implicitly.  \n- It learns sequences needed to complete high-level actions.  \n\nSlide Topic: Attention-Based Model Architecture  \nSummary:\n- The model processes current and previous instructions, initial and current world states.\n- It computes attention encodings for these inputs at each step.\n- The model predicts the next action using attention mechanisms.\n- An RNN decoder is used to generate actions.\n- The architecture is illustrated in a figure showing action generation.\n\nSlide Topic: Learning Algorithm: SESTRA  \nSummary:\n- SESTRA addresses the exploration problem by observing rewards for all actions in each state.\n- It maximizes the immediate expected reward for visited states.\n- The algorithm iterates over training data, sampling executions with up to M actions.\n- It computes entropy and observes immediate rewards at each step.\n- RMSPROP is used to update the parameters effectively.\n\nSlide Topic: Evaluation and Results  \nSummary:  \n- Evaluated approach on SCONE across three domains.  \n- Focus on recovering predicate logic for sequential instructions.  \n- Achieved 89.1% accuracy for single instruction tasks.  \n- Achieved 62.7% accuracy for complete sequences.  \n- Outperformed POLICY GRADIENT and CONTEXTUAL BANDIT (see Table 3).\n\nSlide Topic: Comparison with Related Work  \nSummary:\n- Previous studies used the SAIL corpus for navigation tasks.\n- Focus was on high-level logic and low-level actions.\n- Related works include Branavan et al. and Vogel & Jurafsky.\n- Our work addresses dynamic environments during execution.\n- We emphasize generating direct environment actions.\n- This approach is more applicable to robotic agents.\n\nSlide Topic: Error Analysis and Model Sensitivity  \nSummary:\n- Analyzed instruction-level errors in top models with correct initial states.\n- Studied 50 examples per domain to identify failure types.\n- Table 5 lists major error categories: state reference and multi-turn reference errors.\n- Significant errors found due to ambiguous or incorrect instructions.\n\nSlide Topic: Discussion and Future Directions  \nSummary:  \n- We propose a model for context-dependent instructional language.  \n- The model relies on interaction history and world state.  \n- Future work may use intermediate world states from previous interactions.  \n- Our learning approach, SESTRA, helps recover from early learning biases.  \n- A future direction is using an estimator to predict rewards for all actions.  ",
    "Slide Topic: Introduction to Language Identification  \nSummary:\n- Language identification is vital in natural language processing.\n- It aids in tasks like letter-to-phoneme conversion and machine transliteration.\n- Traditional methods use character-level n-gram language models.\n- This approach suggests using Support Vector Machines (SVMs) for short texts.\n- SVMs outperform language models on datasets of personal names.\n- Language identification can enhance transliteration by pre-processing data.\n\nSlide Topic: Previous Work on Language Identification  \nSummary:\n- N-gram models are popular for language identification.\n- Cavnar and Trenkle (1994) achieved 99.8% accuracy with texts of 300+ characters.\n- Accuracy drops for shorter texts, with 90.2% for 50-byte texts (Kruengkrai et al., 2005).\n- SVMs reached 99.7% accuracy on the same task.\n- Konstantopoulos (2007) noted low F1 scores for proper nouns with n-grams, improved with smaller datasets.\n- Li et al. (2007) achieved 94.8% accuracy for single-name identification using n-grams.\n\nSlide Topic: SVMs for Language Identification  \nSummary:\n- SVMs use character n-gram counts as features for language identification.\n- They effectively handle and weigh a large number of features.\n- Linear, sigmoid, and RBF kernels showed the best performance.\n- Optimal n-gram lengths were found through cross-validation.\n- Models were built using the entire training dataset.\n- LIBLINEAR and LIBSVM packages were utilized, with preprocessing steps like lowercasing and removing short names.\n\nSlide Topic: Intrinsic Evaluation of SVM Approach  \nSummary:\n- SVM tested on Transfermarkt and CEJ corpora of names.\n- Transfermarkt corpus includes names from 13 languages.\n- SVM outperformed language models, especially on last names.\n- CEJ corpus classifies names as Chinese, English, or Japanese.\n- Linear kernel reduced error rate by over 50% compared to language models.\n- RBF and sigmoid kernels were slower; only linear kernel tested.\n\nSlide Topic: Application to Machine Transliteration  \nSummary:\n- Language identification aids in machine transliteration by influencing pronunciation.\n- Tested on an English-Hindi corpus, classifying names as Indian or non-Indian.\n- Achieved 80% accuracy with linear kernel and 84% with RBF kernel.\n- Preprocessing with language identification did not improve transliteration accuracy.\n- Reduced data size for training separate models was a key factor.\n- Experiment confirmed data size reduction impacts transliteration effectiveness.\n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Combined language identification with transliteration by splitting data into disjoint classes.  \n- Used DIRECTL transliteration system and trained language ID model on 1000 tagged names.  \n- Data splitting resulted in a combined top-1 accuracy of 46.0%, compared to 47.0% for a single model.  \n- Lack of improvement attributed to reduced data size, confirmed by an experiment.  \n- Transliteration system may leverage language origin information implicitly.  \n- Closer integration of modules could enhance accuracy.  \n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Introduced a novel SVM approach using n-gram counts for language identification.  \n- Outperformed traditional language models in language identification.  \n- Language identification showed potential but did not significantly improve transliteration accuracy.  \n- Future work will explore integrating language identification in transliteration.  \n- Consider using language identification probabilities as features.  \n- Plan to allow non-disjoint data sets for model training.  ",
    "Slide Topic: Introduction to Extractive Summarization Challenges  \nSummary:  \n- Extractive summarization involves selecting key word sequences to form a summary.  \n- Supervised methods often use human abstracts for annotation.  \n- A major challenge is the imbalance in tagging, with most words labeled as 0s.  \n- Some source words are paraphrases, leading to inaccurate labeling.  \n- These issues complicate using human abstracts for effective supervision.  \n\nSlide Topic: Neural Abstractive vs. Extractive Summarization  \nSummary:  \n- Abstractive summarization can copy or generate new words.  \n- It addresses some limitations of extractive summarization.  \n- A key challenge is maintaining factual accuracy.  \n- Extractive summarization aims for semantic closeness to human abstracts.  \n- It often shares common words with the source text.\n\nSlide Topic: Cloze-Style Question Generation for Training  \nSummary:\n- Introduces a new training paradigm for extractive summarization.\n- Converts human abstracts into Cloze-style comprehension questions.\n- Encourages system summaries to preserve key source content.\n- Makes summaries effective for predicting correct answers.\n- Uses an attention mechanism to locate relevant summary segments.\n- Allows summaries to answer multiple questions efficiently.\n\nSlide Topic: Reinforcement Learning for Extractive Summarization  \nSummary:  \n- Reinforcement learning explores the space of possible extractive summaries.  \n- The approach rewards summaries that preserve salient content.  \n- Summaries are optimized to answer key questions effectively.  \n- A question-focused reward mechanism enhances summary quality.  \n- Common words with abstracts are prioritized for relevance.\n\nSlide Topic: Question-Focused Reward Mechanism  \nSummary:\n- Rewards summaries that answer key questions effectively.\n- Generates question-answer pairs from human abstracts.\n- Uses Bi-LSTM to encode questions and summaries.\n- Attention mechanism identifies relevant summary parts.\n- QA reward is based on log-likelihood of correct answers.\n- Ensures summaries are semantically close to human abstracts.\n\nSlide Topic: Experimental Setup and Hyperparameters  \nSummary:  \n- Training, validation, and testing conducted on the CNN dataset.  \n- Bi-LSTM hidden state size set to 256; single-direction LSTM encoder size is 30.  \n- Dropout rate of 0.2 and minibatch size of 256 are used.  \n- Early stopping applied; Adam optimizer with a learning rate of 1e-4.  \n- Source vocabulary includes 150K words; 100-dimensional word embeddings initialized by GloVe.  \n\nSlide Topic: Comparison with State-of-the-Art Systems  \nSummary:  \n- The study compares its methods with state-of-the-art systems.  \n- Both extractive and abstractive approaches are evaluated.  \n- Two variants, 'Entity Q' and 'Keyword Q', are tested.  \n- Both variants outperform baseline systems on a benchmark dataset.  \n- R-1, R-2, and R-L scores indicate superior results.  \n- R-2 F-scores are 11.5 and 11.6, respectively.  \n\nSlide Topic: Future Directions and Usability Studies  \nSummary:  \n- Investigate methods to automatically cluster selected summary segments.  \n- Focus on capturing unique aspects of the content.  \n- Conduct usability studies inspired by recent research.  \n- Test how summary highlights assist users in answering key questions.  \n- Provide an alternative evaluation strategy for the proposed method.  \n- Compare against extractive and abstractive baselines.  ",
    "Slide Topic: Introduction to Multimodal Social Media and Name Tagging Challenges  \nSummary:\n- Social platforms like Snapchat, Twitter, Instagram, and Pinterest are essential for communication.\n- These platforms are increasingly multimodal, combining images, videos, audios, and texts.\n- Understanding textual components requires visual context due to multimodal nature.\n- Name tagging is challenging due to short texts, linguistic variations, and colloquial language.\n- This work extends models to incorporate visual information using an attention mechanism.\n\nSlide Topic: Proposed Model: Visual Attention and Modulation Gate  \nSummary:\n- Utilizes visual context for name tagging in text.\n- Employs an attention-based model to extract visual features from images.\n- Combines textual features from BLSTM with visual features via a modulation gate.\n- Feeds combined features into a CRF layer for tag prediction.\n- Modulates word-level multimodal features.\n- Enhances name tagging in multimodal social media.\n\nSlide Topic: BLSTM-CRF Sequence Labeling and Character-level Representation  \nSummary:\n- Name tagging is treated as a sequence labeling problem.\n- BLSTM captures long-range dependencies in sequences.\n- Character-level representation is generated using another BLSTM.\n- Combines prefix, suffix, and spelling information.\n- Final word representation is a mix of word embedding and character-level data.\n- CRFs learn label constraints and predict the best label sequence.\n\n**Slide Topic: Visual Feature Representation and Attention Mechanism**\n\nSummary:\n- Visual features are extracted using CNNs, specifically ResNet.\n- ResNet captures both regional and global visual representations.\n- The attention mechanism targets image regions related to text.\n- It maps queries and key-value pairs to outputs via weighted sums.\n- This enhances the model's focus on relevant features.\n- Improved focus boosts name tagging accuracy.\n\nSlide Topic: Datasets: Twitter and Snapchat Multimodal Data  \nSummary:\n- Evaluated on Twitter and Snapchat multimodal datasets.\n- Includes named entities: Location, Person, Organization, Miscellaneous.\n- Twitter data: tweets with images.\n- Snapchat data: snaps with captions.\n- Twitter texts are longer and more contextually related to images.\n- Serve as benchmarks for multimodal name tagging.\n\nSlide Topic: Experimentation and Model Evaluation  \nSummary:  \n- Experiments include tokenization, labeling schema, and embeddings.  \n- Models use word and character embeddings with pretrained CNNs.  \n- Optimization is done via mini-batch stochastic gradient descent.  \n- Specific hyperparameters are applied for model optimization.  \n- Proposed model uses visual attention and modulation gate.  \n- Achieves best F1 scores, validating visual features' effectiveness.  \n\nSlide Topic: Results and Attention Visualization  \nSummary:\n- The model excels on Twitter data due to longer sentences providing more context.\n- Visual features significantly enhance name tagging in well-aligned multimodal contexts.\n- Attention visualization highlights the model's focus on relevant image regions.\n- Accurate name tagging is aided by attention to key image areas.\n- Challenges include poor alignment, blurred images, and incorrect attention.\n\nSlide Topic: Related Work in Name Tagging and Visual Attention  \nSummary:\n- Recent advancements in name tagging use neural network architectures like BLSTM-CRF.\n- These models outperform traditional methods but focus mainly on newswire data.\n- For social media, adaptations include leveraging unlabeled data and multi-task approaches.\n- Visual attention mechanisms enhance name tagging by focusing on relevant image regions.\n- These mechanisms are adapted from tasks like Image Captioning and Visual Question Answering.\n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- The gated Visual Attention model improves name tagging in multimodal social media.  \n- Creation of Twitter and Snapchat datasets supports further research.  \n- Future work includes expanding to fine-grained name tagging and entity linking.  \n- Aim to leverage visual features for specific entity types like sports teams and artists.",
    "Slide Topic: Introduction to Text Coherence  \nSummary:  \n- Coherence is crucial for conveying a text's purpose and meaning.  \n- It reduces reading time by making the text easier to understand.  \n- A coherent text integrates as a whole with related sentences.  \n- Effective organization enhances text coherence.  \n- Good organization helps readers anticipate upcoming information.  \n\nSlide Topic: Existing Coherence Models and Their Limitations  \nSummary:  \n- Computational models like the Entity Grid use supervised learning for text coherence.  \n- These models often favor the original text order and struggle with poorly organized texts.  \n- Supervised models face issues like data sparsity, domain dependence, and high training costs.  \n- Guinaudeau and Strube introduced the unsupervised Entity Graph model.  \n- The Entity Graph model assumes coherence through local cohesion.\n\nSlide Topic: Proposed Unsupervised Text Coherence Model  \nSummary:\n- Introduces an unsupervised text coherence model addressing past research limitations.\n- Captures contributions of related but non-identical entities.\n- Utilizes an unsupervised learning approach to reduce data sparsity and domain dependence.\n- Encodes text into a graph with sentences as vertices and semantic relations as edges.\n- Measures text coherence through semantic similarity.\n\nSlide Topic: Semantic Similarity Graph Construction  \nSummary:  \n- Propose three methods for graph construction based on semantic similarity.  \n- Methods: Preceding Adjacent Vertex (PAV), Single Similar Vertex (SSV), Multiple Similar Vertex (MSV).  \n- PAV mimics reading by linking to preceding sentences.  \n- SSV establishes edges without directional constraints.  \n- MSV allows multiple outgoing edges using a similarity threshold.  \n- Aim to capture semantic dependencies among sentences.  \n\nSlide Topic: Evaluation Methodologies  \nSummary:  \n- Evaluate methods on two tasks: document discrimination and insertion.  \n- Document discrimination involves identifying the original text from permuted sentences.  \n- Insertion task assesses coherence by estimating original sentence positions.  \n- Compare performance with reimplementations of Entity Grid and Entity Graph models.\n\nSlide Topic: Document Discrimination Task Results  \nSummary:  \n- Entity Grid without saliency performed best.  \n- Followed by Entity Grid with saliency, PAV, MSV, Entity Graph, and SSV.  \n- PAV and MSV outperformed Entity Graph.  \n- Suggests importance of semantic similarity between sentences.  \n- PAV-MSV pair shared most judgments.  \n- Indicates local coherence is sufficient for this task.  \n\n**Slide Topic: Insertion Task Results**\n\nSummary:\n- PAV-based graph construction outperformed Entity Grid and Entity Graph baselines.\n- PAV achieved best performance at \u03b1 = 0.0.\n- MSV performed best at \u03b8 = 0.8.\n- No statistically significant accuracy difference due to limited data size.\n- SSV and MSV may better capture distant sentence relations than PAV.\n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Introduced three novel unsupervised text coherence scoring methods.  \n- PAV method outperformed unsupervised baseline in document discrimination.  \n- In insertion task, PAV surpassed both baselines, but not significantly.  \n- Larger data experiments are needed for further validation.  \n- Error analysis suggests improving sentence vector representation and similarity calculation.  ",
    "Slide Topic: Introduction to User Profiling in Social Media  \nSummary:  \n- Online social networks enable comprehensive user text analysis.  \n- Analysis includes social networks, spatial-temporal data, and personal attributes.  \n- Language use is influenced by attributes like location, gender, and age.  \n- User profiling aids sociolinguistic studies and recommender systems.  \n- Language patterns in social media may indicate occupational class.\n\nSlide Topic: Hypothesis on Language Use and Occupational Class  \nSummary:\n- Social status influences language use.\n- Language on social media may indicate occupational class.\n- Executives often write about business topics.\n- Manufacturing workers might focus on personal interests.\n- We predict occupational class using Twitter data.\n- This has applications in sociology and recruitment.\n\nSlide Topic: Data Collection and Standard Occupational Classification  \nSummary:\n- Utilized the UK Standard Occupational Classification (SOC) system.\n- Created a dataset of Twitter users mapped to occupations.\n- Used self-disclosed job titles from SOC unit groups.\n- Filtered data includes 5,191 users from 55 minor groups.\n- Dataset spans nine major SOC groups with 10,796,836 tweets.\n- Data is openly available for study purposes.\n\nSlide Topic: Feature Extraction for Occupational Class Prediction  \nSummary:\n- Features are divided into user-level and textual categories.\n- User-level features include general user info and tweet statistics.\n- Textual features are derived from aggregated tweets.\n- SVD word embeddings and NPMI clusters represent words and topics.\n- Spectral clustering creates word clusters for model analysis.\n- Neural embeddings capture syntactic information for better word representation.\n\nSlide Topic: Gaussian Processes for Classification  \nSummary:\n- Gaussian Processes (GPs) provide a Bayesian non-parametric framework for classification.\n- They define a prior on functions using a kernel to model covariance in response values.\n- For classification, GPs squash the latent function output through a logistic function.\n- Expectation Propagation is used for inference, approximating the non-Gaussian joint posterior with a Gaussian one.\n- GPs offer advantages like not requiring a validation set and providing interpretability.\n- Automatic Relevance Determination enhances interpretability by identifying important features.\n\nSlide Topic: Experimental Results and Predictive Accuracy  \nSummary:  \n- Compared classification methods on held-out data with various feature sets.  \n- User-level features are ineffective for predicting job class, indicating task difficulty.  \n- Textual features enhance performance; clusters outperform embeddings.  \n- Gaussian Processes (GPs) generally outperform Support Vector Machines (SVMs) and offer interpretability.  \n- Dataset enables study of finer occupation class granularity.  \n- Prediction accuracy: 33.9% for sub-major groups, 29.2% for minor groups.  \n\nSlide Topic: Qualitative Analysis of Language Use Across Occupations  \nSummary:  \n- Word clusters enable analysis of language use across occupations.  \n- Topics like 'Corporate', 'Software Engineering', and 'Health' are informative.  \n- Cumulative Distribution Functions show topic prevalence and language divergence.  \n- Higher skilled jobs discuss 'Politics' more; 'Arts' distinguishes class 5.  \n- Informal topics like 'Elongated Words' are common in lower skilled jobs.  \n- Jensen-Shannon divergence quantifies topic distribution differences.  \n\nSlide Topic: Related Work and Sociolinguistic Context  \nSummary:  \n- Occupational class prediction is studied in psychology and economics.  \n- Studies link social status to language use.  \n- Previous work includes user classification on platforms like Sina Weibo.  \n- Our study focuses on Twitter with a large-scale dataset.  \n- We use a model allowing qualitative interpretation.  \n- Builds on sociolinguistic studies highlighting social status in language use.  \n\nSlide Topic: Conclusions and Implications for Future Research  \nSummary:  \n- Conducted large-scale analysis of language use on social media to infer occupational class.  \n- Introduced a labeled Twitter dataset and used a GP framework for classification.  \n- Achieved good predictive accuracy, showing occupation influences text use.  \n- Derived topics capture occupation-specific interests and class-based behaviors.  \n- Reflects Twitter population, suggesting potential for user profiling applications.  \n- Offers insights for future sociolinguistic research.  ",
    "Slide Topic: Introduction to Social Media as a Data Source for Epidemic Detection  \nSummary:  \n- Social media platforms facilitate widespread sharing of personal information.  \n- Twitter is a key resource for researchers due to its micro-blogging nature.  \n- Social media can act as 'social sensors' for event detection, including disease outbreaks.  \n- Over 30 Twitter-based systems have been developed for detecting seasonal influenza.  \n- These systems assume real-world flu spread is mirrored in tweets.  \n- Daily flu-related tweet counts are aggregated to estimate current patient status.  \n\n**Slide Topic: Challenges in Using Twitter for Influenza Surveillance**\n\nSummary:\n- Twitter provides valuable data for influenza surveillance but presents challenges.\n- Tweets can indicate actual illness or mere suspicion, introducing noise.\n- Previous studies focused on removing noise like suspicion and news-related tweets.\n- This study uses noisy tweets, assuming words like 'fever' signal outbreaks.\n- The main challenge is distinguishing tweets reflecting actual patient status.\n- Accurate differentiation is crucial for precise influenza surveillance.\n\n**Slide Topic: Forecasting Words and Time Lag Analysis**\n\nSummary:\n- Identifies 'forecasting words' like 'fever' appearing before influenza onset.\n- Observes a time lag between word frequency and patient numbers.\n- Example: 'Fever' shows a 16-day time lag before patient increase.\n- Early knowledge of time lags can help estimate patient numbers.\n- Examines flu prevention words like 'shot' with longer time lags.\n- Challenges include estimating optimal time lags and integrating them into models.\n\n**Slide Topic: Methodology: Time-Shifted Word Matrix and Nowcasting Model**\n\nSummary:\n- Construct a linear model to estimate current and forecast future influenza epidemics.\n- Determine optimal time shifts for each word using cross-correlation with patient numbers.\n- Build a matrix with shifted word frequencies.\n- Develop a nowcasting model from this matrix using regularization methods like Lasso and Elastic Net.\n- Extend the model to forecast by further shifting word frequencies into the future.\n\nSlide Topic: Parameter Estimation Using Lasso and Elastic Net  \nSummary:\n- Parameters are estimated by minimizing squared error with a penalty term.\n- Lasso is used for parameter selection but may overfit with large vocabulary sizes.\n- Elastic Net combines L1 and L2 penalties to address overfitting.\n- It selects groups of correlated variables, improving performance over Lasso alone.\n- Elastic Net enables automatic variable selection and continuous shrinkage.\n\nSlide Topic: Experimentation and Results: Nowcasting and Forecasting Performance  \nSummary:  \n- Nowcasting model evaluated using actual influenza reports.  \n- Time-shifted models improved by 0.1 points over baseline.  \n- Forecasting model outperformed baseline up to 3 weeks ahead.  \n- Performance decreased with longer time shifts.  \n- Equivalent performance maintained up to 10 days into the future.  \n- Results demonstrate effective epidemic prediction capability.  \n\nSlide Topic: Discussion on Model Limitations and Improvements  \nSummary:\n- The approach showed basic feasibility but had exceptions like time shift inconsistencies.\n- Words like 'vaccination' correlated in one season but not others, indicating annual cycling.\n- The model overestimated before outbreaks and underestimated after peaks.\n- A technique is needed to distinguish actual forecasting words from noise.\n- Despite limitations, time shift was effective for social media-based surveillance.\n- The model enables prediction despite noted challenges.\n\n**Slide Topic: Conclusion and Future Applications of the Model**\n\nSummary:\n- Introduced a novel social media-based influenza surveillance system using Twitter data.\n- Achieved the best nowcasting performance and sufficient forecasting accuracy.\n- First model to enable prediction of future epidemics.\n- Potential applications for predicting other events.\n- Demonstrates the utility of social media data in epidemic detection and forecasting.",
    "- Simultaneous Interpretation (SI) is a challenging task with high cognitive demands.\n- Interpreters must accurately and promptly translate source speech into a target language.\n- They use strategies like generalization and summarization for efficient message delivery.\n- Human memory and stamina limits lead to increased errors after minutes of interpreting.\n- Performance estimation involves predicting when interpreters are smooth or struggling.\n\nSlide Topic: Quality Estimation in Machine Translation and Its Application to Interpretation  \nSummary:\n- Quality Estimation (QE) in MT was first proposed by Blatz et al. (2004).\n- Traditional metrics like BLEU require reference translations, which are not always available.\n- QE has gained significant attention and is a task in the Workshop on Statistical Machine Translation since 2012.\n- Approaches to QE include linear classifiers and neural models.\n- QuEst++ is a prominent QE pipeline supporting word, sentence, and document-level analysis.\n- Its effectiveness and flexibility make it suitable for the proposed task.\n\nSlide Topic: Adapting Quality Estimation for Interpreter Output  \nSummary:\n- Augment baseline features with four additional types for better quality estimation.\n- Increased hesitations or incomplete words may indicate interpreter struggles.\n- Both corpora used are annotated for pauses and partial word renditions.\n- Key features include ratios of pauses, hesitations, and incomplete words.\n- Also consider ratios of non-specific words and 'quasi-' cognates.\n- Evaluate the ratio of the total number of words for comprehensive analysis.\n\nSlide Topic: Interpretation-Specific Features for Quality Estimation  \nSummary:  \n- Interpreters often compress output by omitting or replacing common nouns.  \n- This avoids specific terminology and reduces redundancy or cognitive load.  \n- Example: \"The chairman explained the proposal to the delegates\" becomes \"he explained it to them.\"  \n- A feature checks for words from a seed list of pronouns and demonstrative adjectives.  \n- Transliterated words may indicate facilitated translation due to language proximity.  \n- They can also represent an approximation of unknown words by the interpreter.  \n\nSlide Topic: Evaluation Metrics for Interpreter Performance  \nSummary:\n- Assess novice interpreters on omissions, additions, and inaccurate renditions.\n- Recovery of content and correct terminology are highly valued.\n- No large manually annotated corpus exists for these measures.\n- Metrics align with challenges in machine translation (MT) evaluation.\n- Key decision: choose the right evaluation metric for our QE system.\n- We use METEOR, which considers paraphrases and content-function word distinctions.\n\nSlide Topic: Data and Corpora for Interpretation Quality Experiments  \nSummary:\n- EN-JA data is trained using seven TED Talks from the NAIST TED SI corpus.\n- The corpus includes human-transcribed SI output from three proficiency levels.\n- EN-FR and EN-IT data are sourced from the EPTIC corpus.\n- EPTIC provides source and interpreter transcripts from European Parliament speeches.\n- The EN-FR dataset contains 739 utterances.\n- The EN-IT dataset contains 731 utterances.\n\nSlide Topic: Results and Analysis of Quality Estimation Models  \nSummary:\n- Evaluated QE system using Pearson's r correlation with true METEOR scores.\n- Interpreter-focused model outperforms in all language settings.\n- Notable gains for EN-JA (A-Rank) and highest accuracy on EN-FR dataset.\n- Gains are statistically significant at p < 0.05 via pairwise bootstrap.\n- Model captures issues better due to interpretation-specific qualities.\n\nSlide Topic: Conclusion and Future Directions in Interpreter Quality Estimation  \nSummary:\n- Introduced a novel application of QE for evaluating interpreter output.\n- Enables CAI systems to assist struggling interpreters selectively.\n- Utilizes METEOR for assessing interpreter performance.\n- Future work includes developing fine-grained performance measures.\n- Aims to evaluate various aspects of interpreter performance.",
    "Slide Topic: Introduction to Sentence Representations in Deep Learning  \nSummary:\n- Deep learning enables automatic learning of continuous sentence representations.\n- These representations can be tailored for tasks like sentiment analysis and semantic similarity.\n- Learning can occur in both unsupervised and supervised ways.\n- Neural Machine Translation (NMT) models are a notable approach.\n- Significant advancements have been made with the introduction of attention mechanisms.\n- This paper explores the impact of these improvements on sentence representations.\n\nSlide Topic: Advancements in Neural Machine Translation and Attention Mechanisms  \nSummary:  \n- Significant improvements in NMT since Hill et al. (2016).  \n- Introduction of attention mechanisms enhances translation quality.  \n- Attention allows dynamic focus on key parts of source sentences.  \n- Eliminates the need for static vector representation.  \n- Proposes a novel encoder-decoder architecture.  \n- Maintains structured fixed-size representation with dynamic attention.  \n\nSlide Topic: Proposed Encoder-Decoder Architecture with Compound Attention  \nSummary:\n- Incorporates compound attention in both encoder and decoder.\n- Encoder uses bidirectional GRU to process input sequences.\n- Inner attention computes weighted averages of encoder states.\n- Decoder applies attention for implicit alignment of sequences.\n- Aims to improve sentence representation and translation quality.\n\nSlide Topic: Evaluation of Sentence Representations Across Tasks  \nSummary:  \n- Continuous sentence representations are evaluated using SentEval tasks.  \n- Includes 10 classification and 7 similarity tasks.  \n- Performance assessed with Pearson and Spearman correlation metrics.  \n- Paraphrase representation evaluated with COCO and HyTER datasets.  \n- Metrics include cluster classification and nearest-neighbor retrieval accuracy.  \n\nSlide Topic: Experimental Setup and Training Details  \nSummary:  \n- Train English-to-German and English-to-Czech NMT models using Neural Monkey.  \n- Use datasets like Multi30K and CzEng 1.7 with preprocessing steps.  \n- Apply truecasing and byte-pair encoding for data preparation.  \n- Specify training details: number of epochs, batch size, and vocabulary size.  \n- Evaluate models on respective development and test sets.  \n\nSlide Topic: Translation Quality and BLEU Score Analysis  \nSummary:  \n- Translation quality is evaluated using BLEU scores.  \n- The ATTN model achieves the highest BLEU score.  \n- The Transformer and ATTN-ATTN models follow in performance.  \n- More attention heads correlate with higher BLEU scores.  \n- Larger representations generally improve translation quality.  \n- Manual ranking supports the impact of model configurations.  \n\nSlide Topic: Comparison with Existing Models and Baselines  \nSummary:  \n- Proposed models are compared with InferSent, GloVe-BOW, and SkipThought.  \n- InferSent outperforms other models in most tasks.  \n- Proposed models show competitive performance, especially in NLI tasks.  \n- Comparison highlights strengths and weaknesses of different models.  \n- Provides insights into effectiveness in sentence representation.  \n\n**Slide Topic: Discussion on Representation Metrics and Translation Quality**\n\nSummary:\n- Explores the relationship between representation metrics and translation quality.\n- Reveals a negative correlation between BLEU scores and representation metrics.\n- Suggests models focusing on translation quality may miss sentence meaning.\n- Includes a heatmap of Pearson correlations to illustrate metric interplay.\n- Highlights challenges in achieving both high translation quality and meaningful sentence representations.",
    "Slide Topic: Introduction to Speech-to-Text Translation for Low-Resource Languages  \nSummary:\n- Speech-to-Text (ST) aids language documentation and crisis relief.\n- Traditional ST needs transcribed audio and parallel text, often lacking in low-resource languages.\n- End-to-end ST uses source audio with target text translations.\n- Recent studies show end-to-end ST is effective but resource-intensive.\n- Using data from high-resource languages can enhance low-resource ST performance.\n\nSlide Topic: Challenges in Traditional Speech-to-Text Systems  \nSummary:  \n- Traditional systems use a pipeline of ASR and MT.  \n- They require transcribed audio and parallel text.  \n- These resources are scarce for low-resource languages.  \n- Training effective systems is challenging in these settings.  \n- End-to-end ST needs large amounts of translated audio.  \n- This is a major hurdle in low-resource environments.  \n\nSlide Topic: Leveraging High-Resource Language Data for Low-Resource ST  \nSummary:\n- Utilize high-resource language data to enhance low-resource ST.\n- Train a single ASR model on multiple languages for effectiveness.\n- Apply transfer learning in MT by fine-tuning pre-trained models.\n- Pre-train models on high-resource ASR tasks.\n- Fine-tune these models for low-resource ST to achieve improvements.\n\nSlide Topic: Methodology: Encoder-Decoder Model with Attention  \nSummary:  \n- Utilizes an encoder-decoder model with attention, adapted from previous works.  \n- Consistent architecture across ASR and ST tasks for parameter transfer.  \n- Pre-trained ASR models initialize ST models.  \n- All model parameters are updated during ST training.  \n- Decoder shares vocabulary across tasks for high-resource target languages.  \n\nSlide Topic: Experimental Setup and Data Sets  \nSummary:  \n- Utilized datasets: Switchboard for English ASR, GlobalPhone for French ASR, Fisher Spanish for Spanish-English ST.  \n- Simulated low-resource conditions with smaller training corpora.  \n- Employed Mboshi-French parallel corpus for Mboshi-French ST.  \n- Speech preprocessing involved converting raw input to MFCCs.  \n- Text modeled using byte pair encoding.  \n\n**Slide Topic: Evaluation Metrics and Baselines**\n\nSummary:\n- BLEU scores are reported for all models.\n- BLEU can be hard to interpret in low-resource settings.\n- METEOR reports word-level unigram precision and recall.\n- METEOR considers stem, synonym, and paraphrase matches.\n- Naive baselines predict the most frequent words.\n- Baselines provide an empirical lower bound on precision and recall.\n\nSlide Topic: Results and Analysis of Transfer Learning  \nSummary:  \n- Transfer learning significantly enhances ST performance, especially in low-resource settings.  \n- Fine-tuning an English ASR model for Spanish-English ST improves BLEU scores and precision/recall.  \n- Models trained with minimal ST data achieve results comparable to those with larger datasets.  \n- The main advantage is from transferring encoder parameters.  \n- Encoder parameters capture language-independent acoustic features.  \n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Pre-training on high-resource ASR data boosts low-resource ST performance.  \n- Pre-trained models train faster and achieve higher BLEU scores than baselines.  \n- Future work could explore multilingual training approaches.  \n- Investigate pre-trained multilingual features for further improvements.  \n- Consider alternative language modeling strategies to enhance ST systems.  ",
    "Slide Topic: Introduction to SEQ2SEQ Models and Their Limitations  \nSummary:  \n- SEQ2SEQ models excel in tasks like machine translation and parsing.  \n- They require large parallel datasets, posing a limitation.  \n- Research explores using non-parallel corpora to improve performance.  \n- Unsupervised SEQ2SEQ models often struggle with tasks like sentence compression.  \n- These models rely on reinforcement learning, which is unstable and inefficient.  \n\nSlide Topic: Proposed SEQ3 Model for Unsupervised Abstractive Sentence Compression  \nSummary:\n- SEQ3 is a sequence-to-sequence-to-sequence autoencoder.\n- It is trained end-to-end using gradient-based optimization.\n- Utilizes differentiable approximations for categorical sampling.\n- Outperforms reinforcement learning in sentence compression.\n- SEQ3 can be extended to tasks like machine translation and semantic parsing.\n- Demonstrates state-of-the-art performance in unsupervised abstractive sentence compression.\n\nSlide Topic: Technical Details of the SEQ3 Model Architecture  \nSummary:\n- SEQ3 includes two attentional encoder-decoder pairs: a compressor and a reconstructor.\n- The compressor generates a summary from input text, serving as a latent variable.\n- The reconstructor reproduces the input from the summary, minimizing reconstruction loss.\n- A pretrained language model acts as a prior, adding a loss for human-readable summaries.\n- The model incorporates a topic loss, rewarding summaries with similar topic-indicating words.\n\n**Slide Topic: Differentiable Word Sampling Techniques**\n\nSummary:\n- SEQ3 uses differentiable word sampling techniques for summary generation.\n- Direct word sampling is non-differentiable; a soft-argmax function is used instead.\n- During training, a weighted sum of all vocabulary word embeddings is passed.\n- The Gumbel-Softmax trick approximates sampling from categorical distributions.\n- A Straight-Through estimator discretizes embeddings in the forward pass.\n- Gradients are computed using Gumbel-Softmax in the backward pass.\n\nSlide Topic: Loss Functions and Their Roles in SEQ3  \nSummary:\n- SEQ3 employs multiple loss functions to guide training.\n- Reconstruction loss ensures accurate input text reproduction.\n- LM prior loss, using KL divergence, maintains summary readability.\n- Topic loss preserves topic-indicating words in summaries.\n- Length penalty aids in predicting end-of-sequence tokens.\n\nSlide Topic: Experimental Setup and Datasets Used  \nSummary:  \n- SEQ3 trained on Gigaword sentence compression dataset.  \n- Dataset includes news article sentences and their headlines.  \n- Tested on DUC-2003 and DUC-2004 tasks without retraining.  \n- Evaluated against other unsupervised models and baselines.  \n- Performance measured using ROUGE scores.  \n\nSlide Topic: Comparison with Other Models and Baselines  \nSummary:  \n- SEQ3 significantly outperforms the unsupervised Pretrained Generator model across all metrics.  \n- It surpasses the LEAD-8 baseline in performance.  \n- Performance drops when the LM prior is removed, highlighting its importance.  \n- SEQ3 performs well on DUC-2003 and DUC-2004 datasets.  \n- It does not surpass the PREFIX baseline in DUC-2003.  \n- Results demonstrate SEQ3's effective loss functions and high-quality summary generation.  \n\nSlide Topic: Limitations and Future Work  \nSummary:  \n- SEQ3 tends to copy the first words of the input sentence.  \n- This issue is linked to the autoregressive nature of the reconstructor.  \n- Early errors can lead to cascading effects in the output.  \n- Future work could modify SEQ3 to address this limitation.  \n- Potential applications include unsupervised machine translation and caption generation.  ",
    "Slide Topic: Introduction to Domain-Specific Sentiment Analysis  \nSummary:\n- Word choice for opinions varies by domain.\n- 'Entertaining' and 'boring' are common in movies, rare in electronics.\n- Some words change sentiment across domains, e.g., 'unpredictable.'\n- 'Unpredictable' is positive in movies, negative in automobiles.\n- Supervised algorithms struggle to generalize across domains.\n- Cross-domain performance often degrades.\n\nSlide Topic: Challenges in Cross-Domain Sentiment Classification  \nSummary:\n- Supervised learning requires re-training for each new domain.\n- Manual annotation of data is costly and time-consuming.\n- Numerous domains make manual re-training impractical.\n- Transfer learning reduces labeled data needs but has limitations.\n- Algorithms often ignore words with changing polarity across domains.\n- Changing polarity words negatively impact cross-domain performance.\n\nSlide Topic: Significant Consistent Polarity (SCP) Words  \nSummary:  \n- SCP words have consistent polarity across domains, aiding cross-domain sentiment analysis.  \n- \u03c7\u00b2 test is effective for identifying word significance and polarity in labeled corpora.  \n- No statistical technique exists for unlabeled corpora.  \n- Identifying SCP words in unlabeled data is challenging.\n\nSlide Topic: Proposed SCP Extraction Technique  \nSummary:\n- Introduces a novel technique using \u03c7\u00b2 test and cosine-similarity.\n- Identifies Significant Consistent Polarity (SCP) words across domains.\n- Analyzes significance and polarity of words in their respective domains.\n- Selects words significant in both domains with consistent polarity.\n- Initiates cross-domain adaptation algorithm using these words.\n\nSlide Topic: Ensemble-Based Adaptation Algorithm  \nSummary:\n- Proposes an ensemble-based adaptation algorithm.\n- Uses a classifier trained on SCP words in the source domain as a seed.\n- Initiates a classifier on target-specific features.\n- Combines classifiers in a weighted ensemble.\n- Enhances cross-domain classification performance.\n- Overcomes errors of individual classifiers.\n\nSlide Topic: Comparison with Existing Methods  \nSummary:  \n- Our approach shows statistically significant improvement over SCL and common unigrams.  \n- It enhances the identification of transferable words.  \n- This leads to a more accurate sentiment classifier in the target domain.  \n- SCP words from our method represent more accurate transferable information.  \n- Our approach outperforms SCL and common unigrams in accuracy.  \n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Compared SCP-based and SCL-based domain adaptation across four domains: Electronics, Kitchen, Books, and DVD.  \n- SCP words extracted from training data.  \n- Weights for classifiers based on target domain validation accuracy.  \n- SCP approach outperforms SCL and common unigrams.  \n- Achieved a strong positive correlation of 0.78 between F-score and cross-domain accuracy.  \n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- SCP words represent transferable information for cross-domain sentiment classification.  \n- A strong positive correlation of 0.78 was observed with sentiment classification accuracy.  \n- Our technique uses \u03c7\u00b2 test and cosine-similarity for accurate SCP word identification.  \n- It outperforms SCL and common unigrams in identifying SCP words.  \n- Future work could enhance cross-domain sentiment classification accuracy further.  ",
    "Slide Topic: Introduction to Semantic Parsing and Graph-Structured Representations  \nSummary:\n- Recent years have seen increased interest in semantic parsing.\n- Progress is driven by graph-structured semantic representations.\n- Key representations include AMR, EDS, and DMRS.\n- These are grounded in type-logical semantics.\n- Linguistic elements are treated as roots in semantic graphs.\n- This flat structure is crucial for modeling natural language semantics.\n\n**Slide Topic: Challenges in Existing DAG Transducers for NLG**\n\n**Summary:**\n- The current DAG transducer by Quernheim and Knight is for DAG-to-tree transformation.\n- It is unsuitable for meaning graphs from type-logical semantics.\n- Lacks ability to reverse edge directions and handle multiple roots.\n- Struggles with unconnected nodes with direct dependencies.\n- These limitations hinder generating natural language sentences.\n\nSlide Topic: Proposed Novel DAG Transducer Design  \nSummary:\n- Introduces a novel DAG transducer for graph-to-program transformation.\n- Converts semantic graphs into programs using a declarative language.\n- Executing the program generates a surface string easily.\n- Enhances flexibility in handling diverse semantic graph structures.\n- Can be extended to other linguistic structures.\n\n**Slide Topic: Declarative Programming for Graph-to-String Transformation**\n\nSummary:\n- Utilizes declarative programming to specify what the program should achieve.\n- Transforms type-logical semantic graphs into strings.\n- Executes transformation through program execution.\n- Syntax includes string concatenation and variable assignments.\n- Enables construction of target structures efficiently.\n\nSlide Topic: DAG Transduction-based Natural Language Generation  \nSummary:\n- Different languages have unique morphosyntactic properties affecting NLG.\n- Our DAG transducer converts semantic graphs into sequential lemmas.\n- It considers syntactic and semantic properties during conversion.\n- A neural sequence-to-sequence model generates final surface strings.\n- This model addresses morpho-syntactic properties effectively.\n- The two-step strategy provides a unified framework for DAG-based NLG.\n\nSlide Topic: Inducing Transduction Rules from Semantic Graphs  \nSummary:  \n- Conducted an empirical study on DAG transduction-based NLG using variable-free MRS representations, specifically EDS.  \n- Utilized the DeepBank 1.1 dataset for analysis.  \n- Introduced constraints to ensure reasonable string generation.  \n- Employed a fine-to-coarse strategy to enhance robustness.  \n- Extracted induced rules from training data.  \n- Developed extended and dynamic rules to address data sparseness and decoding challenges.  \n\nSlide Topic: Evaluation and Analysis of the DAG Transducer  \nSummary:\n- Demonstrated effectiveness of DAG transducer on DeepBank 1.1 for NLG.\n- Achieved a BLEU-4 score of 68.07 with high accuracy and efficiency.\n- Generates over 5 sentences per second on average.\n- Outperforms pure sequence-to-sequence models.\n- Confirms utility in handling EDS graphs and achieving full-coverage robustness.\n\nSlide Topic: Conclusion and Future Implications  \nSummary:  \n- Extended work on DAG automata with a flexible DAG transducer.  \n- Utilizes declarative programming to reduce computation burden.  \n- System is accurate, efficient, and robust for NLG tasks.  \n- Confirms the usefulness of DAG transducers in NLP.  \n- Design benefits NLP tasks involving graph manipulation.  \n- Demonstrated by semantic-graph-to-string task.  ",
    "Slide Topic: Introduction to Machine Translation Challenges and Solutions  \nSummary:  \n- Statistical Machine Translation (SMT) is widely used but costly due to the need for large parallel corpora.  \n- Rule-Based Machine Translation (RBMT) offers a solution using a monolingual corpus in the target language.  \n- RBMT selects target words based on co-occurrence relationships from the monolingual corpus.  \n- Word sense disambiguation is developed for Japanese-Korean and Korean-Japanese translations.\n\nSlide Topic: Overview of Toshiba's Rule-Based Machine Translation System  \nSummary:\n- The RBMT system is a transfer-based machine translation framework.\n- It includes morphological, syntactic, and semantic analysis.\n- Key processes: target word selection, structural transfer, and generation.\n- Supported by extensive rules and word dictionaries for high performance.\n- Offers customization flexibility for various translation needs.\n- Syntactic analysis and transfer are omitted for Japanese-Korean translation due to grammatical similarities.\n\nSlide Topic: Statistical Post Editing and Its Role in Domain Adaptation  \nSummary:\n- SPE uses phrase-based SMT to adapt translation output to target domains.\n- Source sentences from ASPEC and JPO Patent Corpus were initially translated using RBMT.\n- A phrase-based model was trained between translated and reference sentences using Moses.\n- Training involved 1M sentences for ja-en, en-ja, JPC zh-ja, and JPC ko-ja, and 0.67M for ja-zh and zh-ja.\n- Language models were trained using KenLM with specific distortion limits for different language pairs.\n\nSlide Topic: System Combination Using Recurrent Neural Network Language Models  \nSummary:\n- SPE and SMT generate different translation candidates using statistical models.\n- System combination achieved through n-best reranking with RNN Language Models.\n- Combined 100-best candidates from SPE and SMT for the dev-set.\n- Tuning performed with MERT by incorporating RNNLM scores.\n- RNNLMs trained with 500 hidden layers and 50 classes.\n- Training utilized 500k sentences from the training data.\n\nSlide Topic: Preprocessing and Postprocessing Techniques for Improved Translation  \nSummary:\n- Selected technical term dictionaries for each task.\n- Used additional patent dictionaries for specific language pairs.\n- Applied English word correction for misspelled words.\n- Performed Japanese KATAKANA normalization for standardization.\n- Translated unknown words in SMT results using RBMT dictionaries.\n- Reduced unknown words and improved translation quality.\n\nSlide Topic: Experimental Results and Performance Metrics  \nSummary:  \n- SPE improved RBMT translation results on BLEU and RIBES scores.  \n- Reranking enhanced BLEU scores by 0.3-0.5 in most tasks.  \n- System combination (COMB) outperformed other systems.  \n- In JPOko-ja, SMT, SPE, and COMB achieved nearly 70 BLEU.  \n- SMT and reranking achieved the highest scores.  \n- Tables 1 and 2 show BLEU and RIBES scores, highlighting system combination effectiveness.  \n\nSlide Topic: Official Results and Human Evaluation Insights  \nSummary:  \n- Toshiba systems ranked in the top three for ja-en, ja-zh, and JPO zh-ja tasks.  \n- Ja-zh achieved the highest human evaluation score despite a lower BLEU score.  \n- JPO ko-ja had a high BLEU score but a lower human evaluation score.  \n- System combination improved human scores for Chinese translations in scientific documents.  \n- Results suggest complementary translation capabilities.  \n\nSlide Topic: Discussion on Evaluation Discrepancies and System Improvements  \nSummary:\n- Analysis of crowdsourcing results showed evaluation discrepancies.\n- Online A scored high in HUMAN evaluation but was outperformed by COMB in pair-wise tests.\n- Discrepancies may stem from handling number expressions, like brackets in JPOko-ja.\n- Highlighted the need for a multi-system combination for practical use.\n- Emphasized leveraging unique characteristics of each system.",
    "Slide Topic: Introduction to the ACL Anthology  \nSummary:\n- The ACL Anthology provides open access to ACL-sponsored conference proceedings and journal articles.\n- It hosts additional computational linguistics literature from sister organizations.\n- Supports text and faceted search of indexed papers and author-specific pages.\n- Incorporates third-party metadata and services.\n- Hosts over 43,000 papers and handles over 4,500 daily requests.\n- Code is available under a Creative Commons license, maintained by volunteers coordinated by the Anthology editor.\n\nSlide Topic: Current State and Technical Infrastructure  \nSummary:  \n- The ACL Anthology was proposed in 2001 and launched in 2002, with a second version in 2012.  \n- It provides access to papers in PDF format and metadata in multiple formats.  \n- The technology stack includes Ruby on Rails, Solr, PostgreSQL, Nginx/Jetty, and Debian GNU-Linux.  \n- Hosted at Saarland University, with future hosting planned under the ACL umbrella.  \n- The process of importing and indexing new papers is semi-automated and integrated with ACL conference management software.  \n- Scripts are used for processing metadata and PDFs.  \n\nSlide Topic: Community-Driven Development and Challenges  \nSummary:\n- The Anthology operates as a community project with volunteers from ACL conferences.\n- Researchers often develop new features using the Anthology as a resource.\n- Volunteer involvement is unpredictable, challenging operational knowledge preservation.\n- The Anthology editor ensures continuity but poses a single point of failure risk.\n- Efforts are underway to improve documentation for smoother volunteer transitions.\n- ACL Executive considers commercial support, but volunteer-driven development is preferred.\n\nSlide Topic: Future Proofing and Technological Upgrades  \nSummary:\n- Code, documentation, and feature requests are hosted on GitHub.\n- Improved documentation reduces the learning curve for new volunteers.\n- Core code remains stable; addressing challenges from dependency deprecation.\n- Replacing deprecated libraries and using Docker containers for better security.\n- Docker simplifies server setup and enhances security through virtualization.\n- Development versions of Docker images enable easy mirroring and experimentation.\n\nSlide Topic: Security and Stability Concerns  \nSummary:  \n- Older dependencies increase exposure to unpatched bugs.  \n- No personal data stored, but server compromise risks exist.  \n- Docker containers abstract server setup and manage dependencies.  \n- Hosting institutions can apply their own security policies.  \n- Larger network of mirrors enhances robustness and reduces downtime.\n\n**Slide Topic: Potential Extensions: Anonymous Pre-prints and Reviewer Matching**\n\n**Summary:**\n- The Anthology faces challenges with non-anonymous pre-prints, which can introduce bias.\n- ACL executive has adopted guidelines for double-blinded submissions.\n- Anonymous pre-prints are being considered to address bias.\n- Two options: collaborate with an existing pre-print service or host within the Anthology.\n- Reviewer matching can leverage Anthology data to support conference organizers.\n- Anthology could provide data on reviewer interests and expertise to improve peer review assignments.\n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- The ACL Anthology is a crucial resource for the NLP community.  \n- Enhanced documentation and server setup will aid volunteer recruitment and training.  \n- Community members are encouraged to experiment with Anthology images and contribute.  \n- Key needs include system administration, software development, and database management.  \n- Docker integration is a current focus area for improvement.  \n- Open invitation for community involvement to enhance capabilities and address challenges.  ",
    "Slide Topic: Introduction to Satire and Satire Detection  \nSummary:\n- Satire is an art form that critiques through humor, irony, and exaggeration.\n- It uses stylistic devices like parody and caricature for entertainment.\n- Satirical news mimics regular news but doesn't aim to deceive.\n- Satire detection distinguishes satirical from regular news, a challenging task.\n- Automatic satire detection systems aid in understanding satire's characteristics.\n\nSlide Topic: Challenges in Automatic Satire Detection  \nSummary:  \n- Satire detection is difficult even for human readers.  \n- Models often rely on corpora labeled by publication sources.  \n- This can lead to learning publisher traits instead of satire.  \n- Misleading model interpretation affects generalization.  \n- Generalization issues arise with unseen sources.  \n\n**Slide Topic: Limitations of Previous Satire Detection Methods**\n\nSummary:\n- Relied heavily on handcrafted features.\n- Primarily focused on English satire detection.\n- Classifiers often learned publication-specific patterns.\n- Risk of misclassifying non-satire as satire.\n- Lack of publication source data limits effectiveness analysis.\n\nSlide Topic: Proposed Model with Adversarial Training  \nSummary:  \n- Introduce a model using adversarial training to address publication source bias.  \n- Incorporate a satire detector and a publication identifier.  \n- Both components share a common feature extractor.  \n- Adversarial training updates parameters to mislead the publication identifier.  \n- Ensures the model emphasizes satire characteristics.\n\nSlide Topic: Adversarial Training and Its Impact on Satire Detection  \nSummary:\n- Adversarial training helps models focus on satire, not publication traits.\n- Experiments show satire detection remains effective with adversarial training.\n- Publication identification performance decreases with adversarial components.\n- The model still learns to identify satire effectively.\n\nSlide Topic: Creation and Utilization of a New German Satire Dataset  \nSummary:\n- Introduced a large German dataset for satire detection, the first of its kind.\n- Includes publication sources to enhance research capabilities.\n- Enables experiments to test if models learn publisher traits over satire.\n- Comprises nearly 330k articles for comprehensive analysis.\n- Offers realistic representation of publication-specific phrases and common words.\n\nSlide Topic: Evaluation Metrics and Results Analysis  \nSummary:  \n- Evaluation metrics: precision, recall, and F1 score.  \n- Adversarial training model matches baseline in satire detection.  \n- Significant drop in publication identification precision.  \n- Indicates overlapping features in satire and publication classification.  \n- Suggests tasks cannot be completely separated.  \n\nSlide Topic: Future Directions and Potential Confounding Variables  \nSummary:  \n- Explore additional confounding variables in satire detection.  \n- Consider the impact of time and region distribution of articles.  \n- Conduct more quantitative analyses for deeper insights.  \n- Implement qualitative analyses to understand classifier behavior.  \n- Evaluate different classifier configurations for improved accuracy.",
    "Slide Topic: Introduction to Zero-Shot Learning in Text Classification  \nSummary:\n- Zero-shot learning (ZSL) classifies unseen classes without prior examples.\n- Inspired by human ability to transfer knowledge to new domains.\n- Utilizes high-level descriptions and semantic representations.\n- Leverages semantic knowledge like class descriptions and relationships.\n- Infers features of unseen classes from patterns in seen classes.\n- Employs semantic attributes, concept ontology, and word embeddings.\n\nSlide Topic: Challenges and Limitations of Existing Approaches  \nSummary:\n- Traditional models struggle with new classes in dynamic environments.\n- Zero-shot learning often depends on semantic links, limiting strict zero-shot scenarios.\n- Some methods use similar classes in training and testing, not truly testing zero-shot capabilities.\n- Few studies explore combining different semantic knowledge types in zero-shot text classification.\n\nSlide Topic: Proposed Two-Phase Framework for Zero-Shot Text Classification  \nSummary:\n- Introduces a two-phase framework for zero-shot text classification.\n- Comprises coarse-grained and fine-grained classification phases.\n- No need for semantic correspondence between training and inference classes.\n- Utilizes convolutional neural networks and four types of semantic knowledge.\n- Integrates word embeddings, class descriptions, class hierarchy, and ConceptNet.\n- First phase identifies seen or unseen classes; second phase specifies the class.\n\nSlide Topic: Data Augmentation through Topic Translation  \nSummary:\n- Introduces a novel data augmentation technique called topic translation.\n- Enhances detection of documents from unseen classes.\n- Translates content from a seen class to an unseen class using analogy.\n- Utilizes word embeddings to solve analogy questions for translation.\n- Creates augmented examples for unseen classes.\n- Augmented examples serve as additional negative samples in training.\n\nSlide Topic: Feature Augmentation Using Semantic Knowledge  \nSummary:\n- Feature augmentation leverages semantic knowledge to transfer insights from seen to unseen classes.\n- A relationship vector indicates word-class relatedness using the ConceptNet knowledge graph.\n- This vector is built by analyzing connections between words and class-related nodes within a set number of hops.\n- Augmented features enhance zero-shot classifiers by providing additional information.\n- This approach improves the framework's ability to generalize zero-shot reasoning.\n\nSlide Topic: Experimental Setup and Datasets  \nSummary:  \n- Experiments used DBpedia and 20 newsgroups datasets.  \n- DBpedia includes 14 non-overlapping classes from Wikipedia.  \n- 20 newsgroups dataset contains 20 distinct topics.  \n- Tested with 50% and 25% unseen class rates.  \n- Results averaged over ten random selections of class groups.  \n- Classifiers implemented with TensorLayer, TensorFlow, and GloVe embeddings.  \n\n**Slide Topic: Evaluation Metrics and Baseline Comparisons**\n\nSummary:\n- Compared framework performance with baselines like DOC, RNN AutoEncoder, and CNN + FC.\n- Phase 1 evaluated using binary classification accuracy.\n- Phase 2 and overall framework assessed with multi-class classification accuracy.\n- Achieved competitive accuracy on unseen classes.\n- Maintained accuracy on seen classes.\n- Highest overall accuracy compared to baselines.\n\nSlide Topic: Results and Discussion on Framework Performance  \nSummary:\n- Phase 1 showed the framework outperformed DOC in detecting seen classes.\n- Improved accuracy on unseen classes with augmented data.\n- Phase 2's zero-shot classifier achieved highest accuracy using combined embeddings.\n- Semantic knowledge integration effectively transferred knowledge to unseen classes.\n- Overall framework achieved highest accuracy across all settings.\n- Demonstrated effectiveness in zero-shot text classification.\n\nSlide Topic: Related Work in Zero-Shot Learning and Data Augmentation  \nSummary:\n- Traditional zero-shot text classification uses semantic relationships via embeddings.\n- The new framework utilizes word embeddings and knowledge graphs for augmentation.\n- Data augmentation in NLP is less common than in computer vision.\n- Recent studies explore augmentation in machine translation and text classification.\n- The technique substitutes unavailable labeled data for unseen classes.\n\n**Slide Topic: Conclusion and Future Directions**\n\nSummary:\n- The CNN-based two-phase framework effectively addresses zero-shot text classification.\n- Data and feature augmentation enhance the detection of unseen classes.\n- Topic translation improves knowledge transfer capabilities.\n- The framework achieves the highest accuracy compared to baselines.\n- Future work includes extending to multi-label classification with larger datasets.\n- Explore the use of linguistically defined semantic units in zero-shot scenarios.",
    "Slide Topic: Introduction to Natural Language Inference (NLI)  \nSummary:  \n- NLI is a key task in natural language understanding.  \n- It involves analyzing two sentences: premise and hypothesis.  \n- The goal is to identify the relationship: entailment, neutral, or contradiction.  \n- Recent advancements are due to high-quality datasets like SNLI and MultiNLI.\n\nSlide Topic: Role of Discourse Markers in NLI  \nSummary:  \n- Most research focuses on premise-hypothesis interaction, overlooking discourse relations.  \n- Discourse relations are crucial for natural language understanding.  \n- Certain words, like 'but' and 'and', express these relations and are called discourse markers.  \n- Discourse markers connect sentence relations and align with NLI intents.  \n- Examples include 'but' for contradiction and 'so' for entailment.  \n\nSlide Topic: Discourse Marker Augmented Network  \nSummary:\n- Introduces a network for natural language inference (NLI).\n- Transfers knowledge from Discourse Marker Prediction (DMP) task.\n- Proposes a sentence encoder model for learning sentence representations.\n- Integrates the encoder into the NLI network.\n- Solves NLI by leveraging knowledge from a supervised task.\n\nSlide Topic: Sentence Encoder Model and Training  \nSummary:  \n- Use BookCorpus dataset for training to ensure domain neutrality.  \n- Transform sentences into vectors using GloVe, word by word.  \n- Input vectors into a bi-directional LSTM for processing.  \n- Apply max pooling on concatenated hidden states for feature extraction.  \n- Project combined representation to a vector of label size.  \n- Normalize probability distribution using the softmax function.  \n\nSlide Topic: Reinforcement Learning for Label Confidence  \nSummary:\n- NLI datasets are manually annotated, leading to varied initial labels.\n- Annotators reach a consensus, but label confidence varies.\n- Reinforcement learning is used to address label confidence.\n- A reward is defined by the uniformity of original labels.\n- A new objective function combines log probabilities and dataset properties.\n- This approach trains the model to better handle label confidence.\n\nSlide Topic: Experimental Setup and Datasets  \nSummary:  \n- Pre-trained sentence encoder using BookCorpus dataset.  \n- Preprocessed data to collect discourse markers.  \n- SNLI dataset: 570k human-annotated sentence pairs.  \n- SNLI labels: entailment, contradiction, semantic independence.  \n- MultiNLI: 433k sentence pairs, diverse text range.  \n- MultiNLI includes auxiliary test set for cross-genre evaluation.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:  \n- Ensemble results presented for two datasets.  \n- Model consists of 10 single models with identical architecture.  \n- Models initialized with different parameters.  \n- Achieved 89.6% on SNLI dataset.  \n- Scored 80.3% on matched MultiNLI dataset.  \n- Reached 79.4% on mismatched MultiNLI dataset.  \n\nSlide Topic: Ablation and Semantic Analysis  \nSummary:\n- Conducted ablation experiment on SNLI development dataset.\n- Evaluated individual contributions of model components.\n- Sentence embedding from discourse markers alone is unsatisfactory.\n- Significant performance drop when sentence encoder is removed.\n- Discourse markers deeply connect logical relations between sentences.\n- Showed performance on three relation labels with different pre-trained discourse marker sets.",
    "Slide Topic: Introduction to Metaphor Processing  \nSummary:\n- Metaphors enrich language and enhance communication, cognition, and decision-making.\n- Computational metaphor processing models non-literal expressions like metaphor and metonymy.\n- It improves NLP tasks such as Machine Translation and Sentiment Analysis.\n- Example: Google Translate struggles with metaphor translation, e.g., \"devour\" in \"She devoured his novels.\"\n- Word embeddings are popular for metaphor identification, avoiding reliance on hand-crafted knowledge.\n\nSlide Topic: Challenges in Metaphor Identification  \nSummary:\n- Metaphor identification often uses word embeddings to parse sentences into phrases.\n- These methods struggle with identifying word-level metaphors.\n- Important contextual information is frequently discarded.\n- This leads to failures when both words in a phrase are metaphorical.\n- Example: \"climb the social ladder\" is parsed as \"climb ladder,\" appearing literal.\n\nSlide Topic: Proposed Unsupervised Metaphor Processing Model  \nSummary:  \n- Introduces an unsupervised model for word-level metaphor identification and interpretation.  \n- Utilizes word embedding methods and WordNet for lexical relations.  \n- Operates without constraints on metaphor source domains or labeled training data.  \n- Identifies metaphors by modeling the distance between literal and metaphorical senses.  \n- Functions at the sentence level without preprocessing.  \n\nSlide Topic: Technical Methodology and Framework  \nSummary:\n- Framework based on two hypotheses: domain difference and frequency of literal senses.\n- Hypothesis 1: Metaphorical words differ in domain from their literal sense.\n- Hypothesis 2: Literal senses are more common in corpora than metaphorical ones.\n- Process involves training word embeddings and separating target and context words.\n- Constructs candidate word set and identifies best fit using cosine similarity.\n- Utilizes CBOW and Skip-gram embeddings to enhance performance.\n\nSlide Topic: Evaluation and Experimental Design  \nSummary:  \n- Evaluated model against three strong baselines for metaphor identification.  \n- Conducted extensive experiments on a publicly available dataset.  \n- Model significantly outperforms unsupervised learning baselines in phrase and sentence evaluation.  \n- Additional experiments show metaphor processing supports machine translation (MT).  \n- Significant improvements observed in metaphoric translation tasks.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:  \n- Our model outperforms unsupervised baselines in sentence and phrase evaluations.  \n- Achieves highest F1 score of 75% in sentence-level metaphor identification with SIM-CBOW I+O.  \n- SIM-SG I+O excels in phrase-level evaluation, surpassing word embedding methods.  \n- Matches performance of deep supervised methods in phrase-level tasks.  \n- Results are stable across different thresholds.  \n- Enhances translation accuracy in machine translation systems.  \n\nSlide Topic: Application in Machine Translation  \nSummary:  \n- Conducted experiments on metaphor processing in English-Chinese translation.  \n- Paraphrased metaphorical words with best-fit alternatives.  \n- Significant improvement in translation engine performance.  \n- Dramatic improvement for metaphorical sentences in Google and Bing Translate.  \n- Overall translation performance improved by 11% for Google and 9% for Bing.  \n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Proposed a framework for identifying and interpreting metaphors at the word level.  \n- Utilized an unsupervised learning approach to improve translation accuracy.  \n- Demonstrated the effectiveness of using WordNet for paraphrasing metaphors.  \n- Future work will introduce weighted CBOW and Skip-gram models.  \n- Aim to learn positional information within sentences.  \n- Enhance metaphor processing in practical tasks.  ",
    "Slide Topic: Introduction to Neural Vector Representations  \nSummary:\n- Neural vector representations are essential in natural language processing.\n- Word vectors exhibit properties like linear substructures and mutual information.\n- Sentence embeddings convert variable-length sentences into fixed-length vectors.\n- Evaluation often relies on cosine similarity reflecting human semantic judgments.\n- Datasets like SICK are used for assessing semantic relatedness.\n- Semantic properties of sentence embeddings are still largely unexplored.\n\nSlide Topic: Properties and Evaluation of Sentence Embeddings  \nSummary:\n- Sentence embeddings predict word occurrence, order, and length.\n- Used in classification tasks like sentiment and question type.\n- Conneau et al. studied syntactic and semantic classification.\n- Specific semantic properties in embeddings remain unclear.\n- Our framework uses sentence triplets to analyze syntactic and semantic changes.\n\nSlide Topic: Sentence Modification Schemes  \nSummary:  \n- Transformations generate altered sentences for semantic analysis.  \n- Techniques include Not-Negation, Quantifier-Negation, and Synonym Substitution.  \n- Other methods: Embedded Clause Extraction, Passivization, Argument Reordering.  \n- Fixed Point Inversion is also used for sentence modification.  \n- Each transformation tests different semantic aspects like negation detection.  \n- Focus is on altering structure while maintaining or changing meaning.  \n\n**Slide Topic: Sentence Triplet Generation and Evaluation Datasets**\n\nSummary:\n- Five evaluation datasets are created: Negation Detection, Negation Variants, Clause Relatedness, Argument Sensitivity, and Fixed Point Reorder.\n- Each dataset tests specific semantic properties of sentence embeddings.\n- Focus areas include distinguishing similar structures with opposite meanings and assessing semantic relatedness despite structural differences.\n- Datasets are derived from sources like SICK and Penn Treebank WSJ.\n- Human annotators ensure grammaticality and semantic accuracy.\n\nSlide Topic: Embedding Methods Compared  \nSummary:  \n- Compare three sentence embedding methods: GloVe Averaging, Concatenated P-Mean Embeddings, and Sent2Vec.  \n- GloVe Averaging computes the average of word vectors.  \n- P-Means concatenates different p-means of word vectors.  \n- Sent2Vec learns word and n-gram embeddings for quality sentence vectors.  \n- Discuss Skip-Thought Vector using neighbor prediction at the sentence level.  \n- InferSent uses supervision from tasks like the Stanford NLI dataset.  \n\nSlide Topic: Results and Discussion on Negation Detection  \nSummary:\n- Table 2 presents results for the Negation Detection dataset.\n- S, S+, S* represent original, Synonym Substitution, and Not-Negation versions.\n- GloVe Avg. often confuses synonyms, affecting accuracy.\n- InferSent and SkipThought effectively distinguish negated from unnegated sentences.\n- Accuracy is measured by similarity between original and synonym-substituted versions versus Not-Negation.\n- InferSent and SkipThought excel in negation detection.\n\nSlide Topic: Analysis of Negation Variants and Clause Relatedness  \nSummary:\n- Table 3 analyzes Negation Variants: S, S+, S*.\n- InferSent excels in understanding negation quantifiers.\n- Table 4 examines Clause Relatedness.\n- Sent2Vec effectively distinguishes embedded clauses from negations.\n- SkipThought's accuracy varies with sentence structure, performing better with similar word order.\n- InferSent's performance also varies based on sentence structure.\n\nSlide Topic: Argument Sensitivity and Fixed Point Reorder  \nSummary:\n- Table 5 examines Argument Sensitivity with original, Passivization, and Argument Reordering versions.\n- None of the approaches effectively separate semantic from structural information.\n- Table 6 explores Fixed Point Reorder with original, semantically equivalent, and Fixed Point Inversion versions.\n- Sentence embeddings using means or concatenation fail to distinguish fixed point inverted sentences.\n- SkipThought and InferSent perform better with similar structures.\n\n**Slide Topic: Conclusion on Sentence Embedding Methods**\n\nSummary:\n- Proposed method inspects sentence embeddings for semantic properties.\n- Analyzed three popular methods: SkipThought, InferSent, and neural networks.\n- SkipThought and InferSent distinguish negation from synonymy.\n- InferSent excels in identifying semantic equivalence regardless of word order.\n- SkipThought is effective when sentence semantics align with structure but struggles with varied word order.\n- Neural network-based embeddings outperform simple word embedding averaging.",
    "Slide Topic: Introduction to Sentence Acceptability and Grammaticality  \nSummary:\n- Sentence acceptability measures how natural a sentence feels to native speakers.\n- It includes semantic, syntactic, and pragmatic factors, plus non-linguistic elements like memory limits.\n- Grammaticality focuses on the syntactic correctness of a sentence.\n- Grammaticality is a theoretical concept, challenging for non-experts to assess.\n- The research aims to predict acceptability judgments, building on Lau et al.'s methods.\n- This study explores how context affects human acceptability judgments.\n\nSlide Topic: Impact of Document Context on Acceptability Judgements  \nSummary:  \n- Constructed a dataset of sentences with acceptability ratings, with and without context.  \n- Extracted 100 random articles from English Wikipedia, sampling one sentence per article.  \n- Found a strong correlation (Pearson's r = 0.80) between ratings with and without context.  \n- Adding context generally improves acceptability, especially for ill-formed sentences.  \n- For highly acceptable sentences, context can reduce rated acceptability.\n\nSlide Topic: Experimental Design and Data Collection  \nSummary:  \n- Used Moses MT system for round-trip translation of sentences.  \n- Collected acceptability judgments via Amazon Mechanical Turk.  \n- Employed a 4-point scale for acceptability ratings.  \n- Conducted annotation tasks with and without context.  \n- Gathered 20 judgments per sentence, totaling 20,000 annotations.  \n- Ensured reliability by presenting sentences in groups of five.  \n\nSlide Topic: Analysis of Human Judgements with and without Context  \nSummary:  \n- Strong correlation (Pearson's r = 0.80) between ratings with and without context.  \n- Adding context compresses the distribution of mean ratings.  \n- Context shifts extreme ratings towards the middle, reducing separability.  \n- This compression lowers correlation between good and bad sentences.  \n- Workers focus more on global semantic and pragmatic coherence with context.  \n\nSlide Topic: Modelling Sentence Acceptability with Enriched Language Models  \nSummary:\n- Lau et al. (2015, 2016) explored unsupervised models for sentence acceptability.\n- Models included n-gram, Bayesian HMMs, LDA-based, and simple recurrent networks.\n- This paper experiments with neural models, specifically LSTM and TDLM.\n- TDLM is a joint model of topic and language.\n- The topic model uses a convolutional layer and trainable topic embeddings.\n- The language model combines topic vectors with LSTM's hidden state for context.\n\nSlide Topic: Performance Evaluation of Language Models  \nSummary:  \n- Pearson's r computed against mean human ratings for performance assessment.  \n- Achieved a correlation of 0.584 with lstm- against h-, similar to previous reports.  \n- SLOR outperforms all other measures, aligning with Lau et al. (2015) findings.  \n- Using context at test time improves model performance across all models.  \n- Context consideration enhances modeling acceptability, with or without context-based judgments.  \n\nSlide Topic: Comparison with Related Work  \nSummary:\n- Extensive research exists on grammatical error detection.\n- Limited studies focus on acceptability prediction.\n- Heilman et al. (2014) used a linear regression model with features like spelling errors and n-gram scores.\n- Lau et al. (2015, 2016) found a simple RNN to be the best performing in unsupervised learning.\n- Both studies predict acceptability without considering external contextual factors.\n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Context enhances acceptability for ill-formed sentences but reduces it for well-formed ones.  \n- Incorporating context improves model performance.  \n- Prediction accuracy drops with context-based judgments, challenging our initial hypothesis.  \n- Human agreement decreases with context, making ratings less predictable.  \n- Future work will explore this issue and test a broader range of models.  \n- We will experiment with sentence embedding methodologies.  ",
    "Slide Topic: Introduction to Tree-Based NMT Systems  \nSummary:  \n- NMT advancements include string-to-string, tree-to-string, and string-to-tree systems.  \n- Tree-based systems use syntactic information and prior knowledge.  \n- They are a key focus in NMT research.  \n- Represented by tree-structured neural networks or linearization.  \n- Linearization is computationally simpler for large corpora.  \n\nSlide Topic: Challenges in Forest-Based NMT  \nSummary:\n- Tree-based NMT systems rely on the 1-best parse tree.\n- This reliance can cause translation errors due to parsing inaccuracies.\n- Forest-based methods use packed forests to represent multiple parse trees.\n- These methods are complex due to no fixed traversal order.\n- Hyperedge-attachment nature of weights adds to complexity.\n- Complexity hinders efficient development of forest-based NMT systems.\n\nSlide Topic: Proposed Forest-Based NMT Approach  \nSummary:  \n- Inspired by tree-based NMT methods.  \n- Proposes a forest-based NMT approach.  \n- Encodes syntactic information from a packed forest.  \n- Utilizes a novel weighted linearization method.  \n- Allows decoding under a simple sequence-to-sequence framework.  \n- Enhances translation accuracy using syntactic richness.  \n\nSlide Topic: Linearization of Packed Forests  \nSummary:\n- Linearizing a packed forest is challenging due to its DAG structure.\n- The edge-based weight system adds complexity.\n- We propose a novel algorithm preserving word order and parent-child relationships.\n- Operators are used to combine different parts of the forest.\n- This method effectively encodes syntax information.\n- It enhances Neural Machine Translation (NMT) performance.\n\nSlide Topic: Encoding and Decoding in Forest-Based NMT  \nSummary:\n- Linearized packed forest is used as input to the encoder.\n- This approach differs from traditional sequence-to-sequence models.\n- It incorporates both symbol and score sequences.\n- Two frameworks are proposed: Score-on-Embedding (SoE) and Score-on-Attention (SoA).\n- These frameworks integrate sequences to leverage syntactic information.\n- Enhances NMT system's encoding and decoding processes.\n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Evaluated forest-based NMT systems on English-to-Chinese and English-to-Japanese tasks.  \n- Used character-level BLEU scores for performance assessment.  \n- Models, especially the SoA framework, outperformed string-to-string and tree-based baselines.  \n- Demonstrated the utility of syntactic information in improving translation accuracy.  \n- Highlighted the effectiveness of packed forests in enhancing model performance.  \n\nSlide Topic: Qualitative Analysis of Translation Outputs  \nSummary:  \n- Packed forests enhance grammatical accuracy in translations.  \n- They improve attention to detail in translation outputs.  \n- Phrases ignored in 1-best tree translations are correctly translated.  \n- Multiple syntactic structures are effectively incorporated.  \n- This method highlights the advantage of diverse syntactic analysis.  \n\nSlide Topic: Comparison with Related Work  \nSummary:  \n- Our approach efficiently encodes multiple trees using linearized packed forests.  \n- Previous methods rely on single trees or latent graphs.  \n- This technique enhances the utilization of syntactic information.  \n- Our experimental results outperform state-of-the-art tree-based systems.\n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Introduced a novel NMT framework using linear-structured neural networks.  \n- Achieved significant improvements over traditional NMT systems.  \n- Future work will refine score layer integration.  \n- Plan to apply linearization method to other tasks.  \n- Aim to further enhance translation performance.  ",
    "Slide Topic: Introduction to Semantic Graphs and Relation Prediction  \nSummary:\n- Semantic graphs encode language structure and human knowledge.\n- They connect semantic concepts through edges representing relations like hypernymy.\n- Local properties are well-studied for relation prediction using distributional methods.\n- Global properties are rarely used in prediction settings.\n- Global semantic graph features can aid local tasks like relation prediction.\n- Features from the entire graph help constrain predictions to structurally sound ones.\n\nSlide Topic: Global Graph Features in Local Relation Prediction  \nSummary:\n- Global semantic graph features enhance local relation prediction tasks.\n- These features often appear as aggregate counts of small subgraph structures, called motifs.\n- Examples include nodes with multiple outgoing edges or cycles of length 3.\n- The study explores global graph properties' impact on link structure using WordNet.\n- Findings show that entire graph features help ensure structurally sound local predictions.\n\nSlide Topic: Exponential Random Graph Models and Their Limitations  \nSummary:\n- ERGMs are log-linear models for networks using global graph features.\n- Graph likelihood is calculated by exponentiating a weighted sum of features.\n- Normalization over all possible graphs grows exponentially with nodes.\n- This makes fitting ERGMs on large graphs, like WordNet, challenging.\n- Approximations are required for large graphs.\n- ERGMs currently scale only to thousands of nodes, insufficient for WordNet.\n\nSlide Topic: Max-Margin Markov Graph Model (M3GM) Framework  \nSummary:\n- M3GM extends the ERGM framework with a margin-based objective.\n- It compares observed networks against alternatives using a local relational model.\n- Estimation complexity is linear in the number of edges.\n- Scales up to 10^5 nodes, as demonstrated with WordNet.\n- Incorporates labeled motifs and links graph-level features with distributional information.\n\nSlide Topic: Graph Motifs as Features in Semantic Graphs  \nSummary:\n- Graph motifs are small subgraph structures used as features.\n- Common features include edge count, cycle count, and transitivity.\n- Semantic graphs are multigraphs with multiple relationships on common nodes.\n- Features are extracted for all combinations of relation types.\n- This approach significantly scales the number of features.\n\nSlide Topic: Parameter Estimation and Proposal Distribution  \nSummary:\n- Parameter estimation in ERGMs involves approximating the normalization term.\n- Monte Carlo Maximum Likelihood Estimation (MCMLE) is used for this process.\n- A large number of graphs are randomly generated in MCMLE.\n- Proposal distribution samples negative edges based on local association scores.\n- This method reduces the likelihood of false positives in M3GM parameters.\n\nSlide Topic: Relation Prediction and Evaluation on WN18RR Dataset  \nSummary:\n- WN18RR is a key relation prediction dataset for WordNet with 18 relations and ~41,000 synsets.\n- It addresses leakage issues from the original WN18 by removing certain relations.\n- Evaluation metrics include Mean Rank (MR), Mean Reciprocal Rank (MRR), and Hits at k (H@k).\n- The M3GM model is used as a re-ranker, enhancing performance across all metrics.\n\nSlide Topic: Analysis of M3GM's Impact on Relation Prediction  \nSummary:\n- M3GM excels in re-ranking top local candidates using score signals.\n- Results improve by weighting graph scores against association components per relation.\n- The model captures inter-relational properties, like target scattering preferences.\n- It highlights the undesirability of multiple unique hypernyms.\n- Adding graph motifs qualifying nodes by syntactic or semantic categories may be beneficial.",
    "**Introduction to Chinese Spelling Check Challenges**\n\n- Chinese spelling checkers are difficult to develop due to the lack of word delimiters.\n- The vast number of Chinese characters, each with unique contexts, adds complexity.\n- Phonological and visual similarities among multi-character words complicate the process.\n- These words are semantically distinct, making error detection challenging.\n- An effective checker must accurately identify errors and suggest corrections.\n\nSlide Topic: Overview of SIGHAN 2015 Bake-off  \nSummary:  \n- Provided a platform for developing Chinese spelling checkers.  \n- Focused on essays by learners of Chinese as a Foreign Language.  \n- Task involved identifying spelling errors and suggesting corrections.  \n- Built on experiences from SIGHAN 2013 and 2014.\n\nSlide Topic: Task Description and Objectives  \nSummary:  \n- Evaluate the capability of Chinese spelling checkers.  \n- Provide passages with or without spelling errors.  \n- Checkers identify incorrect characters and suggest corrections.  \n- Specific output formats for error-free and erroneous passages.  \n- Focus solely on spelling error correction.  \n- Exclude grammatical errors and word ordering issues.  \n\nSlide Topic: Data Preparation and Corpus Details  \nSummary:  \n- Learner corpus sourced from Test of Chinese as a Foreign Language essays.  \n- Annotated by native speakers for accuracy.  \n- Training set includes 970 essays and 3,143 errors.  \n- Dry run set provided for familiarization.  \n- Test set comprises 1,100 passages.  \n- Participants focused on spelling errors, using additional resources for error detection and correction.  \n\nSlide Topic: Evaluation Metrics and Performance Criteria  \nSummary:  \n- Performance evaluated at detection and correction levels.  \n- Used a confusion matrix to measure true positives, false positives, true negatives, and false negatives.  \n- Key metrics: precision, recall, and F1 score.  \n- Focus on reducing false positives.  \n- CAS team achieved the highest F1 score for error detection and correction.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:  \n- CAS team excelled in detection and correction, achieving high F1 scores.  \n- Utilized HMM-based approach and two-stage filter process.  \n- NCTU & NTUT performed well with word vector/conditional random field detector.  \n- No team excelled in all metrics.  \n- CAS and NCTU & NTUT showed strong overall performance.  \n\nSlide Topic: Approaches and Methodologies of Participant Teams  \nSummary:  \n- CAS team proposed a unified framework with HMM-based sentence segmentation.  \n- They implemented a two-stage filter for candidate re-ranking.  \n- NCTU & NTUT used a word vector/conditional random field-based detector.  \n- Their approach guided language model rescoring.  \n- Both methods aimed to efficiently detect and correct spelling errors.  \n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- The SIGHAN 2015 Bake-off enhanced Chinese spelling checkers.  \n- All submissions provided valuable insights.  \n- Datasets and evaluation tools are publicly available.  \n- Future work will target Chinese grammatical error correction.  \n- New resources for computer-aided language learning will be developed.  ",
    "Slide Topic: Introduction to Word Embeddings and OOV Challenges  \nSummary:  \n- Word embeddings enable generalization to unseen words using large unlabeled datasets.  \n- They embed lexical features into a low-dimensional Euclidean space.  \n- These embeddings capture distributional similarity, sharing information among contextually similar words.  \n- Complete vocabulary enumeration is impossible, leading to out-of-vocabulary (OOV) challenges.  \n- OOV issues are significant in low-resource languages with limited data availability.\n\nSlide Topic: MIMICK-RNN: A Quasi-Generative Approach  \nSummary:\n- MIMICK-RNN learns word embeddings from written forms.\n- Trains an RNN at the character level with embeddings as targets.\n- Predicts vectors for out-of-vocabulary (OOV) words in tasks.\n- Mimics a word's spelling to generate its distributional embedding.\n- Nearest-neighbor analysis shows vectors capture word-shape and lexical features.\n\nSlide Topic: Character-Based Embeddings and Nearest-Neighbor Analysis  \nSummary:\n- Character-based embeddings capture word-shape and lexical features.\n- Effective for out-of-vocabulary abbreviations, names, and errors.\n- Provides reasonable nearest neighbors for novel compounds.\n- Nearest-neighbor analysis validates the embedding protocol.\n- Tested on languages like English, Hebrew, Tamil, and Spanish.\n\nSlide Topic: Evaluation on Stanford RareWord Dataset and Universal Dependencies  \nSummary:\n- Character-based embeddings capture word similarity for rare and unseen words.\n- Experiments conducted on joint prediction of POS tags and morphosyntactic attributes.\n- Evaluation includes a diverse set of 23 languages from the Universal Dependencies dataset.\n- Model shows significant improvement over single UNK-embedding backoff method.\n\n**Slide Topic: Comparison with Related Work and Supervised Subword Models**\n\nSummary:\n- Compositional models use morphological and orthographic info for embedding rare words.\n- They predict embeddings for unseen words based on internal structure.\n- Supervised subword models learn task-specific embeddings in end-to-end systems.\n- These methods face challenges in low-resource and morphologically rich languages.\n- They also struggle with Chinese due to its large character set.\n\nSlide Topic: MIMICK Word Embeddings: Methodology and Implementation  \nSummary:\n- Approach OOV embeddings as a generation problem.\n- Assume a generative wordform-based protocol for embeddings.\n- Use Word Type Character Bi-LSTM as the predictive function.\n- Train the model to approximate pre-trained word embeddings.\n- Objective: Match predicted embeddings to pre-trained ones.\n- Minimize squared Euclidean distance for accuracy.\n\n**Slide Topic: Joint Tagging of Parts-of-Speech and Morphosyntactic Attributes**\n\nSummary:\n- Universal Dependencies use 17 POS tags with language-specific features.\n- Our approach extends POS tagging to morphosyntactic attributes.\n- We duplicate the projection layer for each attribute type.\n- A multilayer perceptron processes hidden states for each token.\n- Hidden states are generated by an underlying LSTM.\n\nSlide Topic: Experimental Settings and Results Across Languages  \nSummary:  \n- Word morphological complexity varies across languages.  \n- Selected languages represent diverse morphological patterns.  \n- Approach utilizes Polyglot word embeddings.  \n- Part-of-speech tagging measured by token-level accuracy.  \n- Performance reported in low-resource and full-resource settings.  \n- Significant improvements noted in Slavic languages and Chinese.  ",
    "Slide Topic: Introduction to Adversarial Attacks in Machine Learning  \nSummary:  \n- Adversarial attacks modify model inputs to achieve goals like misclassification.  \n- They expose model vulnerabilities to inputs outside the training distribution.  \n- These attacks pose security risks for safety-critical applications, e.g., self-driving cars.  \n- Initially focused on computer vision, they now challenge NLP due to text's discrete nature.  \n- Even minor text changes are noticeable to humans, complicating NLP adversarial attacks.  \n\nSlide Topic: Challenges of Adversarial Attacks in NLP  \nSummary:\n- Adversarial attacks in NLP are challenging due to text's discrete nature.\n- Even minor changes, like a single word, are easily noticeable.\n- Unlike images, text lacks a canonical distance metric.\n- Evaluating semantic similarity between sentences is difficult.\n- Defining adversarial perturbations is complex, with meaning preservation crucial.\n\nSlide Topic: Proposed Criterion for Adversarial Examples in NLP  \nSummary:  \n- Focuses on meaning-preserving perturbations on the source side.  \n- Emphasizes meaning-destroying effects on the target side.  \n- Contrasts with previous work lacking explicit meaning preservation evaluation.  \n- Enables evaluation of adversarial attacks using human evaluation.  \n- Allows for approximations without human intervention.  \n\nSlide Topic: Framework for Evaluating Adversarial Attacks  \nSummary:\n- Introduces a framework for evaluating adversarial attacks on seq2seq models.\n- Focuses on the trade-off between source and target similarity.\n- Evaluation includes measures of target and source similarity scores.\n- Recommends reporting both similarity scores when presenting results.\n- Framework can be extended to other tasks by adapting similarity metrics.\n\nSlide Topic: Gradient-Based Adversarial Attacks  \nSummary:  \n- Replace words in a sentence to maximize adversarial loss.  \n- Use a differentiable function for optimal word substitutions.  \n- Apply constraints like kNN and CharSwap to preserve meaning.  \n- Method is computationally efficient.  \n- Supports adversarial training.  \n\nSlide Topic: Experiments and Evaluation Metrics  \nSummary:  \n- Experiments conducted on the IWSLT 2016 dataset.  \n- Models used: LSTM and Transformer.  \n- Evaluation framework compares various adversarial attacks.  \n- chrF shows higher correlation with human judgment than BLEU and METEOR.  \n- Results highlight effectiveness of meaning-preserving attacks.  \n- Demonstrates robustness of different model architectures.  \n\nSlide Topic: Adversarial Training with Meaning-Preserving Attacks  \nSummary:  \n- Adversarial training uses adversarial examples to improve model robustness.  \n- It enhances robustness without affecting test performance.  \n- Meaning-preserving attacks, like CharSwap, are more effective than unconstrained attacks.  \n- Results show constrained adversarial training benefits model performance.\n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Emphasizes the importance of meaning-preserving adversarial perturbations in NLP models.  \n- Proposes a general evaluation framework for assessing these perturbations.  \n- Compares various automatic metrics as proxies for human judgment.  \n- Confirms that naive attacks fail to preserve meaning.  \n- Highlights the usefulness of adversarial training.  \n- Suggests future work should focus on consistent evaluation of meaning conservation.  ",
    "Slide Topic: Introduction to Semantic Role Labeling and Current Challenges  \nSummary:\n- Semantic Role Labeling (SRL) captures predicate-argument relations like 'who did what to whom.'\n- Recent SRL models use BIO-taggers to label argument spans for single predicates.\n- These models are evaluated with gold predicates and require error-prone predicate identification for deployment.\n- A major challenge is modeling overlapping spans across different predicates.\n- The proposed span-graph formulation aims to address this challenge.\n\nSlide Topic: Proposed End-to-End SRL Model and Its Advantages  \nSummary:\n- Introduces an end-to-end model for predicting predicates and argument spans in one pass.\n- Utilizes learned, contextualized span representations for direct SRL graph prediction.\n- Capable of modeling overlapping spans without assuming given predicates.\n- Achieves state-of-the-art performance on PropBank.\n- Offers potential applications in tasks like syntactic parsing and relation extraction.\n\nSlide Topic: Model Architecture and Span Representation  \nSummary:\n- The model treats all tokens as potential predicates and spans as arguments.\n- It predicts relations for each predicate-argument pair, including no relation.\n- Scoring is based on unary scores for predicates and arguments.\n- A label-specific score is used for the relation.\n- This architecture enables dynamic decision-making on spans and roles.\n\nSlide Topic: Learning and Beam Pruning Techniques  \nSummary:  \n- The model minimizes negative log likelihood for each input's gold structure.  \n- Beam pruning addresses computational complexity of O(n^3 |L|) possible relations.  \n- Two beams store candidate arguments and predicates, ranked by unary scores.  \n- This reduces evaluated relational factors to O(n^2 |L|).  \n- Limits maximum span width, further reducing computational load.  \n\nSlide Topic: Neural Architecture and Contextualized Representations  \nSummary:  \n- The model creates contextualized representations for argument spans and predicate words.  \n- It uses BiLSTM outputs to build these representations.  \n- Word-level contexts are formed by combining pre-trained word embeddings with character-based representations.  \n- A bidirectional LSTM is used to contextualize these combined representations.  \n- Argument representations include BiLSTM endpoints, a soft head word, and embedded span width features.  \n- Predicate representations are derived from BiLSTM outputs at specific positions.  \n\nSlide Topic: Experimental Setup and Results  \nSummary:  \n- Experiments conducted on CoNLL 2005 and CoNLL 2012 benchmarks.  \n- Used end-to-end and gold predicate setups.  \n- Model outperformed previous systems by F1 difference of 1.3 to 6.0.  \n- Significant improvement on out-of-domain and nominal predicate datasets.  \n- ELMo embeddings added for orthogonal performance improvements.  \n- Model predicted over 40% of sentences completely correctly across datasets.  \n\nSlide Topic: Analysis of Model Performance and Beam Pruning Effectiveness  \nSummary:\n- The model's architecture differs in input and decision space from previous BIO systems.\n- Beam pruning effectively reduces edges by pruning unlikely argument spans and predicates.\n- The model excels at predicting arguments distant from predicates due to direct connections.\n- It achieves comparable syntactic agreement to syntax-based systems.\n- The model struggles with global consistency due to strong independence assumptions.\n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Proposed a new SRL model for joint predicate and argument span prediction.  \n- Model excels at long-range dependencies and syntactic boundary agreement.  \n- Weakness observed in global consistency.  \n- Future work: explore higher-order inference methods.  \n- Consider combining span-based architecture with self-attention layers.",
    "Slide Topic: Introduction to Incremental Domain Adaptation  \nSummary:  \n- Enhances statistical and neural machine translation systems.  \n- Used in computer-aided translation (CAT) for immediate adaptation.  \n- Adapts systems to specific documents using user translations.  \n- Beneficial in collaborative translation for quick in-domain adaptation.  \n- Highlights importance of measuring lexical acquisition speed.  \n- Proposes metrics to evaluate adaptation responsiveness.  \n\nSlide Topic: Challenges in Evaluating Machine Translation Adaptation  \nSummary:\n- Evaluating machine translation adaptation is challenging, especially in collaborative scenarios.\n- Traditional metrics like BLEU and TER focus on string similarity.\n- These metrics may not reflect the system's ability to handle domain-specific vocabulary.\n- Specialized metrics are needed to evaluate vocabulary acquisition.\n- NMT models often struggle with content words compared to phrase-based MT.\n- The focus is on translation performance concerning content words, not word order.\n\nSlide Topic: Metrics for Measuring Lexical Acquisition  \nSummary:\n- Proposes three metrics: one-shot, zero-shot, and combined.\n- Focuses on recall of target-language content words.\n- Allows automatic computation by comparing translation hypotheses to references.\n- Operates on a set-level without clipping or alignment.\n- Evaluates system's ability to translate content words after limited exposure.\n\n**Slide Topic: Impact of Fine-Tuning on Vocabulary Recall**\n\nSummary:\n- Fine-tuning improves one-shot vocabulary recall.\n- It degrades zero-shot vocabulary recall.\n- Evaluated various adaptation techniques.\n- Fine-tuning all parameters maximizes one-shot but harms zero-shot recall.\n- Group lasso regularization balances zero-shot and one-shot acquisition.\n- Maintains high corpus-level translation quality.\n\nSlide Topic: Adaptation Techniques and Their Trade-offs  \nSummary:\n- Explores full parameter adaptation, bias adaptation, and group lasso regularization.\n- Each method has trade-offs in vocabulary acquisition and translation quality.\n- Full adaptation is effective but may be infeasible due to many parameters.\n- Bias adaptation introduces a second bias term.\n- Group lasso allows selection of a fixed number of parameters after updates.\n- Lasso technique balances zero-shot and one-shot recall, promising for immediate adaptation.\n\nSlide Topic: Experimental Evaluation and Surprising Results  \nSummary:\n- Experimental evaluation reveals surprising results.\n- Methods with similar corpus quality metrics can differ in zero-shot and one-shot vocabulary acquisition.\n- Lasso technique excels in zero-shot recall.\n- Full adaptation shows high one-shot recall but reduces zero-shot recall.\n- Traditional metrics may not fully capture adaptation effectiveness.\n- Highlights the need for specialized metrics for immediate adaptation performance.\n\nSlide Topic: Analysis of Novel Word Adaptation  \nSummary:\n- Focuses on adaptation of novel words using the lasso method.\n- Lasso improves zero-shot recall by learning new subword combinations.\n- Full adaptation shows high one-shot recall.\n- Provides insights into adaptation behavior of different techniques.\n- Highlights the importance of evaluating novel word adaptation in machine translation systems.\n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- Fine-tuning improves overall translation quality but harms zero-shot recall.  \n- This impact can negatively affect user experience.  \n- Current regularization methods help but need more effective solutions.  \n- Proposed metrics R0, R1, and R0+1 are useful for measuring adaptation performance.  \n- Future work should enhance zero-shot recall without reducing translation quality.  ",
    "Slide Topic: Introduction to Presuppositions and Triggers  \nSummary:\n- Presuppositions are shared assumptions in communication.\n- They are indicated by triggers like definite descriptions and factive verbs.\n- Example: \"again\" in \"John is going to the restaurant again\" presupposes prior visits.\n- Presuppositions remain unaffected by negation.\n- Focus on adverbial triggers like \"again,\" \"also,\" and \"still\" for event recurrence.\n- Understanding these triggers can improve language applications like summarization.\n\nSlide Topic: Adverbial Presupposition Triggers in Language  \nSummary:\n- Adverbial triggers like 'again', 'also', and 'still' indicate event recurrence or continuation.\n- They are common in English journalistic texts, second only to existential triggers.\n- These triggers are complex and often span multiple sentences.\n- Understanding them is crucial for pragmatic reasoning.\n- They enhance readability and coherence in language technology systems.\n- The study suggests a binary classification task to detect contexts for these triggers using datasets from the Penn Treebank and English Gigaword.\n\nSlide Topic: Dataset Creation and Extraction Methodology  \nSummary:  \n- Datasets are sourced from Penn Treebank and English Gigaword corpora.  \n- Focus is on five target adverbs: 'too', 'again', 'also', 'still', and 'yet'.  \n- Task is a binary classification to predict adverbial presupposition presence.  \n- Samples include a label, token list, and corresponding POS tags.  \n- Positive contexts contain a triggering adverb; negative contexts do not.  \n- Extraction process balances positive and negative samples, controlling for confounding factors.  \n\nSlide Topic: Attention-Based Deep Learning Models  \nSummary:\n- Attention models detect adverbial presupposition triggers.\n- They weigh information from multiple context points.\n- This helps infer long-range dependencies effectively.\n- Useful for detecting recurring events, like repeated mentions.\n- Attention-based RNNs predict article definiteness successfully.\n- Highlights potential of attention mechanisms in this task.\n\nSlide Topic: Weighted Pooling Attention Mechanism  \nSummary:\n- Introduces a new weighted pooling attention mechanism for predicting adverbial presupposition triggers.\n- Allows weighted averaging of RNN hidden states informed by inputs.\n- No new parameters compared to the LSTM baseline.\n- Utilizes self-attention with the input sequence as both query and key/value.\n- Aggregates learned attention scores through an additional attention layer.\n- Enhances prediction performance without extra computational cost.\n\nSlide Topic: Experimental Setup and Baseline Comparisons  \nSummary:  \n- The setup compares a weighted pooling model with several baselines: most-frequent-class, logistic regression, LSTM, and CNN.  \n- Experiments are conducted on datasets from the Penn Treebank and English Gigaword corpora.  \n- The impact of POS tags and attention mechanisms on prediction accuracy is investigated.  \n- The weighted pooling model uses the same bidirectional LSTM as the baseline LSTM for fair comparison.  \n- The model shows superior performance in predicting adverbial presupposition triggers.  \n\nSlide Topic: Analysis of Model Performance and Results  \nSummary:  \n- The weighted pooling model achieves 82.42% accuracy on the Gigaword dataset.  \n- It outperforms baseline models in predicting the adverb 'also'.  \n- Confusion matrices show balanced misclassifications with presupposition triggers.  \n- PTB dataset examples illustrate detection of presuppositions despite complex coreferences.  \n- Results highlight effectiveness in capturing long-range dependencies and interactions.  \n- Demonstrates the utility of attention mechanisms in this task.  ",
    "Slide Topic: Introduction to Deep Learning in Question Answering  \nSummary:\n- Deep learning is applied to question answering tasks across images, data, and text.\n- Developers and academics are keen to explore these models' capabilities.\n- Model effectiveness is measured by error rates on test sets.\n- High accuracy is meaningful if the test set reflects real-world tasks.\n- Ensuring test and training sets are representative is challenging.\n\nSlide Topic: Sensitivity Analysis of Deep Learning Models  \nSummary:\n- Propose techniques to analyze model sensitivity to question words.\n- Use attribution and generate adversarial questions for analysis.\n- Example: Model answers \"how symmetrical\" and \"how spherical\" similarly.\n- Indicates reliance on a few key words in question answering.\n- Helps identify important question words and their roles.\n\nSlide Topic: Integrated Gradients for Attribution  \nSummary:  \n- Integrated Gradients (IG) is used to identify key question words in deep learning systems.  \n- IG attributes predictions at input x relative to a baseline input x0, an empty question.  \n- It aggregates gradients of probability with respect to input using a path integral.  \n- Chosen for its ease and efficiency of implementation.  \n- IG is supported by axiomatic justification.  \n\n**Slide Topic: Overstability in Question Answering Networks**\n\nSummary:\n- Introduce an overstability test for question answering networks.\n- Reading comprehension networks are overly stable to semantic changes.\n- Overstability also affects questions and applies to visual and tabular networks.\n- Define a general-purpose test to measure overstability.\n- Test measures accuracy changes as words are systematically dropped.\n\nSlide Topic: Visual Question Answering Analysis  \nSummary:\n- Visual Question Answering requires systems to answer questions about images.\n- Analyzed Kazemi and Elqursh's deep network achieving 61.1% accuracy on validation set.\n- Network often ignores question words, relying heavily on image content.\n- Model retains over 50% accuracy even when most question words are removed.\n- Indicates a strong reliance on images rather than questions for answers.\n\nSlide Topic: Question Answering on Tables  \nSummary:  \n- Analyzed Neural Programmer (NP) for answering questions on tabular data.  \n- NP selects a sequence of operations to apply on the table.  \n- Operation selection is influenced more by content-free words than key words.  \n- Removing content-free words drops validation accuracy from 33.5% to 28.5%.  \n- Indicates reliance on irrelevant words over question content.  \n\nSlide Topic: Reading Comprehension and Adversarial Attacks  \nSummary:\n- Reading comprehension involves finding an answer span in a context paragraph.\n- Analysis of Yu et al. (2018) shows networks often ignore important words.\n- Jia and Liang (2017) introduced attacks by adding sentences to paragraphs.\n- These added sentences should not change answers but sometimes do.\n- Attacks succeed more when added sentences include key question words.\n\nSlide Topic: Conclusion and Implications for QA Models  \nSummary:  \n- Analyzed three QA models using an attribution technique.  \n- Attributions identified model weaknesses more effectively than conventional methods.  \n- Attribution-based workflow aids developers in improving model quality.  \n- Exposed weaknesses, though attacks may seem unrealistic, affecting QA product usage.  \n- Attribution methods can directly gauge the extent of these problems.  ",
    "Slide Topic: Introduction to Conversational Interfaces and Use Cases  \nSummary:\n- Advances in speech recognition and language understanding boost conversational interfaces.\n- Popular interfaces include Alexa, Cortana, and Siri.\n- Two main use cases: task completion (e.g., playing music) and answering questions.\n- Questions are answered by querying a knowledge graph or database.\n- Two distinct systems handle these use cases under a common interface.\n- The system for task completion is called a spoken language understanding (SLU) system.\n\nSlide Topic: Executable Semantic Parsing as a Unified Framework  \nSummary:\n- Proposes executable semantic parsing as a unified framework for SLU and Q&A.\n- Frames SLU as executable semantic parsing to unify both use cases.\n- Parses Q&A inputs into logical forms for machine-readable questions.\n- Parses SLU inputs into machine-readable representations of user intent and slots.\n- Enhances SLU by handling complex linguistic phenomena like coordinated intents.\n\nSlide Topic: Challenges in Semantic Parsing and Solutions  \nSummary:  \n- Annotated logical forms for training parsers are costly and scarce.  \n- Distant supervision using question-answer pairs is a viable solution.  \n- Leveraging annotated forms from different domains can be effective.  \n- Focus on scenarios with limited data for new domains.  \n- Apply transfer learning techniques to address data scarcity.  \n\nSlide Topic: Neural Transition-based Parser with Stack-LSTMs  \nSummary:\n- Focuses on the neural executable semantic parser by Cheng et al. (2017).\n- Utilizes a transition-based parsing paradigm.\n- Differs from traditional systems by not consuming words from the buffer.\n- No strict alignments between input words and tree nodes.\n- Employs a Bi-LSTM to encode the buffer.\n- Uses a Stack-LSTM for stack operations, allowing push and pop actions.\n\nSlide Topic: Handling Data Sparsity: Delexicalization vs. Copy Mechanism  \nSummary:\n- Data sparsity issues arise with rare or unknown words.\n- Two main solutions: delexicalization and copy mechanism.\n- Copy mechanism generally outperforms delexicalization.\n- Tested on datasets of varying sizes, showing consistent results.\n- In NLmaps and SLU domains, copy mechanism improves accuracy by 16% over baseline.\n\nSlide Topic: Transfer Learning Techniques for Low-Resource Domains  \nSummary:\n- Large training corpora exist for some domains, but new domains often lack data.\n- Transfer learning can help bootstrap parsers for low-resource domains.\n- Two approaches: pre-training and multi-task learning (MTL).\n- No clear winner between pre-training and MTL for SLU tasks.\n- Both methods outperform the baseline, highlighting their effectiveness.\n- Transfer learning is crucial for smaller domains.\n\nSlide Topic: Experiments and Results on Q&A and SLU Data Sets  \nSummary:  \n- Conducted experiments on single-task semantic parsing across three data sources.  \n- Analyzed the impact of an attention mechanism on performance.  \n- Compared delexicalization and copy mechanisms for handling data sparsity.  \n- Evaluated parsers using exact match accuracy, measuring correctly parsed sentences.  \n- Baseline model achieved a score of 0.846, outperforming previous results.  \n- Best F1 score of 0.874 obtained with the addition of the copy mechanism.  \n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- SLU is framed as an executable semantic parsing task, addressing limitations in current systems.  \n- The framework is effective for both Q&A and SLU across various datasets.  \n- Explored learning a semantic parser for new domains with limited data.  \n- Utilized high-resource domains to aid low-resource domain learning.  \n- Preliminary results in transfer learning across heterogeneous datasets are promising.  \n- Future work will focus on enhancing transfer learning techniques.  ",
    "Slide Topic: Introduction to Stance Classification  \nSummary:\n- Stance classification identifies users' positions on a specific target from text.\n- Traditionally, it uses a target-specific classifier trained on the same target.\n- Cross-target classification adapts classifiers from related targets.\n- This approach leverages existing knowledge for new targets.\n- Example: A classifier for 'climate change concern' can be adapted for mining project approvals.\n\n**Slide Topic: Challenges in Cross-Target Stance Classification**\n\nSummary:\n- Cross-target stance classification is challenging due to language model incompatibility.\n- Models struggle to generalize across different targets.\n- Domain-specific aspects can aid generalization for related targets.\n- Common concerns, like 'reef destruction,' link discussions on climate change and mining.\n- Identifying shared domain concerns helps improve classification accuracy.\n\nSlide Topic: Proposed Model: CrossNet  \nSummary:\n- CrossNet is a novel neural model for cross-target stance classification.\n- It leverages the self-attention mechanism for improved learning.\n- The model learns domain-specific aspects from a source target.\n- These aspects are applied to a destination target for classification.\n- Preliminary analysis shows improved classification performance.\n- CrossNet effectively extracts useful domain-specific information.\n\nSlide Topic: Embedding and Context Encoding Layers  \nSummary:  \n- CrossNet's Embedding Layer uses word embeddings for stance-bearing sentences and targets.  \n- The Context Encoding Layer employs a bi-directional LSTM (BiLSTM) for contextual information.  \n- Conditional encoding models sentence-target dependencies.  \n- Sentence BiLSTM is initialized with final states of the target BiLSTM.\n\n**Slide Topic: Aspect Attention Layer and Domain-Specific Aspects**\n\nSummary:\n- Identifies domain-specific aspects crucial for stance inference.\n- Utilizes self-attention to find core sentence parts.\n- Ensures semantic compatibility with the entire sentence.\n- Captures recurring domain aspects, like 'reef' in mining discussions.\n- Central to understanding users' stances.\n\nSlide Topic: Prediction Layer and Model Training  \nSummary:\n- The Prediction Layer uses a multilayer perceptron (MLP) for stance label prediction.\n- Domain aspect encoding is the basis for predictions.\n- Model training employs multi-class cross-entropy loss with L2-regularization.\n- Training and validation occur on a source target; testing is on a destination target.\n- The ADAM optimizer is used for optimization.\n- Stratified 10-fold cross-validation ensures robust evaluation.\n\nSlide Topic: Experimental Setup and Evaluation Metrics  \nSummary:  \n- Utilize pre-trained GloVe word vectors for model training.  \n- Train the model on a specified source target.  \n- Evaluate using F1-score with micro and macro averages.  \n- Use transfer ratio metric to assess target adaptation.  \n- Compare cross-target and in-target performance for effectiveness.  \n\nSlide Topic: Classification Performance and Results  \nSummary:  \n- CrossNet outperforms baseline models in cross-target stance classification.  \n- Excels in domains like Women's Rights and American Politics.  \n- Aspect attention mechanism enhances target-level generalization.  \n- Significant improvements observed in cross-target tasks.  \n- Transfer ratio results highlight the importance of destination target-specific information.  \n\nSlide Topic: Visualization of Attention and Learned Aspects  \nSummary:\n- Self-attention visualization highlights domain-relevant sentence parts.\n- CrossNet effectively identifies key terms like 'feminist' and 'rights.'\n- The model captures domain-specific information for stance inference.\n- It aids in stance inference across related targets.\n- Sentiment-carrying words are identified for stance prediction.\n\nSlide Topic: Conclusion and Future Work  \nSummary:  \n- Proposed a self-attention neural model for cross-target stance classification.  \n- Achieved superior results in specific domains.  \n- Future work includes exploring effective source target selection.  \n- Consider multiple source targets for knowledge transfer.  \n- Evaluate the model's multilingual generalization ability.  ",
    "Slide Topic: Introduction to Semantic Parsing and Challenges  \nSummary:\n- Semantic parsing maps language utterances to executable programs.\n- Early methods used supervised learning, requiring expert annotators.\n- This approach faced scalability issues due to expert knowledge needs.\n- Weakly-supervised parsing uses utterance-denotation pairs, easing data collection.\n- Challenges include search complexity and spuriousness.\n- Spuriousness occurs when incorrect programs yield correct denotations, posing significant issues.\n\nSlide Topic: Weakly-Supervised Semantic Parsing and Spuriousness  \nSummary:\n- Involves training with utterance-denotation pairs, simplifying data collection.\n- Complicates training due to a vast search space of programs.\n- Spuriousness occurs when incorrect programs yield correct denotations.\n- This misleads the learning process and is underexplored in literature.\n- The CNLVR dataset offers a chance to study spuriousness further.\n- Its binary nature increases spuriousness, as correct denotations can occur by chance.\n\nSlide Topic: Cornell Natural Language for Visual Reasoning (CNLVR) Dataset  \nSummary:\n- CNLVR dataset includes images with diverse objects and complex language statements.\n- The task is to assess the truthfulness of these statements.\n- It provides two input types: images and synthesized knowledge bases (KBs).\n- The setup frames CNLVR as a semantic parsing problem.\n- The goal is to translate language into programs executed against the KB.\n- The binary nature of the task presents challenges with spuriousness.\n\nSlide Topic: Proposed Semantic Parser for CNLVR  \nSummary:\n- Introduces the first semantic parser for the CNLVR dataset.\n- Maps words to program constants and language composition to operators.\n- Utilizes closed-world scenarios with clear semantic types to minimize lexical issues.\n- Employs abstract representations to share information across examples.\n- Enhances search efficiency and reduces spuriousness.\n- Achieves 82.5% accuracy, a 14.7% improvement over the state-of-the-art.\n\nSlide Topic: Abstract Representations and Lexicon Construction  \nSummary:\n- Language compositionality is a key challenge in visual reasoning domains.\n- We use a compact KB schema and typing system to address lexical mapping.\n- A small lexicon maps lexical items to typed program constants.\n- Abstract representations help generalize across examples, reducing search issues.\n- Seven abstract clusters correspond to main semantic types.\n- 25 mappings define abstract representations, enhancing training and search accuracy.\n\nSlide Topic: Model Architecture and Search Techniques  \nSummary:\n- Utilizes an encoder-decoder architecture with a bi-directional LSTM for encoding.\n- Employs a feed-forward network with attention for decoding.\n- Beam search and semantic typing prune the search space by ensuring syntactically valid programs.\n- A globally-normalized re-ranker addresses label bias by scoring final beam programs.\n- Focuses on promising program prefixes to enhance search efficiency and accuracy.\n\nSlide Topic: Training from Weak Supervision and Data Augmentation  \nSummary:\n- Treat programs as latent variables and approximately marginalize them.\n- Redefine training examples to include multiple KBs, ensuring correct denotations.\n- Use abstract examples to generate new training data, enhancing model coverage.\n- Data augmentation creates non-abstract pairs from abstract examples for better generalization.\n- Combine with caching to address search and spuriousness challenges.\n\nSlide Topic: Experimental Evaluation and Results  \nSummary:  \n- Our weakly-supervised semantic parser achieves 84.0% accuracy and 65.0% consistency on the public test set.  \n- It attains 82.5% accuracy on the hidden test set, improving by 14.7 points over the state-of-the-art.  \n- The rule-based parser serves as a reasonable baseline.  \n- The supervised parser trained on augmented data shows better performance.  \n- Our approach significantly improves over the supervised parser, highlighting the effectiveness of abstract examples and data augmentation.  \n- Error analysis reveals challenges with complex utterances and strengths in handling spatial relations.  ",
    "Slide Topic: Introduction to BioASQ Challenge and Task B  \nSummary:\n- BioASQ is a challenge in semantic indexing, question answering, and information extraction.\n- We participated in Task B, focusing on biomedical question answering.\n- Phase B involves returning answer candidates for given questions and gold-standard snippets.\n- The fifth BioASQ challenge includes five batches of 100 questions, released bi-weekly.\n- Our work targets factoid and list questions, evaluated using MRR and F1 score, respectively.\n\nSlide Topic: Traditional vs. Neural Network QA Systems  \nSummary:\n- Traditional QA systems use discrete steps like named-entity recognition and question classification.\n- These systems require extensive resources and feature engineering.\n- Our system uses a neural network QA architecture trained end-to-end.\n- It builds on FastQA, trained on SQuAD, and fine-tuned on BioASQ.\n- This approach restricts responses to substrings in provided snippets.\n- It generalizes the output layer to handle list questions.\n\nSlide Topic: System Architecture and Modifications  \nSummary:\n- The system is a neural network that processes questions and context to output token pointers.\n- Utilizes FastQA with modified input vectors and an output layer for list answers.\n- Input layer maps tokens to high-dimensional vectors using GloVe, character, and biomedical Word2Vec embeddings.\n- One-hot encoding of question type is concatenated with these vectors.\n- Output layer computes start, end, and span probabilities for multiple spans in list questions.\n\nSlide Topic: Training Methodology and Dataset Preparation  \nSummary:  \n- Train the network in two steps: pre-training on SQuAD and fine-tuning on BioASQ.  \n- Use Adam optimizer with exponentially decaying learning rate.  \n- Start with learning rates of 10^-3 for pre-training and 10^-4 for fine-tuning.  \n- Extract answer spans from BioASQ by identifying gold standard answers in snippets.  \n- Process snippets independently to reduce false positives and negatives.  \n- Aggregate answer spans based on their probability.  \n\nSlide Topic: Decoding and Answer Selection Process  \nSummary:\n- Retrieve top 20 answer spans using beam search with a beam size of 20.\n- Remove duplicate strings from the sorted list.\n- For factoid questions, output the top five answers as a ranked list.\n- For list questions, apply a probability cutoff optimized for list F1 score.\n- Ensure selection of the most probable answer spans for each question type.\n\nSlide Topic: Ensemble Model and Performance Optimization  \nSummary:\n- Built an ensemble model using five single models with 5-fold cross-validation.\n- Combined models by averaging start and end scores to compute span probabilities.\n- Submitted both the best single model and the ensemble model to the challenge.\n- Ensemble model generally outperforms, especially on factoid questions.\n- Achieved better results in three out of five batches with the ensemble model.\n\nSlide Topic: Results and Discussion on BioASQ 5B  \nSummary:\n- Results reported for all five test batches of BioASQ 5 (Task 5b, Phase B).\n- Single and ensemble systems perform similarly, with ensemble slightly better on average.\n- System excels in factoid questions, winning three out of five batches.\n- Performance on list questions varies due to lack of list questions in pre-training dataset (SQuAD).\n- Yes/no question responses submitted as a baseline, but not discussed due to simplicity.",
    "Slide Topic: Introduction to Span-Based Neural Constituency Parsing  \nSummary:  \n- Span-based parsing is known for high accuracy and simplicity.  \n- It differs from other parsers by not modeling tree structures.  \n- Utilizes bidirectional RNNs to model input sequences.  \n- Enables efficient dynamic programming algorithms like CKY.  \n- Decomposes output space for better manageability and efficiency.  \n\n**Slide Topic: Limitations of Existing Span-Based Parsers**\n\nSummary:\n- Span-based parsers struggle with search efficiency.\n- Greedy parsers are fast but explore only one path.\n- Chart-based parsers are accurate but slow for long sentences.\n- This trade-off limits their use in complex tasks.\n- Affects end-to-end discourse parsing and sentence boundary detection.\n\nSlide Topic: Proposed Linear-Time Span-Based Neural Parser  \nSummary:\n- Combines strengths of greedy and chart-based approaches.\n- Achieves linear-time parsing over an exponentially large space.\n- Utilizes left-to-right dynamic programming in an action-synchronous style.\n- Practical runtime of O(n b^2) using beam search.\n- Cube pruning improves runtime to O(n b log b).\n- Effectively linear in sentence length.\n\nSlide Topic: Dynamic Programming and Score Decomposition  \nSummary:\n- The parser breaks down the tree score into a sum of span scores.\n- This decomposition enables efficient dynamic programming.\n- Each span's label is selected independently.\n- Independent labeling simplifies the parsing process.\n- Decomposition is key to the parser's efficiency and accuracy.\n\nSlide Topic: Graph-Structured Stack without Bookkeeping  \nSummary:\n- Introduces a new graph-structured stack formulation.\n- Eliminates the need for extra bookkeeping.\n- Maintains predecessor states for each parsing state.\n- Enables efficient span combination without added complexity.\n- Time complexity remains O(n^4) with an extra O(n) for step index.\n\nSlide Topic: Action-Synchronous Beam Search and Cube Pruning  \nSummary:\n- Utilizes action-synchronous beam search to maintain top parsing states.\n- Reduces runtime complexity significantly.\n- Introduces cube pruning to manage beam size.\n- Optimizes runtime to O(n b log b).\n- Ensures efficient parsing with larger beam sizes.\n\nSlide Topic: Training with Structured SVM and Cross-Span Loss  \nSummary:\n- Utilizes a structured SVM approach with a novel cross-span loss function.\n- Penalizes incorrect spans that cross gold-tree spans.\n- Employs max-violation updates for model optimization.\n- Ensures the model scores the gold tree higher than any other tree.\n- Achieves this by maintaining a specified margin.\n\nSlide Topic: Experiments and Results on Penn Treebank and Discourse Parsing  \nSummary:\n- Experiments show the parser's efficiency and accuracy.\n- Achieves state-of-the-art results with a beam size of 20.\n- Demonstrates significant speed improvements over existing methods.\n- Handles longer sequences effectively in discourse parsing.",
    "Slide Topic: Introduction to Machine Translation and Semantic Representations  \nSummary:  \n- Machine translation aims to convert source language into fluent target language with the same meaning.  \n- Recent methods use statistical techniques on parallel data for learning transformations.  \n- Unified semantic representations, like Abstract Meaning Representation (AMR), are gaining interest.  \n- These representations enhance natural language understanding and generation tasks.  \n- Deeper models offer more efficient translation by handling related words and concepts uniformly.\n\nSlide Topic: Related Work and Historical Context  \nSummary:  \n- Significant research exists on deep language representation in NLP and MT.  \n- Early systems from the 1950s used transfer methods for translation.  \n- Notable examples include KANT, Systran, and TectoMT.  \n- AMR is used for abstractive summarization and semantic graph-based MT.  \n- Discriminative models in statistical MT leverage deep representations.  \n- These models focus on capturing semantic relations over surface correspondences.  \n\nSlide Topic: Semantic Representation and Logical Form  \nSummary:\n- Sentence semantics are represented using Logical Form (LF).\n- LF consists of labeled directed graphs with nodes for content words.\n- Edge labels indicate semantic relations; nodes store linguistic info.\n- LF can be converted to conform with AMR specifications.\n- Semantic graphs are used for discriminative translation modeling.\n- Our approach bridges LF and AMR in semantic representation.\n\nSlide Topic: Graph-to-String Translation Model  \nSummary:\n- Develops models for semantic graph-to-string translation.\n- Views translation as selecting target words based on source representation.\n- Assumes alignment between target words and source graph nodes.\n- Generates target words by transitioning through graph nodes.\n- Stops once the target sentence is complete.\n- Uses a feature-rich approach for translation reranking.\n\nSlide Topic: Alignment of Semantic Graph Nodes  \nSummary:  \n- Experimented with aligning source-side semantic graph nodes to target-side words.  \n- Used a generative HMM-style model to align target words to source nodes via graph paths.  \n- Employed Gibbs sampling to incorporate a sparse prior in parameter estimation.  \n- Direct GIZA++ aligns linearized semantic graph nodes to target sentences.  \n- Composed alignments involve two stages: aligning graph nodes to source words and source to target words.  \n- Combining gold alignment with GIZA++ yielded superior results.  \n\nSlide Topic: Discriminative Model and Feature Set  \nSummary:\n- Utilizes a maximum entropy model for learning conditional probabilities.\n- Features include lemma, part of speech, and graph node bits.\n- Captures agreement phenomena and semantic context.\n- Additional features manage multiple target word generation.\n- Previous tokens serve as discriminative language model features.\n- Maintains state during MT decoding with conventional complexity.\n\nSlide Topic: Experiments and Results  \nSummary:  \n- Tested model in n-best re-ranking for English \u2192 French MT.  \n- System optimized with minimum error rate training.  \n- Re-ranking added model scores to n-best lists.  \n- Rescored test n-best lists evaluated using BLEU.  \n- Achieved modest but consistent improvement.  \n- Suggests potential gains with direct decoder integration.  \n\nSlide Topic: Conclusion and Future Directions  \nSummary:  \n- Introduced semantic features in statistical MT using discriminative training.  \n- Initial gains are modest; further feature engineering is needed.  \n- Improvements depend on semantic parser accuracy.  \n- Exploring robust features or combining parsers could be beneficial.  \n- Separate evaluation of morphological, syntactic, and semantic features is recommended.  \n- Integrating semantic plausibility models can enhance output quality.  ",
    "**Slide Topic: Introduction and Objectives of the BioASQ Challenge 2017**\n\nSummary:\n- Overview of data from the 2017 BioASQ challenge.\n- Presentation of participating systems and their performance.\n- Tasks conducted from February to May 2017.\n- Evaluation of systems' effectiveness in the challenge.\n\nSlide Topic: Overview of the Tasks in the BioASQ Challenge  \nSummary:  \n- The challenge includes three main tasks.  \n- Task 5a: Large-scale semantic indexing.  \n- Task 5b: Question answering.  \n- Task 5c: Funding information extraction.  \n- Detailed descriptions follow in subsequent sections.  \n\nSlide Topic: Large-scale Semantic Indexing (Task 5a)  \nSummary:  \n- Classify PubMed documents into MeSH hierarchy concepts.  \n- Use unannotated PubMed articles as test sets for evaluation.  \n- Include articles from all journals in test data sets.  \n- Evaluate system performance with flat and hierarchical measures.  \n- Task divided into three batches of five weekly test sets each.  \n\nSlide Topic: Biomedical Semantic Question Answering (Task 5b)  \nSummary:  \n- Task 5b aimed to tackle large-scale biomedical question answering.  \n- It involved four question types: yes/no, factoid, list, and summary.  \n- Phase A required responses with relevant elements from specific resources.  \n- BioASQ released 100 questions for participants in Phase A.  \n- Phase B included questions with manually selected articles and snippets.  \n- Participants provided exact answers and natural language summaries in Phase B.  \n\nSlide Topic: Funding Information Extraction (Task 5c)  \nSummary:\n- Task 5c was introduced this year to extract grant information from Biomedical articles.\n- Funding data helps assess the impact of agency funding in scientific literature.\n- It also identifies agencies supporting specific research directions.\n- MEDLINE citations include annotations about funding from specified agencies.\n- Systems were tasked with extracting grant information from full texts.\n- Author-provided funding details may not always be mentioned in articles.\n\nSlide Topic: System Approaches and Methodologies  \nSummary:  \n- Overview of systems and their approaches is provided.  \n- Systems without available descriptions are excluded.  \n- 'Search system' developed as UIMA-based text and data mining workflow.  \n- Different search strategies used to annotate documents with MeSH terms.  \n- 'MZ' systems applied Binary Relevance classification with TF-IDF features.  \n- LDA models used label frequencies per journal for prior frequencies, with regression for threshold prediction.  \n\nSlide Topic: Evaluation Metrics and Results  \nSummary:  \n- Task 5a: Each batch evaluated independently using flat and hierarchical measures.  \n- Micro F-measure (MiF) and Lowest Common Ancestor F-measure (LCA-F) determined winners.  \n- Task 5b Phase A: Systems ranked by Mean Average Precision (MAP).  \n- Task 5b Phase B: Ranked by accuracy for yes/no, MRR for factoids, and mean F-measure for lists.  \n- Task 5c: Micro-recall used for all sub-tasks.  \n\nSlide Topic: Conclusions and Future Directions  \nSummary:  \n- Overview of the fifth BioASQ challenge with three tasks: semantic indexing, question answering, and funding information extraction.  \n- Best systems outperformed strong baselines, indicating advances over the state of the art.  \n- Future editions aim to provide more benchmark data from a community-driven acquisition process.  \n- Plan to design a multi-batch scenario for Task 5c, similar to other tasks.",
    "Slide Topic: Introduction to Phrase-based Statistical Machine Translation  \nSummary:\n- Phrase-based SMT is a data-driven translation approach.\n- Widely used for over a decade in machine translation.\n- Moses toolkit is a key open-source tool developed at the University of Edinburgh.\n- Training involves crucial word and phrase alignment.\n- Alignment quality directly impacts translation accuracy.\n\nSlide Topic: Word and Phrase Alignment in PB-SMT  \nSummary:\n- Essential step in training PB-SMT.\n- Establishes translation relationships in sentence-aligned bi-corpus.\n- Crucial for translation quality.\n- One of the most time-consuming processes.\n- Involves determining best alignment links between source and target.\n\nSlide Topic: Probabilistic and Associative Alignment Approaches  \nSummary:  \n- Probabilistic approaches use statistical models for word alignment.  \n- IBM models and HMM alignment models employ the EM algorithm.  \n- GIZA++ is a popular implementation of these probabilistic models.  \n- Associative approaches rely on statistical measures like the Dice coefficient.  \n- Mutual information is used to process sentences independently.  \n- Both methods offer distinct strategies for machine translation alignment.  \n\n**Slide Topic: Sampling-based Multilingual Alignment and Hierarchical Sub-sentential Alignment**\n\nSummary:\n- Two associative approaches: Anymalign and Cutnalign.\n- Anymalign samples sub-corpora for word occurrence distributions.\n- Cutnalign uses recursive binary segmentation for alignment.\n- Both methods aim to reduce training time.\n- They maintain alignment quality effectively.\n\nSlide Topic: Data and Tools Used for Chinese-Japanese Translation  \nSummary:  \n- Utilized the Chinese-Japanese JPO Patent Corpus from WAT 2015.  \n- Contains 1 million parallel sentences in Chemistry, Electricity, Mechanical Engineering, and Physics.  \n- Employed Stanford Segmenter for Chinese segmentation.  \n- Used Juman for Japanese segmentation.  \n- Preprocessed data with tokenization, lowercase conversion, and cleaning.\n\nSlide Topic: Bilingual Hierarchical Sub-sentential Alignment Method  \nSummary:\n- Cutnalign is a bilingual hierarchical alignment method.\n- It uses recursive binary segmentation for sentence pairs.\n- Measures translation link strength and computes joint clustering.\n- Evaluates translation probabilities for optimal alignment.\n- Utilizes a bipartite graph, shown in alignment matrices.\n- Provides segmentation examples for clarity.\n\nSlide Topic: Improvements and Optimizations in Alignment Methods  \nSummary:  \n- Implemented multi-processing to accelerate alignment methods.  \n- Re-implemented core components in C for faster processing.  \n- Used approximations in N cut computation for speed improvements.  \n- Reduced search space in hierarchical sub-sentential alignment.  \n- Enhancements aim to boost efficiency without losing translation quality.  \n\nSlide Topic: Experimental Setup and Evaluation Metrics  \nSummary:  \n- Experiments used GIZA++ or MGIZA for alignment.  \n- Phrase tables were extracted with grow-diag-final-and heuristic in Moses.  \n- Systems were evaluated using BLEU and RIBES metrics.  \n- Various timeouts and versions of Anymalign and Cutnalign were tested.  \n- Impact on training time and translation quality was assessed.  \n\n**Slide Topic: Results and Analysis of Different Alignment Methods**\n\nSummary:\n- No significant differences in translation quality across alignment methods.\n- Training times varied significantly between methods.\n- Fastest training achieved with Moses, Anymalign, and Cutnalign configuration.\n- This configuration reduced training time to one-fifth of GIZA++ or MGIZA.\n- Confidence intervals and p-values confirmed result reliability.\n\nSlide Topic: Conclusion and Impact on SMT Development  \nSummary:  \n- Significant reductions in SMT training time are achievable.  \n- Translation quality remains unaffected by reduced training time.  \n- Bilingual hierarchical sub-sentential alignment is a promising method.  \n- Efficient alignment methods accelerate SMT development.  \n- Steady translation quality is maintained with less computation time.  ",
    "Slide Topic: Introduction to Multi-task Learning in NLP  \nSummary:  \n- Traditional NLP uses separate models for each task with specific labeled data.  \n- Many NLP tasks are intrinsically linked, benefiting from shared learning.  \n- Multi-task Learning (MTL) enables knowledge transfer across related tasks.  \n- MTL often involves sharing parameters in neural network architectures.  \n- Studies show MTL improves performance in tasks like machine translation and parsing.\n\nSlide Topic: Challenges in Low-resource Language Settings  \nSummary:\n- NLP tasks often lack training data for low-resource languages.\n- Annotating data for all languages is impractical, especially for complex tasks.\n- Some applications need rapid NLP system development for very low-resource languages.\n- Focus is on enhancing supervised models in low-resource settings.\n- Leverage knowledge from related high-resource languages and tasks.\n\nSlide Topic: Proposed Multi-lingual Multi-task Architecture  \nSummary:\n- Addresses data sparsity in low-resource languages.\n- Combines transfer models in a unified architecture.\n- Implements two levels of parameter sharing.\n- First level shares character embeddings, CNNs, and LSTM layers.\n- Second level uses different sharing strategies for transfer schemes.\n- Shares output layer for all Name Tagging tasks to enhance task-specific knowledge.\n\nSlide Topic: Model Architecture and Parameter Sharing  \nSummary:\n- The architecture is based on the LSTM-CNNs model.\n- Each word is represented by a word embedding and character feature vector.\n- A bidirectional LSTM encodes words and context into fixed-size vectors.\n- A linear layer converts these vectors into score vectors.\n- A CRF layer generates the best tagging path.\n- Our implementation omits additional symbolic features used in LSTM-CNNs.\n\nSlide Topic: Experiments and Data Sets  \nSummary:\n- Name tagging uses data from CoNLL 2002, 2003, LDC2016E95, and TAC KBP 2017.\n- POS tagging utilizes data from the CoNLL 2017 shared task.\n- Word embeddings are trained using the word2vec package.\n- MUSE library's unsupervised model maps monolingual word embeddings.\n- Parameters are optimized with Stochastic Gradient Descent with momentum.\n- Techniques include gradient clipping and exponential learning rate decay.\n\nSlide Topic: Comparison of Different Models  \nSummary:\n- Our model is compared with mono-lingual LSTM-CNN, cross-task, and cross-lingual transfer models.\n- It significantly outperforms the mono-lingual single-task baseline.\n- Achieves notable gains over single transfer models.\n- Cross-lingual transfer excels with fewer than 50 training sentences.\n- Consistently surpasses cross-task transfer in low-resource settings.\n\nSlide Topic: Qualitative Analysis and Ablation Studies  \nSummary:  \n- Table 4 compares Name Tagging results of baseline and our model.  \n- Shared character-level networks transfer morphological and semantic information.  \n- Ablation studies on Dutch Name Tagging show performance enhancement with each component.  \n- Impact of components varies with the size of the target task data.\n\n**Slide Topic: Effect of Auxiliary Task Data and Related Work**\n\nSummary:\n- Evaluated model sensitivity by fixing main task data and downsampling auxiliary data.\n- Performance improved as auxiliary data sample rate increased from 1% to 20%.\n- Multi-task learning applied in NLP areas like translation, classification, parsing, and labeling.\n- Our model integrates cross-task and cross-lingual transfer in a unified architecture.\n- Enables simultaneous knowledge transfer from multiple auxiliary tasks.",
    "Slide Topic: Introduction to Query Auto-Completion and Personalization  \nSummary:\n- Query auto-completion (QAC) suggests queries as users type.\n- It saves time and reduces cognitive load for users.\n- Most QAC systems use the Most Popular Completion (MPC) algorithm.\n- MPC suggests based on popular queries matching the prefix.\n- Personalization tailors suggestions to individual user needs and interests.\n\nSlide Topic: Advancements in Language Models for QAC  \nSummary:\n- Park and Chiba introduced a novel approach using a character LSTM language model for QAC.\n- This method replaces ranking completions from a database.\n- It can complete queries with unseen prefixes during training.\n- The approach offers significant memory savings by eliminating the need for a large query database.\n- Personalized QAC is enhanced by combining personalization with the model's effectiveness.\n- The model effectively handles rare and previously unseen prefixes.\n\nSlide Topic: FactorCell Model for Personalized QAC  \nSummary:\n- FactorCell model uses an embedding vector to transform recurrent layer weights.\n- It employs a low-rank matrix for additive weight transformation.\n- Allows more weight changes during personalization than traditional methods.\n- Applies adaptive language models to QAC personalization.\n- Demonstrates RNN language model adaptation to unseen contexts.\n\nSlide Topic: Learning and Adapting User Embeddings  \nSummary:  \n- User embeddings capture latent demographic factors.  \n- Users with fewer than 15 queries are grouped together.  \n- Embeddings are learned via back-propagation in the model.  \n- Online updates occur during evaluation for unseen users.\n\nSlide Topic: Recurrent Layer Adaptation Techniques  \nSummary:  \n- Three model architectures are explored for recurrent layer adaptation.  \n- The unadapted LM does not perform personalization.  \n- ConcatCell model concatenates user embedding with character embedding at each input step.  \n- FactorCell model transforms recurrent layer weights using a low-rank adaptation matrix.  \n- FactorCell provides stronger adaptation than ConcatCell.  \n\nSlide Topic: Experimental Setup and Data Utilization  \nSummary:  \n- Utilized AOL Query data from 2006, comprising 12 million queries from 173,000 users.  \n- Models trained for six epochs using the Adam optimizer.  \n- Employed a single-layer character-level LSTM with coupled input and forget gates.  \n- Incorporated layer normalization in the language model.  \n- Conducted experiments on two configurations: small and large.  \n- Varied hidden state sizes and user embedding dimensions across configurations.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:  \n- Results are evaluated using Mean Reciprocal Rank (MRR).  \n- Neural models outperform the MPC baseline.  \n- Personalized models perform better than unadapted ones.  \n- Factor Cell model is the best overall, especially for seen prefixes.  \n- Figure 1 shows MRR improvement over an unpersonalized model.  \n- Factor Cell outperforms Concat Cell over time with more user queries.  \n\nSlide Topic: Comparative Analysis with Existing Models  \nSummary:  \n- FactorCell adapts faster to repetitive search behaviors than ConcatCell.  \n- Case studies show FactorCell examples are more semantically coherent.  \n- FactorCell identifies queries more relevant to user context.  \n- ConcatCell focuses more on orthography than semantics.\n\n**Conclusion and Future Directions**\n\n- LSTM model improves with personalization; Factor Cell model offers an advantage.\n- Adapted weight matrix is efficient, computed once per query.\n- Personalization benefits extend to unseen users, increasing over time.\n- Future work could integrate language model with MPC.\n- Consider including time as an additional conditioning factor.",
    "Slide Topic: Introduction to Word Embeddings and Their Limitations  \nSummary:\n- Word embeddings map words into low-dimensional vector spaces.\n- Widely used in NLP tasks like Machine Translation and syntactic parsing.\n- Struggle with differentiating multiple meanings of the same word.\n- Efforts to address this involve inducing word senses from corpora or bilingual data.\n- These approaches often lack interpretability and are hard to align with lexical resources.\n\nSlide Topic: Existing Approaches to Sense Disambiguation  \nSummary:\n- Various methods address limitations of word embeddings.\n- Disambiguation systems like Babelfy learn sense embeddings.\n- These methods often replace words with intended senses.\n- Some rely on resources like WordNet, limiting applicability.\n- Models align vector spaces but are restricted to nominal instances.\n\nSlide Topic: Proposed SW2V Model: Integrating Words and Senses  \nSummary:\n- SW2V model learns embeddings for words and senses.\n- Utilizes text corpora and semantic networks.\n- Embeddings are represented in the same vector space.\n- Offers flexibility for various predictive models.\n- Scalable for large semantic networks and corpora.\n- Addresses limitations of previous approaches with a unified vector space.\n\nSlide Topic: Shallow Word-Sense Connectivity Algorithm  \nSummary:  \n- Connects words and senses using a semantic network.  \n- Operates linearly with corpus size.  \n- Allows words to have multiple associated senses.  \n- Associates words with the most connected senses in context.  \n- Speeds up training and improves accuracy over existing systems.  \n\nSlide Topic: Joint Training of Words and Senses  \nSummary:\n- SW2V model enhances word embeddings with explicit knowledge integration.\n- Utilizes a shared vector space for both words and senses.\n- Updates embeddings based on intrinsic relationships.\n- Compatible with architectures like CBOW and Skip-Gram.\n- Predicts words and senses in context.\n- Aims to create a semantically coherent vector space.\n\nSlide Topic: Analysis of Model Components and Configurations  \nSummary:  \n- Evaluated SW2V components and model configurations.  \n- Focused on shallow word-sense connectivity algorithm.  \n- Tested on word similarity datasets.  \n- Configurations with only senses in input layer perform best.  \n- Shallow algorithm outperforms state-of-the-art disambiguation systems.  \n- Provides better results and faster processing times.  \n\n**Slide Topic: Evaluation of SW2V on Word Similarity and Sense Clustering**\n\nSummary:\n- SW2V is evaluated on word similarity tasks using datasets like SimLex-999 and MEN.\n- It outperforms other sense-based models in these tasks.\n- Excels in sense clustering, capturing semantics accurately.\n- The model's shared space of words and senses offers advantages.\n- It avoids conflating different meanings into the same vector, unlike retrofitting approaches.\n\nSlide Topic: Interconnectivity of Word and Sense Embeddings  \nSummary:  \n- Tested through a Word Sense Disambiguation task.  \n- SW2V's shared vector space provides a strong baseline.  \n- Outperforms other models in semantic coherence.  \n- Highlights SW2V's value for resource-poor languages.  \n- Useful for large knowledge resources.  ",
    "Slide Topic: Introduction to Search-based Structured Prediction  \nSummary:\n- Models natural language structure generation as a search problem.\n- Gains attention for competitive accuracy and efficiency.\n- Involves learning a stochastic policy by imitating a reference policy.\n- Challenges arise from ambiguities between training and testing phases.\n- Techniques like ensemble and exploration during training address these issues.\n\nSlide Topic: Challenges in Imitation Learning for Structured Prediction  \nSummary:\n- Imitation learning faces challenges in structured prediction tasks.\n- Ambiguities arise when multiple actions lead to the optimal structure.\n- Only one action is chosen for training, causing potential issues.\n- Discrepancies occur between training and testing environments.\n- Learned policies may encounter non-optimal states during testing.\n- These challenges affect model generalization and performance.\n\nSlide Topic: Knowledge Distillation as a Solution  \nSummary:\n- Enhances generalization of search-based structured prediction models.\n- Transfers knowledge from a large teacher model to a smaller student model.\n- Uses a distillation loss with the teacher's output as a 'soft target.'\n- Addresses limitations of traditional training methods.\n- Balances reference and exploration states for improved learning.\n\nSlide Topic: Distillation from Reference and Exploration  \nSummary:\n- Proposes a method to distill knowledge from reference and exploration states.\n- Reference distillation involves learning from states produced by the reference policy.\n- Exploration distillation involves learning from states encountered during exploration.\n- Combining these methods enhances model performance.\n- Leverages the generalization ability of the ensemble model.\n- Workflow is illustrated in Figure 1.\n\nSlide Topic: Experimental Setup and Methodology  \nSummary:  \n- Experiments conducted on dependency parsing and machine translation tasks.  \n- Parsing uses the Penn Treebank dataset.  \n- Translation uses German-to-English IWSLT 2014 dataset.  \n- Models utilize LSTM-based architectures.  \n- Performance evaluated with LAS and BLEU scores.  \n- Setup includes varying hyperparameters and multiple seeded runs for robustness.  \n\nSlide Topic: Results and Performance Analysis  \nSummary:\n- The proposed knowledge distillation method is effective.\n- Parsing experiments show a 1.32 improvement in LAS.\n- Translation experiments achieve a 2.65 increase in BLEU score.\n- Distilled models outperform strong baselines and other models.\n- Ensemble model's output distribution enhances generalization.\n\nSlide Topic: Ensemble Behavior on Problematic States  \nSummary:\n- Ensemble model excels in ambiguous or non-optimal states.\n- Dynamic oracle assesses parser accuracy in these states.\n- Ensemble significantly outperforms the baseline.\n- Demonstrates strong generalization on problematic states.\n- Distillation model, learned from exploration, surpasses both baseline and ensemble.\n\nSlide Topic: Impact of Distillation Loss and Learning Stability  \nSummary:\n- Distillation loss impact is studied by varying parameter \u03b1.\n- Larger \u03b1 values improve model performance.\n- Distillation method enhances learning stability.\n- Stability shown by smaller performance score deviations.\n- Stability linked to less sharp minimizers from distillation loss.",
    "**Introduction to Complaints in Natural Language**\n\n- Complaining is a speech act expressing a negative mismatch between reality and expectations.\n- Understanding complaints is crucial for linguists, psychologists, organizations, and NLP development.\n- Complaints differ from negative sentiment by focusing on breached expectations.\n- This analysis includes a publicly available dataset of Twitter complaints.\n- It features a quantitative analysis of linguistic features and high-performance predictive models.\n\nSlide Topic: Related Work and Linguistic Background  \nSummary:  \n- Complaints are studied in linguistics and marketing as breaches of expectations.  \n- They are distinct speech acts that can threaten both positive and negative face.  \n- Research has explored complaints across cultures and socio-demographic traits.  \n- Studies cover various forms of communication.  \n- Previous computational work defined complaints by negative connotation, not breached expectations.  \n\nSlide Topic: Data Collection and Annotation Process  \nSummary:\n- Created a dataset of complaints using Twitter as the data source.\n- Focused on tweets directed to customer service accounts.\n- Employed a sampling method to increase the hit rate of complaints.\n- Manually annotated tweets to identify complaints.\n- Achieved substantial agreement with a Cohen's Kappa of 0.731.\n- Dataset includes 1,232 complaints and 739 non-complaints, with tweets anonymized and preprocessed.\n\nSlide Topic: Linguistic Feature Analysis of Complaints  \nSummary:\n- Conducted quantitative analysis using univariate Pearson correlation.\n- Complaints frequently use negations, possessive pronouns, and temporal references.\n- Personal pronouns and exclamation marks are often avoided.\n- Common patterns include possessive pronouns followed by nouns and past participle verbs.\n- Focus is on describing issues rather than expressing sentiment.\n- Question marks are frequently used in complaints.\n\nSlide Topic: Predictive Modeling of Complaints  \nSummary:\n- Developed predictive models using various features and machine learning methods.\n- Utilized Logistic Regression with Elastic Net and neural methods like MLP and LSTM.\n- Best performance achieved with bag-of-word features, reaching an F1 score of 77.5.\n- Combining all features increased accuracy to an F1 score of 78.\n- Neural network approaches were comparable but did not surpass feature-based models.\n\nSlide Topic: Distant Supervision and Domain Adaptation  \nSummary:  \n- Distant supervision identified extra complaint data, enhancing predictive performance.  \n- Tweets with complaint-related hashtags were collected.  \n- Combined tweets with annotated data using domain adaptation.  \n- EasyAdapt algorithm improved F1 score to 79 and ROC AUC by 0.012.  \n- Simply pooling data reduced performance, emphasizing domain adaptation's importance.  \n\nSlide Topic: Domain-Specific and Cross-Domain Experiments  \nSummary:\n- Models tested with in-domain, out-of-domain, and domain adaptation methods.\n- Out-of-domain data generally improved performance.\n- Domain adaptation was beneficial in most cases.\n- Cross-domain experiments showed consistent performance, with some exceptions.\n- Apparel domain exhibited unique features due to complaint nature.\n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Introduced a computational approach to modeling complaints with annotated Twitter data.  \n- Achieved up to 79% Macro F1 in predictive models.  \n- Domain adaptation improved model accuracy.  \n- Future work includes identifying complaint targets and using additional context.  \n- Plans to study complaints in personal and political domains.  \n- Data and code are available for further research.  ",
    "Slide Topic: Introduction to Emotion Classification  \nSummary:  \n- Human emotions are complex and challenging to define.  \n- Ekman identified six basic emotions shared across cultures: anger, disgust, fear, guilt, joy, and sadness.  \n- Seven emotional states, including shame, were extracted from a psychological corpus.  \n- Statements were tokenized, grouped into trigrams, and analyzed as Context Vectors.  \n- Vectors were POS tagged, with TF and TF-IDF scores measured for feature importance.  \n- Affinity Scores were calculated using distance metrics, enabling classification methods like Naive Bayes and Decision Trees.  \n\nSlide Topic: Related Work and Existing Resources  \nSummary:  \n- Strapparava and Valitutti (2004) developed WORDNET-AFFECT for affective labeling of synsets.  \n- Wiebe et al. (2005) introduced an annotation scheme for opinions and emotions.  \n- Kobayashi et al. (2004) created an opinion lexicon from annotated corpora.  \n- Esuli and Sebastiani (2006) adapted SentiWordNet for synset classification using ternary classifiers.  \n- Mohammad et al. (2010) analyzed emotion annotations; Bandyopadhyay (2009, 2010) developed Bengali emotion lexicons.  \n- SenticNet and EmotiNet are resources for emotion detection, with EmotiNet being extendable.  \n\nSlide Topic: Data Preprocessing and Corpus Preparation  \nSummary:  \n- Emotional statements were sourced from the ISEAR database, with 3,000 respondents from 37 countries.  \n- Statements were tokenized into words and grouped into trigrams to form Context Windows (CWs).  \n- WordNet Affect list identified affect words within the CWs.  \n- Anger class had the most trigrams; joy class had the fewest.  \n- Patterns like 'advices, about, problems' (Anger) and 'always, joyous, one' (Joy) were identified.  \n- Stop words were prevalent, with fear having the most in certain positions.  \n\nSlide Topic: Context Vector Formation and Affinity Score Calculation  \nSummary:\n- Context windows are mapped into vector space for emotion classification.\n- Vectors are formed based on word occurrence in CW patterns (NAW1, AW, NAW2).\n- Affinity scores are calculated using a formula inspired by Newton's law of gravitation.\n- Higher affinity scores indicate stronger relationships between vectors.\n- Distance metrics like Chebyshev, Euclidean, and Hamming measure vector proximity.\n\nSlide Topic: Feature Selection and Analysis  \nSummary:  \n- Feature selection is essential for effective pattern classification.  \n- Sentences were POS tagged, and Context Windows were extracted as PTCWs.  \n- POS tag sequences were named PTWs.  \n- 'Fear' had the most Context Windows and unique PTCWs; 'anger' had the most unique PTWs.  \n- TF and TF-IDF scores were calculated for CWs, PTCWs, and PTWs.  \n- 'Guilt' had the highest Max_TF and variance; 'anger' and 'disgust' had the lowest.  \n- Ranking scores for CWs were calculated using SentiWordNet to identify key emotional phrases.  \n\nSlide Topic: Classification Methods and Results  \nSummary:  \n- Classifiers used: Bayes Net, J48, Naive Bayes Simple, Decision Tree.  \n- Bayes Net achieved 100% accuracy on test data with Euclidean distance.  \n- Bayes Net scored 97.91% on 10-fold cross-validation.  \n- J48 achieved 77% on test data and 83.54% on cross-validation.  \n- Naive Bayes Simple had 92.30% on test data, 27.07% on cross-validation.  \n- Decision Tree performed well with 98.30% on test data, 98.10% on cross-validation.  \n- Bayes Net also excelled with Hamming and Chebyshev distances.  \n\nSlide Topic: Conclusions and Future Work  \nSummary:  \n- Conducted vector formation for Context Windows and calculated TF and TF-IDF measures.  \n- Developed affinity scores inspired by Newton's law of gravitation.  \n- Utilized classifiers such as BayesNet, J48, Naive Bayes Simple, and Decision Table.  \n- Future work includes incorporating more lexicons.  \n- Plan to use associative learning to identify classification rules.  "
]